{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    " <a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/video_notebooks/01_pytorch_workflow_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeG__6p8FZHC"
   },
   "source": [
    "# PyTorch Workflow\n",
    "\n",
    "Let's explore a an example PyTorch end-to-end workflow.\n",
    "\n",
    "Resources:\n",
    "* Ground truth notebook - https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb\n",
    "* Book version of notebook - https://www.learnpytorch.io/01_pytorch_workflow/\n",
    "* Ask a question - https://github.com/mrdbourke/pytorch-deep-learning/discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:56.567097Z",
     "start_time": "2024-04-27T14:07:56.356728200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_n_NlLzFwEN",
    "outputId": "0f9c66d7-e8af-4020-d53c-17c2e1ede55f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'data (prepare and load)',\n",
       " 2: 'build model',\n",
       " 3: 'fitting the model to data (training)',\n",
       " 4: 'making predictions and evaluting a model (inference)',\n",
       " 5: 'saving and loading a model',\n",
       " 6: 'putting it all together'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what_were_covering = {1: \"data (prepare and load)\",\n",
    "                      2: \"build model\",\n",
    "                      3: \"fitting the model to data (training)\",\n",
    "                      4: \"making predictions and evaluting a model (inference)\",\n",
    "                      5: \"saving and loading a model\",\n",
    "                      6: \"putting it all together\"}\n",
    "\n",
    "what_were_covering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.547970500Z",
     "start_time": "2024-04-27T14:07:56.372615600Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OJN3I__OGWOe",
    "outputId": "1e270c8b-bbb2-4901-b1c7-bcf3e0f1e9f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks \n",
    "\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg1IBDfEG207"
   },
   "source": [
    "## 1. Data (preparing and loading)\n",
    "\n",
    "Data can be almost anything... in machine learning.\n",
    "\n",
    "* Excel speadsheet\n",
    "* Images of any kind\n",
    "* Videos (YouTube has lots of data...)\n",
    "* Audio like songs or podcasts\n",
    "* DNA \n",
    "* Text\n",
    "\n",
    "Machine learning is a game of two parts: \n",
    "1. Get data into a numerical representation.\n",
    "2. Build a model to learn patterns in that numerical representation.\n",
    "\n",
    "To showcase this, let's create some *known* data using the linear regression formula.\n",
    "\n",
    "We'll use a linear regression formula to make a straight line with *known* **parameters**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.561009800Z",
     "start_time": "2024-04-27T14:07:57.546970100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hCumNpHHCTU",
    "outputId": "ca51def4-8b84-4b2a-80f8-2da542907aed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create *known* parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias \n",
    "\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.619361300Z",
     "start_time": "2024-04-27T14:07:57.561009800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrPJV_XkJgRT",
    "outputId": "a7c152d0-59d6-455f-c61d-4445f1311014"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GTAEnvLJlq6"
   },
   "source": [
    "### Splitting data into training and test sets (one of the most important concepts in machine learning in general)\n",
    "\n",
    "Let's create a training and test set with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.620886100Z",
     "start_time": "2024-04-27T14:07:57.576188800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpMm7mp_KtNH",
    "outputId": "ff199d0c-6974-47f7-8ba7-e51b7df4c6b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a train/test split\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:] \n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqArrYcENbhp"
   },
   "source": [
    "How might we better visualize our data?\n",
    "\n",
    "This is where the data explorer's motto comes in!\n",
    "\n",
    "\"Visualize, visualize, visualize!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.620886100Z",
     "start_time": "2024-04-27T14:07:57.590108200Z"
    },
    "id": "Bgb1fH7FL0O8"
   },
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "  \"\"\"\n",
    "  Plots training data, test data and compares predictions.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  # Plot training data in blue\n",
    "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "\n",
    "  # Plot test data in green\n",
    "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "  # Are there predictions?\n",
    "  if predictions is not None:\n",
    "    # Plot the predictions if they exist\n",
    "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "  \n",
    "  # Show the legend\n",
    "  plt.legend(prop={\"size\": 14});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0z7q5QhLOv_q"
   },
   "source": [
    "## 2. Build model \n",
    "\n",
    "Our first PyTorch model!\n",
    "\n",
    "This is very exciting... let's do it!\n",
    "\n",
    "Because we're going to be building classes throughout the course, I'd recommend getting familiar with OOP in Python, to do so you can use the following resource from Real Python: https://realpython.com/python3-object-oriented-programming/\n",
    "\n",
    "What our model does:\n",
    "* Start with random values (weight & bias)\n",
    "* Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight & bias values we used to create the data)\n",
    "\n",
    "How does it do so?\n",
    "\n",
    "Through two main algorithms:\n",
    "1. Gradient descent - https://youtu.be/IHZwWFHWa-w\n",
    "2. Backpropagation - https://youtu.be/Ilg3gGewQ5U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.638576300Z",
     "start_time": "2024-04-27T14:07:57.606227700Z"
    },
    "id": "qirhP4VUkOky"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create linear regression model class\n",
    "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch inherhits from nn.Module\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.weights = nn.Parameter(torch.randn(1, # <- start with a random weight and try to adjust it to the ideal weight\n",
    "                                            requires_grad=True, # <- can this parameter be updated via gradient descent?\n",
    "                                            dtype=torch.float)) # <- PyTorch loves the datatype torch.float32\n",
    "    \n",
    "    self.bias = nn.Parameter(torch.randn(1, # <- start with a random bias and try to adjust it to the ideal bias\n",
    "                                         requires_grad=True, # <- can this parameter be updated via gradient descent?\n",
    "                                         dtype=torch.float)) # <- PyTorch loves the datatype torch.float32 \n",
    "    \n",
    "  # Forward method to define the computation in the model\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data\n",
    "    return self.weights * x + self.bias # this is the linear regression formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JH_vD6ICnRUO"
   },
   "source": [
    "### PyTorch model building essentials\n",
    "\n",
    "* torch.nn - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)\n",
    "* torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us \n",
    "* torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()\n",
    "* torch.optim - this where the optimizers in PyTorch live, they will help with gradient descent\n",
    "* def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation \n",
    "\n",
    "See more of these essential modules via the PyTorch cheatsheet - https://pytorch.org/tutorials/beginner/ptcheat.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_8fP6B0rYnN"
   },
   "source": [
    "### Checking the contents of our PyTorch model\n",
    "\n",
    "Now we've created a model, let's see what's inside...\n",
    "\n",
    "So we can check our model parameters or what's inside our model using `.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.638576300Z",
     "start_time": "2024-04-27T14:07:57.623027Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0737rQGNtDxP",
    "outputId": "2a477df1-d234-4db1-c25d-f6eb734cbf86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the model (this is a subclass of nn.Module)\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "# Check out the parameters\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.713208900Z",
     "start_time": "2024-04-27T14:07:57.637577Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdzvifGftWYZ",
    "outputId": "983cdf2a-c582-4fbf-e8d8-9060bb32bb30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List named parameters\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlAsG4S-uJO5"
   },
   "source": [
    "### Making prediction using `torch.inference_mode()`\n",
    "\n",
    "To check our model's predictive power, let's see how well it predicts `y_test` based on `X_test`.\n",
    "\n",
    "When we pass data through our model, it's going to run it through the `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.726136600Z",
     "start_time": "2024-04-27T14:07:57.652868800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_nRrqGMwm0N",
    "outputId": "f9cbe561-5994-4f99-e4e2-96f70b06fbb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982],\n",
       "        [0.4049],\n",
       "        [0.4116],\n",
       "        [0.4184],\n",
       "        [0.4251],\n",
       "        [0.4318],\n",
       "        [0.4386],\n",
       "        [0.4453],\n",
       "        [0.4520],\n",
       "        [0.4588]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model_0(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.727136500Z",
     "start_time": "2024-04-27T14:07:57.671711Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCASe_bouVKL",
    "outputId": "cdcc4351-3173-44d2-b854-7bdecedcd0b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3982],\n",
       "        [0.4049],\n",
       "        [0.4116],\n",
       "        [0.4184],\n",
       "        [0.4251],\n",
       "        [0.4318],\n",
       "        [0.4386],\n",
       "        [0.4453],\n",
       "        [0.4520],\n",
       "        [0.4588]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode():\n",
    "  y_preds = model_0(X_test)\n",
    "  \n",
    "\n",
    "# # You can also do something similar with torch.no_grad(), however, torch.inference_mode() is preferred\n",
    "# with torch.no_grad():\n",
    "#   y_preds = model_0(X_test)\n",
    "\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYHvIyDsxL65"
   },
   "source": [
    "See more on inference mode here - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=cnKavO9iTgwQ-rfri6u7PQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.727136500Z",
     "start_time": "2024-04-27T14:07:57.685042300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVREWa_BvzI0",
    "outputId": "a89181a5-a12c-4f52-ef03-e01943da9561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8600],\n",
       "        [0.8740],\n",
       "        [0.8880],\n",
       "        [0.9020],\n",
       "        [0.9160],\n",
       "        [0.9300],\n",
       "        [0.9440],\n",
       "        [0.9580],\n",
       "        [0.9720],\n",
       "        [0.9860]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC7cHOnqwWi8"
   },
   "source": [
    "## 3. Train model\n",
    "\n",
    "The whole idea of training is for a model to move from some *unknown* parameters (these may be random) to some *known* parameters.\n",
    "\n",
    "Or in other words from a poor representation of the data to a better representation of the data.\n",
    "\n",
    "One way to measure how poor or how wrong your models predictions are is to use a loss function.\n",
    "\n",
    "* Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function.\n",
    "\n",
    "Things we need to train:\n",
    "\n",
    "* **Loss function:** A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.\n",
    "* **Optimizer:** Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function - https://pytorch.org/docs/stable/optim.html#module-torch.optim\n",
    "  * Inside the optimizer you'll often have to set two parameters:\n",
    "    * `params` - the model parameters you'd like to optimize, for example `params=model_0.parameters()`\n",
    "    * `lr` (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step (a small `lr` results in small changes, a large `lr` results in large changes)\n",
    "\n",
    "And specifically for PyTorch, we need:\n",
    "* A training loop\n",
    "* A testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.740252400Z",
     "start_time": "2024-04-27T14:07:57.700669800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gR7Xy8FqQ1x1",
    "outputId": "85ea9234-292b-4269-c450-3a9088e3d65a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:57.761552500Z",
     "start_time": "2024-04-27T14:07:57.718216700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MeA8r0whRzVM",
    "outputId": "3bf7b029-3a6d-4f4d-cb37-e493fff66f16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out our model's parameters (a parameter is a value that the model sets itself)\n",
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:58.575652200Z",
     "start_time": "2024-04-27T14:07:57.732244200Z"
    },
    "id": "FhpnOr3vR5jI"
   },
   "outputs": [],
   "source": [
    "# Setup a loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "# Setup an optimizer (stochastic gradient descent)\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), # we want to optimize the parameters present in our model\n",
    "                            lr=0.001) # lr = learning rate = possibly the most important hyperparameter you can set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zR0wku4LUzr7"
   },
   "source": [
    "> **Q:** Which loss function and optimizer should I use?\n",
    ">\n",
    "> **A:** This will be problem specific. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.\n",
    ">\n",
    "> For example, for a regression problem (like ours), a loss function of `nn.L1Loss()` and an optimizer like `torch.optim.SGD()` will suffice.\n",
    ">\n",
    "> But for a classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of `nn.BCELoss()` (binary cross entropy loss). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffr8kYJTXwkD"
   },
   "source": [
    "### Building a training loop (and a testing loop) in PyTorch\n",
    "\n",
    "A couple of things we need in a training loop:\n",
    "0. Loop through the data and do...\n",
    "1. Forward pass (this involves data moving through our model's `forward()` functions) to make predictions on data - also called forward propagation \n",
    "2. Calculate the loss (compare forward pass predictions to ground truth labels)\n",
    "3. Optimizer zero grad\n",
    "4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (**backpropagation** - https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent** - https://youtu.be/IHZwWFHWa-w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:58.687738400Z",
     "start_time": "2024-04-27T14:07:58.578924Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TV8WOxYPaTlP",
    "outputId": "e05110b7-2289-48c9-e99c-cfca09e0f9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.31288138031959534 | Test loss: 0.4931890368461609\n",
      "OrderedDict([('weights', tensor([0.3371])), ('bias', tensor([0.1298]))])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# An epoch is one loop through the data... (this is a hyperparameter because we've set it ourselves)\n",
    "epochs = 200\n",
    "\n",
    "# Track different values\n",
    "epoch_count = [] \n",
    "loss_values = []\n",
    "test_loss_values = [] \n",
    "\n",
    "### Training\n",
    "# 0. Loop through the data\n",
    "for epoch in range(epochs): \n",
    "  # Set the model to training mode\n",
    "  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require gradients \n",
    "\n",
    "  # 1. Forward pass\n",
    "  y_pred = model_0(X_train)\n",
    "\n",
    "  # 2. Calculate the loss\n",
    "  loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "  # 3. Optimizer zero grad\n",
    "  optimizer.zero_grad() \n",
    "\n",
    "  # 4. Perform backpropagation on the loss with respect to the parameters of the model (calculate gradients of each parameter)\n",
    "  loss.backward()\n",
    "\n",
    "  # 5. Step the optimizer (perform gradient descent)\n",
    "  optimizer.step() # by default how the optimizer changes will acculumate through the loop so... we have to zero them above in step 3 for the next iteration of the loop\n",
    "\n",
    "  ### Testing\n",
    "  model_0.eval() # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)\n",
    "  with torch.inference_mode(): # turns off gradient tracking & a couple more things behind the scenes - https://twitter.com/PyTorch/status/1437838231505096708?s=20&t=aftDZicoiUGiklEP179x7A\n",
    "  # with torch.no_grad(): # you may also see torch.no_grad() in older PyTorch code\n",
    "    # 1. Do the forward pass \n",
    "    test_pred = model_0(X_test)\n",
    "\n",
    "    # 2. Calculate the loss\n",
    "    test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "  # Print out what's happening'\n",
    "  if epoch % 10000 == 0:\n",
    "    epoch_count.append(epoch)\n",
    "    loss_values.append(loss)\n",
    "    test_loss_values.append(test_loss)\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")\n",
    "    # Print out model state_dict()\n",
    "    print(model_0.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:07:59.737689500Z",
     "start_time": "2024-04-27T14:07:59.690678500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6EZVQi1759Y",
    "outputId": "e1ea02bd-efe8-4655-eea3-50e88e951189"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.31288138], dtype=float32), [tensor(0.4932)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(torch.tensor(loss_values).numpy()), test_loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-27T14:08:01.456657Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ccr-GEYe7da1",
    "is_executing": true,
    "outputId": "68b6980f-85ab-4811-cd53-be0d1091ff10"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='83de7541-82cf-408e-86ff-bc2b052a9694'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "plt.plot(epoch_count, np.array(torch.tensor(loss_values).numpy()), label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T14:03:27.385540Z",
     "start_time": "2024-04-27T14:03:27.027595Z"
    },
    "id": "vfl9oAt_09Fd"
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "  y_preds_new = model_0(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T13:24:36.268605100Z",
     "start_time": "2024-04-27T13:24:36.228002300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3Y-ilS3jCdG",
    "outputId": "4bc413d2-3b5b-4f4d-c822-5f3693f3796d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.4146])), ('bias', tensor([0.3250]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T13:24:38.337343200Z",
     "start_time": "2024-04-27T13:24:38.318126100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cE8uVRUeg39p",
    "outputId": "9abe949e-3ba1-4d7e-91ee-f0dc59371641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-27T13:24:42.202413300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "f_gboBMSl13p",
    "outputId": "3c5bce1a-ad41-44ac-bf7f-326d4b5c4308"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='0bbaae5e-8d1d-444d-b934-0453220b6d5c'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "9y-u_rVC16XJ",
    "outputId": "354933d7-7c14-46c5-9a71-eee4cb7200c2"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='b71c0638-440f-4cfc-bc81-3928c06605f0'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds_new);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXusQ2JP1_S1"
   },
   "source": [
    "## Saving a model in PyTorch\n",
    "\n",
    "There are three main methods you should about for saving and loading models in PyTorch.\n",
    "\n",
    "1. `torch.save()` - allows you save a PyTorch object in Python's pickle format \n",
    "2. `torch.load()` - allows you load a saved PyTorch object\n",
    "3. `torch.nn.Module.load_state_dict()` - this allows to load a model's saved state dictionary \n",
    "\n",
    "PyTorch save & load code tutorial + extra-curriculum - https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a0iaBiX5JAG",
    "outputId": "dbe254f6-1696-4fa8-a3d3-619aebf930aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models\\01_pytorch_workflow_model_0.pth\n"
     ]
    }
   ],
   "source": [
    "# Saving our PyTorch model\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path\n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_0.state_dict(),\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Os6BGzXT54Xq",
    "outputId": "2529729e-a3c5-486a-e913-b3031a5fe9ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls -l models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uib6vQMB7Ds1"
   },
   "source": [
    "## Loading a PyTorch model\n",
    "\n",
    "Since we saved our model's `state_dict()` rather the entire model, we'll create a new instance of our model class and load the saved `state_dict()` into that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9U-uXVaC85PP",
    "outputId": "a2c8ea18-fbb5-49a7-d985-eab4a76b2875"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.4146])), ('bias', tensor([0.3250]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTghUxOH89lz",
    "outputId": "354c3ac5-96d5-499b-d734-d3e56b8ba3e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load in a saved state_dict we have to instantiate a new instance of our model class\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "\n",
    "# Load the saved state_dict of model_0 (this will update the new instance with updated parameters)\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_44x4te89OjW",
    "outputId": "a8537d36-f89a-4409-90b3-3563f836231e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.4146])), ('bias', tensor([0.3250]))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxPDLIU09Q0i",
    "outputId": "887f395e-1719-4f68-f3d9-23e17088eb14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6566],\n",
       "        [0.6649],\n",
       "        [0.6732],\n",
       "        [0.6815],\n",
       "        [0.6898],\n",
       "        [0.6981],\n",
       "        [0.7064],\n",
       "        [0.7147],\n",
       "        [0.7230],\n",
       "        [0.7313]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions with our loaded model\n",
    "loaded_model_0.eval()\n",
    "with torch.inference_mode():\n",
    "  loaded_model_preds = loaded_model_0(X_test)\n",
    "\n",
    "loaded_model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BP9cufq99K-",
    "outputId": "191daad3-6df1-4018-95c9-de755eb4f381"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6566],\n",
       "        [0.6649],\n",
       "        [0.6732],\n",
       "        [0.6815],\n",
       "        [0.6898],\n",
       "        [0.6981],\n",
       "        [0.7064],\n",
       "        [0.7147],\n",
       "        [0.7230],\n",
       "        [0.7313]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some models preds\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "  y_preds = model_0(X_test)\n",
    "\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sSbACbvI94XX",
    "outputId": "853b8d5f-7830-41c2-ffd2-93a7fc0b2270"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare loaded model preds with original model preds\n",
    "y_preds == loaded_model_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQHmmLBo9_ji"
   },
   "source": [
    "## 6. Putting it all together\n",
    "\n",
    "Let's go back through the steps above and see it all in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TY16oebx_4yK",
    "outputId": "d0d09199-ab74-45b0-fa8f-ee86ecaa5f3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu121'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import PyTorch and matplotlib\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l91cJBlNAZ7m"
   },
   "source": [
    "Create device-agnostic code.\n",
    "\n",
    "This means if we've got access to a GPU, our code will use it (for potentially faster computing).\n",
    "\n",
    "If no GPU is available, the code will default to using CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hRCrpBhAj9G",
    "outputId": "d1f41115-e110-49ba-822b-748f48d86bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lMKxKvN_1yP"
   },
   "source": [
    "### 6.1 Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StBcwzuA_4GP",
    "outputId": "620dca7c-3409-47f9-9a50-f051505125d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create some data using the linear regression formula of y = weight * X + bias\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "# Create range values\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "\n",
    "# Create X and y (features and labels)\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will pop up\n",
    "y = weight * X + bias\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGpxaIDsCDBN",
    "outputId": "8ff5f597-3f3f-4324-c015-8b6b641163de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "T4tidX_5CnVx"
   },
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "  \"\"\"\n",
    "  Plots training data, test data and compares predictions.\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(10, 7))\n",
    "\n",
    "  # Plot training data in blue\n",
    "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "\n",
    "  # Plot test data in green\n",
    "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "  # Are there predictions?\n",
    "  if predictions is not None:\n",
    "    # Plot the predictions if they exist\n",
    "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "  \n",
    "  # Show the legend\n",
    "  plt.legend(prop={\"size\": 14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "2go898QeCXJY",
    "outputId": "ca6406c0-6604-4ff3-8dd4-25aae5a44333"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='3fd47e46-ec33-4d80-976c-1a13db398640'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "# Note: if you don't have the plot_predictions() function loaded, this will error\n",
    "%matplotlib notebook\n",
    "plot_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2jU9bN1Cgt0"
   },
   "source": [
    "### 6.2 Building a PyTorch Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wx8c6kF5Cyp_",
    "outputId": "eeb9bcc0-a227-4cca-cc4c-96021e6c0d23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModelV2(\n",
       "   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       " ),\n",
       " OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "              ('linear_layer.bias', tensor([0.8300]))]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a linear model by subclassing nn.Module\n",
    "class LinearRegressionModelV2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Use nn.Linear() for creating the model parameters / also called: linear transform, probing layer, fully connected layer, dense layer\n",
    "    self.linear_layer = nn.Linear(in_features=1,\n",
    "                                  out_features=1)\n",
    "    \n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return self.linear_layer(x)\n",
    "\n",
    "# Set the manual seed\n",
    "torch.manual_seed(42)\n",
    "model_1 = LinearRegressionModelV2()\n",
    "model_1, model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9DmOPoTFIC4",
    "outputId": "4f37c379-8cb8-4080-9615-f2d936abd829"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n",
       "             ('linear_layer.bias', tensor([0.8300]))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iq9DxUEyEEvX",
    "outputId": "66921f3d-c706-40dc-99ed-6b35f7ec92dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3gCbh6Am8go",
    "outputId": "f2121256-84b6-4908-d3bd-084ac636a156"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model current device\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMNMpAavEFnC",
    "outputId": "d3aa990d-455f-4808-fc01-b229be4eb853"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to use the target device\n",
    "model_1.to(device)\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9h7xgGbCnWI1",
    "outputId": "74a3d084-6c7b-443b-e217-401985507575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.7645]], device='cuda:0')),\n",
       "             ('linear_layer.bias', tensor([0.8300], device='cuda:0'))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.state_dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEfyHhtrm45n"
   },
   "source": [
    "### 6.3 Training\n",
    "\n",
    "For training we need:\n",
    "* Loss function\n",
    "* Optimizer\n",
    "* Training loop\n",
    "* Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BjW4zUvtnOrj"
   },
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "loss_fn = nn.L1Loss() # same as MAE\n",
    "\n",
    "# Setup our optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), \n",
    "                            lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GyAumLw3n2Hy",
    "outputId": "207e60c6-83b6-4524-da0b-ddf7d7db52e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.5551779866218567 | Test loss: 0.587445855140686\n",
      "Epoch: 10 | Loss: 0.5551661849021912 | Test loss: 0.5874320268630981\n",
      "Epoch: 20 | Loss: 0.5551544427871704 | Test loss: 0.5874180793762207\n",
      "Epoch: 30 | Loss: 0.5551427006721497 | Test loss: 0.5874042510986328\n",
      "Epoch: 40 | Loss: 0.5551308989524841 | Test loss: 0.5873904228210449\n",
      "Epoch: 50 | Loss: 0.5551191568374634 | Test loss: 0.587376594543457\n",
      "Epoch: 60 | Loss: 0.5551074147224426 | Test loss: 0.5873627662658691\n",
      "Epoch: 70 | Loss: 0.5550956130027771 | Test loss: 0.5873488783836365\n",
      "Epoch: 80 | Loss: 0.5550839304924011 | Test loss: 0.5873350501060486\n",
      "Epoch: 90 | Loss: 0.5550720691680908 | Test loss: 0.5873212218284607\n",
      "Epoch: 100 | Loss: 0.5550603270530701 | Test loss: 0.5873073935508728\n",
      "Epoch: 110 | Loss: 0.5550485849380493 | Test loss: 0.5872935652732849\n",
      "Epoch: 120 | Loss: 0.5550367832183838 | Test loss: 0.587279736995697\n",
      "Epoch: 130 | Loss: 0.5550251007080078 | Test loss: 0.5872658491134644\n",
      "Epoch: 140 | Loss: 0.5550132989883423 | Test loss: 0.5872519612312317\n",
      "Epoch: 150 | Loss: 0.5550015568733215 | Test loss: 0.5872381329536438\n",
      "Epoch: 160 | Loss: 0.554989755153656 | Test loss: 0.5872242450714111\n",
      "Epoch: 170 | Loss: 0.5549780130386353 | Test loss: 0.587210476398468\n",
      "Epoch: 180 | Loss: 0.5549662709236145 | Test loss: 0.5871965885162354\n",
      "Epoch: 190 | Loss: 0.5549545288085938 | Test loss: 0.5871827006340027\n",
      "Epoch: 200 | Loss: 0.554942786693573 | Test loss: 0.5871688723564148\n",
      "Epoch: 210 | Loss: 0.5549309849739075 | Test loss: 0.5871550440788269\n",
      "Epoch: 220 | Loss: 0.5549192428588867 | Test loss: 0.587141215801239\n",
      "Epoch: 230 | Loss: 0.554907500743866 | Test loss: 0.5871273875236511\n",
      "Epoch: 240 | Loss: 0.5548956990242004 | Test loss: 0.5871134996414185\n",
      "Epoch: 250 | Loss: 0.5548839569091797 | Test loss: 0.5870996713638306\n",
      "Epoch: 260 | Loss: 0.5548721551895142 | Test loss: 0.5870858430862427\n",
      "Epoch: 270 | Loss: 0.5548604130744934 | Test loss: 0.5870720148086548\n",
      "Epoch: 280 | Loss: 0.5548486709594727 | Test loss: 0.5870581269264221\n",
      "Epoch: 290 | Loss: 0.5548369288444519 | Test loss: 0.5870442986488342\n",
      "Epoch: 300 | Loss: 0.5548251271247864 | Test loss: 0.5870304107666016\n",
      "Epoch: 310 | Loss: 0.5548133850097656 | Test loss: 0.5870165824890137\n",
      "Epoch: 320 | Loss: 0.5548015832901001 | Test loss: 0.5870027542114258\n",
      "Epoch: 330 | Loss: 0.5547898411750793 | Test loss: 0.5869889259338379\n",
      "Epoch: 340 | Loss: 0.5547780990600586 | Test loss: 0.5869750380516052\n",
      "Epoch: 350 | Loss: 0.5547663569450378 | Test loss: 0.5869612097740173\n",
      "Epoch: 360 | Loss: 0.5547545552253723 | Test loss: 0.5869473814964294\n",
      "Epoch: 370 | Loss: 0.5547428131103516 | Test loss: 0.5869335532188416\n",
      "Epoch: 380 | Loss: 0.5547310709953308 | Test loss: 0.5869197249412537\n",
      "Epoch: 390 | Loss: 0.5547192692756653 | Test loss: 0.586905837059021\n",
      "Epoch: 400 | Loss: 0.5547075271606445 | Test loss: 0.5868920087814331\n",
      "Epoch: 410 | Loss: 0.5546957850456238 | Test loss: 0.5868781208992004\n",
      "Epoch: 420 | Loss: 0.5546841025352478 | Test loss: 0.5868642926216125\n",
      "Epoch: 430 | Loss: 0.5546722412109375 | Test loss: 0.5868504643440247\n",
      "Epoch: 440 | Loss: 0.5546604990959167 | Test loss: 0.586836576461792\n",
      "Epoch: 450 | Loss: 0.5546488165855408 | Test loss: 0.5868227481842041\n",
      "Epoch: 460 | Loss: 0.5546369552612305 | Test loss: 0.5868089199066162\n",
      "Epoch: 470 | Loss: 0.5546252131462097 | Test loss: 0.5867950320243835\n",
      "Epoch: 480 | Loss: 0.5546135306358337 | Test loss: 0.5867812037467957\n",
      "Epoch: 490 | Loss: 0.5546016693115234 | Test loss: 0.5867673754692078\n",
      "Epoch: 500 | Loss: 0.5545899271965027 | Test loss: 0.5867534875869751\n",
      "Epoch: 510 | Loss: 0.5545781850814819 | Test loss: 0.5867396593093872\n",
      "Epoch: 520 | Loss: 0.5545663833618164 | Test loss: 0.5867258310317993\n",
      "Epoch: 530 | Loss: 0.5545546412467957 | Test loss: 0.5867120027542114\n",
      "Epoch: 540 | Loss: 0.5545429587364197 | Test loss: 0.5866981744766235\n",
      "Epoch: 550 | Loss: 0.5545310974121094 | Test loss: 0.5866842865943909\n",
      "Epoch: 560 | Loss: 0.5545193552970886 | Test loss: 0.5866703987121582\n",
      "Epoch: 570 | Loss: 0.5545076727867126 | Test loss: 0.5866565704345703\n",
      "Epoch: 580 | Loss: 0.5544958710670471 | Test loss: 0.5866427421569824\n",
      "Epoch: 590 | Loss: 0.5544840693473816 | Test loss: 0.5866289138793945\n",
      "Epoch: 600 | Loss: 0.5544723868370056 | Test loss: 0.5866150856018066\n",
      "Epoch: 610 | Loss: 0.5544605255126953 | Test loss: 0.586601197719574\n",
      "Epoch: 620 | Loss: 0.5544488430023193 | Test loss: 0.5865873694419861\n",
      "Epoch: 630 | Loss: 0.5544371008872986 | Test loss: 0.5865735411643982\n",
      "Epoch: 640 | Loss: 0.5544253587722778 | Test loss: 0.5865597128868103\n",
      "Epoch: 650 | Loss: 0.5544135570526123 | Test loss: 0.5865458846092224\n",
      "Epoch: 660 | Loss: 0.5544018149375916 | Test loss: 0.5865320563316345\n",
      "Epoch: 670 | Loss: 0.554390013217926 | Test loss: 0.5865181088447571\n",
      "Epoch: 680 | Loss: 0.5543782711029053 | Test loss: 0.5865042805671692\n",
      "Epoch: 690 | Loss: 0.5543665289878845 | Test loss: 0.5864904522895813\n",
      "Epoch: 700 | Loss: 0.5543547868728638 | Test loss: 0.5864765644073486\n",
      "Epoch: 710 | Loss: 0.5543429255485535 | Test loss: 0.5864627361297607\n",
      "Epoch: 720 | Loss: 0.5543312430381775 | Test loss: 0.5864489078521729\n",
      "Epoch: 730 | Loss: 0.5543195009231567 | Test loss: 0.5864350199699402\n",
      "Epoch: 740 | Loss: 0.5543076992034912 | Test loss: 0.5864211916923523\n",
      "Epoch: 750 | Loss: 0.5542959570884705 | Test loss: 0.5864073634147644\n",
      "Epoch: 760 | Loss: 0.5542842149734497 | Test loss: 0.5863934755325317\n",
      "Epoch: 770 | Loss: 0.5542724132537842 | Test loss: 0.5863797068595886\n",
      "Epoch: 780 | Loss: 0.5542606711387634 | Test loss: 0.586365818977356\n",
      "Epoch: 790 | Loss: 0.5542489290237427 | Test loss: 0.5863519906997681\n",
      "Epoch: 800 | Loss: 0.5542371869087219 | Test loss: 0.5863381624221802\n",
      "Epoch: 810 | Loss: 0.5542253851890564 | Test loss: 0.5863243341445923\n",
      "Epoch: 820 | Loss: 0.5542136430740356 | Test loss: 0.5863103866577148\n",
      "Epoch: 830 | Loss: 0.5542019009590149 | Test loss: 0.586296558380127\n",
      "Epoch: 840 | Loss: 0.5541900992393494 | Test loss: 0.5862827301025391\n",
      "Epoch: 850 | Loss: 0.5541783571243286 | Test loss: 0.5862689018249512\n",
      "Epoch: 860 | Loss: 0.5541666150093079 | Test loss: 0.5862550735473633\n",
      "Epoch: 870 | Loss: 0.5541548132896423 | Test loss: 0.5862411856651306\n",
      "Epoch: 880 | Loss: 0.5541431307792664 | Test loss: 0.5862273573875427\n",
      "Epoch: 890 | Loss: 0.554131269454956 | Test loss: 0.5862135291099548\n",
      "Epoch: 900 | Loss: 0.5541195273399353 | Test loss: 0.5861997008323669\n",
      "Epoch: 910 | Loss: 0.5541077852249146 | Test loss: 0.586185872554779\n",
      "Epoch: 920 | Loss: 0.554095983505249 | Test loss: 0.5861720442771912\n",
      "Epoch: 930 | Loss: 0.554084300994873 | Test loss: 0.5861581563949585\n",
      "Epoch: 940 | Loss: 0.5540724992752075 | Test loss: 0.5861442685127258\n",
      "Epoch: 950 | Loss: 0.5540607571601868 | Test loss: 0.5861304402351379\n",
      "Epoch: 960 | Loss: 0.5540489554405212 | Test loss: 0.5861165523529053\n",
      "Epoch: 970 | Loss: 0.5540372133255005 | Test loss: 0.5861027836799622\n",
      "Epoch: 980 | Loss: 0.5540254712104797 | Test loss: 0.5860888957977295\n",
      "Epoch: 990 | Loss: 0.554013729095459 | Test loss: 0.5860750079154968\n",
      "Epoch: 1000 | Loss: 0.5540019869804382 | Test loss: 0.5860611796379089\n",
      "Epoch: 1010 | Loss: 0.5539901852607727 | Test loss: 0.586047351360321\n",
      "Epoch: 1020 | Loss: 0.553978443145752 | Test loss: 0.5860335230827332\n",
      "Epoch: 1030 | Loss: 0.5539667010307312 | Test loss: 0.5860196948051453\n",
      "Epoch: 1040 | Loss: 0.5539548993110657 | Test loss: 0.5860058069229126\n",
      "Epoch: 1050 | Loss: 0.5539431571960449 | Test loss: 0.5859919786453247\n",
      "Epoch: 1060 | Loss: 0.5539313554763794 | Test loss: 0.5859781503677368\n",
      "Epoch: 1070 | Loss: 0.5539196133613586 | Test loss: 0.5859643220901489\n",
      "Epoch: 1080 | Loss: 0.5539078712463379 | Test loss: 0.5859504342079163\n",
      "Epoch: 1090 | Loss: 0.5538961291313171 | Test loss: 0.5859366059303284\n",
      "Epoch: 1100 | Loss: 0.5538843274116516 | Test loss: 0.5859227180480957\n",
      "Epoch: 1110 | Loss: 0.5538725852966309 | Test loss: 0.5859088897705078\n",
      "Epoch: 1120 | Loss: 0.5538607835769653 | Test loss: 0.5858950614929199\n",
      "Epoch: 1130 | Loss: 0.5538490414619446 | Test loss: 0.585881233215332\n",
      "Epoch: 1140 | Loss: 0.5538372993469238 | Test loss: 0.5858673453330994\n",
      "Epoch: 1150 | Loss: 0.5538255572319031 | Test loss: 0.5858535170555115\n",
      "Epoch: 1160 | Loss: 0.5538137555122375 | Test loss: 0.5858396887779236\n",
      "Epoch: 1170 | Loss: 0.5538020133972168 | Test loss: 0.5858258605003357\n",
      "Epoch: 1180 | Loss: 0.553790271282196 | Test loss: 0.5858120322227478\n",
      "Epoch: 1190 | Loss: 0.5537784695625305 | Test loss: 0.5857981443405151\n",
      "Epoch: 1200 | Loss: 0.5537667274475098 | Test loss: 0.5857843160629272\n",
      "Epoch: 1210 | Loss: 0.553754985332489 | Test loss: 0.5857704281806946\n",
      "Epoch: 1220 | Loss: 0.553743302822113 | Test loss: 0.5857565999031067\n",
      "Epoch: 1230 | Loss: 0.5537314414978027 | Test loss: 0.5857427716255188\n",
      "Epoch: 1240 | Loss: 0.553719699382782 | Test loss: 0.5857288837432861\n",
      "Epoch: 1250 | Loss: 0.553708016872406 | Test loss: 0.5857150554656982\n",
      "Epoch: 1260 | Loss: 0.5536961555480957 | Test loss: 0.5857012271881104\n",
      "Epoch: 1270 | Loss: 0.553684413433075 | Test loss: 0.5856873393058777\n",
      "Epoch: 1280 | Loss: 0.553672730922699 | Test loss: 0.5856735110282898\n",
      "Epoch: 1290 | Loss: 0.5536608695983887 | Test loss: 0.5856596827507019\n",
      "Epoch: 1300 | Loss: 0.5536491274833679 | Test loss: 0.5856457948684692\n",
      "Epoch: 1310 | Loss: 0.5536373853683472 | Test loss: 0.5856319665908813\n",
      "Epoch: 1320 | Loss: 0.5536255836486816 | Test loss: 0.5856181383132935\n",
      "Epoch: 1330 | Loss: 0.5536138415336609 | Test loss: 0.5856043100357056\n",
      "Epoch: 1340 | Loss: 0.5536021590232849 | Test loss: 0.5855904817581177\n",
      "Epoch: 1350 | Loss: 0.5535902976989746 | Test loss: 0.585576593875885\n",
      "Epoch: 1360 | Loss: 0.5535785555839539 | Test loss: 0.5855627059936523\n",
      "Epoch: 1370 | Loss: 0.5535668730735779 | Test loss: 0.5855488777160645\n",
      "Epoch: 1380 | Loss: 0.5535550713539124 | Test loss: 0.5855350494384766\n",
      "Epoch: 1390 | Loss: 0.5535432696342468 | Test loss: 0.5855212211608887\n",
      "Epoch: 1400 | Loss: 0.5535315871238708 | Test loss: 0.5855073928833008\n",
      "Epoch: 1410 | Loss: 0.5535197257995605 | Test loss: 0.5854935050010681\n",
      "Epoch: 1420 | Loss: 0.5535080432891846 | Test loss: 0.5854796767234802\n",
      "Epoch: 1430 | Loss: 0.5534963011741638 | Test loss: 0.5854658484458923\n",
      "Epoch: 1440 | Loss: 0.5534845590591431 | Test loss: 0.5854520201683044\n",
      "Epoch: 1450 | Loss: 0.5534727573394775 | Test loss: 0.5854381918907166\n",
      "Epoch: 1460 | Loss: 0.5534610152244568 | Test loss: 0.5854243636131287\n",
      "Epoch: 1470 | Loss: 0.5534492135047913 | Test loss: 0.5854104161262512\n",
      "Epoch: 1480 | Loss: 0.5534374713897705 | Test loss: 0.5853965878486633\n",
      "Epoch: 1490 | Loss: 0.5534257292747498 | Test loss: 0.5853827595710754\n",
      "Epoch: 1500 | Loss: 0.553413987159729 | Test loss: 0.5853688716888428\n",
      "Epoch: 1510 | Loss: 0.5534021258354187 | Test loss: 0.5853550434112549\n",
      "Epoch: 1520 | Loss: 0.5533904433250427 | Test loss: 0.585341215133667\n",
      "Epoch: 1530 | Loss: 0.553378701210022 | Test loss: 0.5853273272514343\n",
      "Epoch: 1540 | Loss: 0.5533668994903564 | Test loss: 0.5853134989738464\n",
      "Epoch: 1550 | Loss: 0.5533551573753357 | Test loss: 0.5852996706962585\n",
      "Epoch: 1560 | Loss: 0.5533434152603149 | Test loss: 0.5852857828140259\n",
      "Epoch: 1570 | Loss: 0.5533316135406494 | Test loss: 0.5852720141410828\n",
      "Epoch: 1580 | Loss: 0.5533198714256287 | Test loss: 0.5852581262588501\n",
      "Epoch: 1590 | Loss: 0.5533081293106079 | Test loss: 0.5852442979812622\n",
      "Epoch: 1600 | Loss: 0.5532963871955872 | Test loss: 0.5852304697036743\n",
      "Epoch: 1610 | Loss: 0.5532845854759216 | Test loss: 0.5852166414260864\n",
      "Epoch: 1620 | Loss: 0.5532728433609009 | Test loss: 0.585202693939209\n",
      "Epoch: 1630 | Loss: 0.5532611012458801 | Test loss: 0.5851888656616211\n",
      "Epoch: 1640 | Loss: 0.5532492995262146 | Test loss: 0.5851750373840332\n",
      "Epoch: 1650 | Loss: 0.5532375574111938 | Test loss: 0.5851612091064453\n",
      "Epoch: 1660 | Loss: 0.5532258152961731 | Test loss: 0.5851473808288574\n",
      "Epoch: 1670 | Loss: 0.5532140135765076 | Test loss: 0.5851334929466248\n",
      "Epoch: 1680 | Loss: 0.5532023310661316 | Test loss: 0.5851196646690369\n",
      "Epoch: 1690 | Loss: 0.5531904697418213 | Test loss: 0.585105836391449\n",
      "Epoch: 1700 | Loss: 0.5531787276268005 | Test loss: 0.5850920081138611\n",
      "Epoch: 1710 | Loss: 0.5531669855117798 | Test loss: 0.5850781798362732\n",
      "Epoch: 1720 | Loss: 0.5531551837921143 | Test loss: 0.5850643515586853\n",
      "Epoch: 1730 | Loss: 0.5531435012817383 | Test loss: 0.5850504636764526\n",
      "Epoch: 1740 | Loss: 0.5531316995620728 | Test loss: 0.58503657579422\n",
      "Epoch: 1750 | Loss: 0.553119957447052 | Test loss: 0.5850227475166321\n",
      "Epoch: 1760 | Loss: 0.5531081557273865 | Test loss: 0.5850088596343994\n",
      "Epoch: 1770 | Loss: 0.5530964136123657 | Test loss: 0.5849950909614563\n",
      "Epoch: 1780 | Loss: 0.553084671497345 | Test loss: 0.5849812030792236\n",
      "Epoch: 1790 | Loss: 0.5530729293823242 | Test loss: 0.584967315196991\n",
      "Epoch: 1800 | Loss: 0.5530611872673035 | Test loss: 0.5849534869194031\n",
      "Epoch: 1810 | Loss: 0.5530493855476379 | Test loss: 0.5849396586418152\n",
      "Epoch: 1820 | Loss: 0.5530376434326172 | Test loss: 0.5849258303642273\n",
      "Epoch: 1830 | Loss: 0.5530259013175964 | Test loss: 0.5849120020866394\n",
      "Epoch: 1840 | Loss: 0.5530140995979309 | Test loss: 0.5848981142044067\n",
      "Epoch: 1850 | Loss: 0.5530023574829102 | Test loss: 0.5848842859268188\n",
      "Epoch: 1860 | Loss: 0.5529905557632446 | Test loss: 0.584870457649231\n",
      "Epoch: 1870 | Loss: 0.5529788136482239 | Test loss: 0.5848566293716431\n",
      "Epoch: 1880 | Loss: 0.5529670715332031 | Test loss: 0.5848427414894104\n",
      "Epoch: 1890 | Loss: 0.5529553294181824 | Test loss: 0.5848289132118225\n",
      "Epoch: 1900 | Loss: 0.5529435276985168 | Test loss: 0.5848150253295898\n",
      "Epoch: 1910 | Loss: 0.5529317855834961 | Test loss: 0.584801197052002\n",
      "Epoch: 1920 | Loss: 0.5529199838638306 | Test loss: 0.5847873687744141\n",
      "Epoch: 1930 | Loss: 0.5529082417488098 | Test loss: 0.5847735404968262\n",
      "Epoch: 1940 | Loss: 0.5528964996337891 | Test loss: 0.5847596526145935\n",
      "Epoch: 1950 | Loss: 0.5528847575187683 | Test loss: 0.5847458243370056\n",
      "Epoch: 1960 | Loss: 0.5528729557991028 | Test loss: 0.5847319960594177\n",
      "Epoch: 1970 | Loss: 0.552861213684082 | Test loss: 0.5847181677818298\n",
      "Epoch: 1980 | Loss: 0.5528494715690613 | Test loss: 0.5847043395042419\n",
      "Epoch: 1990 | Loss: 0.5528376698493958 | Test loss: 0.5846904516220093\n",
      "Epoch: 2000 | Loss: 0.552825927734375 | Test loss: 0.5846766233444214\n",
      "Epoch: 2010 | Loss: 0.5528141856193542 | Test loss: 0.5846627354621887\n",
      "Epoch: 2020 | Loss: 0.5528025031089783 | Test loss: 0.5846489071846008\n",
      "Epoch: 2030 | Loss: 0.552790641784668 | Test loss: 0.5846350789070129\n",
      "Epoch: 2040 | Loss: 0.5527788996696472 | Test loss: 0.5846211910247803\n",
      "Epoch: 2050 | Loss: 0.5527672171592712 | Test loss: 0.5846073627471924\n",
      "Epoch: 2060 | Loss: 0.5527553558349609 | Test loss: 0.5845935344696045\n",
      "Epoch: 2070 | Loss: 0.5527436137199402 | Test loss: 0.5845796465873718\n",
      "Epoch: 2080 | Loss: 0.5527319312095642 | Test loss: 0.5845658183097839\n",
      "Epoch: 2090 | Loss: 0.5527200698852539 | Test loss: 0.584551990032196\n",
      "Epoch: 2100 | Loss: 0.5527083277702332 | Test loss: 0.5845381021499634\n",
      "Epoch: 2110 | Loss: 0.5526965856552124 | Test loss: 0.5845242738723755\n",
      "Epoch: 2120 | Loss: 0.5526847839355469 | Test loss: 0.5845104455947876\n",
      "Epoch: 2130 | Loss: 0.5526730418205261 | Test loss: 0.5844966173171997\n",
      "Epoch: 2140 | Loss: 0.5526613593101501 | Test loss: 0.5844827890396118\n",
      "Epoch: 2150 | Loss: 0.5526494979858398 | Test loss: 0.5844689011573792\n",
      "Epoch: 2160 | Loss: 0.5526377558708191 | Test loss: 0.5844550132751465\n",
      "Epoch: 2170 | Loss: 0.5526260733604431 | Test loss: 0.5844411849975586\n",
      "Epoch: 2180 | Loss: 0.5526142716407776 | Test loss: 0.5844273567199707\n",
      "Epoch: 2190 | Loss: 0.5526024699211121 | Test loss: 0.5844135284423828\n",
      "Epoch: 2200 | Loss: 0.5525907874107361 | Test loss: 0.5843997001647949\n",
      "Epoch: 2210 | Loss: 0.5525789260864258 | Test loss: 0.5843858122825623\n",
      "Epoch: 2220 | Loss: 0.5525672435760498 | Test loss: 0.5843719840049744\n",
      "Epoch: 2230 | Loss: 0.552555501461029 | Test loss: 0.5843581557273865\n",
      "Epoch: 2240 | Loss: 0.5525437593460083 | Test loss: 0.5843443274497986\n",
      "Epoch: 2250 | Loss: 0.5525319576263428 | Test loss: 0.5843304991722107\n",
      "Epoch: 2260 | Loss: 0.552520215511322 | Test loss: 0.5843166708946228\n",
      "Epoch: 2270 | Loss: 0.5525084137916565 | Test loss: 0.5843027234077454\n",
      "Epoch: 2280 | Loss: 0.5524966716766357 | Test loss: 0.5842888951301575\n",
      "Epoch: 2290 | Loss: 0.552484929561615 | Test loss: 0.5842750668525696\n",
      "Epoch: 2300 | Loss: 0.5524731874465942 | Test loss: 0.5842611789703369\n",
      "Epoch: 2310 | Loss: 0.5524613261222839 | Test loss: 0.584247350692749\n",
      "Epoch: 2320 | Loss: 0.552449643611908 | Test loss: 0.5842335224151611\n",
      "Epoch: 2330 | Loss: 0.5524379014968872 | Test loss: 0.5842196345329285\n",
      "Epoch: 2340 | Loss: 0.5524260997772217 | Test loss: 0.5842058062553406\n",
      "Epoch: 2350 | Loss: 0.5524143576622009 | Test loss: 0.5841919779777527\n",
      "Epoch: 2360 | Loss: 0.5524026155471802 | Test loss: 0.58417809009552\n",
      "Epoch: 2370 | Loss: 0.5523908138275146 | Test loss: 0.5841643214225769\n",
      "Epoch: 2380 | Loss: 0.5523790717124939 | Test loss: 0.5841504335403442\n",
      "Epoch: 2390 | Loss: 0.5523673295974731 | Test loss: 0.5841366052627563\n",
      "Epoch: 2400 | Loss: 0.5523555874824524 | Test loss: 0.5841227769851685\n",
      "Epoch: 2410 | Loss: 0.5523437857627869 | Test loss: 0.5841089487075806\n",
      "Epoch: 2420 | Loss: 0.5523320436477661 | Test loss: 0.5840950012207031\n",
      "Epoch: 2430 | Loss: 0.5523203015327454 | Test loss: 0.5840811729431152\n",
      "Epoch: 2440 | Loss: 0.5523084998130798 | Test loss: 0.5840673446655273\n",
      "Epoch: 2450 | Loss: 0.5522967576980591 | Test loss: 0.5840535163879395\n",
      "Epoch: 2460 | Loss: 0.5522850155830383 | Test loss: 0.5840396881103516\n",
      "Epoch: 2470 | Loss: 0.5522732138633728 | Test loss: 0.5840258002281189\n",
      "Epoch: 2480 | Loss: 0.5522615313529968 | Test loss: 0.584011971950531\n",
      "Epoch: 2490 | Loss: 0.5522496700286865 | Test loss: 0.5839981436729431\n",
      "Epoch: 2500 | Loss: 0.5522379279136658 | Test loss: 0.5839843153953552\n",
      "Epoch: 2510 | Loss: 0.552226185798645 | Test loss: 0.5839704871177673\n",
      "Epoch: 2520 | Loss: 0.5522143840789795 | Test loss: 0.5839566588401794\n",
      "Epoch: 2530 | Loss: 0.5522027015686035 | Test loss: 0.5839427709579468\n",
      "Epoch: 2540 | Loss: 0.552190899848938 | Test loss: 0.5839288830757141\n",
      "Epoch: 2550 | Loss: 0.5521791577339172 | Test loss: 0.5839150547981262\n",
      "Epoch: 2560 | Loss: 0.5521673560142517 | Test loss: 0.5839011669158936\n",
      "Epoch: 2570 | Loss: 0.552155613899231 | Test loss: 0.5838873982429504\n",
      "Epoch: 2580 | Loss: 0.5521438717842102 | Test loss: 0.5838735103607178\n",
      "Epoch: 2590 | Loss: 0.5521321296691895 | Test loss: 0.5838596224784851\n",
      "Epoch: 2600 | Loss: 0.5521203875541687 | Test loss: 0.5838457942008972\n",
      "Epoch: 2610 | Loss: 0.5521085858345032 | Test loss: 0.5838319659233093\n",
      "Epoch: 2620 | Loss: 0.5520968437194824 | Test loss: 0.5838181376457214\n",
      "Epoch: 2630 | Loss: 0.5520851016044617 | Test loss: 0.5838043093681335\n",
      "Epoch: 2640 | Loss: 0.5520732998847961 | Test loss: 0.5837904214859009\n",
      "Epoch: 2650 | Loss: 0.5520615577697754 | Test loss: 0.583776593208313\n",
      "Epoch: 2660 | Loss: 0.5520497560501099 | Test loss: 0.5837627649307251\n",
      "Epoch: 2670 | Loss: 0.5520380139350891 | Test loss: 0.5837489366531372\n",
      "Epoch: 2680 | Loss: 0.5520262718200684 | Test loss: 0.5837350487709045\n",
      "Epoch: 2690 | Loss: 0.5520145297050476 | Test loss: 0.5837212204933167\n",
      "Epoch: 2700 | Loss: 0.5520027279853821 | Test loss: 0.583707332611084\n",
      "Epoch: 2710 | Loss: 0.5519909858703613 | Test loss: 0.5836935043334961\n",
      "Epoch: 2720 | Loss: 0.5519791841506958 | Test loss: 0.5836796760559082\n",
      "Epoch: 2730 | Loss: 0.551967442035675 | Test loss: 0.5836658477783203\n",
      "Epoch: 2740 | Loss: 0.5519556999206543 | Test loss: 0.5836519598960876\n",
      "Epoch: 2750 | Loss: 0.5519439578056335 | Test loss: 0.5836381316184998\n",
      "Epoch: 2760 | Loss: 0.551932156085968 | Test loss: 0.5836243033409119\n",
      "Epoch: 2770 | Loss: 0.5519204139709473 | Test loss: 0.583610475063324\n",
      "Epoch: 2780 | Loss: 0.5519086718559265 | Test loss: 0.5835966467857361\n",
      "Epoch: 2790 | Loss: 0.551896870136261 | Test loss: 0.5835827589035034\n",
      "Epoch: 2800 | Loss: 0.5518851280212402 | Test loss: 0.5835689306259155\n",
      "Epoch: 2810 | Loss: 0.5518733859062195 | Test loss: 0.5835550427436829\n",
      "Epoch: 2820 | Loss: 0.5518617033958435 | Test loss: 0.583541214466095\n",
      "Epoch: 2830 | Loss: 0.5518498420715332 | Test loss: 0.5835273861885071\n",
      "Epoch: 2840 | Loss: 0.5518380999565125 | Test loss: 0.5835134983062744\n",
      "Epoch: 2850 | Loss: 0.5518264174461365 | Test loss: 0.5834996700286865\n",
      "Epoch: 2860 | Loss: 0.5518145561218262 | Test loss: 0.5834858417510986\n",
      "Epoch: 2870 | Loss: 0.5518028140068054 | Test loss: 0.583471953868866\n",
      "Epoch: 2880 | Loss: 0.5517911314964294 | Test loss: 0.5834581255912781\n",
      "Epoch: 2890 | Loss: 0.5517792701721191 | Test loss: 0.5834442973136902\n",
      "Epoch: 2900 | Loss: 0.5517675280570984 | Test loss: 0.5834304094314575\n",
      "Epoch: 2910 | Loss: 0.5517557859420776 | Test loss: 0.5834165811538696\n",
      "Epoch: 2920 | Loss: 0.5517439842224121 | Test loss: 0.5834027528762817\n",
      "Epoch: 2930 | Loss: 0.5517322421073914 | Test loss: 0.5833889245986938\n",
      "Epoch: 2940 | Loss: 0.5517205595970154 | Test loss: 0.583375096321106\n",
      "Epoch: 2950 | Loss: 0.5517086982727051 | Test loss: 0.5833612084388733\n",
      "Epoch: 2960 | Loss: 0.5516969561576843 | Test loss: 0.5833473205566406\n",
      "Epoch: 2970 | Loss: 0.5516852736473083 | Test loss: 0.5833334922790527\n",
      "Epoch: 2980 | Loss: 0.5516734719276428 | Test loss: 0.5833196640014648\n",
      "Epoch: 2990 | Loss: 0.5516616702079773 | Test loss: 0.583305835723877\n",
      "Epoch: 3000 | Loss: 0.5516499876976013 | Test loss: 0.5832920074462891\n",
      "Epoch: 3010 | Loss: 0.551638126373291 | Test loss: 0.5832781195640564\n",
      "Epoch: 3020 | Loss: 0.551626443862915 | Test loss: 0.5832642912864685\n",
      "Epoch: 3030 | Loss: 0.5516147017478943 | Test loss: 0.5832504630088806\n",
      "Epoch: 3040 | Loss: 0.5516029596328735 | Test loss: 0.5832366347312927\n",
      "Epoch: 3050 | Loss: 0.551591157913208 | Test loss: 0.5832228064537048\n",
      "Epoch: 3060 | Loss: 0.5515794157981873 | Test loss: 0.5832089781761169\n",
      "Epoch: 3070 | Loss: 0.5515676140785217 | Test loss: 0.5831950306892395\n",
      "Epoch: 3080 | Loss: 0.551555871963501 | Test loss: 0.5831812024116516\n",
      "Epoch: 3090 | Loss: 0.5515441298484802 | Test loss: 0.5831673741340637\n",
      "Epoch: 3100 | Loss: 0.5515323877334595 | Test loss: 0.583153486251831\n",
      "Epoch: 3110 | Loss: 0.5515205264091492 | Test loss: 0.5831396579742432\n",
      "Epoch: 3120 | Loss: 0.5515088438987732 | Test loss: 0.5831258296966553\n",
      "Epoch: 3130 | Loss: 0.5514971017837524 | Test loss: 0.5831119418144226\n",
      "Epoch: 3140 | Loss: 0.5514853000640869 | Test loss: 0.5830981135368347\n",
      "Epoch: 3150 | Loss: 0.5514735579490662 | Test loss: 0.5830842852592468\n",
      "Epoch: 3160 | Loss: 0.5514618158340454 | Test loss: 0.5830703973770142\n",
      "Epoch: 3170 | Loss: 0.5514500141143799 | Test loss: 0.583056628704071\n",
      "Epoch: 3180 | Loss: 0.5514382719993591 | Test loss: 0.5830427408218384\n",
      "Epoch: 3190 | Loss: 0.5514265298843384 | Test loss: 0.5830289125442505\n",
      "Epoch: 3200 | Loss: 0.5514147877693176 | Test loss: 0.5830150842666626\n",
      "Epoch: 3210 | Loss: 0.5514029860496521 | Test loss: 0.5830012559890747\n",
      "Epoch: 3220 | Loss: 0.5513912439346313 | Test loss: 0.5829873085021973\n",
      "Epoch: 3230 | Loss: 0.5513795018196106 | Test loss: 0.5829734802246094\n",
      "Epoch: 3240 | Loss: 0.5513677000999451 | Test loss: 0.5829596519470215\n",
      "Epoch: 3250 | Loss: 0.5513559579849243 | Test loss: 0.5829458236694336\n",
      "Epoch: 3260 | Loss: 0.5513442158699036 | Test loss: 0.5829319953918457\n",
      "Epoch: 3270 | Loss: 0.551332414150238 | Test loss: 0.582918107509613\n",
      "Epoch: 3280 | Loss: 0.5513207316398621 | Test loss: 0.5829042792320251\n",
      "Epoch: 3290 | Loss: 0.5513088703155518 | Test loss: 0.5828904509544373\n",
      "Epoch: 3300 | Loss: 0.551297128200531 | Test loss: 0.5828766226768494\n",
      "Epoch: 3310 | Loss: 0.5512853860855103 | Test loss: 0.5828627943992615\n",
      "Epoch: 3320 | Loss: 0.5512735843658447 | Test loss: 0.5828489661216736\n",
      "Epoch: 3330 | Loss: 0.5512619018554688 | Test loss: 0.5828350782394409\n",
      "Epoch: 3340 | Loss: 0.5512501001358032 | Test loss: 0.5828211903572083\n",
      "Epoch: 3350 | Loss: 0.5512383580207825 | Test loss: 0.5828073620796204\n",
      "Epoch: 3360 | Loss: 0.5512265563011169 | Test loss: 0.5827934741973877\n",
      "Epoch: 3370 | Loss: 0.5512148141860962 | Test loss: 0.5827797055244446\n",
      "Epoch: 3380 | Loss: 0.5512030720710754 | Test loss: 0.5827658176422119\n",
      "Epoch: 3390 | Loss: 0.5511913299560547 | Test loss: 0.5827519297599792\n",
      "Epoch: 3400 | Loss: 0.5511795878410339 | Test loss: 0.5827381014823914\n",
      "Epoch: 3410 | Loss: 0.5511677861213684 | Test loss: 0.5827242732048035\n",
      "Epoch: 3420 | Loss: 0.5511560440063477 | Test loss: 0.5827104449272156\n",
      "Epoch: 3430 | Loss: 0.5511443018913269 | Test loss: 0.5826966166496277\n",
      "Epoch: 3440 | Loss: 0.5511325001716614 | Test loss: 0.582682728767395\n",
      "Epoch: 3450 | Loss: 0.5511207580566406 | Test loss: 0.5826689004898071\n",
      "Epoch: 3460 | Loss: 0.5511089563369751 | Test loss: 0.5826550722122192\n",
      "Epoch: 3470 | Loss: 0.5510972142219543 | Test loss: 0.5826412439346313\n",
      "Epoch: 3480 | Loss: 0.5510854721069336 | Test loss: 0.5826273560523987\n",
      "Epoch: 3490 | Loss: 0.5510737299919128 | Test loss: 0.5826135277748108\n",
      "Epoch: 3500 | Loss: 0.5510619282722473 | Test loss: 0.5825996398925781\n",
      "Epoch: 3510 | Loss: 0.5510501861572266 | Test loss: 0.5825858116149902\n",
      "Epoch: 3520 | Loss: 0.551038384437561 | Test loss: 0.5825719833374023\n",
      "Epoch: 3530 | Loss: 0.5510266423225403 | Test loss: 0.5825581550598145\n",
      "Epoch: 3540 | Loss: 0.5510149002075195 | Test loss: 0.5825442671775818\n",
      "Epoch: 3550 | Loss: 0.5510031580924988 | Test loss: 0.5825304388999939\n",
      "Epoch: 3560 | Loss: 0.5509913563728333 | Test loss: 0.582516610622406\n",
      "Epoch: 3570 | Loss: 0.5509796142578125 | Test loss: 0.5825027823448181\n",
      "Epoch: 3580 | Loss: 0.5509678721427917 | Test loss: 0.5824889540672302\n",
      "Epoch: 3590 | Loss: 0.5509560704231262 | Test loss: 0.5824750661849976\n",
      "Epoch: 3600 | Loss: 0.5509443283081055 | Test loss: 0.5824612379074097\n",
      "Epoch: 3610 | Loss: 0.5509325861930847 | Test loss: 0.582447350025177\n",
      "Epoch: 3620 | Loss: 0.5509209036827087 | Test loss: 0.5824335217475891\n",
      "Epoch: 3630 | Loss: 0.5509090423583984 | Test loss: 0.5824196934700012\n",
      "Epoch: 3640 | Loss: 0.5508973002433777 | Test loss: 0.5824058055877686\n",
      "Epoch: 3650 | Loss: 0.5508856177330017 | Test loss: 0.5823919773101807\n",
      "Epoch: 3660 | Loss: 0.5508737564086914 | Test loss: 0.5823781490325928\n",
      "Epoch: 3670 | Loss: 0.5508620142936707 | Test loss: 0.5823642611503601\n",
      "Epoch: 3680 | Loss: 0.5508503317832947 | Test loss: 0.5823504328727722\n",
      "Epoch: 3690 | Loss: 0.5508384704589844 | Test loss: 0.5823366045951843\n",
      "Epoch: 3700 | Loss: 0.5508267283439636 | Test loss: 0.5823227167129517\n",
      "Epoch: 3710 | Loss: 0.5508149862289429 | Test loss: 0.5823088884353638\n",
      "Epoch: 3720 | Loss: 0.5508031845092773 | Test loss: 0.5822950601577759\n",
      "Epoch: 3730 | Loss: 0.5507914423942566 | Test loss: 0.582281231880188\n",
      "Epoch: 3740 | Loss: 0.5507797598838806 | Test loss: 0.5822674036026001\n",
      "Epoch: 3750 | Loss: 0.5507678985595703 | Test loss: 0.5822535157203674\n",
      "Epoch: 3760 | Loss: 0.5507561564445496 | Test loss: 0.5822396278381348\n",
      "Epoch: 3770 | Loss: 0.5507444739341736 | Test loss: 0.5822257995605469\n",
      "Epoch: 3780 | Loss: 0.5507326722145081 | Test loss: 0.582211971282959\n",
      "Epoch: 3790 | Loss: 0.5507208704948425 | Test loss: 0.5821981430053711\n",
      "Epoch: 3800 | Loss: 0.5507091879844666 | Test loss: 0.5821843147277832\n",
      "Epoch: 3810 | Loss: 0.5506973266601562 | Test loss: 0.5821704268455505\n",
      "Epoch: 3820 | Loss: 0.5506856441497803 | Test loss: 0.5821565985679626\n",
      "Epoch: 3830 | Loss: 0.5506739020347595 | Test loss: 0.5821427702903748\n",
      "Epoch: 3840 | Loss: 0.5506621599197388 | Test loss: 0.5821289420127869\n",
      "Epoch: 3850 | Loss: 0.5506503582000732 | Test loss: 0.582115113735199\n",
      "Epoch: 3860 | Loss: 0.5506386160850525 | Test loss: 0.5821012854576111\n",
      "Epoch: 3870 | Loss: 0.550626814365387 | Test loss: 0.5820873379707336\n",
      "Epoch: 3880 | Loss: 0.5506150722503662 | Test loss: 0.5820735096931458\n",
      "Epoch: 3890 | Loss: 0.5506033301353455 | Test loss: 0.5820596814155579\n",
      "Epoch: 3900 | Loss: 0.5505915880203247 | Test loss: 0.5820457935333252\n",
      "Epoch: 3910 | Loss: 0.5505797266960144 | Test loss: 0.5820319652557373\n",
      "Epoch: 3920 | Loss: 0.5505680441856384 | Test loss: 0.5820181369781494\n",
      "Epoch: 3930 | Loss: 0.5505563020706177 | Test loss: 0.5820042490959167\n",
      "Epoch: 3940 | Loss: 0.5505445003509521 | Test loss: 0.5819904208183289\n",
      "Epoch: 3950 | Loss: 0.5505327582359314 | Test loss: 0.581976592540741\n",
      "Epoch: 3960 | Loss: 0.5505210161209106 | Test loss: 0.5819627046585083\n",
      "Epoch: 3970 | Loss: 0.5505092144012451 | Test loss: 0.5819489359855652\n",
      "Epoch: 3980 | Loss: 0.5504974722862244 | Test loss: 0.5819350481033325\n",
      "Epoch: 3990 | Loss: 0.5504857301712036 | Test loss: 0.5819212198257446\n",
      "Epoch: 4000 | Loss: 0.5504739880561829 | Test loss: 0.5819073915481567\n",
      "Epoch: 4010 | Loss: 0.5504621863365173 | Test loss: 0.5818935632705688\n",
      "Epoch: 4020 | Loss: 0.5504504442214966 | Test loss: 0.5818796157836914\n",
      "Epoch: 4030 | Loss: 0.5504387021064758 | Test loss: 0.5818657875061035\n",
      "Epoch: 4040 | Loss: 0.5504269003868103 | Test loss: 0.5818519592285156\n",
      "Epoch: 4050 | Loss: 0.5504151582717896 | Test loss: 0.5818381309509277\n",
      "Epoch: 4060 | Loss: 0.5504034161567688 | Test loss: 0.5818243026733398\n",
      "Epoch: 4070 | Loss: 0.5503916144371033 | Test loss: 0.5818104147911072\n",
      "Epoch: 4080 | Loss: 0.5503799319267273 | Test loss: 0.5817965865135193\n",
      "Epoch: 4090 | Loss: 0.550368070602417 | Test loss: 0.5817827582359314\n",
      "Epoch: 4100 | Loss: 0.5503563284873962 | Test loss: 0.5817689299583435\n",
      "Epoch: 4110 | Loss: 0.5503445863723755 | Test loss: 0.5817551016807556\n",
      "Epoch: 4120 | Loss: 0.55033278465271 | Test loss: 0.5817412734031677\n",
      "Epoch: 4130 | Loss: 0.550321102142334 | Test loss: 0.5817273855209351\n",
      "Epoch: 4140 | Loss: 0.5503093004226685 | Test loss: 0.5817134976387024\n",
      "Epoch: 4150 | Loss: 0.5502975583076477 | Test loss: 0.5816996693611145\n",
      "Epoch: 4160 | Loss: 0.5502857565879822 | Test loss: 0.5816857814788818\n",
      "Epoch: 4170 | Loss: 0.5502740144729614 | Test loss: 0.5816720128059387\n",
      "Epoch: 4180 | Loss: 0.5502622723579407 | Test loss: 0.581658124923706\n",
      "Epoch: 4190 | Loss: 0.5502505302429199 | Test loss: 0.5816442370414734\n",
      "Epoch: 4200 | Loss: 0.5502387881278992 | Test loss: 0.5816304087638855\n",
      "Epoch: 4210 | Loss: 0.5502269864082336 | Test loss: 0.5816165804862976\n",
      "Epoch: 4220 | Loss: 0.5502152442932129 | Test loss: 0.5816027522087097\n",
      "Epoch: 4230 | Loss: 0.5502035021781921 | Test loss: 0.5815889239311218\n",
      "Epoch: 4240 | Loss: 0.5501917004585266 | Test loss: 0.5815750360488892\n",
      "Epoch: 4250 | Loss: 0.5501799583435059 | Test loss: 0.5815612077713013\n",
      "Epoch: 4260 | Loss: 0.5501681566238403 | Test loss: 0.5815473794937134\n",
      "Epoch: 4270 | Loss: 0.5501564145088196 | Test loss: 0.5815335512161255\n",
      "Epoch: 4280 | Loss: 0.5501446723937988 | Test loss: 0.5815196633338928\n",
      "Epoch: 4290 | Loss: 0.5501329302787781 | Test loss: 0.5815058350563049\n",
      "Epoch: 4300 | Loss: 0.5501211285591125 | Test loss: 0.5814919471740723\n",
      "Epoch: 4310 | Loss: 0.5501093864440918 | Test loss: 0.5814781188964844\n",
      "Epoch: 4320 | Loss: 0.5500975847244263 | Test loss: 0.5814642906188965\n",
      "Epoch: 4330 | Loss: 0.5500858426094055 | Test loss: 0.5814504623413086\n",
      "Epoch: 4340 | Loss: 0.5500741004943848 | Test loss: 0.5814365744590759\n",
      "Epoch: 4350 | Loss: 0.550062358379364 | Test loss: 0.581422746181488\n",
      "Epoch: 4360 | Loss: 0.5500505566596985 | Test loss: 0.5814089179039001\n",
      "Epoch: 4370 | Loss: 0.5500388145446777 | Test loss: 0.5813950896263123\n",
      "Epoch: 4380 | Loss: 0.550027072429657 | Test loss: 0.5813812613487244\n",
      "Epoch: 4390 | Loss: 0.5500152707099915 | Test loss: 0.5813673734664917\n",
      "Epoch: 4400 | Loss: 0.5500035285949707 | Test loss: 0.5813535451889038\n",
      "Epoch: 4410 | Loss: 0.54999178647995 | Test loss: 0.5813396573066711\n",
      "Epoch: 4420 | Loss: 0.549980103969574 | Test loss: 0.5813258290290833\n",
      "Epoch: 4430 | Loss: 0.5499682426452637 | Test loss: 0.5813120007514954\n",
      "Epoch: 4440 | Loss: 0.5499565005302429 | Test loss: 0.5812981128692627\n",
      "Epoch: 4450 | Loss: 0.5499448180198669 | Test loss: 0.5812842845916748\n",
      "Epoch: 4460 | Loss: 0.5499329566955566 | Test loss: 0.5812704563140869\n",
      "Epoch: 4470 | Loss: 0.5499212145805359 | Test loss: 0.5812565684318542\n",
      "Epoch: 4480 | Loss: 0.5499095320701599 | Test loss: 0.5812427401542664\n",
      "Epoch: 4490 | Loss: 0.5498976707458496 | Test loss: 0.5812289118766785\n",
      "Epoch: 4500 | Loss: 0.5498859286308289 | Test loss: 0.5812150239944458\n",
      "Epoch: 4510 | Loss: 0.5498741865158081 | Test loss: 0.5812011957168579\n",
      "Epoch: 4520 | Loss: 0.5498623847961426 | Test loss: 0.58118736743927\n",
      "Epoch: 4530 | Loss: 0.5498506426811218 | Test loss: 0.5811735391616821\n",
      "Epoch: 4540 | Loss: 0.5498389601707458 | Test loss: 0.5811597108840942\n",
      "Epoch: 4550 | Loss: 0.5498270988464355 | Test loss: 0.5811458230018616\n",
      "Epoch: 4560 | Loss: 0.5498153567314148 | Test loss: 0.5811319351196289\n",
      "Epoch: 4570 | Loss: 0.5498036742210388 | Test loss: 0.581118106842041\n",
      "Epoch: 4580 | Loss: 0.5497918725013733 | Test loss: 0.5811042785644531\n",
      "Epoch: 4590 | Loss: 0.5497800707817078 | Test loss: 0.5810904502868652\n",
      "Epoch: 4600 | Loss: 0.5497683882713318 | Test loss: 0.5810766220092773\n",
      "Epoch: 4610 | Loss: 0.5497565269470215 | Test loss: 0.5810627341270447\n",
      "Epoch: 4620 | Loss: 0.5497448444366455 | Test loss: 0.5810489058494568\n",
      "Epoch: 4630 | Loss: 0.5497331023216248 | Test loss: 0.5810350775718689\n",
      "Epoch: 4640 | Loss: 0.549721360206604 | Test loss: 0.581021249294281\n",
      "Epoch: 4650 | Loss: 0.5497095584869385 | Test loss: 0.5810074210166931\n",
      "Epoch: 4660 | Loss: 0.5496978163719177 | Test loss: 0.5809935927391052\n",
      "Epoch: 4670 | Loss: 0.5496860146522522 | Test loss: 0.5809796452522278\n",
      "Epoch: 4680 | Loss: 0.5496742725372314 | Test loss: 0.5809658169746399\n",
      "Epoch: 4690 | Loss: 0.5496625304222107 | Test loss: 0.580951988697052\n",
      "Epoch: 4700 | Loss: 0.5496507883071899 | Test loss: 0.5809381008148193\n",
      "Epoch: 4710 | Loss: 0.5496389269828796 | Test loss: 0.5809242725372314\n",
      "Epoch: 4720 | Loss: 0.5496272444725037 | Test loss: 0.5809104442596436\n",
      "Epoch: 4730 | Loss: 0.5496155023574829 | Test loss: 0.5808965563774109\n",
      "Epoch: 4740 | Loss: 0.5496037006378174 | Test loss: 0.580882728099823\n",
      "Epoch: 4750 | Loss: 0.5495919585227966 | Test loss: 0.5808688998222351\n",
      "Epoch: 4760 | Loss: 0.5495802164077759 | Test loss: 0.5808550119400024\n",
      "Epoch: 4770 | Loss: 0.5495684146881104 | Test loss: 0.5808412432670593\n",
      "Epoch: 4780 | Loss: 0.5495566725730896 | Test loss: 0.5808273553848267\n",
      "Epoch: 4790 | Loss: 0.5495449304580688 | Test loss: 0.5808135271072388\n",
      "Epoch: 4800 | Loss: 0.5495331883430481 | Test loss: 0.5807996988296509\n",
      "Epoch: 4810 | Loss: 0.5495213866233826 | Test loss: 0.580785870552063\n",
      "Epoch: 4820 | Loss: 0.5495096445083618 | Test loss: 0.5807719230651855\n",
      "Epoch: 4830 | Loss: 0.5494979023933411 | Test loss: 0.5807580947875977\n",
      "Epoch: 4840 | Loss: 0.5494861006736755 | Test loss: 0.5807442665100098\n",
      "Epoch: 4850 | Loss: 0.5494743585586548 | Test loss: 0.5807304382324219\n",
      "Epoch: 4860 | Loss: 0.549462616443634 | Test loss: 0.580716609954834\n",
      "Epoch: 4870 | Loss: 0.5494508147239685 | Test loss: 0.5807027220726013\n",
      "Epoch: 4880 | Loss: 0.5494391322135925 | Test loss: 0.5806888937950134\n",
      "Epoch: 4890 | Loss: 0.5494272708892822 | Test loss: 0.5806750655174255\n",
      "Epoch: 4900 | Loss: 0.5494155287742615 | Test loss: 0.5806612372398376\n",
      "Epoch: 4910 | Loss: 0.5494037866592407 | Test loss: 0.5806474089622498\n",
      "Epoch: 4920 | Loss: 0.5493919849395752 | Test loss: 0.5806335806846619\n",
      "Epoch: 4930 | Loss: 0.5493803024291992 | Test loss: 0.5806196928024292\n",
      "Epoch: 4940 | Loss: 0.5493685007095337 | Test loss: 0.5806058049201965\n",
      "Epoch: 4950 | Loss: 0.5493567585945129 | Test loss: 0.5805919766426086\n",
      "Epoch: 4960 | Loss: 0.5493449568748474 | Test loss: 0.580578088760376\n",
      "Epoch: 4970 | Loss: 0.5493332147598267 | Test loss: 0.5805643200874329\n",
      "Epoch: 4980 | Loss: 0.5493214726448059 | Test loss: 0.5805504322052002\n",
      "Epoch: 4990 | Loss: 0.5493097305297852 | Test loss: 0.5805365443229675\n",
      "Epoch: 5000 | Loss: 0.5492979884147644 | Test loss: 0.5805227160453796\n",
      "Epoch: 5010 | Loss: 0.5492861866950989 | Test loss: 0.5805088877677917\n",
      "Epoch: 5020 | Loss: 0.5492744445800781 | Test loss: 0.5804950594902039\n",
      "Epoch: 5030 | Loss: 0.5492627024650574 | Test loss: 0.580481231212616\n",
      "Epoch: 5040 | Loss: 0.5492509007453918 | Test loss: 0.5804673433303833\n",
      "Epoch: 5050 | Loss: 0.5492391586303711 | Test loss: 0.5804535150527954\n",
      "Epoch: 5060 | Loss: 0.5492273569107056 | Test loss: 0.5804396867752075\n",
      "Epoch: 5070 | Loss: 0.5492156147956848 | Test loss: 0.5804258584976196\n",
      "Epoch: 5080 | Loss: 0.5492038726806641 | Test loss: 0.580411970615387\n",
      "Epoch: 5090 | Loss: 0.5491921305656433 | Test loss: 0.5803981423377991\n",
      "Epoch: 5100 | Loss: 0.5491803288459778 | Test loss: 0.5803842544555664\n",
      "Epoch: 5110 | Loss: 0.549168586730957 | Test loss: 0.5803704261779785\n",
      "Epoch: 5120 | Loss: 0.5491567850112915 | Test loss: 0.5803565979003906\n",
      "Epoch: 5130 | Loss: 0.5491450428962708 | Test loss: 0.5803427696228027\n",
      "Epoch: 5140 | Loss: 0.54913330078125 | Test loss: 0.5803288817405701\n",
      "Epoch: 5150 | Loss: 0.5491215586662292 | Test loss: 0.5803150534629822\n",
      "Epoch: 5160 | Loss: 0.5491097569465637 | Test loss: 0.5803012251853943\n",
      "Epoch: 5170 | Loss: 0.549098014831543 | Test loss: 0.5802873969078064\n",
      "Epoch: 5180 | Loss: 0.5490862727165222 | Test loss: 0.5802735686302185\n",
      "Epoch: 5190 | Loss: 0.5490744709968567 | Test loss: 0.5802596807479858\n",
      "Epoch: 5200 | Loss: 0.5490627288818359 | Test loss: 0.580245852470398\n",
      "Epoch: 5210 | Loss: 0.5490509867668152 | Test loss: 0.5802319645881653\n",
      "Epoch: 5220 | Loss: 0.5490393042564392 | Test loss: 0.5802181363105774\n",
      "Epoch: 5230 | Loss: 0.5490274429321289 | Test loss: 0.5802043080329895\n",
      "Epoch: 5240 | Loss: 0.5490157008171082 | Test loss: 0.5801904201507568\n",
      "Epoch: 5250 | Loss: 0.5490040183067322 | Test loss: 0.580176591873169\n",
      "Epoch: 5260 | Loss: 0.5489921569824219 | Test loss: 0.580162763595581\n",
      "Epoch: 5270 | Loss: 0.5489804148674011 | Test loss: 0.5801488757133484\n",
      "Epoch: 5280 | Loss: 0.5489687323570251 | Test loss: 0.5801350474357605\n",
      "Epoch: 5290 | Loss: 0.5489568710327148 | Test loss: 0.5801212191581726\n",
      "Epoch: 5300 | Loss: 0.5489451289176941 | Test loss: 0.5801073312759399\n",
      "Epoch: 5310 | Loss: 0.5489333868026733 | Test loss: 0.580093502998352\n",
      "Epoch: 5320 | Loss: 0.5489215850830078 | Test loss: 0.5800796747207642\n",
      "Epoch: 5330 | Loss: 0.5489098429679871 | Test loss: 0.5800658464431763\n",
      "Epoch: 5340 | Loss: 0.5488981604576111 | Test loss: 0.5800520181655884\n",
      "Epoch: 5350 | Loss: 0.5488862991333008 | Test loss: 0.5800381302833557\n",
      "Epoch: 5360 | Loss: 0.54887455701828 | Test loss: 0.580024242401123\n",
      "Epoch: 5370 | Loss: 0.548862874507904 | Test loss: 0.5800104141235352\n",
      "Epoch: 5380 | Loss: 0.5488510727882385 | Test loss: 0.5799965858459473\n",
      "Epoch: 5390 | Loss: 0.548839271068573 | Test loss: 0.5799827575683594\n",
      "Epoch: 5400 | Loss: 0.548827588558197 | Test loss: 0.5799689292907715\n",
      "Epoch: 5410 | Loss: 0.5488157272338867 | Test loss: 0.5799550414085388\n",
      "Epoch: 5420 | Loss: 0.5488040447235107 | Test loss: 0.5799412131309509\n",
      "Epoch: 5430 | Loss: 0.54879230260849 | Test loss: 0.579927384853363\n",
      "Epoch: 5440 | Loss: 0.5487805604934692 | Test loss: 0.5799135565757751\n",
      "Epoch: 5450 | Loss: 0.5487687587738037 | Test loss: 0.5798997282981873\n",
      "Epoch: 5460 | Loss: 0.548757016658783 | Test loss: 0.5798859000205994\n",
      "Epoch: 5470 | Loss: 0.5487452149391174 | Test loss: 0.5798719525337219\n",
      "Epoch: 5480 | Loss: 0.5487334728240967 | Test loss: 0.579858124256134\n",
      "Epoch: 5490 | Loss: 0.5487217307090759 | Test loss: 0.5798442959785461\n",
      "Epoch: 5500 | Loss: 0.5487099885940552 | Test loss: 0.5798304080963135\n",
      "Epoch: 5510 | Loss: 0.5486981272697449 | Test loss: 0.5798165798187256\n",
      "Epoch: 5520 | Loss: 0.5486864447593689 | Test loss: 0.5798027515411377\n",
      "Epoch: 5530 | Loss: 0.5486747026443481 | Test loss: 0.579788863658905\n",
      "Epoch: 5540 | Loss: 0.5486629009246826 | Test loss: 0.5797750353813171\n",
      "Epoch: 5550 | Loss: 0.5486511588096619 | Test loss: 0.5797612071037292\n",
      "Epoch: 5560 | Loss: 0.5486394166946411 | Test loss: 0.5797473192214966\n",
      "Epoch: 5570 | Loss: 0.5486276149749756 | Test loss: 0.5797335505485535\n",
      "Epoch: 5580 | Loss: 0.5486158728599548 | Test loss: 0.5797196626663208\n",
      "Epoch: 5590 | Loss: 0.5486041307449341 | Test loss: 0.5797058343887329\n",
      "Epoch: 5600 | Loss: 0.5485923886299133 | Test loss: 0.579692006111145\n",
      "Epoch: 5610 | Loss: 0.5485805869102478 | Test loss: 0.5796781778335571\n",
      "Epoch: 5620 | Loss: 0.548568844795227 | Test loss: 0.5796642303466797\n",
      "Epoch: 5630 | Loss: 0.5485571026802063 | Test loss: 0.5796504020690918\n",
      "Epoch: 5640 | Loss: 0.5485453009605408 | Test loss: 0.5796365737915039\n",
      "Epoch: 5650 | Loss: 0.54853355884552 | Test loss: 0.579622745513916\n",
      "Epoch: 5660 | Loss: 0.5485218167304993 | Test loss: 0.5796089172363281\n",
      "Epoch: 5670 | Loss: 0.5485100150108337 | Test loss: 0.5795950293540955\n",
      "Epoch: 5680 | Loss: 0.5484983325004578 | Test loss: 0.5795812010765076\n",
      "Epoch: 5690 | Loss: 0.5484864711761475 | Test loss: 0.5795673727989197\n",
      "Epoch: 5700 | Loss: 0.5484747290611267 | Test loss: 0.5795535445213318\n",
      "Epoch: 5710 | Loss: 0.548462986946106 | Test loss: 0.5795397162437439\n",
      "Epoch: 5720 | Loss: 0.5484511852264404 | Test loss: 0.579525887966156\n",
      "Epoch: 5730 | Loss: 0.5484395027160645 | Test loss: 0.5795120000839233\n",
      "Epoch: 5740 | Loss: 0.5484277009963989 | Test loss: 0.5794981122016907\n",
      "Epoch: 5750 | Loss: 0.5484159588813782 | Test loss: 0.5794842839241028\n",
      "Epoch: 5760 | Loss: 0.5484041571617126 | Test loss: 0.5794703960418701\n",
      "Epoch: 5770 | Loss: 0.5483924150466919 | Test loss: 0.579456627368927\n",
      "Epoch: 5780 | Loss: 0.5483806729316711 | Test loss: 0.5794427394866943\n",
      "Epoch: 5790 | Loss: 0.5483689308166504 | Test loss: 0.5794288516044617\n",
      "Epoch: 5800 | Loss: 0.5483571887016296 | Test loss: 0.5794150233268738\n",
      "Epoch: 5810 | Loss: 0.5483453869819641 | Test loss: 0.5794011950492859\n",
      "Epoch: 5820 | Loss: 0.5483336448669434 | Test loss: 0.579387366771698\n",
      "Epoch: 5830 | Loss: 0.5483219027519226 | Test loss: 0.5793735384941101\n",
      "Epoch: 5840 | Loss: 0.5483101010322571 | Test loss: 0.5793596506118774\n",
      "Epoch: 5850 | Loss: 0.5482983589172363 | Test loss: 0.5793458223342896\n",
      "Epoch: 5860 | Loss: 0.5482865571975708 | Test loss: 0.5793319940567017\n",
      "Epoch: 5870 | Loss: 0.54827481508255 | Test loss: 0.5793181657791138\n",
      "Epoch: 5880 | Loss: 0.5482630729675293 | Test loss: 0.5793042778968811\n",
      "Epoch: 5890 | Loss: 0.5482513308525085 | Test loss: 0.5792904496192932\n",
      "Epoch: 5900 | Loss: 0.548239529132843 | Test loss: 0.5792765617370605\n",
      "Epoch: 5910 | Loss: 0.5482277870178223 | Test loss: 0.5792627334594727\n",
      "Epoch: 5920 | Loss: 0.5482159852981567 | Test loss: 0.5792489051818848\n",
      "Epoch: 5930 | Loss: 0.548204243183136 | Test loss: 0.5792350769042969\n",
      "Epoch: 5940 | Loss: 0.5481925010681152 | Test loss: 0.5792211890220642\n",
      "Epoch: 5950 | Loss: 0.5481807589530945 | Test loss: 0.5792073607444763\n",
      "Epoch: 5960 | Loss: 0.548168957233429 | Test loss: 0.5791935324668884\n",
      "Epoch: 5970 | Loss: 0.5481572151184082 | Test loss: 0.5791797041893005\n",
      "Epoch: 5980 | Loss: 0.5481454730033875 | Test loss: 0.5791658759117126\n",
      "Epoch: 5990 | Loss: 0.5481336712837219 | Test loss: 0.57915198802948\n",
      "Epoch: 6000 | Loss: 0.5481219291687012 | Test loss: 0.5791381597518921\n",
      "Epoch: 6010 | Loss: 0.5481101870536804 | Test loss: 0.5791242718696594\n",
      "Epoch: 6020 | Loss: 0.5480985045433044 | Test loss: 0.5791104435920715\n",
      "Epoch: 6030 | Loss: 0.5480866432189941 | Test loss: 0.5790966153144836\n",
      "Epoch: 6040 | Loss: 0.5480749011039734 | Test loss: 0.579082727432251\n",
      "Epoch: 6050 | Loss: 0.5480632185935974 | Test loss: 0.5790688991546631\n",
      "Epoch: 6060 | Loss: 0.5480513572692871 | Test loss: 0.5790550708770752\n",
      "Epoch: 6070 | Loss: 0.5480396151542664 | Test loss: 0.5790411829948425\n",
      "Epoch: 6080 | Loss: 0.5480279326438904 | Test loss: 0.5790273547172546\n",
      "Epoch: 6090 | Loss: 0.5480160713195801 | Test loss: 0.5790135264396667\n",
      "Epoch: 6100 | Loss: 0.5480043292045593 | Test loss: 0.5789996385574341\n",
      "Epoch: 6110 | Loss: 0.5479925870895386 | Test loss: 0.5789858102798462\n",
      "Epoch: 6120 | Loss: 0.547980785369873 | Test loss: 0.5789719820022583\n",
      "Epoch: 6130 | Loss: 0.5479690432548523 | Test loss: 0.5789581537246704\n",
      "Epoch: 6140 | Loss: 0.5479573607444763 | Test loss: 0.5789443254470825\n",
      "Epoch: 6150 | Loss: 0.547945499420166 | Test loss: 0.5789304375648499\n",
      "Epoch: 6160 | Loss: 0.5479337573051453 | Test loss: 0.5789165496826172\n",
      "Epoch: 6170 | Loss: 0.5479220747947693 | Test loss: 0.5789027214050293\n",
      "Epoch: 6180 | Loss: 0.5479102730751038 | Test loss: 0.5788888931274414\n",
      "Epoch: 6190 | Loss: 0.5478984713554382 | Test loss: 0.5788750648498535\n",
      "Epoch: 6200 | Loss: 0.5478867888450623 | Test loss: 0.5788612365722656\n",
      "Epoch: 6210 | Loss: 0.547874927520752 | Test loss: 0.578847348690033\n",
      "Epoch: 6220 | Loss: 0.547863245010376 | Test loss: 0.5788335204124451\n",
      "Epoch: 6230 | Loss: 0.5478515028953552 | Test loss: 0.5788196921348572\n",
      "Epoch: 6240 | Loss: 0.5478397607803345 | Test loss: 0.5788058638572693\n",
      "Epoch: 6250 | Loss: 0.547827959060669 | Test loss: 0.5787920355796814\n",
      "Epoch: 6260 | Loss: 0.5478162169456482 | Test loss: 0.5787782073020935\n",
      "Epoch: 6270 | Loss: 0.5478044152259827 | Test loss: 0.5787642598152161\n",
      "Epoch: 6280 | Loss: 0.5477926731109619 | Test loss: 0.5787504315376282\n",
      "Epoch: 6290 | Loss: 0.5477809309959412 | Test loss: 0.5787366032600403\n",
      "Epoch: 6300 | Loss: 0.5477691888809204 | Test loss: 0.5787227153778076\n",
      "Epoch: 6310 | Loss: 0.5477573275566101 | Test loss: 0.5787088871002197\n",
      "Epoch: 6320 | Loss: 0.5477456450462341 | Test loss: 0.5786950588226318\n",
      "Epoch: 6330 | Loss: 0.5477339029312134 | Test loss: 0.5786811709403992\n",
      "Epoch: 6340 | Loss: 0.5477221012115479 | Test loss: 0.5786673426628113\n",
      "Epoch: 6350 | Loss: 0.5477103590965271 | Test loss: 0.5786535143852234\n",
      "Epoch: 6360 | Loss: 0.5476986169815063 | Test loss: 0.5786396265029907\n",
      "Epoch: 6370 | Loss: 0.5476868152618408 | Test loss: 0.5786258578300476\n",
      "Epoch: 6380 | Loss: 0.5476750731468201 | Test loss: 0.5786119699478149\n",
      "Epoch: 6390 | Loss: 0.5476633310317993 | Test loss: 0.578598141670227\n",
      "Epoch: 6400 | Loss: 0.5476515889167786 | Test loss: 0.5785843133926392\n",
      "Epoch: 6410 | Loss: 0.547639787197113 | Test loss: 0.5785704851150513\n",
      "Epoch: 6420 | Loss: 0.5476280450820923 | Test loss: 0.5785565376281738\n",
      "Epoch: 6430 | Loss: 0.5476163029670715 | Test loss: 0.5785427093505859\n",
      "Epoch: 6440 | Loss: 0.547604501247406 | Test loss: 0.578528881072998\n",
      "Epoch: 6450 | Loss: 0.5475927591323853 | Test loss: 0.5785150527954102\n",
      "Epoch: 6460 | Loss: 0.5475810170173645 | Test loss: 0.5785012245178223\n",
      "Epoch: 6470 | Loss: 0.547569215297699 | Test loss: 0.5784873366355896\n",
      "Epoch: 6480 | Loss: 0.547557532787323 | Test loss: 0.5784735083580017\n",
      "Epoch: 6490 | Loss: 0.5475456714630127 | Test loss: 0.5784596800804138\n",
      "Epoch: 6500 | Loss: 0.5475339293479919 | Test loss: 0.5784458518028259\n",
      "Epoch: 6510 | Loss: 0.5475221872329712 | Test loss: 0.578432023525238\n",
      "Epoch: 6520 | Loss: 0.5475103855133057 | Test loss: 0.5784181952476501\n",
      "Epoch: 6530 | Loss: 0.5474987030029297 | Test loss: 0.5784043073654175\n",
      "Epoch: 6540 | Loss: 0.5474869012832642 | Test loss: 0.5783904194831848\n",
      "Epoch: 6550 | Loss: 0.5474751591682434 | Test loss: 0.5783765912055969\n",
      "Epoch: 6560 | Loss: 0.5474633574485779 | Test loss: 0.5783627033233643\n",
      "Epoch: 6570 | Loss: 0.5474516153335571 | Test loss: 0.5783489346504211\n",
      "Epoch: 6580 | Loss: 0.5474398732185364 | Test loss: 0.5783350467681885\n",
      "Epoch: 6590 | Loss: 0.5474281311035156 | Test loss: 0.5783211588859558\n",
      "Epoch: 6600 | Loss: 0.5474163889884949 | Test loss: 0.5783073306083679\n",
      "Epoch: 6610 | Loss: 0.5474045872688293 | Test loss: 0.57829350233078\n",
      "Epoch: 6620 | Loss: 0.5473928451538086 | Test loss: 0.5782796740531921\n",
      "Epoch: 6630 | Loss: 0.5473811030387878 | Test loss: 0.5782658457756042\n",
      "Epoch: 6640 | Loss: 0.5473693013191223 | Test loss: 0.5782519578933716\n",
      "Epoch: 6650 | Loss: 0.5473575592041016 | Test loss: 0.5782381296157837\n",
      "Epoch: 6660 | Loss: 0.547345757484436 | Test loss: 0.5782243013381958\n",
      "Epoch: 6670 | Loss: 0.5473340153694153 | Test loss: 0.5782104730606079\n",
      "Epoch: 6680 | Loss: 0.5473222732543945 | Test loss: 0.5781965851783752\n",
      "Epoch: 6690 | Loss: 0.5473105311393738 | Test loss: 0.5781827569007874\n",
      "Epoch: 6700 | Loss: 0.5472987294197083 | Test loss: 0.5781688690185547\n",
      "Epoch: 6710 | Loss: 0.5472869873046875 | Test loss: 0.5781550407409668\n",
      "Epoch: 6720 | Loss: 0.547275185585022 | Test loss: 0.5781412124633789\n",
      "Epoch: 6730 | Loss: 0.5472634434700012 | Test loss: 0.578127384185791\n",
      "Epoch: 6740 | Loss: 0.5472517013549805 | Test loss: 0.5781134963035583\n",
      "Epoch: 6750 | Loss: 0.5472399592399597 | Test loss: 0.5780996680259705\n",
      "Epoch: 6760 | Loss: 0.5472281575202942 | Test loss: 0.5780858397483826\n",
      "Epoch: 6770 | Loss: 0.5472164154052734 | Test loss: 0.5780720114707947\n",
      "Epoch: 6780 | Loss: 0.5472046732902527 | Test loss: 0.5780581831932068\n",
      "Epoch: 6790 | Loss: 0.5471928715705872 | Test loss: 0.5780442953109741\n",
      "Epoch: 6800 | Loss: 0.5471811294555664 | Test loss: 0.5780304670333862\n",
      "Epoch: 6810 | Loss: 0.5471693873405457 | Test loss: 0.5780165791511536\n",
      "Epoch: 6820 | Loss: 0.5471577048301697 | Test loss: 0.5780027508735657\n",
      "Epoch: 6830 | Loss: 0.5471458435058594 | Test loss: 0.5779889225959778\n",
      "Epoch: 6840 | Loss: 0.5471341013908386 | Test loss: 0.5779750347137451\n",
      "Epoch: 6850 | Loss: 0.5471224188804626 | Test loss: 0.5779612064361572\n",
      "Epoch: 6860 | Loss: 0.5471105575561523 | Test loss: 0.5779473781585693\n",
      "Epoch: 6870 | Loss: 0.5470988154411316 | Test loss: 0.5779334902763367\n",
      "Epoch: 6880 | Loss: 0.5470871329307556 | Test loss: 0.5779196619987488\n",
      "Epoch: 6890 | Loss: 0.5470752716064453 | Test loss: 0.5779058337211609\n",
      "Epoch: 6900 | Loss: 0.5470635294914246 | Test loss: 0.5778919458389282\n",
      "Epoch: 6910 | Loss: 0.5470517873764038 | Test loss: 0.5778781175613403\n",
      "Epoch: 6920 | Loss: 0.5470399856567383 | Test loss: 0.5778642892837524\n",
      "Epoch: 6930 | Loss: 0.5470282435417175 | Test loss: 0.5778504610061646\n",
      "Epoch: 6940 | Loss: 0.5470165610313416 | Test loss: 0.5778366327285767\n",
      "Epoch: 6950 | Loss: 0.5470046997070312 | Test loss: 0.577822744846344\n",
      "Epoch: 6960 | Loss: 0.5469929575920105 | Test loss: 0.5778088569641113\n",
      "Epoch: 6970 | Loss: 0.5469812750816345 | Test loss: 0.5777950286865234\n",
      "Epoch: 6980 | Loss: 0.546969473361969 | Test loss: 0.5777812004089355\n",
      "Epoch: 6990 | Loss: 0.5469576716423035 | Test loss: 0.5777673721313477\n",
      "Epoch: 7000 | Loss: 0.5469459891319275 | Test loss: 0.5777535438537598\n",
      "Epoch: 7010 | Loss: 0.5469341278076172 | Test loss: 0.5777396559715271\n",
      "Epoch: 7020 | Loss: 0.5469224452972412 | Test loss: 0.5777258276939392\n",
      "Epoch: 7030 | Loss: 0.5469107031822205 | Test loss: 0.5777119994163513\n",
      "Epoch: 7040 | Loss: 0.5468989610671997 | Test loss: 0.5776981711387634\n",
      "Epoch: 7050 | Loss: 0.5468871593475342 | Test loss: 0.5776843428611755\n",
      "Epoch: 7060 | Loss: 0.5468754172325134 | Test loss: 0.5776705145835876\n",
      "Epoch: 7070 | Loss: 0.5468636155128479 | Test loss: 0.5776565670967102\n",
      "Epoch: 7080 | Loss: 0.5468518733978271 | Test loss: 0.5776427388191223\n",
      "Epoch: 7090 | Loss: 0.5468401312828064 | Test loss: 0.5776289105415344\n",
      "Epoch: 7100 | Loss: 0.5468283891677856 | Test loss: 0.5776150226593018\n",
      "Epoch: 7110 | Loss: 0.5468165278434753 | Test loss: 0.5776011943817139\n",
      "Epoch: 7120 | Loss: 0.5468048453330994 | Test loss: 0.577587366104126\n",
      "Epoch: 7130 | Loss: 0.5467931032180786 | Test loss: 0.5775734782218933\n",
      "Epoch: 7140 | Loss: 0.5467813014984131 | Test loss: 0.5775596499443054\n",
      "Epoch: 7150 | Loss: 0.5467695593833923 | Test loss: 0.5775458216667175\n",
      "Epoch: 7160 | Loss: 0.5467578172683716 | Test loss: 0.5775319337844849\n",
      "Epoch: 7170 | Loss: 0.546746015548706 | Test loss: 0.5775181651115417\n",
      "Epoch: 7180 | Loss: 0.5467342734336853 | Test loss: 0.5775042772293091\n",
      "Epoch: 7190 | Loss: 0.5467225313186646 | Test loss: 0.5774904489517212\n",
      "Epoch: 7200 | Loss: 0.5467107892036438 | Test loss: 0.5774766206741333\n",
      "Epoch: 7210 | Loss: 0.5466989874839783 | Test loss: 0.5774627923965454\n",
      "Epoch: 7220 | Loss: 0.5466872453689575 | Test loss: 0.577448844909668\n",
      "Epoch: 7230 | Loss: 0.5466755032539368 | Test loss: 0.5774350166320801\n",
      "Epoch: 7240 | Loss: 0.5466637015342712 | Test loss: 0.5774211883544922\n",
      "Epoch: 7250 | Loss: 0.5466519594192505 | Test loss: 0.5774073600769043\n",
      "Epoch: 7260 | Loss: 0.5466402173042297 | Test loss: 0.5773935317993164\n",
      "Epoch: 7270 | Loss: 0.5466284155845642 | Test loss: 0.5773796439170837\n",
      "Epoch: 7280 | Loss: 0.5466167330741882 | Test loss: 0.5773658156394958\n",
      "Epoch: 7290 | Loss: 0.5466048717498779 | Test loss: 0.577351987361908\n",
      "Epoch: 7300 | Loss: 0.5465931296348572 | Test loss: 0.5773381590843201\n",
      "Epoch: 7310 | Loss: 0.5465813875198364 | Test loss: 0.5773243308067322\n",
      "Epoch: 7320 | Loss: 0.5465695858001709 | Test loss: 0.5773105025291443\n",
      "Epoch: 7330 | Loss: 0.5465579032897949 | Test loss: 0.5772966146469116\n",
      "Epoch: 7340 | Loss: 0.5465461015701294 | Test loss: 0.577282726764679\n",
      "Epoch: 7350 | Loss: 0.5465343594551086 | Test loss: 0.5772688984870911\n",
      "Epoch: 7360 | Loss: 0.5465225577354431 | Test loss: 0.5772550106048584\n",
      "Epoch: 7370 | Loss: 0.5465108156204224 | Test loss: 0.5772412419319153\n",
      "Epoch: 7380 | Loss: 0.5464990735054016 | Test loss: 0.5772273540496826\n",
      "Epoch: 7390 | Loss: 0.5464873313903809 | Test loss: 0.57721346616745\n",
      "Epoch: 7400 | Loss: 0.5464755892753601 | Test loss: 0.5771996378898621\n",
      "Epoch: 7410 | Loss: 0.5464637875556946 | Test loss: 0.5771858096122742\n",
      "Epoch: 7420 | Loss: 0.5464520454406738 | Test loss: 0.5771719813346863\n",
      "Epoch: 7430 | Loss: 0.5464403033256531 | Test loss: 0.5771581530570984\n",
      "Epoch: 7440 | Loss: 0.5464285016059875 | Test loss: 0.5771442651748657\n",
      "Epoch: 7450 | Loss: 0.5464167594909668 | Test loss: 0.5771304368972778\n",
      "Epoch: 7460 | Loss: 0.5464049577713013 | Test loss: 0.5771166086196899\n",
      "Epoch: 7470 | Loss: 0.5463932156562805 | Test loss: 0.577102780342102\n",
      "Epoch: 7480 | Loss: 0.5463814735412598 | Test loss: 0.5770888924598694\n",
      "Epoch: 7490 | Loss: 0.546369731426239 | Test loss: 0.5770750641822815\n",
      "Epoch: 7500 | Loss: 0.5463579297065735 | Test loss: 0.5770611763000488\n",
      "Epoch: 7510 | Loss: 0.5463461875915527 | Test loss: 0.5770473480224609\n",
      "Epoch: 7520 | Loss: 0.5463343858718872 | Test loss: 0.577033519744873\n",
      "Epoch: 7530 | Loss: 0.5463226437568665 | Test loss: 0.5770196914672852\n",
      "Epoch: 7540 | Loss: 0.5463109016418457 | Test loss: 0.5770058035850525\n",
      "Epoch: 7550 | Loss: 0.546299159526825 | Test loss: 0.5769919753074646\n",
      "Epoch: 7560 | Loss: 0.5462873578071594 | Test loss: 0.5769781470298767\n",
      "Epoch: 7570 | Loss: 0.5462756156921387 | Test loss: 0.5769643187522888\n",
      "Epoch: 7580 | Loss: 0.5462638735771179 | Test loss: 0.5769504904747009\n",
      "Epoch: 7590 | Loss: 0.5462520718574524 | Test loss: 0.5769366025924683\n",
      "Epoch: 7600 | Loss: 0.5462403297424316 | Test loss: 0.5769227743148804\n",
      "Epoch: 7610 | Loss: 0.5462285876274109 | Test loss: 0.5769088864326477\n",
      "Epoch: 7620 | Loss: 0.5462169051170349 | Test loss: 0.5768950581550598\n",
      "Epoch: 7630 | Loss: 0.5462050437927246 | Test loss: 0.5768812298774719\n",
      "Epoch: 7640 | Loss: 0.5461933016777039 | Test loss: 0.5768673419952393\n",
      "Epoch: 7650 | Loss: 0.5461816191673279 | Test loss: 0.5768535137176514\n",
      "Epoch: 7660 | Loss: 0.5461697578430176 | Test loss: 0.5768396854400635\n",
      "Epoch: 7670 | Loss: 0.5461580157279968 | Test loss: 0.5768257975578308\n",
      "Epoch: 7680 | Loss: 0.5461463332176208 | Test loss: 0.5768119692802429\n",
      "Epoch: 7690 | Loss: 0.5461344718933105 | Test loss: 0.576798141002655\n",
      "Epoch: 7700 | Loss: 0.5461227297782898 | Test loss: 0.5767842531204224\n",
      "Epoch: 7710 | Loss: 0.546110987663269 | Test loss: 0.5767704248428345\n",
      "Epoch: 7720 | Loss: 0.5460991859436035 | Test loss: 0.5767565965652466\n",
      "Epoch: 7730 | Loss: 0.5460874438285828 | Test loss: 0.5767427682876587\n",
      "Epoch: 7740 | Loss: 0.5460757613182068 | Test loss: 0.5767289400100708\n",
      "Epoch: 7750 | Loss: 0.5460638999938965 | Test loss: 0.5767150521278381\n",
      "Epoch: 7760 | Loss: 0.5460521578788757 | Test loss: 0.5767011642456055\n",
      "Epoch: 7770 | Loss: 0.5460404753684998 | Test loss: 0.5766873359680176\n",
      "Epoch: 7780 | Loss: 0.5460286736488342 | Test loss: 0.5766735076904297\n",
      "Epoch: 7790 | Loss: 0.5460168719291687 | Test loss: 0.5766596794128418\n",
      "Epoch: 7800 | Loss: 0.5460051894187927 | Test loss: 0.5766458511352539\n",
      "Epoch: 7810 | Loss: 0.5459933280944824 | Test loss: 0.5766319632530212\n",
      "Epoch: 7820 | Loss: 0.5459816455841064 | Test loss: 0.5766181349754333\n",
      "Epoch: 7830 | Loss: 0.5459699034690857 | Test loss: 0.5766043066978455\n",
      "Epoch: 7840 | Loss: 0.5459581613540649 | Test loss: 0.5765904784202576\n",
      "Epoch: 7850 | Loss: 0.5459463596343994 | Test loss: 0.5765766501426697\n",
      "Epoch: 7860 | Loss: 0.5459346175193787 | Test loss: 0.5765628218650818\n",
      "Epoch: 7870 | Loss: 0.5459228157997131 | Test loss: 0.5765488743782043\n",
      "Epoch: 7880 | Loss: 0.5459110736846924 | Test loss: 0.5765350461006165\n",
      "Epoch: 7890 | Loss: 0.5458993315696716 | Test loss: 0.5765212178230286\n",
      "Epoch: 7900 | Loss: 0.5458875894546509 | Test loss: 0.5765073299407959\n",
      "Epoch: 7910 | Loss: 0.5458757281303406 | Test loss: 0.576493501663208\n",
      "Epoch: 7920 | Loss: 0.5458640456199646 | Test loss: 0.5764796733856201\n",
      "Epoch: 7930 | Loss: 0.5458523035049438 | Test loss: 0.5764657855033875\n",
      "Epoch: 7940 | Loss: 0.5458405017852783 | Test loss: 0.5764519572257996\n",
      "Epoch: 7950 | Loss: 0.5458287596702576 | Test loss: 0.5764381289482117\n",
      "Epoch: 7960 | Loss: 0.5458170175552368 | Test loss: 0.576424241065979\n",
      "Epoch: 7970 | Loss: 0.5458052158355713 | Test loss: 0.5764104723930359\n",
      "Epoch: 7980 | Loss: 0.5457934737205505 | Test loss: 0.5763965845108032\n",
      "Epoch: 7990 | Loss: 0.5457817316055298 | Test loss: 0.5763827562332153\n",
      "Epoch: 8000 | Loss: 0.545769989490509 | Test loss: 0.5763689279556274\n",
      "Epoch: 8010 | Loss: 0.5457581877708435 | Test loss: 0.5763550996780396\n",
      "Epoch: 8020 | Loss: 0.5457464456558228 | Test loss: 0.5763411521911621\n",
      "Epoch: 8030 | Loss: 0.545734703540802 | Test loss: 0.5763273239135742\n",
      "Epoch: 8040 | Loss: 0.5457229018211365 | Test loss: 0.5763134956359863\n",
      "Epoch: 8050 | Loss: 0.5457111597061157 | Test loss: 0.5762996673583984\n",
      "Epoch: 8060 | Loss: 0.545699417591095 | Test loss: 0.5762858390808105\n",
      "Epoch: 8070 | Loss: 0.5456876158714294 | Test loss: 0.5762719511985779\n",
      "Epoch: 8080 | Loss: 0.5456759333610535 | Test loss: 0.57625812292099\n",
      "Epoch: 8090 | Loss: 0.5456640720367432 | Test loss: 0.5762442946434021\n",
      "Epoch: 8100 | Loss: 0.5456523299217224 | Test loss: 0.5762304663658142\n",
      "Epoch: 8110 | Loss: 0.5456405878067017 | Test loss: 0.5762166380882263\n",
      "Epoch: 8120 | Loss: 0.5456287860870361 | Test loss: 0.5762028098106384\n",
      "Epoch: 8130 | Loss: 0.5456171035766602 | Test loss: 0.5761889219284058\n",
      "Epoch: 8140 | Loss: 0.5456053018569946 | Test loss: 0.5761750340461731\n",
      "Epoch: 8150 | Loss: 0.5455935597419739 | Test loss: 0.5761612057685852\n",
      "Epoch: 8160 | Loss: 0.5455817580223083 | Test loss: 0.5761473178863525\n",
      "Epoch: 8170 | Loss: 0.5455700159072876 | Test loss: 0.5761335492134094\n",
      "Epoch: 8180 | Loss: 0.5455582737922668 | Test loss: 0.5761196613311768\n",
      "Epoch: 8190 | Loss: 0.5455465316772461 | Test loss: 0.5761057734489441\n",
      "Epoch: 8200 | Loss: 0.5455347895622253 | Test loss: 0.5760919451713562\n",
      "Epoch: 8210 | Loss: 0.5455229878425598 | Test loss: 0.5760781168937683\n",
      "Epoch: 8220 | Loss: 0.5455112457275391 | Test loss: 0.5760642886161804\n",
      "Epoch: 8230 | Loss: 0.5454995036125183 | Test loss: 0.5760504603385925\n",
      "Epoch: 8240 | Loss: 0.5454877018928528 | Test loss: 0.5760365724563599\n",
      "Epoch: 8250 | Loss: 0.545475959777832 | Test loss: 0.576022744178772\n",
      "Epoch: 8260 | Loss: 0.5454641580581665 | Test loss: 0.5760089159011841\n",
      "Epoch: 8270 | Loss: 0.5454524159431458 | Test loss: 0.5759950876235962\n",
      "Epoch: 8280 | Loss: 0.545440673828125 | Test loss: 0.5759811997413635\n",
      "Epoch: 8290 | Loss: 0.5454289317131042 | Test loss: 0.5759673714637756\n",
      "Epoch: 8300 | Loss: 0.5454171299934387 | Test loss: 0.575953483581543\n",
      "Epoch: 8310 | Loss: 0.545405387878418 | Test loss: 0.5759396553039551\n",
      "Epoch: 8320 | Loss: 0.5453935861587524 | Test loss: 0.5759258270263672\n",
      "Epoch: 8330 | Loss: 0.5453818440437317 | Test loss: 0.5759119987487793\n",
      "Epoch: 8340 | Loss: 0.5453701019287109 | Test loss: 0.5758981108665466\n",
      "Epoch: 8350 | Loss: 0.5453583598136902 | Test loss: 0.5758842825889587\n",
      "Epoch: 8360 | Loss: 0.5453465580940247 | Test loss: 0.5758704543113708\n",
      "Epoch: 8370 | Loss: 0.5453348159790039 | Test loss: 0.575856626033783\n",
      "Epoch: 8380 | Loss: 0.5453230738639832 | Test loss: 0.5758427977561951\n",
      "Epoch: 8390 | Loss: 0.5453112721443176 | Test loss: 0.5758289098739624\n",
      "Epoch: 8400 | Loss: 0.5452995300292969 | Test loss: 0.5758150815963745\n",
      "Epoch: 8410 | Loss: 0.5452877879142761 | Test loss: 0.5758011937141418\n",
      "Epoch: 8420 | Loss: 0.5452761054039001 | Test loss: 0.575787365436554\n",
      "Epoch: 8430 | Loss: 0.5452642440795898 | Test loss: 0.5757735371589661\n",
      "Epoch: 8440 | Loss: 0.5452525019645691 | Test loss: 0.5757596492767334\n",
      "Epoch: 8450 | Loss: 0.5452408194541931 | Test loss: 0.5757458209991455\n",
      "Epoch: 8460 | Loss: 0.5452289581298828 | Test loss: 0.5757319927215576\n",
      "Epoch: 8470 | Loss: 0.5452172160148621 | Test loss: 0.575718104839325\n",
      "Epoch: 8480 | Loss: 0.5452055335044861 | Test loss: 0.5757042765617371\n",
      "Epoch: 8490 | Loss: 0.5451936721801758 | Test loss: 0.5756904482841492\n",
      "Epoch: 8500 | Loss: 0.545181930065155 | Test loss: 0.5756765604019165\n",
      "Epoch: 8510 | Loss: 0.5451701879501343 | Test loss: 0.5756627321243286\n",
      "Epoch: 8520 | Loss: 0.5451583862304688 | Test loss: 0.5756489038467407\n",
      "Epoch: 8530 | Loss: 0.545146644115448 | Test loss: 0.5756350755691528\n",
      "Epoch: 8540 | Loss: 0.545134961605072 | Test loss: 0.5756212472915649\n",
      "Epoch: 8550 | Loss: 0.5451231002807617 | Test loss: 0.5756073594093323\n",
      "Epoch: 8560 | Loss: 0.545111358165741 | Test loss: 0.5755934715270996\n",
      "Epoch: 8570 | Loss: 0.545099675655365 | Test loss: 0.5755796432495117\n",
      "Epoch: 8580 | Loss: 0.5450878739356995 | Test loss: 0.5755658149719238\n",
      "Epoch: 8590 | Loss: 0.5450760722160339 | Test loss: 0.5755519866943359\n",
      "Epoch: 8600 | Loss: 0.545064389705658 | Test loss: 0.575538158416748\n",
      "Epoch: 8610 | Loss: 0.5450525283813477 | Test loss: 0.5755242705345154\n",
      "Epoch: 8620 | Loss: 0.5450408458709717 | Test loss: 0.5755104422569275\n",
      "Epoch: 8630 | Loss: 0.5450291037559509 | Test loss: 0.5754966139793396\n",
      "Epoch: 8640 | Loss: 0.5450173616409302 | Test loss: 0.5754827857017517\n",
      "Epoch: 8650 | Loss: 0.5450055599212646 | Test loss: 0.5754689574241638\n",
      "Epoch: 8660 | Loss: 0.5449938178062439 | Test loss: 0.5754551291465759\n",
      "Epoch: 8670 | Loss: 0.5449820160865784 | Test loss: 0.5754411816596985\n",
      "Epoch: 8680 | Loss: 0.5449702739715576 | Test loss: 0.5754273533821106\n",
      "Epoch: 8690 | Loss: 0.5449585318565369 | Test loss: 0.5754135251045227\n",
      "Epoch: 8700 | Loss: 0.5449467897415161 | Test loss: 0.57539963722229\n",
      "Epoch: 8710 | Loss: 0.5449349284172058 | Test loss: 0.5753858089447021\n",
      "Epoch: 8720 | Loss: 0.5449232459068298 | Test loss: 0.5753719806671143\n",
      "Epoch: 8730 | Loss: 0.5449115037918091 | Test loss: 0.5753580927848816\n",
      "Epoch: 8740 | Loss: 0.5448997020721436 | Test loss: 0.5753442645072937\n",
      "Epoch: 8750 | Loss: 0.5448879599571228 | Test loss: 0.5753304362297058\n",
      "Epoch: 8760 | Loss: 0.544876217842102 | Test loss: 0.5753165483474731\n",
      "Epoch: 8770 | Loss: 0.5448644161224365 | Test loss: 0.57530277967453\n",
      "Epoch: 8780 | Loss: 0.5448526740074158 | Test loss: 0.5752888917922974\n",
      "Epoch: 8790 | Loss: 0.544840931892395 | Test loss: 0.5752750635147095\n",
      "Epoch: 8800 | Loss: 0.5448291897773743 | Test loss: 0.5752612352371216\n",
      "Epoch: 8810 | Loss: 0.5448173880577087 | Test loss: 0.5752474069595337\n",
      "Epoch: 8820 | Loss: 0.544805645942688 | Test loss: 0.5752334594726562\n",
      "Epoch: 8830 | Loss: 0.5447939038276672 | Test loss: 0.5752196311950684\n",
      "Epoch: 8840 | Loss: 0.5447821021080017 | Test loss: 0.5752058029174805\n",
      "Epoch: 8850 | Loss: 0.544770359992981 | Test loss: 0.5751919746398926\n",
      "Epoch: 8860 | Loss: 0.5447586178779602 | Test loss: 0.5751781463623047\n",
      "Epoch: 8870 | Loss: 0.5447468161582947 | Test loss: 0.575164258480072\n",
      "Epoch: 8880 | Loss: 0.5447351336479187 | Test loss: 0.5751504302024841\n",
      "Epoch: 8890 | Loss: 0.5447232723236084 | Test loss: 0.5751366019248962\n",
      "Epoch: 8900 | Loss: 0.5447115302085876 | Test loss: 0.5751227736473083\n",
      "Epoch: 8910 | Loss: 0.5446997880935669 | Test loss: 0.5751089453697205\n",
      "Epoch: 8920 | Loss: 0.5446879863739014 | Test loss: 0.5750951170921326\n",
      "Epoch: 8930 | Loss: 0.5446763038635254 | Test loss: 0.5750812292098999\n",
      "Epoch: 8940 | Loss: 0.5446645021438599 | Test loss: 0.5750673413276672\n",
      "Epoch: 8950 | Loss: 0.5446527600288391 | Test loss: 0.5750535130500793\n",
      "Epoch: 8960 | Loss: 0.5446409583091736 | Test loss: 0.5750396251678467\n",
      "Epoch: 8970 | Loss: 0.5446292161941528 | Test loss: 0.5750258564949036\n",
      "Epoch: 8980 | Loss: 0.5446174740791321 | Test loss: 0.5750119686126709\n",
      "Epoch: 8990 | Loss: 0.5446057319641113 | Test loss: 0.5749980807304382\n",
      "Epoch: 9000 | Loss: 0.5445939898490906 | Test loss: 0.5749842524528503\n",
      "Epoch: 9010 | Loss: 0.544582188129425 | Test loss: 0.5749704241752625\n",
      "Epoch: 9020 | Loss: 0.5445704460144043 | Test loss: 0.5749565958976746\n",
      "Epoch: 9030 | Loss: 0.5445587038993835 | Test loss: 0.5749427676200867\n",
      "Epoch: 9040 | Loss: 0.544546902179718 | Test loss: 0.574928879737854\n",
      "Epoch: 9050 | Loss: 0.5445351600646973 | Test loss: 0.5749150514602661\n",
      "Epoch: 9060 | Loss: 0.5445233583450317 | Test loss: 0.5749012231826782\n",
      "Epoch: 9070 | Loss: 0.544511616230011 | Test loss: 0.5748873949050903\n",
      "Epoch: 9080 | Loss: 0.5444998741149902 | Test loss: 0.5748735070228577\n",
      "Epoch: 9090 | Loss: 0.5444881319999695 | Test loss: 0.5748596787452698\n",
      "Epoch: 9100 | Loss: 0.544476330280304 | Test loss: 0.5748457908630371\n",
      "Epoch: 9110 | Loss: 0.5444645881652832 | Test loss: 0.5748319625854492\n",
      "Epoch: 9120 | Loss: 0.5444527864456177 | Test loss: 0.5748181343078613\n",
      "Epoch: 9130 | Loss: 0.5444410443305969 | Test loss: 0.5748043060302734\n",
      "Epoch: 9140 | Loss: 0.5444293022155762 | Test loss: 0.5747904181480408\n",
      "Epoch: 9150 | Loss: 0.5444175601005554 | Test loss: 0.5747765898704529\n",
      "Epoch: 9160 | Loss: 0.5444057583808899 | Test loss: 0.574762761592865\n",
      "Epoch: 9170 | Loss: 0.5443940162658691 | Test loss: 0.5747489333152771\n",
      "Epoch: 9180 | Loss: 0.5443822741508484 | Test loss: 0.5747351050376892\n",
      "Epoch: 9190 | Loss: 0.5443704724311829 | Test loss: 0.5747212171554565\n",
      "Epoch: 9200 | Loss: 0.5443587303161621 | Test loss: 0.5747073888778687\n",
      "Epoch: 9210 | Loss: 0.5443469882011414 | Test loss: 0.574693500995636\n",
      "Epoch: 9220 | Loss: 0.5443353056907654 | Test loss: 0.5746796727180481\n",
      "Epoch: 9230 | Loss: 0.5443234443664551 | Test loss: 0.5746658444404602\n",
      "Epoch: 9240 | Loss: 0.5443117022514343 | Test loss: 0.5746519565582275\n",
      "Epoch: 9250 | Loss: 0.5443000197410583 | Test loss: 0.5746381282806396\n",
      "Epoch: 9260 | Loss: 0.544288158416748 | Test loss: 0.5746243000030518\n",
      "Epoch: 9270 | Loss: 0.5442764163017273 | Test loss: 0.5746104121208191\n",
      "Epoch: 9280 | Loss: 0.5442647337913513 | Test loss: 0.5745965838432312\n",
      "Epoch: 9290 | Loss: 0.544252872467041 | Test loss: 0.5745827555656433\n",
      "Epoch: 9300 | Loss: 0.5442411303520203 | Test loss: 0.5745688676834106\n",
      "Epoch: 9310 | Loss: 0.5442293882369995 | Test loss: 0.5745550394058228\n",
      "Epoch: 9320 | Loss: 0.544217586517334 | Test loss: 0.5745412111282349\n",
      "Epoch: 9330 | Loss: 0.5442058444023132 | Test loss: 0.574527382850647\n",
      "Epoch: 9340 | Loss: 0.5441941618919373 | Test loss: 0.5745135545730591\n",
      "Epoch: 9350 | Loss: 0.544182300567627 | Test loss: 0.5744996666908264\n",
      "Epoch: 9360 | Loss: 0.5441705584526062 | Test loss: 0.5744857788085938\n",
      "Epoch: 9370 | Loss: 0.5441588759422302 | Test loss: 0.5744719505310059\n",
      "Epoch: 9380 | Loss: 0.5441470742225647 | Test loss: 0.574458122253418\n",
      "Epoch: 9390 | Loss: 0.5441352725028992 | Test loss: 0.5744442939758301\n",
      "Epoch: 9400 | Loss: 0.5441235899925232 | Test loss: 0.5744304656982422\n",
      "Epoch: 9410 | Loss: 0.5441117286682129 | Test loss: 0.5744165778160095\n",
      "Epoch: 9420 | Loss: 0.5441000461578369 | Test loss: 0.5744027495384216\n",
      "Epoch: 9430 | Loss: 0.5440883040428162 | Test loss: 0.5743889212608337\n",
      "Epoch: 9440 | Loss: 0.5440765619277954 | Test loss: 0.5743750929832458\n",
      "Epoch: 9450 | Loss: 0.5440647602081299 | Test loss: 0.574361264705658\n",
      "Epoch: 9460 | Loss: 0.5440530180931091 | Test loss: 0.5743474364280701\n",
      "Epoch: 9470 | Loss: 0.5440412163734436 | Test loss: 0.5743334889411926\n",
      "Epoch: 9480 | Loss: 0.5440294742584229 | Test loss: 0.5743196606636047\n",
      "Epoch: 9490 | Loss: 0.5440177321434021 | Test loss: 0.5743058323860168\n",
      "Epoch: 9500 | Loss: 0.5440059900283813 | Test loss: 0.5742919445037842\n",
      "Epoch: 9510 | Loss: 0.543994128704071 | Test loss: 0.5742781162261963\n",
      "Epoch: 9520 | Loss: 0.5439824461936951 | Test loss: 0.5742642879486084\n",
      "Epoch: 9530 | Loss: 0.5439707040786743 | Test loss: 0.5742504000663757\n",
      "Epoch: 9540 | Loss: 0.5439589023590088 | Test loss: 0.5742365717887878\n",
      "Epoch: 9550 | Loss: 0.543947160243988 | Test loss: 0.5742227435112\n",
      "Epoch: 9560 | Loss: 0.5439354181289673 | Test loss: 0.5742088556289673\n",
      "Epoch: 9570 | Loss: 0.5439236164093018 | Test loss: 0.5741950869560242\n",
      "Epoch: 9580 | Loss: 0.543911874294281 | Test loss: 0.5741811990737915\n",
      "Epoch: 9590 | Loss: 0.5439001321792603 | Test loss: 0.5741673707962036\n",
      "Epoch: 9600 | Loss: 0.5438883900642395 | Test loss: 0.5741535425186157\n",
      "Epoch: 9610 | Loss: 0.543876588344574 | Test loss: 0.5741397142410278\n",
      "Epoch: 9620 | Loss: 0.5438648462295532 | Test loss: 0.5741257667541504\n",
      "Epoch: 9630 | Loss: 0.5438531041145325 | Test loss: 0.5741119384765625\n",
      "Epoch: 9640 | Loss: 0.5438413023948669 | Test loss: 0.5740981101989746\n",
      "Epoch: 9650 | Loss: 0.5438295602798462 | Test loss: 0.5740842819213867\n",
      "Epoch: 9660 | Loss: 0.5438178181648254 | Test loss: 0.5740704536437988\n",
      "Epoch: 9670 | Loss: 0.5438060164451599 | Test loss: 0.5740565657615662\n",
      "Epoch: 9680 | Loss: 0.5437943339347839 | Test loss: 0.5740427374839783\n",
      "Epoch: 9690 | Loss: 0.5437824726104736 | Test loss: 0.5740289092063904\n",
      "Epoch: 9700 | Loss: 0.5437707304954529 | Test loss: 0.5740150809288025\n",
      "Epoch: 9710 | Loss: 0.5437589883804321 | Test loss: 0.5740012526512146\n",
      "Epoch: 9720 | Loss: 0.5437471866607666 | Test loss: 0.5739874243736267\n",
      "Epoch: 9730 | Loss: 0.5437355041503906 | Test loss: 0.573973536491394\n",
      "Epoch: 9740 | Loss: 0.5437237024307251 | Test loss: 0.5739596486091614\n",
      "Epoch: 9750 | Loss: 0.5437119603157043 | Test loss: 0.5739458203315735\n",
      "Epoch: 9760 | Loss: 0.5437001585960388 | Test loss: 0.5739319324493408\n",
      "Epoch: 9770 | Loss: 0.5436884164810181 | Test loss: 0.5739181637763977\n",
      "Epoch: 9780 | Loss: 0.5436766743659973 | Test loss: 0.573904275894165\n",
      "Epoch: 9790 | Loss: 0.5436649322509766 | Test loss: 0.5738903880119324\n",
      "Epoch: 9800 | Loss: 0.5436531901359558 | Test loss: 0.5738765597343445\n",
      "Epoch: 9810 | Loss: 0.5436413884162903 | Test loss: 0.5738627314567566\n",
      "Epoch: 9820 | Loss: 0.5436296463012695 | Test loss: 0.5738489031791687\n",
      "Epoch: 9830 | Loss: 0.5436179041862488 | Test loss: 0.5738350749015808\n",
      "Epoch: 9840 | Loss: 0.5436061024665833 | Test loss: 0.5738211870193481\n",
      "Epoch: 9850 | Loss: 0.5435943603515625 | Test loss: 0.5738073587417603\n",
      "Epoch: 9860 | Loss: 0.543582558631897 | Test loss: 0.5737935304641724\n",
      "Epoch: 9870 | Loss: 0.5435708165168762 | Test loss: 0.5737797021865845\n",
      "Epoch: 9880 | Loss: 0.5435590744018555 | Test loss: 0.5737658143043518\n",
      "Epoch: 9890 | Loss: 0.5435473322868347 | Test loss: 0.5737519860267639\n",
      "Epoch: 9900 | Loss: 0.5435355305671692 | Test loss: 0.5737380981445312\n",
      "Epoch: 9910 | Loss: 0.5435237884521484 | Test loss: 0.5737242698669434\n",
      "Epoch: 9920 | Loss: 0.5435119867324829 | Test loss: 0.5737104415893555\n",
      "Epoch: 9930 | Loss: 0.5435002446174622 | Test loss: 0.5736966133117676\n",
      "Epoch: 9940 | Loss: 0.5434885025024414 | Test loss: 0.5736827254295349\n",
      "Epoch: 9950 | Loss: 0.5434767603874207 | Test loss: 0.573668897151947\n",
      "Epoch: 9960 | Loss: 0.5434649586677551 | Test loss: 0.5736550688743591\n",
      "Epoch: 9970 | Loss: 0.5434532165527344 | Test loss: 0.5736412405967712\n",
      "Epoch: 9980 | Loss: 0.5434414744377136 | Test loss: 0.5736274123191833\n",
      "Epoch: 9990 | Loss: 0.5434296727180481 | Test loss: 0.5736135244369507\n",
      "Epoch: 10000 | Loss: 0.5434179306030273 | Test loss: 0.5735996961593628\n",
      "Epoch: 10010 | Loss: 0.5434061884880066 | Test loss: 0.5735858082771301\n",
      "Epoch: 10020 | Loss: 0.5433945059776306 | Test loss: 0.5735719799995422\n",
      "Epoch: 10030 | Loss: 0.5433826446533203 | Test loss: 0.5735581517219543\n",
      "Epoch: 10040 | Loss: 0.5433709025382996 | Test loss: 0.5735442638397217\n",
      "Epoch: 10050 | Loss: 0.5433592200279236 | Test loss: 0.5735304355621338\n",
      "Epoch: 10060 | Loss: 0.5433473587036133 | Test loss: 0.5735166072845459\n",
      "Epoch: 10070 | Loss: 0.5433356165885925 | Test loss: 0.5735027194023132\n",
      "Epoch: 10080 | Loss: 0.5433239340782166 | Test loss: 0.5734888911247253\n",
      "Epoch: 10090 | Loss: 0.5433120727539062 | Test loss: 0.5734750628471375\n",
      "Epoch: 10100 | Loss: 0.5433003306388855 | Test loss: 0.5734611749649048\n",
      "Epoch: 10110 | Loss: 0.5432885885238647 | Test loss: 0.5734473466873169\n",
      "Epoch: 10120 | Loss: 0.5432767868041992 | Test loss: 0.573433518409729\n",
      "Epoch: 10130 | Loss: 0.5432650446891785 | Test loss: 0.5734196901321411\n",
      "Epoch: 10140 | Loss: 0.5432533621788025 | Test loss: 0.5734058618545532\n",
      "Epoch: 10150 | Loss: 0.5432415008544922 | Test loss: 0.5733919739723206\n",
      "Epoch: 10160 | Loss: 0.5432297587394714 | Test loss: 0.5733780860900879\n",
      "Epoch: 10170 | Loss: 0.5432180762290955 | Test loss: 0.5733642578125\n",
      "Epoch: 10180 | Loss: 0.5432062745094299 | Test loss: 0.5733504295349121\n",
      "Epoch: 10190 | Loss: 0.5431944727897644 | Test loss: 0.5733366012573242\n",
      "Epoch: 10200 | Loss: 0.5431827902793884 | Test loss: 0.5733227729797363\n",
      "Epoch: 10210 | Loss: 0.5431709289550781 | Test loss: 0.5733088850975037\n",
      "Epoch: 10220 | Loss: 0.5431592464447021 | Test loss: 0.5732950568199158\n",
      "Epoch: 10230 | Loss: 0.5431475043296814 | Test loss: 0.5732812285423279\n",
      "Epoch: 10240 | Loss: 0.5431357622146606 | Test loss: 0.57326740026474\n",
      "Epoch: 10250 | Loss: 0.5431239604949951 | Test loss: 0.5732535719871521\n",
      "Epoch: 10260 | Loss: 0.5431122183799744 | Test loss: 0.5732397437095642\n",
      "Epoch: 10270 | Loss: 0.5431004166603088 | Test loss: 0.5732257962226868\n",
      "Epoch: 10280 | Loss: 0.5430886745452881 | Test loss: 0.5732119679450989\n",
      "Epoch: 10290 | Loss: 0.5430769324302673 | Test loss: 0.573198139667511\n",
      "Epoch: 10300 | Loss: 0.5430651903152466 | Test loss: 0.5731842517852783\n",
      "Epoch: 10310 | Loss: 0.5430533289909363 | Test loss: 0.5731704235076904\n",
      "Epoch: 10320 | Loss: 0.5430416464805603 | Test loss: 0.5731565952301025\n",
      "Epoch: 10330 | Loss: 0.5430299043655396 | Test loss: 0.5731427073478699\n",
      "Epoch: 10340 | Loss: 0.543018102645874 | Test loss: 0.573128879070282\n",
      "Epoch: 10350 | Loss: 0.5430063605308533 | Test loss: 0.5731150507926941\n",
      "Epoch: 10360 | Loss: 0.5429946184158325 | Test loss: 0.5731011629104614\n",
      "Epoch: 10370 | Loss: 0.542982816696167 | Test loss: 0.5730873942375183\n",
      "Epoch: 10380 | Loss: 0.5429710745811462 | Test loss: 0.5730735063552856\n",
      "Epoch: 10390 | Loss: 0.5429593324661255 | Test loss: 0.5730596780776978\n",
      "Epoch: 10400 | Loss: 0.5429475903511047 | Test loss: 0.5730458498001099\n",
      "Epoch: 10410 | Loss: 0.5429357886314392 | Test loss: 0.573032021522522\n",
      "Epoch: 10420 | Loss: 0.5429240465164185 | Test loss: 0.5730180740356445\n",
      "Epoch: 10430 | Loss: 0.5429123044013977 | Test loss: 0.5730042457580566\n",
      "Epoch: 10440 | Loss: 0.5429005026817322 | Test loss: 0.5729904174804688\n",
      "Epoch: 10450 | Loss: 0.5428887605667114 | Test loss: 0.5729765892028809\n",
      "Epoch: 10460 | Loss: 0.5428770184516907 | Test loss: 0.572962760925293\n",
      "Epoch: 10470 | Loss: 0.5428652167320251 | Test loss: 0.5729488730430603\n",
      "Epoch: 10480 | Loss: 0.5428535342216492 | Test loss: 0.5729350447654724\n",
      "Epoch: 10490 | Loss: 0.5428416728973389 | Test loss: 0.5729212164878845\n",
      "Epoch: 10500 | Loss: 0.5428299307823181 | Test loss: 0.5729073882102966\n",
      "Epoch: 10510 | Loss: 0.5428181886672974 | Test loss: 0.5728935599327087\n",
      "Epoch: 10520 | Loss: 0.5428063869476318 | Test loss: 0.5728797316551208\n",
      "Epoch: 10530 | Loss: 0.5427947044372559 | Test loss: 0.5728658437728882\n",
      "Epoch: 10540 | Loss: 0.5427829027175903 | Test loss: 0.5728519558906555\n",
      "Epoch: 10550 | Loss: 0.5427711606025696 | Test loss: 0.5728381276130676\n",
      "Epoch: 10560 | Loss: 0.542759358882904 | Test loss: 0.572824239730835\n",
      "Epoch: 10570 | Loss: 0.5427476167678833 | Test loss: 0.5728104710578918\n",
      "Epoch: 10580 | Loss: 0.5427358746528625 | Test loss: 0.5727965831756592\n",
      "Epoch: 10590 | Loss: 0.5427241325378418 | Test loss: 0.5727826952934265\n",
      "Epoch: 10600 | Loss: 0.542712390422821 | Test loss: 0.5727688670158386\n",
      "Epoch: 10610 | Loss: 0.5427005887031555 | Test loss: 0.5727550387382507\n",
      "Epoch: 10620 | Loss: 0.5426888465881348 | Test loss: 0.5727412104606628\n",
      "Epoch: 10630 | Loss: 0.542677104473114 | Test loss: 0.572727382183075\n",
      "Epoch: 10640 | Loss: 0.5426653027534485 | Test loss: 0.5727134943008423\n",
      "Epoch: 10650 | Loss: 0.5426535606384277 | Test loss: 0.5726996660232544\n",
      "Epoch: 10660 | Loss: 0.5426417589187622 | Test loss: 0.5726858377456665\n",
      "Epoch: 10670 | Loss: 0.5426300168037415 | Test loss: 0.5726720094680786\n",
      "Epoch: 10680 | Loss: 0.5426182746887207 | Test loss: 0.572658121585846\n",
      "Epoch: 10690 | Loss: 0.5426065325737 | Test loss: 0.5726442933082581\n",
      "Epoch: 10700 | Loss: 0.5425947308540344 | Test loss: 0.5726304054260254\n",
      "Epoch: 10710 | Loss: 0.5425829887390137 | Test loss: 0.5726165771484375\n",
      "Epoch: 10720 | Loss: 0.5425711870193481 | Test loss: 0.5726027488708496\n",
      "Epoch: 10730 | Loss: 0.5425594449043274 | Test loss: 0.5725889205932617\n",
      "Epoch: 10740 | Loss: 0.5425477027893066 | Test loss: 0.572575032711029\n",
      "Epoch: 10750 | Loss: 0.5425359606742859 | Test loss: 0.5725612044334412\n",
      "Epoch: 10760 | Loss: 0.5425241589546204 | Test loss: 0.5725473761558533\n",
      "Epoch: 10770 | Loss: 0.5425124168395996 | Test loss: 0.5725335478782654\n",
      "Epoch: 10780 | Loss: 0.5425006747245789 | Test loss: 0.5725197196006775\n",
      "Epoch: 10790 | Loss: 0.5424888730049133 | Test loss: 0.5725058317184448\n",
      "Epoch: 10800 | Loss: 0.5424771308898926 | Test loss: 0.5724920034408569\n",
      "Epoch: 10810 | Loss: 0.5424653887748718 | Test loss: 0.5724781155586243\n",
      "Epoch: 10820 | Loss: 0.5424537062644958 | Test loss: 0.5724642872810364\n",
      "Epoch: 10830 | Loss: 0.5424418449401855 | Test loss: 0.5724504590034485\n",
      "Epoch: 10840 | Loss: 0.5424301028251648 | Test loss: 0.5724365711212158\n",
      "Epoch: 10850 | Loss: 0.5424184203147888 | Test loss: 0.5724227428436279\n",
      "Epoch: 10860 | Loss: 0.5424065589904785 | Test loss: 0.57240891456604\n",
      "Epoch: 10870 | Loss: 0.5423948168754578 | Test loss: 0.5723950266838074\n",
      "Epoch: 10880 | Loss: 0.5423831343650818 | Test loss: 0.5723811984062195\n",
      "Epoch: 10890 | Loss: 0.5423712730407715 | Test loss: 0.5723673701286316\n",
      "Epoch: 10900 | Loss: 0.5423595309257507 | Test loss: 0.5723534822463989\n",
      "Epoch: 10910 | Loss: 0.54234778881073 | Test loss: 0.572339653968811\n",
      "Epoch: 10920 | Loss: 0.5423359870910645 | Test loss: 0.5723258256912231\n",
      "Epoch: 10930 | Loss: 0.5423242449760437 | Test loss: 0.5723119974136353\n",
      "Epoch: 10940 | Loss: 0.5423125624656677 | Test loss: 0.5722981691360474\n",
      "Epoch: 10950 | Loss: 0.5423007011413574 | Test loss: 0.5722842812538147\n",
      "Epoch: 10960 | Loss: 0.5422889590263367 | Test loss: 0.572270393371582\n",
      "Epoch: 10970 | Loss: 0.5422772765159607 | Test loss: 0.5722565650939941\n",
      "Epoch: 10980 | Loss: 0.5422654747962952 | Test loss: 0.5722427368164062\n",
      "Epoch: 10990 | Loss: 0.5422536730766296 | Test loss: 0.5722289085388184\n",
      "Epoch: 11000 | Loss: 0.5422419905662537 | Test loss: 0.5722150802612305\n",
      "Epoch: 11010 | Loss: 0.5422301292419434 | Test loss: 0.5722011923789978\n",
      "Epoch: 11020 | Loss: 0.5422184467315674 | Test loss: 0.5721873641014099\n",
      "Epoch: 11030 | Loss: 0.5422067046165466 | Test loss: 0.572173535823822\n",
      "Epoch: 11040 | Loss: 0.5421949625015259 | Test loss: 0.5721597075462341\n",
      "Epoch: 11050 | Loss: 0.5421831607818604 | Test loss: 0.5721458792686462\n",
      "Epoch: 11060 | Loss: 0.5421714186668396 | Test loss: 0.5721320509910583\n",
      "Epoch: 11070 | Loss: 0.5421596169471741 | Test loss: 0.5721181035041809\n",
      "Epoch: 11080 | Loss: 0.5421478748321533 | Test loss: 0.572104275226593\n",
      "Epoch: 11090 | Loss: 0.5421361327171326 | Test loss: 0.5720904469490051\n",
      "Epoch: 11100 | Loss: 0.5421243906021118 | Test loss: 0.5720765590667725\n",
      "Epoch: 11110 | Loss: 0.5421125292778015 | Test loss: 0.5720627307891846\n",
      "Epoch: 11120 | Loss: 0.5421008467674255 | Test loss: 0.5720489025115967\n",
      "Epoch: 11130 | Loss: 0.5420891046524048 | Test loss: 0.572035014629364\n",
      "Epoch: 11140 | Loss: 0.5420773029327393 | Test loss: 0.5720211863517761\n",
      "Epoch: 11150 | Loss: 0.5420655608177185 | Test loss: 0.5720073580741882\n",
      "Epoch: 11160 | Loss: 0.5420538187026978 | Test loss: 0.5719934701919556\n",
      "Epoch: 11170 | Loss: 0.5420420169830322 | Test loss: 0.5719797015190125\n",
      "Epoch: 11180 | Loss: 0.5420302748680115 | Test loss: 0.5719658136367798\n",
      "Epoch: 11190 | Loss: 0.5420185327529907 | Test loss: 0.5719519853591919\n",
      "Epoch: 11200 | Loss: 0.54200679063797 | Test loss: 0.571938157081604\n",
      "Epoch: 11210 | Loss: 0.5419949889183044 | Test loss: 0.5719243288040161\n",
      "Epoch: 11220 | Loss: 0.5419832468032837 | Test loss: 0.5719103813171387\n",
      "Epoch: 11230 | Loss: 0.5419715046882629 | Test loss: 0.5718965530395508\n",
      "Epoch: 11240 | Loss: 0.5419597029685974 | Test loss: 0.5718827247619629\n",
      "Epoch: 11250 | Loss: 0.5419479608535767 | Test loss: 0.571868896484375\n",
      "Epoch: 11260 | Loss: 0.5419362187385559 | Test loss: 0.5718550682067871\n",
      "Epoch: 11270 | Loss: 0.5419244170188904 | Test loss: 0.5718411803245544\n",
      "Epoch: 11280 | Loss: 0.5419127345085144 | Test loss: 0.5718273520469666\n",
      "Epoch: 11290 | Loss: 0.5419008731842041 | Test loss: 0.5718135237693787\n",
      "Epoch: 11300 | Loss: 0.5418891310691833 | Test loss: 0.5717996954917908\n",
      "Epoch: 11310 | Loss: 0.5418773889541626 | Test loss: 0.5717858672142029\n",
      "Epoch: 11320 | Loss: 0.5418655872344971 | Test loss: 0.571772038936615\n",
      "Epoch: 11330 | Loss: 0.5418539047241211 | Test loss: 0.5717581510543823\n",
      "Epoch: 11340 | Loss: 0.5418421030044556 | Test loss: 0.5717442631721497\n",
      "Epoch: 11350 | Loss: 0.5418303608894348 | Test loss: 0.5717304348945618\n",
      "Epoch: 11360 | Loss: 0.5418185591697693 | Test loss: 0.5717165470123291\n",
      "Epoch: 11370 | Loss: 0.5418068170547485 | Test loss: 0.571702778339386\n",
      "Epoch: 11380 | Loss: 0.5417950749397278 | Test loss: 0.5716888904571533\n",
      "Epoch: 11390 | Loss: 0.541783332824707 | Test loss: 0.5716750025749207\n",
      "Epoch: 11400 | Loss: 0.5417715907096863 | Test loss: 0.5716611742973328\n",
      "Epoch: 11410 | Loss: 0.5417597889900208 | Test loss: 0.5716473460197449\n",
      "Epoch: 11420 | Loss: 0.541748046875 | Test loss: 0.571633517742157\n",
      "Epoch: 11430 | Loss: 0.5417363047599792 | Test loss: 0.5716196894645691\n",
      "Epoch: 11440 | Loss: 0.5417245030403137 | Test loss: 0.5716058015823364\n",
      "Epoch: 11450 | Loss: 0.541712760925293 | Test loss: 0.5715919733047485\n",
      "Epoch: 11460 | Loss: 0.5417009592056274 | Test loss: 0.5715781450271606\n",
      "Epoch: 11470 | Loss: 0.5416892170906067 | Test loss: 0.5715643167495728\n",
      "Epoch: 11480 | Loss: 0.5416774749755859 | Test loss: 0.5715504288673401\n",
      "Epoch: 11490 | Loss: 0.5416657328605652 | Test loss: 0.5715366005897522\n",
      "Epoch: 11500 | Loss: 0.5416539311408997 | Test loss: 0.5715227127075195\n",
      "Epoch: 11510 | Loss: 0.5416421890258789 | Test loss: 0.5715088844299316\n",
      "Epoch: 11520 | Loss: 0.5416303873062134 | Test loss: 0.5714950561523438\n",
      "Epoch: 11530 | Loss: 0.5416186451911926 | Test loss: 0.5714812278747559\n",
      "Epoch: 11540 | Loss: 0.5416069030761719 | Test loss: 0.5714673399925232\n",
      "Epoch: 11550 | Loss: 0.5415951609611511 | Test loss: 0.5714535117149353\n",
      "Epoch: 11560 | Loss: 0.5415833592414856 | Test loss: 0.5714396834373474\n",
      "Epoch: 11570 | Loss: 0.5415716171264648 | Test loss: 0.5714258551597595\n",
      "Epoch: 11580 | Loss: 0.5415598750114441 | Test loss: 0.5714120268821716\n",
      "Epoch: 11590 | Loss: 0.5415480732917786 | Test loss: 0.571398138999939\n",
      "Epoch: 11600 | Loss: 0.5415363311767578 | Test loss: 0.5713843107223511\n",
      "Epoch: 11610 | Loss: 0.5415245890617371 | Test loss: 0.5713704228401184\n",
      "Epoch: 11620 | Loss: 0.5415129065513611 | Test loss: 0.5713565945625305\n",
      "Epoch: 11630 | Loss: 0.5415010452270508 | Test loss: 0.5713427662849426\n",
      "Epoch: 11640 | Loss: 0.54148930311203 | Test loss: 0.57132887840271\n",
      "Epoch: 11650 | Loss: 0.541477620601654 | Test loss: 0.5713150501251221\n",
      "Epoch: 11660 | Loss: 0.5414657592773438 | Test loss: 0.5713012218475342\n",
      "Epoch: 11670 | Loss: 0.541454017162323 | Test loss: 0.5712873339653015\n",
      "Epoch: 11680 | Loss: 0.541442334651947 | Test loss: 0.5712735056877136\n",
      "Epoch: 11690 | Loss: 0.5414304733276367 | Test loss: 0.5712596774101257\n",
      "Epoch: 11700 | Loss: 0.541418731212616 | Test loss: 0.5712457895278931\n",
      "Epoch: 11710 | Loss: 0.5414069890975952 | Test loss: 0.5712319612503052\n",
      "Epoch: 11720 | Loss: 0.5413951873779297 | Test loss: 0.5712181329727173\n",
      "Epoch: 11730 | Loss: 0.5413834452629089 | Test loss: 0.5712043046951294\n",
      "Epoch: 11740 | Loss: 0.541371762752533 | Test loss: 0.5711904764175415\n",
      "Epoch: 11750 | Loss: 0.5413599014282227 | Test loss: 0.5711765885353088\n",
      "Epoch: 11760 | Loss: 0.5413481593132019 | Test loss: 0.5711627006530762\n",
      "Epoch: 11770 | Loss: 0.5413364768028259 | Test loss: 0.5711488723754883\n",
      "Epoch: 11780 | Loss: 0.5413246750831604 | Test loss: 0.5711350440979004\n",
      "Epoch: 11790 | Loss: 0.5413128733634949 | Test loss: 0.5711212158203125\n",
      "Epoch: 11800 | Loss: 0.5413011908531189 | Test loss: 0.5711073875427246\n",
      "Epoch: 11810 | Loss: 0.5412893295288086 | Test loss: 0.5710934996604919\n",
      "Epoch: 11820 | Loss: 0.5412776470184326 | Test loss: 0.571079671382904\n",
      "Epoch: 11830 | Loss: 0.5412659049034119 | Test loss: 0.5710658431053162\n",
      "Epoch: 11840 | Loss: 0.5412541627883911 | Test loss: 0.5710520148277283\n",
      "Epoch: 11850 | Loss: 0.5412423610687256 | Test loss: 0.5710381865501404\n",
      "Epoch: 11860 | Loss: 0.5412306189537048 | Test loss: 0.5710243582725525\n",
      "Epoch: 11870 | Loss: 0.5412188172340393 | Test loss: 0.571010410785675\n",
      "Epoch: 11880 | Loss: 0.5412070751190186 | Test loss: 0.5709965825080872\n",
      "Epoch: 11890 | Loss: 0.5411953330039978 | Test loss: 0.5709827542304993\n",
      "Epoch: 11900 | Loss: 0.541183590888977 | Test loss: 0.5709688663482666\n",
      "Epoch: 11910 | Loss: 0.5411717295646667 | Test loss: 0.5709550380706787\n",
      "Epoch: 11920 | Loss: 0.5411600470542908 | Test loss: 0.5709412097930908\n",
      "Epoch: 11930 | Loss: 0.54114830493927 | Test loss: 0.5709273219108582\n",
      "Epoch: 11940 | Loss: 0.5411365032196045 | Test loss: 0.5709134936332703\n",
      "Epoch: 11950 | Loss: 0.5411247611045837 | Test loss: 0.5708996653556824\n",
      "Epoch: 11960 | Loss: 0.541113018989563 | Test loss: 0.5708857774734497\n",
      "Epoch: 11970 | Loss: 0.5411012172698975 | Test loss: 0.5708720088005066\n",
      "Epoch: 11980 | Loss: 0.5410894751548767 | Test loss: 0.5708581209182739\n",
      "Epoch: 11990 | Loss: 0.541077733039856 | Test loss: 0.570844292640686\n",
      "Epoch: 12000 | Loss: 0.5410659909248352 | Test loss: 0.5708304643630981\n",
      "Epoch: 12010 | Loss: 0.5410541892051697 | Test loss: 0.5708166360855103\n",
      "Epoch: 12020 | Loss: 0.5410424470901489 | Test loss: 0.5708026885986328\n",
      "Epoch: 12030 | Loss: 0.5410307049751282 | Test loss: 0.5707888603210449\n",
      "Epoch: 12040 | Loss: 0.5410189032554626 | Test loss: 0.570775032043457\n",
      "Epoch: 12050 | Loss: 0.5410071611404419 | Test loss: 0.5707612037658691\n",
      "Epoch: 12060 | Loss: 0.5409954190254211 | Test loss: 0.5707473754882812\n",
      "Epoch: 12070 | Loss: 0.5409836173057556 | Test loss: 0.5707334876060486\n",
      "Epoch: 12080 | Loss: 0.5409719347953796 | Test loss: 0.5707196593284607\n",
      "Epoch: 12090 | Loss: 0.5409600734710693 | Test loss: 0.5707058310508728\n",
      "Epoch: 12100 | Loss: 0.5409483313560486 | Test loss: 0.5706920027732849\n",
      "Epoch: 12110 | Loss: 0.5409365892410278 | Test loss: 0.570678174495697\n",
      "Epoch: 12120 | Loss: 0.5409247875213623 | Test loss: 0.5706643462181091\n",
      "Epoch: 12130 | Loss: 0.5409131050109863 | Test loss: 0.5706504583358765\n",
      "Epoch: 12140 | Loss: 0.5409013032913208 | Test loss: 0.5706365704536438\n",
      "Epoch: 12150 | Loss: 0.5408895611763 | Test loss: 0.5706227421760559\n",
      "Epoch: 12160 | Loss: 0.5408777594566345 | Test loss: 0.5706088542938232\n",
      "Epoch: 12170 | Loss: 0.5408660173416138 | Test loss: 0.5705950856208801\n",
      "Epoch: 12180 | Loss: 0.540854275226593 | Test loss: 0.5705811977386475\n",
      "Epoch: 12190 | Loss: 0.5408424735069275 | Test loss: 0.5705673098564148\n",
      "Epoch: 12200 | Loss: 0.5408307909965515 | Test loss: 0.5705534815788269\n",
      "Epoch: 12210 | Loss: 0.540818989276886 | Test loss: 0.570539653301239\n",
      "Epoch: 12220 | Loss: 0.5408072471618652 | Test loss: 0.5705258250236511\n",
      "Epoch: 12230 | Loss: 0.5407955050468445 | Test loss: 0.5705119967460632\n",
      "Epoch: 12240 | Loss: 0.540783703327179 | Test loss: 0.5704981088638306\n",
      "Epoch: 12250 | Loss: 0.5407719612121582 | Test loss: 0.5704842805862427\n",
      "Epoch: 12260 | Loss: 0.5407601594924927 | Test loss: 0.5704704523086548\n",
      "Epoch: 12270 | Loss: 0.5407484173774719 | Test loss: 0.5704566240310669\n",
      "Epoch: 12280 | Loss: 0.5407366752624512 | Test loss: 0.5704427361488342\n",
      "Epoch: 12290 | Loss: 0.5407249331474304 | Test loss: 0.5704289078712463\n",
      "Epoch: 12300 | Loss: 0.5407131314277649 | Test loss: 0.5704150199890137\n",
      "Epoch: 12310 | Loss: 0.5407013893127441 | Test loss: 0.5704011917114258\n",
      "Epoch: 12320 | Loss: 0.5406895875930786 | Test loss: 0.5703873634338379\n",
      "Epoch: 12330 | Loss: 0.5406778454780579 | Test loss: 0.57037353515625\n",
      "Epoch: 12340 | Loss: 0.5406661033630371 | Test loss: 0.5703596472740173\n",
      "Epoch: 12350 | Loss: 0.5406543612480164 | Test loss: 0.5703458189964294\n",
      "Epoch: 12360 | Loss: 0.5406425595283508 | Test loss: 0.5703319907188416\n",
      "Epoch: 12370 | Loss: 0.5406308174133301 | Test loss: 0.5703181624412537\n",
      "Epoch: 12380 | Loss: 0.5406190752983093 | Test loss: 0.5703043341636658\n",
      "Epoch: 12390 | Loss: 0.5406072735786438 | Test loss: 0.5702904462814331\n",
      "Epoch: 12400 | Loss: 0.540595531463623 | Test loss: 0.5702766180038452\n",
      "Epoch: 12410 | Loss: 0.5405837893486023 | Test loss: 0.5702627301216125\n",
      "Epoch: 12420 | Loss: 0.5405721068382263 | Test loss: 0.5702489018440247\n",
      "Epoch: 12430 | Loss: 0.540560245513916 | Test loss: 0.5702350735664368\n",
      "Epoch: 12440 | Loss: 0.5405485033988953 | Test loss: 0.5702211856842041\n",
      "Epoch: 12450 | Loss: 0.5405368208885193 | Test loss: 0.5702073574066162\n",
      "Epoch: 12460 | Loss: 0.540524959564209 | Test loss: 0.5701935291290283\n",
      "Epoch: 12470 | Loss: 0.5405132174491882 | Test loss: 0.5701796412467957\n",
      "Epoch: 12480 | Loss: 0.5405015349388123 | Test loss: 0.5701658129692078\n",
      "Epoch: 12490 | Loss: 0.540489673614502 | Test loss: 0.5701519846916199\n",
      "Epoch: 12500 | Loss: 0.5404779314994812 | Test loss: 0.5701380968093872\n",
      "Epoch: 12510 | Loss: 0.5404661893844604 | Test loss: 0.5701242685317993\n",
      "Epoch: 12520 | Loss: 0.5404543876647949 | Test loss: 0.5701104402542114\n",
      "Epoch: 12530 | Loss: 0.5404426455497742 | Test loss: 0.5700966119766235\n",
      "Epoch: 12540 | Loss: 0.5404309630393982 | Test loss: 0.5700827836990356\n",
      "Epoch: 12550 | Loss: 0.5404191017150879 | Test loss: 0.570068895816803\n",
      "Epoch: 12560 | Loss: 0.5404073596000671 | Test loss: 0.5700550079345703\n",
      "Epoch: 12570 | Loss: 0.5403956770896912 | Test loss: 0.5700411796569824\n",
      "Epoch: 12580 | Loss: 0.5403838753700256 | Test loss: 0.5700273513793945\n",
      "Epoch: 12590 | Loss: 0.5403720736503601 | Test loss: 0.5700135231018066\n",
      "Epoch: 12600 | Loss: 0.5403603911399841 | Test loss: 0.5699996948242188\n",
      "Epoch: 12610 | Loss: 0.5403485298156738 | Test loss: 0.5699858069419861\n",
      "Epoch: 12620 | Loss: 0.5403368473052979 | Test loss: 0.5699719786643982\n",
      "Epoch: 12630 | Loss: 0.5403251051902771 | Test loss: 0.5699581503868103\n",
      "Epoch: 12640 | Loss: 0.5403133630752563 | Test loss: 0.5699443221092224\n",
      "Epoch: 12650 | Loss: 0.5403015613555908 | Test loss: 0.5699304938316345\n",
      "Epoch: 12660 | Loss: 0.5402898192405701 | Test loss: 0.5699166655540466\n",
      "Epoch: 12670 | Loss: 0.5402780175209045 | Test loss: 0.5699027180671692\n",
      "Epoch: 12680 | Loss: 0.5402662754058838 | Test loss: 0.5698888897895813\n",
      "Epoch: 12690 | Loss: 0.540254533290863 | Test loss: 0.5698750615119934\n",
      "Epoch: 12700 | Loss: 0.5402427911758423 | Test loss: 0.5698611736297607\n",
      "Epoch: 12710 | Loss: 0.540230929851532 | Test loss: 0.5698473453521729\n",
      "Epoch: 12720 | Loss: 0.540219247341156 | Test loss: 0.569833517074585\n",
      "Epoch: 12730 | Loss: 0.5402075052261353 | Test loss: 0.5698196291923523\n",
      "Epoch: 12740 | Loss: 0.5401957035064697 | Test loss: 0.5698058009147644\n",
      "Epoch: 12750 | Loss: 0.540183961391449 | Test loss: 0.5697919726371765\n",
      "Epoch: 12760 | Loss: 0.5401722192764282 | Test loss: 0.5697780847549438\n",
      "Epoch: 12770 | Loss: 0.5401604175567627 | Test loss: 0.5697643160820007\n",
      "Epoch: 12780 | Loss: 0.5401486754417419 | Test loss: 0.5697504281997681\n",
      "Epoch: 12790 | Loss: 0.5401369333267212 | Test loss: 0.5697365999221802\n",
      "Epoch: 12800 | Loss: 0.5401251912117004 | Test loss: 0.5697227716445923\n",
      "Epoch: 12810 | Loss: 0.5401133894920349 | Test loss: 0.5697089433670044\n",
      "Epoch: 12820 | Loss: 0.5401016473770142 | Test loss: 0.569694995880127\n",
      "Epoch: 12830 | Loss: 0.5400899052619934 | Test loss: 0.5696811676025391\n",
      "Epoch: 12840 | Loss: 0.5400781035423279 | Test loss: 0.5696673393249512\n",
      "Epoch: 12850 | Loss: 0.5400663614273071 | Test loss: 0.5696535110473633\n",
      "Epoch: 12860 | Loss: 0.5400546193122864 | Test loss: 0.5696396827697754\n",
      "Epoch: 12870 | Loss: 0.5400428175926208 | Test loss: 0.5696257948875427\n",
      "Epoch: 12880 | Loss: 0.5400311350822449 | Test loss: 0.5696119666099548\n",
      "Epoch: 12890 | Loss: 0.5400192737579346 | Test loss: 0.5695981383323669\n",
      "Epoch: 12900 | Loss: 0.5400075316429138 | Test loss: 0.569584310054779\n",
      "Epoch: 12910 | Loss: 0.5399957895278931 | Test loss: 0.5695704817771912\n",
      "Epoch: 12920 | Loss: 0.5399839878082275 | Test loss: 0.5695566534996033\n",
      "Epoch: 12930 | Loss: 0.5399723052978516 | Test loss: 0.5695427656173706\n",
      "Epoch: 12940 | Loss: 0.539960503578186 | Test loss: 0.5695288777351379\n",
      "Epoch: 12950 | Loss: 0.5399487614631653 | Test loss: 0.56951504945755\n",
      "Epoch: 12960 | Loss: 0.5399369597434998 | Test loss: 0.5695011615753174\n",
      "Epoch: 12970 | Loss: 0.539925217628479 | Test loss: 0.5694873929023743\n",
      "Epoch: 12980 | Loss: 0.5399134755134583 | Test loss: 0.5694735050201416\n",
      "Epoch: 12990 | Loss: 0.5399016737937927 | Test loss: 0.5694596171379089\n",
      "Epoch: 13000 | Loss: 0.5398899912834167 | Test loss: 0.569445788860321\n",
      "Epoch: 13010 | Loss: 0.5398781895637512 | Test loss: 0.5694319605827332\n",
      "Epoch: 13020 | Loss: 0.5398664474487305 | Test loss: 0.5694181323051453\n",
      "Epoch: 13030 | Loss: 0.5398547053337097 | Test loss: 0.5694043040275574\n",
      "Epoch: 13040 | Loss: 0.5398429036140442 | Test loss: 0.5693904161453247\n",
      "Epoch: 13050 | Loss: 0.5398311614990234 | Test loss: 0.5693765878677368\n",
      "Epoch: 13060 | Loss: 0.5398193597793579 | Test loss: 0.5693627595901489\n",
      "Epoch: 13070 | Loss: 0.5398076176643372 | Test loss: 0.569348931312561\n",
      "Epoch: 13080 | Loss: 0.5397958755493164 | Test loss: 0.5693350434303284\n",
      "Epoch: 13090 | Loss: 0.5397841334342957 | Test loss: 0.5693212151527405\n",
      "Epoch: 13100 | Loss: 0.5397723317146301 | Test loss: 0.5693073272705078\n",
      "Epoch: 13110 | Loss: 0.5397605895996094 | Test loss: 0.5692934989929199\n",
      "Epoch: 13120 | Loss: 0.5397487878799438 | Test loss: 0.569279670715332\n",
      "Epoch: 13130 | Loss: 0.5397370457649231 | Test loss: 0.5692658424377441\n",
      "Epoch: 13140 | Loss: 0.5397253036499023 | Test loss: 0.5692519545555115\n",
      "Epoch: 13150 | Loss: 0.5397135615348816 | Test loss: 0.5692381262779236\n",
      "Epoch: 13160 | Loss: 0.5397017598152161 | Test loss: 0.5692242980003357\n",
      "Epoch: 13170 | Loss: 0.5396900177001953 | Test loss: 0.5692104697227478\n",
      "Epoch: 13180 | Loss: 0.5396782755851746 | Test loss: 0.5691966414451599\n",
      "Epoch: 13190 | Loss: 0.539666473865509 | Test loss: 0.5691827535629272\n",
      "Epoch: 13200 | Loss: 0.5396547317504883 | Test loss: 0.5691689252853394\n",
      "Epoch: 13210 | Loss: 0.5396429896354675 | Test loss: 0.5691550374031067\n",
      "Epoch: 13220 | Loss: 0.5396313071250916 | Test loss: 0.5691412091255188\n",
      "Epoch: 13230 | Loss: 0.5396194458007812 | Test loss: 0.5691273808479309\n",
      "Epoch: 13240 | Loss: 0.5396077036857605 | Test loss: 0.5691134929656982\n",
      "Epoch: 13250 | Loss: 0.5395960211753845 | Test loss: 0.5690996646881104\n",
      "Epoch: 13260 | Loss: 0.5395841598510742 | Test loss: 0.5690858364105225\n",
      "Epoch: 13270 | Loss: 0.5395724177360535 | Test loss: 0.5690719485282898\n",
      "Epoch: 13280 | Loss: 0.5395607352256775 | Test loss: 0.5690581202507019\n",
      "Epoch: 13290 | Loss: 0.5395488739013672 | Test loss: 0.569044291973114\n",
      "Epoch: 13300 | Loss: 0.5395371317863464 | Test loss: 0.5690304040908813\n",
      "Epoch: 13310 | Loss: 0.5395253896713257 | Test loss: 0.5690165758132935\n",
      "Epoch: 13320 | Loss: 0.5395135879516602 | Test loss: 0.5690027475357056\n",
      "Epoch: 13330 | Loss: 0.5395018458366394 | Test loss: 0.5689889192581177\n",
      "Epoch: 13340 | Loss: 0.5394901633262634 | Test loss: 0.5689750909805298\n",
      "Epoch: 13350 | Loss: 0.5394783020019531 | Test loss: 0.5689612030982971\n",
      "Epoch: 13360 | Loss: 0.5394665598869324 | Test loss: 0.5689473152160645\n",
      "Epoch: 13370 | Loss: 0.5394548773765564 | Test loss: 0.5689334869384766\n",
      "Epoch: 13380 | Loss: 0.5394430756568909 | Test loss: 0.5689196586608887\n",
      "Epoch: 13390 | Loss: 0.5394312739372253 | Test loss: 0.5689058303833008\n",
      "Epoch: 13400 | Loss: 0.5394195914268494 | Test loss: 0.5688920021057129\n",
      "Epoch: 13410 | Loss: 0.5394077301025391 | Test loss: 0.5688781142234802\n",
      "Epoch: 13420 | Loss: 0.5393960475921631 | Test loss: 0.5688642859458923\n",
      "Epoch: 13430 | Loss: 0.5393843054771423 | Test loss: 0.5688504576683044\n",
      "Epoch: 13440 | Loss: 0.5393725633621216 | Test loss: 0.5688366293907166\n",
      "Epoch: 13450 | Loss: 0.539360761642456 | Test loss: 0.5688228011131287\n",
      "Epoch: 13460 | Loss: 0.5393490195274353 | Test loss: 0.5688089728355408\n",
      "Epoch: 13470 | Loss: 0.5393372178077698 | Test loss: 0.5687950253486633\n",
      "Epoch: 13480 | Loss: 0.539325475692749 | Test loss: 0.5687811970710754\n",
      "Epoch: 13490 | Loss: 0.5393137335777283 | Test loss: 0.5687673687934875\n",
      "Epoch: 13500 | Loss: 0.5393019914627075 | Test loss: 0.5687534809112549\n",
      "Epoch: 13510 | Loss: 0.5392901301383972 | Test loss: 0.568739652633667\n",
      "Epoch: 13520 | Loss: 0.5392784476280212 | Test loss: 0.5687258243560791\n",
      "Epoch: 13530 | Loss: 0.5392667055130005 | Test loss: 0.5687119364738464\n",
      "Epoch: 13540 | Loss: 0.539254903793335 | Test loss: 0.5686981081962585\n",
      "Epoch: 13550 | Loss: 0.5392431616783142 | Test loss: 0.5686842799186707\n",
      "Epoch: 13560 | Loss: 0.5392314195632935 | Test loss: 0.568670392036438\n",
      "Epoch: 13570 | Loss: 0.5392196178436279 | Test loss: 0.5686566233634949\n",
      "Epoch: 13580 | Loss: 0.5392078757286072 | Test loss: 0.5686427354812622\n",
      "Epoch: 13590 | Loss: 0.5391961336135864 | Test loss: 0.5686289072036743\n",
      "Epoch: 13600 | Loss: 0.5391843914985657 | Test loss: 0.5686150789260864\n",
      "Epoch: 13610 | Loss: 0.5391725897789001 | Test loss: 0.5686012506484985\n",
      "Epoch: 13620 | Loss: 0.5391608476638794 | Test loss: 0.5685873031616211\n",
      "Epoch: 13630 | Loss: 0.5391491055488586 | Test loss: 0.5685734748840332\n",
      "Epoch: 13640 | Loss: 0.5391373038291931 | Test loss: 0.5685596466064453\n",
      "Epoch: 13650 | Loss: 0.5391255617141724 | Test loss: 0.5685458183288574\n",
      "Epoch: 13660 | Loss: 0.5391138195991516 | Test loss: 0.5685319900512695\n",
      "Epoch: 13670 | Loss: 0.5391020178794861 | Test loss: 0.5685181021690369\n",
      "Epoch: 13680 | Loss: 0.5390903353691101 | Test loss: 0.568504273891449\n",
      "Epoch: 13690 | Loss: 0.5390784740447998 | Test loss: 0.5684904456138611\n",
      "Epoch: 13700 | Loss: 0.539066731929779 | Test loss: 0.5684766173362732\n",
      "Epoch: 13710 | Loss: 0.5390549898147583 | Test loss: 0.5684627890586853\n",
      "Epoch: 13720 | Loss: 0.5390431880950928 | Test loss: 0.5684489607810974\n",
      "Epoch: 13730 | Loss: 0.5390315055847168 | Test loss: 0.5684350728988647\n",
      "Epoch: 13740 | Loss: 0.5390197038650513 | Test loss: 0.5684211850166321\n",
      "Epoch: 13750 | Loss: 0.5390079617500305 | Test loss: 0.5684073567390442\n",
      "Epoch: 13760 | Loss: 0.538996160030365 | Test loss: 0.5683934688568115\n",
      "Epoch: 13770 | Loss: 0.5389844179153442 | Test loss: 0.5683797001838684\n",
      "Epoch: 13780 | Loss: 0.5389726758003235 | Test loss: 0.5683658123016357\n",
      "Epoch: 13790 | Loss: 0.538960874080658 | Test loss: 0.5683519244194031\n",
      "Epoch: 13800 | Loss: 0.538949191570282 | Test loss: 0.5683380961418152\n",
      "Epoch: 13810 | Loss: 0.5389373898506165 | Test loss: 0.5683242678642273\n",
      "Epoch: 13820 | Loss: 0.5389256477355957 | Test loss: 0.5683104395866394\n",
      "Epoch: 13830 | Loss: 0.538913905620575 | Test loss: 0.5682966113090515\n",
      "Epoch: 13840 | Loss: 0.5389021039009094 | Test loss: 0.5682827234268188\n",
      "Epoch: 13850 | Loss: 0.5388903617858887 | Test loss: 0.568268895149231\n",
      "Epoch: 13860 | Loss: 0.5388785600662231 | Test loss: 0.5682550668716431\n",
      "Epoch: 13870 | Loss: 0.5388668179512024 | Test loss: 0.5682412385940552\n",
      "Epoch: 13880 | Loss: 0.5388550758361816 | Test loss: 0.5682273507118225\n",
      "Epoch: 13890 | Loss: 0.5388433337211609 | Test loss: 0.5682135224342346\n",
      "Epoch: 13900 | Loss: 0.5388315320014954 | Test loss: 0.568199634552002\n",
      "Epoch: 13910 | Loss: 0.5388197898864746 | Test loss: 0.5681858062744141\n",
      "Epoch: 13920 | Loss: 0.5388079881668091 | Test loss: 0.5681719779968262\n",
      "Epoch: 13930 | Loss: 0.5387962460517883 | Test loss: 0.5681581497192383\n",
      "Epoch: 13940 | Loss: 0.5387845039367676 | Test loss: 0.5681442618370056\n",
      "Epoch: 13950 | Loss: 0.5387727618217468 | Test loss: 0.5681304335594177\n",
      "Epoch: 13960 | Loss: 0.5387609601020813 | Test loss: 0.5681166052818298\n",
      "Epoch: 13970 | Loss: 0.5387492179870605 | Test loss: 0.5681027770042419\n",
      "Epoch: 13980 | Loss: 0.5387374758720398 | Test loss: 0.568088948726654\n",
      "Epoch: 13990 | Loss: 0.5387256741523743 | Test loss: 0.5680750608444214\n",
      "Epoch: 14000 | Loss: 0.5387139320373535 | Test loss: 0.5680612325668335\n",
      "Epoch: 14010 | Loss: 0.5387021899223328 | Test loss: 0.5680473446846008\n",
      "Epoch: 14020 | Loss: 0.5386905074119568 | Test loss: 0.5680335164070129\n",
      "Epoch: 14030 | Loss: 0.5386786460876465 | Test loss: 0.568019688129425\n",
      "Epoch: 14040 | Loss: 0.5386669039726257 | Test loss: 0.5680058002471924\n",
      "Epoch: 14050 | Loss: 0.5386552214622498 | Test loss: 0.5679919719696045\n",
      "Epoch: 14060 | Loss: 0.5386433601379395 | Test loss: 0.5679781436920166\n",
      "Epoch: 14070 | Loss: 0.5386316180229187 | Test loss: 0.5679642558097839\n",
      "Epoch: 14080 | Loss: 0.5386199355125427 | Test loss: 0.567950427532196\n",
      "Epoch: 14090 | Loss: 0.5386080741882324 | Test loss: 0.5679365992546082\n",
      "Epoch: 14100 | Loss: 0.5385963320732117 | Test loss: 0.5679227113723755\n",
      "Epoch: 14110 | Loss: 0.5385845899581909 | Test loss: 0.5679088830947876\n",
      "Epoch: 14120 | Loss: 0.5385727882385254 | Test loss: 0.5678950548171997\n",
      "Epoch: 14130 | Loss: 0.5385610461235046 | Test loss: 0.5678812265396118\n",
      "Epoch: 14140 | Loss: 0.5385493636131287 | Test loss: 0.5678673982620239\n",
      "Epoch: 14150 | Loss: 0.5385375022888184 | Test loss: 0.5678535103797913\n",
      "Epoch: 14160 | Loss: 0.5385257601737976 | Test loss: 0.5678396224975586\n",
      "Epoch: 14170 | Loss: 0.5385140776634216 | Test loss: 0.5678257942199707\n",
      "Epoch: 14180 | Loss: 0.5385022759437561 | Test loss: 0.5678119659423828\n",
      "Epoch: 14190 | Loss: 0.5384904742240906 | Test loss: 0.5677981376647949\n",
      "Epoch: 14200 | Loss: 0.5384787917137146 | Test loss: 0.567784309387207\n",
      "Epoch: 14210 | Loss: 0.5384669303894043 | Test loss: 0.5677704215049744\n",
      "Epoch: 14220 | Loss: 0.5384552478790283 | Test loss: 0.5677565932273865\n",
      "Epoch: 14230 | Loss: 0.5384435057640076 | Test loss: 0.5677427649497986\n",
      "Epoch: 14240 | Loss: 0.5384317636489868 | Test loss: 0.5677289366722107\n",
      "Epoch: 14250 | Loss: 0.5384199619293213 | Test loss: 0.5677151083946228\n",
      "Epoch: 14260 | Loss: 0.5384082198143005 | Test loss: 0.5677012801170349\n",
      "Epoch: 14270 | Loss: 0.538396418094635 | Test loss: 0.5676873326301575\n",
      "Epoch: 14280 | Loss: 0.5383846759796143 | Test loss: 0.5676735043525696\n",
      "Epoch: 14290 | Loss: 0.5383729338645935 | Test loss: 0.5676596760749817\n",
      "Epoch: 14300 | Loss: 0.5383611917495728 | Test loss: 0.567645788192749\n",
      "Epoch: 14310 | Loss: 0.5383493304252625 | Test loss: 0.5676319599151611\n",
      "Epoch: 14320 | Loss: 0.5383376479148865 | Test loss: 0.5676181316375732\n",
      "Epoch: 14330 | Loss: 0.5383259057998657 | Test loss: 0.5676042437553406\n",
      "Epoch: 14340 | Loss: 0.5383141040802002 | Test loss: 0.5675904154777527\n",
      "Epoch: 14350 | Loss: 0.5383023619651794 | Test loss: 0.5675765872001648\n",
      "Epoch: 14360 | Loss: 0.5382906198501587 | Test loss: 0.5675626993179321\n",
      "Epoch: 14370 | Loss: 0.5382788181304932 | Test loss: 0.567548930644989\n",
      "Epoch: 14380 | Loss: 0.5382670760154724 | Test loss: 0.5675350427627563\n",
      "Epoch: 14390 | Loss: 0.5382553339004517 | Test loss: 0.5675212144851685\n",
      "Epoch: 14400 | Loss: 0.5382435917854309 | Test loss: 0.5675073862075806\n",
      "Epoch: 14410 | Loss: 0.5382317900657654 | Test loss: 0.5674935579299927\n",
      "Epoch: 14420 | Loss: 0.5382200479507446 | Test loss: 0.5674796104431152\n",
      "Epoch: 14430 | Loss: 0.5382083058357239 | Test loss: 0.5674657821655273\n",
      "Epoch: 14440 | Loss: 0.5381965041160583 | Test loss: 0.5674519538879395\n",
      "Epoch: 14450 | Loss: 0.5381847620010376 | Test loss: 0.5674381256103516\n",
      "Epoch: 14460 | Loss: 0.5381730198860168 | Test loss: 0.5674242973327637\n",
      "Epoch: 14470 | Loss: 0.5381612181663513 | Test loss: 0.567410409450531\n",
      "Epoch: 14480 | Loss: 0.5381495356559753 | Test loss: 0.5673965811729431\n",
      "Epoch: 14490 | Loss: 0.538137674331665 | Test loss: 0.5673827528953552\n",
      "Epoch: 14500 | Loss: 0.5381259322166443 | Test loss: 0.5673689246177673\n",
      "Epoch: 14510 | Loss: 0.5381141901016235 | Test loss: 0.5673550963401794\n",
      "Epoch: 14520 | Loss: 0.538102388381958 | Test loss: 0.5673412680625916\n",
      "Epoch: 14530 | Loss: 0.538090705871582 | Test loss: 0.5673273801803589\n",
      "Epoch: 14540 | Loss: 0.5380789041519165 | Test loss: 0.5673134922981262\n",
      "Epoch: 14550 | Loss: 0.5380671620368958 | Test loss: 0.5672996640205383\n",
      "Epoch: 14560 | Loss: 0.5380553603172302 | Test loss: 0.5672857761383057\n",
      "Epoch: 14570 | Loss: 0.5380436182022095 | Test loss: 0.5672720074653625\n",
      "Epoch: 14580 | Loss: 0.5380318760871887 | Test loss: 0.5672581195831299\n",
      "Epoch: 14590 | Loss: 0.5380200743675232 | Test loss: 0.5672442317008972\n",
      "Epoch: 14600 | Loss: 0.5380083918571472 | Test loss: 0.5672304034233093\n",
      "Epoch: 14610 | Loss: 0.5379965901374817 | Test loss: 0.5672165751457214\n",
      "Epoch: 14620 | Loss: 0.5379848480224609 | Test loss: 0.5672027468681335\n",
      "Epoch: 14630 | Loss: 0.5379731059074402 | Test loss: 0.5671889185905457\n",
      "Epoch: 14640 | Loss: 0.5379613041877747 | Test loss: 0.567175030708313\n",
      "Epoch: 14650 | Loss: 0.5379495620727539 | Test loss: 0.5671612024307251\n",
      "Epoch: 14660 | Loss: 0.5379377603530884 | Test loss: 0.5671473741531372\n",
      "Epoch: 14670 | Loss: 0.5379260182380676 | Test loss: 0.5671335458755493\n",
      "Epoch: 14680 | Loss: 0.5379142761230469 | Test loss: 0.5671196579933167\n",
      "Epoch: 14690 | Loss: 0.5379025340080261 | Test loss: 0.5671058297157288\n",
      "Epoch: 14700 | Loss: 0.5378907322883606 | Test loss: 0.5670919418334961\n",
      "Epoch: 14710 | Loss: 0.5378789901733398 | Test loss: 0.5670781135559082\n",
      "Epoch: 14720 | Loss: 0.5378671884536743 | Test loss: 0.5670642852783203\n",
      "Epoch: 14730 | Loss: 0.5378554463386536 | Test loss: 0.5670504570007324\n",
      "Epoch: 14740 | Loss: 0.5378437042236328 | Test loss: 0.5670365691184998\n",
      "Epoch: 14750 | Loss: 0.5378319621086121 | Test loss: 0.5670227408409119\n",
      "Epoch: 14760 | Loss: 0.5378201603889465 | Test loss: 0.567008912563324\n",
      "Epoch: 14770 | Loss: 0.5378084182739258 | Test loss: 0.5669950842857361\n",
      "Epoch: 14780 | Loss: 0.537796676158905 | Test loss: 0.5669812560081482\n",
      "Epoch: 14790 | Loss: 0.5377848744392395 | Test loss: 0.5669673681259155\n",
      "Epoch: 14800 | Loss: 0.5377731323242188 | Test loss: 0.5669535398483276\n",
      "Epoch: 14810 | Loss: 0.537761390209198 | Test loss: 0.566939651966095\n",
      "Epoch: 14820 | Loss: 0.537749707698822 | Test loss: 0.5669258236885071\n",
      "Epoch: 14830 | Loss: 0.5377378463745117 | Test loss: 0.5669119954109192\n",
      "Epoch: 14840 | Loss: 0.537726104259491 | Test loss: 0.5668981075286865\n",
      "Epoch: 14850 | Loss: 0.537714421749115 | Test loss: 0.5668842792510986\n",
      "Epoch: 14860 | Loss: 0.5377025604248047 | Test loss: 0.5668704509735107\n",
      "Epoch: 14870 | Loss: 0.5376908183097839 | Test loss: 0.5668565630912781\n",
      "Epoch: 14880 | Loss: 0.537679135799408 | Test loss: 0.5668427348136902\n",
      "Epoch: 14890 | Loss: 0.5376672744750977 | Test loss: 0.5668289065361023\n",
      "Epoch: 14900 | Loss: 0.5376555323600769 | Test loss: 0.5668150186538696\n",
      "Epoch: 14910 | Loss: 0.5376437902450562 | Test loss: 0.5668011903762817\n",
      "Epoch: 14920 | Loss: 0.5376319885253906 | Test loss: 0.5667873620986938\n",
      "Epoch: 14930 | Loss: 0.5376202464103699 | Test loss: 0.566773533821106\n",
      "Epoch: 14940 | Loss: 0.5376085638999939 | Test loss: 0.5667597055435181\n",
      "Epoch: 14950 | Loss: 0.5375967025756836 | Test loss: 0.5667458176612854\n",
      "Epoch: 14960 | Loss: 0.5375849604606628 | Test loss: 0.5667319297790527\n",
      "Epoch: 14970 | Loss: 0.5375732779502869 | Test loss: 0.5667181015014648\n",
      "Epoch: 14980 | Loss: 0.5375614762306213 | Test loss: 0.566704273223877\n",
      "Epoch: 14990 | Loss: 0.5375496745109558 | Test loss: 0.5666904449462891\n",
      "Epoch: 15000 | Loss: 0.5375379920005798 | Test loss: 0.5666766166687012\n",
      "Epoch: 15010 | Loss: 0.5375261306762695 | Test loss: 0.5666627287864685\n",
      "Epoch: 15020 | Loss: 0.5375144481658936 | Test loss: 0.5666489005088806\n",
      "Epoch: 15030 | Loss: 0.5375027060508728 | Test loss: 0.5666350722312927\n",
      "Epoch: 15040 | Loss: 0.537490963935852 | Test loss: 0.5666212439537048\n",
      "Epoch: 15050 | Loss: 0.5374791622161865 | Test loss: 0.5666074156761169\n",
      "Epoch: 15060 | Loss: 0.5374674201011658 | Test loss: 0.566593587398529\n",
      "Epoch: 15070 | Loss: 0.5374556183815002 | Test loss: 0.5665796399116516\n",
      "Epoch: 15080 | Loss: 0.5374438762664795 | Test loss: 0.5665658116340637\n",
      "Epoch: 15090 | Loss: 0.5374321341514587 | Test loss: 0.5665519833564758\n",
      "Epoch: 15100 | Loss: 0.537420392036438 | Test loss: 0.5665380954742432\n",
      "Epoch: 15110 | Loss: 0.5374085307121277 | Test loss: 0.5665242671966553\n",
      "Epoch: 15120 | Loss: 0.5373968482017517 | Test loss: 0.5665104389190674\n",
      "Epoch: 15130 | Loss: 0.537385106086731 | Test loss: 0.5664965510368347\n",
      "Epoch: 15140 | Loss: 0.5373733043670654 | Test loss: 0.5664827227592468\n",
      "Epoch: 15150 | Loss: 0.5373615622520447 | Test loss: 0.5664688944816589\n",
      "Epoch: 15160 | Loss: 0.5373498201370239 | Test loss: 0.5664550065994263\n",
      "Epoch: 15170 | Loss: 0.5373380184173584 | Test loss: 0.5664412379264832\n",
      "Epoch: 15180 | Loss: 0.5373262763023376 | Test loss: 0.5664273500442505\n",
      "Epoch: 15190 | Loss: 0.5373145341873169 | Test loss: 0.5664135217666626\n",
      "Epoch: 15200 | Loss: 0.5373027920722961 | Test loss: 0.5663996934890747\n",
      "Epoch: 15210 | Loss: 0.5372909903526306 | Test loss: 0.5663858652114868\n",
      "Epoch: 15220 | Loss: 0.5372792482376099 | Test loss: 0.5663719177246094\n",
      "Epoch: 15230 | Loss: 0.5372675061225891 | Test loss: 0.5663580894470215\n",
      "Epoch: 15240 | Loss: 0.5372557044029236 | Test loss: 0.5663442611694336\n",
      "Epoch: 15250 | Loss: 0.5372439622879028 | Test loss: 0.5663304328918457\n",
      "Epoch: 15260 | Loss: 0.5372322201728821 | Test loss: 0.5663166046142578\n",
      "Epoch: 15270 | Loss: 0.5372204184532166 | Test loss: 0.5663027167320251\n",
      "Epoch: 15280 | Loss: 0.5372087359428406 | Test loss: 0.5662888884544373\n",
      "Epoch: 15290 | Loss: 0.5371968746185303 | Test loss: 0.5662750601768494\n",
      "Epoch: 15300 | Loss: 0.5371851325035095 | Test loss: 0.5662612318992615\n",
      "Epoch: 15310 | Loss: 0.5371733903884888 | Test loss: 0.5662474036216736\n",
      "Epoch: 15320 | Loss: 0.5371615886688232 | Test loss: 0.5662335753440857\n",
      "Epoch: 15330 | Loss: 0.5371499061584473 | Test loss: 0.566219687461853\n",
      "Epoch: 15340 | Loss: 0.5371381044387817 | Test loss: 0.5662057995796204\n",
      "Epoch: 15350 | Loss: 0.537126362323761 | Test loss: 0.5661919713020325\n",
      "Epoch: 15360 | Loss: 0.5371145606040955 | Test loss: 0.5661780834197998\n",
      "Epoch: 15370 | Loss: 0.5371028184890747 | Test loss: 0.5661643147468567\n",
      "Epoch: 15380 | Loss: 0.537091076374054 | Test loss: 0.566150426864624\n",
      "Epoch: 15390 | Loss: 0.5370792746543884 | Test loss: 0.5661365389823914\n",
      "Epoch: 15400 | Loss: 0.5370675921440125 | Test loss: 0.5661227107048035\n",
      "Epoch: 15410 | Loss: 0.5370557904243469 | Test loss: 0.5661088824272156\n",
      "Epoch: 15420 | Loss: 0.5370440483093262 | Test loss: 0.5660950541496277\n",
      "Epoch: 15430 | Loss: 0.5370323061943054 | Test loss: 0.5660812258720398\n",
      "Epoch: 15440 | Loss: 0.5370205044746399 | Test loss: 0.5660673379898071\n",
      "Epoch: 15450 | Loss: 0.5370087623596191 | Test loss: 0.5660535097122192\n",
      "Epoch: 15460 | Loss: 0.5369969606399536 | Test loss: 0.5660396814346313\n",
      "Epoch: 15470 | Loss: 0.5369852185249329 | Test loss: 0.5660258531570435\n",
      "Epoch: 15480 | Loss: 0.5369734764099121 | Test loss: 0.5660119652748108\n",
      "Epoch: 15490 | Loss: 0.5369617342948914 | Test loss: 0.5659981369972229\n",
      "Epoch: 15500 | Loss: 0.5369499325752258 | Test loss: 0.5659842491149902\n",
      "Epoch: 15510 | Loss: 0.5369381904602051 | Test loss: 0.5659704208374023\n",
      "Epoch: 15520 | Loss: 0.5369263887405396 | Test loss: 0.5659565925598145\n",
      "Epoch: 15530 | Loss: 0.5369146466255188 | Test loss: 0.5659427642822266\n",
      "Epoch: 15540 | Loss: 0.536902904510498 | Test loss: 0.5659288763999939\n",
      "Epoch: 15550 | Loss: 0.5368911623954773 | Test loss: 0.565915048122406\n",
      "Epoch: 15560 | Loss: 0.5368793606758118 | Test loss: 0.5659012198448181\n",
      "Epoch: 15570 | Loss: 0.536867618560791 | Test loss: 0.5658873915672302\n",
      "Epoch: 15580 | Loss: 0.5368558764457703 | Test loss: 0.5658735632896423\n",
      "Epoch: 15590 | Loss: 0.5368440747261047 | Test loss: 0.5658596754074097\n",
      "Epoch: 15600 | Loss: 0.536832332611084 | Test loss: 0.5658458471298218\n",
      "Epoch: 15610 | Loss: 0.5368205904960632 | Test loss: 0.5658319592475891\n",
      "Epoch: 15620 | Loss: 0.5368089079856873 | Test loss: 0.5658181309700012\n",
      "Epoch: 15630 | Loss: 0.536797046661377 | Test loss: 0.5658043026924133\n",
      "Epoch: 15640 | Loss: 0.5367853045463562 | Test loss: 0.5657904148101807\n",
      "Epoch: 15650 | Loss: 0.5367736220359802 | Test loss: 0.5657765865325928\n",
      "Epoch: 15660 | Loss: 0.5367617607116699 | Test loss: 0.5657627582550049\n",
      "Epoch: 15670 | Loss: 0.5367500185966492 | Test loss: 0.5657488703727722\n",
      "Epoch: 15680 | Loss: 0.5367383360862732 | Test loss: 0.5657350420951843\n",
      "Epoch: 15690 | Loss: 0.5367264747619629 | Test loss: 0.5657212138175964\n",
      "Epoch: 15700 | Loss: 0.5367147326469421 | Test loss: 0.5657073259353638\n",
      "Epoch: 15710 | Loss: 0.5367029905319214 | Test loss: 0.5656934976577759\n",
      "Epoch: 15720 | Loss: 0.5366911888122559 | Test loss: 0.565679669380188\n",
      "Epoch: 15730 | Loss: 0.5366794466972351 | Test loss: 0.5656658411026001\n",
      "Epoch: 15740 | Loss: 0.5366677641868591 | Test loss: 0.5656520128250122\n",
      "Epoch: 15750 | Loss: 0.5366559028625488 | Test loss: 0.5656381249427795\n",
      "Epoch: 15760 | Loss: 0.5366441607475281 | Test loss: 0.5656242370605469\n",
      "Epoch: 15770 | Loss: 0.5366324782371521 | Test loss: 0.565610408782959\n",
      "Epoch: 15780 | Loss: 0.5366206765174866 | Test loss: 0.5655965805053711\n",
      "Epoch: 15790 | Loss: 0.536608874797821 | Test loss: 0.5655827522277832\n",
      "Epoch: 15800 | Loss: 0.5365971922874451 | Test loss: 0.5655689239501953\n",
      "Epoch: 15810 | Loss: 0.5365853309631348 | Test loss: 0.5655550360679626\n",
      "Epoch: 15820 | Loss: 0.5365736484527588 | Test loss: 0.5655412077903748\n",
      "Epoch: 15830 | Loss: 0.536561906337738 | Test loss: 0.5655273795127869\n",
      "Epoch: 15840 | Loss: 0.5365501642227173 | Test loss: 0.565513551235199\n",
      "Epoch: 15850 | Loss: 0.5365383625030518 | Test loss: 0.5654997229576111\n",
      "Epoch: 15860 | Loss: 0.536526620388031 | Test loss: 0.5654858946800232\n",
      "Epoch: 15870 | Loss: 0.5365148186683655 | Test loss: 0.5654719471931458\n",
      "Epoch: 15880 | Loss: 0.5365030765533447 | Test loss: 0.5654581189155579\n",
      "Epoch: 15890 | Loss: 0.536491334438324 | Test loss: 0.56544429063797\n",
      "Epoch: 15900 | Loss: 0.5364795923233032 | Test loss: 0.5654304027557373\n",
      "Epoch: 15910 | Loss: 0.5364677309989929 | Test loss: 0.5654165744781494\n",
      "Epoch: 15920 | Loss: 0.5364560484886169 | Test loss: 0.5654027462005615\n",
      "Epoch: 15930 | Loss: 0.5364443063735962 | Test loss: 0.5653888583183289\n",
      "Epoch: 15940 | Loss: 0.5364325046539307 | Test loss: 0.565375030040741\n",
      "Epoch: 15950 | Loss: 0.5364207625389099 | Test loss: 0.5653612017631531\n",
      "Epoch: 15960 | Loss: 0.5364090204238892 | Test loss: 0.5653473138809204\n",
      "Epoch: 15970 | Loss: 0.5363972187042236 | Test loss: 0.5653335452079773\n",
      "Epoch: 15980 | Loss: 0.5363854765892029 | Test loss: 0.5653196573257446\n",
      "Epoch: 15990 | Loss: 0.5363737344741821 | Test loss: 0.5653058290481567\n",
      "Epoch: 16000 | Loss: 0.5363619923591614 | Test loss: 0.5652920007705688\n",
      "Epoch: 16010 | Loss: 0.5363501906394958 | Test loss: 0.565278172492981\n",
      "Epoch: 16020 | Loss: 0.5363384485244751 | Test loss: 0.5652642250061035\n",
      "Epoch: 16030 | Loss: 0.5363267064094543 | Test loss: 0.5652503967285156\n",
      "Epoch: 16040 | Loss: 0.5363149046897888 | Test loss: 0.5652365684509277\n",
      "Epoch: 16050 | Loss: 0.5363031625747681 | Test loss: 0.5652227401733398\n",
      "Epoch: 16060 | Loss: 0.5362914204597473 | Test loss: 0.565208911895752\n",
      "Epoch: 16070 | Loss: 0.5362796187400818 | Test loss: 0.5651950240135193\n",
      "Epoch: 16080 | Loss: 0.5362679362297058 | Test loss: 0.5651811957359314\n",
      "Epoch: 16090 | Loss: 0.5362560749053955 | Test loss: 0.5651673674583435\n",
      "Epoch: 16100 | Loss: 0.5362443327903748 | Test loss: 0.5651535391807556\n",
      "Epoch: 16110 | Loss: 0.536232590675354 | Test loss: 0.5651397109031677\n",
      "Epoch: 16120 | Loss: 0.5362207889556885 | Test loss: 0.5651258826255798\n",
      "Epoch: 16130 | Loss: 0.5362091064453125 | Test loss: 0.5651119947433472\n",
      "Epoch: 16140 | Loss: 0.536197304725647 | Test loss: 0.5650981068611145\n",
      "Epoch: 16150 | Loss: 0.5361855626106262 | Test loss: 0.5650842785835266\n",
      "Epoch: 16160 | Loss: 0.5361737608909607 | Test loss: 0.565070390701294\n",
      "Epoch: 16170 | Loss: 0.5361620187759399 | Test loss: 0.5650566220283508\n",
      "Epoch: 16180 | Loss: 0.5361502766609192 | Test loss: 0.5650427341461182\n",
      "Epoch: 16190 | Loss: 0.5361384749412537 | Test loss: 0.5650288462638855\n",
      "Epoch: 16200 | Loss: 0.5361267924308777 | Test loss: 0.5650150179862976\n",
      "Epoch: 16210 | Loss: 0.5361149907112122 | Test loss: 0.5650011897087097\n",
      "Epoch: 16220 | Loss: 0.5361032485961914 | Test loss: 0.5649873614311218\n",
      "Epoch: 16230 | Loss: 0.5360915064811707 | Test loss: 0.5649735331535339\n",
      "Epoch: 16240 | Loss: 0.5360797047615051 | Test loss: 0.5649596452713013\n",
      "Epoch: 16250 | Loss: 0.5360679626464844 | Test loss: 0.5649458169937134\n",
      "Epoch: 16260 | Loss: 0.5360561609268188 | Test loss: 0.5649319887161255\n",
      "Epoch: 16270 | Loss: 0.5360444188117981 | Test loss: 0.5649181604385376\n",
      "Epoch: 16280 | Loss: 0.5360326766967773 | Test loss: 0.5649042725563049\n",
      "Epoch: 16290 | Loss: 0.5360209345817566 | Test loss: 0.564890444278717\n",
      "Epoch: 16300 | Loss: 0.5360091328620911 | Test loss: 0.5648765563964844\n",
      "Epoch: 16310 | Loss: 0.5359973907470703 | Test loss: 0.5648627281188965\n",
      "Epoch: 16320 | Loss: 0.5359855890274048 | Test loss: 0.5648488998413086\n",
      "Epoch: 16330 | Loss: 0.535973846912384 | Test loss: 0.5648350715637207\n",
      "Epoch: 16340 | Loss: 0.5359621047973633 | Test loss: 0.564821183681488\n",
      "Epoch: 16350 | Loss: 0.5359503626823425 | Test loss: 0.5648073554039001\n",
      "Epoch: 16360 | Loss: 0.535938560962677 | Test loss: 0.5647935271263123\n",
      "Epoch: 16370 | Loss: 0.5359268188476562 | Test loss: 0.5647796988487244\n",
      "Epoch: 16380 | Loss: 0.5359150767326355 | Test loss: 0.5647658705711365\n",
      "Epoch: 16390 | Loss: 0.53590327501297 | Test loss: 0.5647519826889038\n",
      "Epoch: 16400 | Loss: 0.5358915328979492 | Test loss: 0.5647381544113159\n",
      "Epoch: 16410 | Loss: 0.5358797907829285 | Test loss: 0.5647242665290833\n",
      "Epoch: 16420 | Loss: 0.5358681082725525 | Test loss: 0.5647104382514954\n",
      "Epoch: 16430 | Loss: 0.5358562469482422 | Test loss: 0.5646966099739075\n",
      "Epoch: 16440 | Loss: 0.5358445048332214 | Test loss: 0.5646827220916748\n",
      "Epoch: 16450 | Loss: 0.5358328223228455 | Test loss: 0.5646688938140869\n",
      "Epoch: 16460 | Loss: 0.5358209609985352 | Test loss: 0.564655065536499\n",
      "Epoch: 16470 | Loss: 0.5358092188835144 | Test loss: 0.5646411776542664\n",
      "Epoch: 16480 | Loss: 0.5357975363731384 | Test loss: 0.5646273493766785\n",
      "Epoch: 16490 | Loss: 0.5357856750488281 | Test loss: 0.5646135210990906\n",
      "Epoch: 16500 | Loss: 0.5357739329338074 | Test loss: 0.5645996332168579\n",
      "Epoch: 16510 | Loss: 0.5357621908187866 | Test loss: 0.56458580493927\n",
      "Epoch: 16520 | Loss: 0.5357503890991211 | Test loss: 0.5645719766616821\n",
      "Epoch: 16530 | Loss: 0.5357386469841003 | Test loss: 0.5645581483840942\n",
      "Epoch: 16540 | Loss: 0.5357269644737244 | Test loss: 0.5645443201065063\n",
      "Epoch: 16550 | Loss: 0.5357151031494141 | Test loss: 0.5645304322242737\n",
      "Epoch: 16560 | Loss: 0.5357033610343933 | Test loss: 0.564516544342041\n",
      "Epoch: 16570 | Loss: 0.5356916785240173 | Test loss: 0.5645027160644531\n",
      "Epoch: 16580 | Loss: 0.5356798768043518 | Test loss: 0.5644888877868652\n",
      "Epoch: 16590 | Loss: 0.5356680750846863 | Test loss: 0.5644750595092773\n",
      "Epoch: 16600 | Loss: 0.5356563925743103 | Test loss: 0.5644612312316895\n",
      "Epoch: 16610 | Loss: 0.53564453125 | Test loss: 0.5644473433494568\n",
      "Epoch: 16620 | Loss: 0.535632848739624 | Test loss: 0.5644335150718689\n",
      "Epoch: 16630 | Loss: 0.5356211066246033 | Test loss: 0.564419686794281\n",
      "Epoch: 16640 | Loss: 0.5356093645095825 | Test loss: 0.5644058585166931\n",
      "Epoch: 16650 | Loss: 0.535597562789917 | Test loss: 0.5643920302391052\n",
      "Epoch: 16660 | Loss: 0.5355858206748962 | Test loss: 0.5643782019615173\n",
      "Epoch: 16670 | Loss: 0.5355740189552307 | Test loss: 0.5643642544746399\n",
      "Epoch: 16680 | Loss: 0.53556227684021 | Test loss: 0.564350426197052\n",
      "Epoch: 16690 | Loss: 0.5355505347251892 | Test loss: 0.5643365979194641\n",
      "Epoch: 16700 | Loss: 0.5355387926101685 | Test loss: 0.5643227100372314\n",
      "Epoch: 16710 | Loss: 0.5355269312858582 | Test loss: 0.5643088817596436\n",
      "Epoch: 16720 | Loss: 0.5355152487754822 | Test loss: 0.5642950534820557\n",
      "Epoch: 16730 | Loss: 0.5355035066604614 | Test loss: 0.564281165599823\n",
      "Epoch: 16740 | Loss: 0.5354917049407959 | Test loss: 0.5642673373222351\n",
      "Epoch: 16750 | Loss: 0.5354799628257751 | Test loss: 0.5642535090446472\n",
      "Epoch: 16760 | Loss: 0.5354682207107544 | Test loss: 0.5642396211624146\n",
      "Epoch: 16770 | Loss: 0.5354564189910889 | Test loss: 0.5642258524894714\n",
      "Epoch: 16780 | Loss: 0.5354446768760681 | Test loss: 0.5642119646072388\n",
      "Epoch: 16790 | Loss: 0.5354329347610474 | Test loss: 0.5641981363296509\n",
      "Epoch: 16800 | Loss: 0.5354211926460266 | Test loss: 0.564184308052063\n",
      "Epoch: 16810 | Loss: 0.5354093909263611 | Test loss: 0.5641704797744751\n",
      "Epoch: 16820 | Loss: 0.5353976488113403 | Test loss: 0.5641565322875977\n",
      "Epoch: 16830 | Loss: 0.5353859066963196 | Test loss: 0.5641427040100098\n",
      "Epoch: 16840 | Loss: 0.535374104976654 | Test loss: 0.5641288757324219\n",
      "Epoch: 16850 | Loss: 0.5353623628616333 | Test loss: 0.564115047454834\n",
      "Epoch: 16860 | Loss: 0.5353506207466125 | Test loss: 0.5641012191772461\n",
      "Epoch: 16870 | Loss: 0.535338819026947 | Test loss: 0.5640873312950134\n",
      "Epoch: 16880 | Loss: 0.535327136516571 | Test loss: 0.5640735030174255\n",
      "Epoch: 16890 | Loss: 0.5353152751922607 | Test loss: 0.5640596747398376\n",
      "Epoch: 16900 | Loss: 0.53530353307724 | Test loss: 0.5640458464622498\n",
      "Epoch: 16910 | Loss: 0.5352917909622192 | Test loss: 0.5640320181846619\n",
      "Epoch: 16920 | Loss: 0.5352799892425537 | Test loss: 0.564018189907074\n",
      "Epoch: 16930 | Loss: 0.5352683067321777 | Test loss: 0.5640043020248413\n",
      "Epoch: 16940 | Loss: 0.5352565050125122 | Test loss: 0.5639904141426086\n",
      "Epoch: 16950 | Loss: 0.5352447628974915 | Test loss: 0.5639765858650208\n",
      "Epoch: 16960 | Loss: 0.5352329611778259 | Test loss: 0.5639626979827881\n",
      "Epoch: 16970 | Loss: 0.5352212190628052 | Test loss: 0.563948929309845\n",
      "Epoch: 16980 | Loss: 0.5352094769477844 | Test loss: 0.5639350414276123\n",
      "Epoch: 16990 | Loss: 0.5351976752281189 | Test loss: 0.5639211535453796\n",
      "Epoch: 17000 | Loss: 0.5351859927177429 | Test loss: 0.5639073252677917\n",
      "Epoch: 17010 | Loss: 0.5351741909980774 | Test loss: 0.5638934969902039\n",
      "Epoch: 17020 | Loss: 0.5351624488830566 | Test loss: 0.563879668712616\n",
      "Epoch: 17030 | Loss: 0.5351507067680359 | Test loss: 0.5638658404350281\n",
      "Epoch: 17040 | Loss: 0.5351389050483704 | Test loss: 0.5638519525527954\n",
      "Epoch: 17050 | Loss: 0.5351271629333496 | Test loss: 0.5638381242752075\n",
      "Epoch: 17060 | Loss: 0.5351153612136841 | Test loss: 0.5638242959976196\n",
      "Epoch: 17070 | Loss: 0.5351036190986633 | Test loss: 0.5638104677200317\n",
      "Epoch: 17080 | Loss: 0.5350918769836426 | Test loss: 0.5637965798377991\n",
      "Epoch: 17090 | Loss: 0.5350801348686218 | Test loss: 0.5637827515602112\n",
      "Epoch: 17100 | Loss: 0.5350683331489563 | Test loss: 0.5637688636779785\n",
      "Epoch: 17110 | Loss: 0.5350565910339355 | Test loss: 0.5637550354003906\n",
      "Epoch: 17120 | Loss: 0.53504478931427 | Test loss: 0.5637412071228027\n",
      "Epoch: 17130 | Loss: 0.5350330471992493 | Test loss: 0.5637273788452148\n",
      "Epoch: 17140 | Loss: 0.5350213050842285 | Test loss: 0.5637134909629822\n",
      "Epoch: 17150 | Loss: 0.5350095629692078 | Test loss: 0.5636996626853943\n",
      "Epoch: 17160 | Loss: 0.5349977612495422 | Test loss: 0.5636858344078064\n",
      "Epoch: 17170 | Loss: 0.5349860191345215 | Test loss: 0.5636720061302185\n",
      "Epoch: 17180 | Loss: 0.5349742770195007 | Test loss: 0.5636581778526306\n",
      "Epoch: 17190 | Loss: 0.5349624752998352 | Test loss: 0.563644289970398\n",
      "Epoch: 17200 | Loss: 0.5349507331848145 | Test loss: 0.5636304616928101\n",
      "Epoch: 17210 | Loss: 0.5349389910697937 | Test loss: 0.5636165738105774\n",
      "Epoch: 17220 | Loss: 0.5349273085594177 | Test loss: 0.5636027455329895\n",
      "Epoch: 17230 | Loss: 0.5349154472351074 | Test loss: 0.5635889172554016\n",
      "Epoch: 17240 | Loss: 0.5349037051200867 | Test loss: 0.563575029373169\n",
      "Epoch: 17250 | Loss: 0.5348920226097107 | Test loss: 0.563561201095581\n",
      "Epoch: 17260 | Loss: 0.5348801612854004 | Test loss: 0.5635473728179932\n",
      "Epoch: 17270 | Loss: 0.5348684191703796 | Test loss: 0.5635334849357605\n",
      "Epoch: 17280 | Loss: 0.5348567366600037 | Test loss: 0.5635196566581726\n",
      "Epoch: 17290 | Loss: 0.5348448753356934 | Test loss: 0.5635058283805847\n",
      "Epoch: 17300 | Loss: 0.5348331332206726 | Test loss: 0.563491940498352\n",
      "Epoch: 17310 | Loss: 0.5348213911056519 | Test loss: 0.5634781122207642\n",
      "Epoch: 17320 | Loss: 0.5348095893859863 | Test loss: 0.5634642839431763\n",
      "Epoch: 17330 | Loss: 0.5347978472709656 | Test loss: 0.5634504556655884\n",
      "Epoch: 17340 | Loss: 0.5347861647605896 | Test loss: 0.5634366273880005\n",
      "Epoch: 17350 | Loss: 0.5347743034362793 | Test loss: 0.5634227395057678\n",
      "Epoch: 17360 | Loss: 0.5347625613212585 | Test loss: 0.5634088516235352\n",
      "Epoch: 17370 | Loss: 0.5347508788108826 | Test loss: 0.5633950233459473\n",
      "Epoch: 17380 | Loss: 0.534739077091217 | Test loss: 0.5633811950683594\n",
      "Epoch: 17390 | Loss: 0.5347272753715515 | Test loss: 0.5633673667907715\n",
      "Epoch: 17400 | Loss: 0.5347155928611755 | Test loss: 0.5633535385131836\n",
      "Epoch: 17410 | Loss: 0.5347037315368652 | Test loss: 0.5633396506309509\n",
      "Epoch: 17420 | Loss: 0.5346920490264893 | Test loss: 0.563325822353363\n",
      "Epoch: 17430 | Loss: 0.5346803069114685 | Test loss: 0.5633119940757751\n",
      "Epoch: 17440 | Loss: 0.5346685647964478 | Test loss: 0.5632981657981873\n",
      "Epoch: 17450 | Loss: 0.5346567630767822 | Test loss: 0.5632843375205994\n",
      "Epoch: 17460 | Loss: 0.5346450209617615 | Test loss: 0.5632705092430115\n",
      "Epoch: 17470 | Loss: 0.534633219242096 | Test loss: 0.563256561756134\n",
      "Epoch: 17480 | Loss: 0.5346214771270752 | Test loss: 0.5632427334785461\n",
      "Epoch: 17490 | Loss: 0.5346097350120544 | Test loss: 0.5632289052009583\n",
      "Epoch: 17500 | Loss: 0.5345979928970337 | Test loss: 0.5632150173187256\n",
      "Epoch: 17510 | Loss: 0.5345861315727234 | Test loss: 0.5632011890411377\n",
      "Epoch: 17520 | Loss: 0.5345744490623474 | Test loss: 0.5631873607635498\n",
      "Epoch: 17530 | Loss: 0.5345627069473267 | Test loss: 0.5631734728813171\n",
      "Epoch: 17540 | Loss: 0.5345509052276611 | Test loss: 0.5631596446037292\n",
      "Epoch: 17550 | Loss: 0.5345391631126404 | Test loss: 0.5631458163261414\n",
      "Epoch: 17560 | Loss: 0.5345274209976196 | Test loss: 0.5631319284439087\n",
      "Epoch: 17570 | Loss: 0.5345156192779541 | Test loss: 0.5631181597709656\n",
      "Epoch: 17580 | Loss: 0.5345038771629333 | Test loss: 0.5631042718887329\n",
      "Epoch: 17590 | Loss: 0.5344921350479126 | Test loss: 0.563090443611145\n",
      "Epoch: 17600 | Loss: 0.5344803929328918 | Test loss: 0.5630766153335571\n",
      "Epoch: 17610 | Loss: 0.5344685912132263 | Test loss: 0.5630627870559692\n",
      "Epoch: 17620 | Loss: 0.5344568490982056 | Test loss: 0.5630488395690918\n",
      "Epoch: 17630 | Loss: 0.5344451069831848 | Test loss: 0.5630350112915039\n",
      "Epoch: 17640 | Loss: 0.5344333052635193 | Test loss: 0.563021183013916\n",
      "Epoch: 17650 | Loss: 0.5344215631484985 | Test loss: 0.5630073547363281\n",
      "Epoch: 17660 | Loss: 0.5344098210334778 | Test loss: 0.5629935264587402\n",
      "Epoch: 17670 | Loss: 0.5343980193138123 | Test loss: 0.5629796385765076\n",
      "Epoch: 17680 | Loss: 0.5343863368034363 | Test loss: 0.5629658102989197\n",
      "Epoch: 17690 | Loss: 0.534374475479126 | Test loss: 0.5629519820213318\n",
      "Epoch: 17700 | Loss: 0.5343627333641052 | Test loss: 0.5629381537437439\n",
      "Epoch: 17710 | Loss: 0.5343509912490845 | Test loss: 0.562924325466156\n",
      "Epoch: 17720 | Loss: 0.534339189529419 | Test loss: 0.5629104971885681\n",
      "Epoch: 17730 | Loss: 0.534327507019043 | Test loss: 0.5628966093063354\n",
      "Epoch: 17740 | Loss: 0.5343157052993774 | Test loss: 0.5628827214241028\n",
      "Epoch: 17750 | Loss: 0.5343039631843567 | Test loss: 0.5628688931465149\n",
      "Epoch: 17760 | Loss: 0.5342921614646912 | Test loss: 0.5628550052642822\n",
      "Epoch: 17770 | Loss: 0.5342804193496704 | Test loss: 0.5628412365913391\n",
      "Epoch: 17780 | Loss: 0.5342686772346497 | Test loss: 0.5628273487091064\n",
      "Epoch: 17790 | Loss: 0.5342568755149841 | Test loss: 0.5628134608268738\n",
      "Epoch: 17800 | Loss: 0.5342451930046082 | Test loss: 0.5627996325492859\n",
      "Epoch: 17810 | Loss: 0.5342333912849426 | Test loss: 0.562785804271698\n",
      "Epoch: 17820 | Loss: 0.5342216491699219 | Test loss: 0.5627719759941101\n",
      "Epoch: 17830 | Loss: 0.5342099070549011 | Test loss: 0.5627581477165222\n",
      "Epoch: 17840 | Loss: 0.5341981053352356 | Test loss: 0.5627442598342896\n",
      "Epoch: 17850 | Loss: 0.5341863632202148 | Test loss: 0.5627304315567017\n",
      "Epoch: 17860 | Loss: 0.5341745615005493 | Test loss: 0.5627166032791138\n",
      "Epoch: 17870 | Loss: 0.5341628193855286 | Test loss: 0.5627027750015259\n",
      "Epoch: 17880 | Loss: 0.5341510772705078 | Test loss: 0.5626888871192932\n",
      "Epoch: 17890 | Loss: 0.5341393351554871 | Test loss: 0.5626750588417053\n",
      "Epoch: 17900 | Loss: 0.5341275334358215 | Test loss: 0.5626611709594727\n",
      "Epoch: 17910 | Loss: 0.5341157913208008 | Test loss: 0.5626473426818848\n",
      "Epoch: 17920 | Loss: 0.5341039896011353 | Test loss: 0.5626335144042969\n",
      "Epoch: 17930 | Loss: 0.5340922474861145 | Test loss: 0.562619686126709\n",
      "Epoch: 17940 | Loss: 0.5340805053710938 | Test loss: 0.5626057982444763\n",
      "Epoch: 17950 | Loss: 0.534068763256073 | Test loss: 0.5625919699668884\n",
      "Epoch: 17960 | Loss: 0.5340569615364075 | Test loss: 0.5625781416893005\n",
      "Epoch: 17970 | Loss: 0.5340452194213867 | Test loss: 0.5625643134117126\n",
      "Epoch: 17980 | Loss: 0.534033477306366 | Test loss: 0.5625504851341248\n",
      "Epoch: 17990 | Loss: 0.5340216755867004 | Test loss: 0.5625365972518921\n",
      "Epoch: 18000 | Loss: 0.5340099334716797 | Test loss: 0.5625227689743042\n",
      "Epoch: 18010 | Loss: 0.5339981913566589 | Test loss: 0.5625088810920715\n",
      "Epoch: 18020 | Loss: 0.533986508846283 | Test loss: 0.5624950528144836\n",
      "Epoch: 18030 | Loss: 0.5339746475219727 | Test loss: 0.5624812245368958\n",
      "Epoch: 18040 | Loss: 0.5339629054069519 | Test loss: 0.5624673366546631\n",
      "Epoch: 18050 | Loss: 0.5339512228965759 | Test loss: 0.5624535083770752\n",
      "Epoch: 18060 | Loss: 0.5339393615722656 | Test loss: 0.5624396800994873\n",
      "Epoch: 18070 | Loss: 0.5339276194572449 | Test loss: 0.5624257922172546\n",
      "Epoch: 18080 | Loss: 0.5339159369468689 | Test loss: 0.5624119639396667\n",
      "Epoch: 18090 | Loss: 0.5339040756225586 | Test loss: 0.5623981356620789\n",
      "Epoch: 18100 | Loss: 0.5338923335075378 | Test loss: 0.5623842477798462\n",
      "Epoch: 18110 | Loss: 0.5338805913925171 | Test loss: 0.5623704195022583\n",
      "Epoch: 18120 | Loss: 0.5338687896728516 | Test loss: 0.5623565912246704\n",
      "Epoch: 18130 | Loss: 0.5338570475578308 | Test loss: 0.5623427629470825\n",
      "Epoch: 18140 | Loss: 0.5338453650474548 | Test loss: 0.5623289346694946\n",
      "Epoch: 18150 | Loss: 0.5338335037231445 | Test loss: 0.562315046787262\n",
      "Epoch: 18160 | Loss: 0.5338217616081238 | Test loss: 0.5623011589050293\n",
      "Epoch: 18170 | Loss: 0.5338100790977478 | Test loss: 0.5622873306274414\n",
      "Epoch: 18180 | Loss: 0.5337982773780823 | Test loss: 0.5622735023498535\n",
      "Epoch: 18190 | Loss: 0.5337864756584167 | Test loss: 0.5622596740722656\n",
      "Epoch: 18200 | Loss: 0.5337747931480408 | Test loss: 0.5622458457946777\n",
      "Epoch: 18210 | Loss: 0.5337629318237305 | Test loss: 0.5622319579124451\n",
      "Epoch: 18220 | Loss: 0.5337512493133545 | Test loss: 0.5622181296348572\n",
      "Epoch: 18230 | Loss: 0.5337395071983337 | Test loss: 0.5622043013572693\n",
      "Epoch: 18240 | Loss: 0.533727765083313 | Test loss: 0.5621904730796814\n",
      "Epoch: 18250 | Loss: 0.5337159633636475 | Test loss: 0.5621766448020935\n",
      "Epoch: 18260 | Loss: 0.5337042212486267 | Test loss: 0.5621628165245056\n",
      "Epoch: 18270 | Loss: 0.5336924195289612 | Test loss: 0.5621488690376282\n",
      "Epoch: 18280 | Loss: 0.5336806774139404 | Test loss: 0.5621350407600403\n",
      "Epoch: 18290 | Loss: 0.5336689352989197 | Test loss: 0.5621212124824524\n",
      "Epoch: 18300 | Loss: 0.5336571931838989 | Test loss: 0.5621073246002197\n",
      "Epoch: 18310 | Loss: 0.5336453318595886 | Test loss: 0.5620934963226318\n",
      "Epoch: 18320 | Loss: 0.5336336493492126 | Test loss: 0.562079668045044\n",
      "Epoch: 18330 | Loss: 0.5336219072341919 | Test loss: 0.5620657801628113\n",
      "Epoch: 18340 | Loss: 0.5336101055145264 | Test loss: 0.5620519518852234\n",
      "Epoch: 18350 | Loss: 0.5335983633995056 | Test loss: 0.5620381236076355\n",
      "Epoch: 18360 | Loss: 0.5335866212844849 | Test loss: 0.5620242357254028\n",
      "Epoch: 18370 | Loss: 0.5335748195648193 | Test loss: 0.5620104670524597\n",
      "Epoch: 18380 | Loss: 0.5335630774497986 | Test loss: 0.561996579170227\n",
      "Epoch: 18390 | Loss: 0.5335513353347778 | Test loss: 0.5619827508926392\n",
      "Epoch: 18400 | Loss: 0.5335395932197571 | Test loss: 0.5619689226150513\n",
      "Epoch: 18410 | Loss: 0.5335277915000916 | Test loss: 0.5619550943374634\n",
      "Epoch: 18420 | Loss: 0.5335160493850708 | Test loss: 0.5619411468505859\n",
      "Epoch: 18430 | Loss: 0.53350430727005 | Test loss: 0.561927318572998\n",
      "Epoch: 18440 | Loss: 0.5334925055503845 | Test loss: 0.5619134902954102\n",
      "Epoch: 18450 | Loss: 0.5334807634353638 | Test loss: 0.5618996620178223\n",
      "Epoch: 18460 | Loss: 0.533469021320343 | Test loss: 0.5618858337402344\n",
      "Epoch: 18470 | Loss: 0.5334572196006775 | Test loss: 0.5618719458580017\n",
      "Epoch: 18480 | Loss: 0.5334455370903015 | Test loss: 0.5618581175804138\n",
      "Epoch: 18490 | Loss: 0.5334336757659912 | Test loss: 0.5618442893028259\n",
      "Epoch: 18500 | Loss: 0.5334219336509705 | Test loss: 0.561830461025238\n",
      "Epoch: 18510 | Loss: 0.5334101915359497 | Test loss: 0.5618166327476501\n",
      "Epoch: 18520 | Loss: 0.5333983898162842 | Test loss: 0.5618028044700623\n",
      "Epoch: 18530 | Loss: 0.5333867073059082 | Test loss: 0.5617889165878296\n",
      "Epoch: 18540 | Loss: 0.5333749055862427 | Test loss: 0.5617750287055969\n",
      "Epoch: 18550 | Loss: 0.5333631634712219 | Test loss: 0.561761200428009\n",
      "Epoch: 18560 | Loss: 0.5333513617515564 | Test loss: 0.5617473125457764\n",
      "Epoch: 18570 | Loss: 0.5333396196365356 | Test loss: 0.5617335438728333\n",
      "Epoch: 18580 | Loss: 0.5333278775215149 | Test loss: 0.5617196559906006\n",
      "Epoch: 18590 | Loss: 0.5333160758018494 | Test loss: 0.5617057681083679\n",
      "Epoch: 18600 | Loss: 0.5333043932914734 | Test loss: 0.56169193983078\n",
      "Epoch: 18610 | Loss: 0.5332925915718079 | Test loss: 0.5616781115531921\n",
      "Epoch: 18620 | Loss: 0.5332808494567871 | Test loss: 0.5616642832756042\n",
      "Epoch: 18630 | Loss: 0.5332691073417664 | Test loss: 0.5616504549980164\n",
      "Epoch: 18640 | Loss: 0.5332573056221008 | Test loss: 0.5616365671157837\n",
      "Epoch: 18650 | Loss: 0.5332455635070801 | Test loss: 0.5616227388381958\n",
      "Epoch: 18660 | Loss: 0.5332337617874146 | Test loss: 0.5616089105606079\n",
      "Epoch: 18670 | Loss: 0.5332220196723938 | Test loss: 0.56159508228302\n",
      "Epoch: 18680 | Loss: 0.533210277557373 | Test loss: 0.5615811944007874\n",
      "Epoch: 18690 | Loss: 0.5331985354423523 | Test loss: 0.5615673661231995\n",
      "Epoch: 18700 | Loss: 0.5331867337226868 | Test loss: 0.5615534782409668\n",
      "Epoch: 18710 | Loss: 0.533174991607666 | Test loss: 0.5615396499633789\n",
      "Epoch: 18720 | Loss: 0.5331631898880005 | Test loss: 0.561525821685791\n",
      "Epoch: 18730 | Loss: 0.5331514477729797 | Test loss: 0.5615119934082031\n",
      "Epoch: 18740 | Loss: 0.533139705657959 | Test loss: 0.5614981055259705\n",
      "Epoch: 18750 | Loss: 0.5331279635429382 | Test loss: 0.5614842772483826\n",
      "Epoch: 18760 | Loss: 0.5331161618232727 | Test loss: 0.5614704489707947\n",
      "Epoch: 18770 | Loss: 0.533104419708252 | Test loss: 0.5614566206932068\n",
      "Epoch: 18780 | Loss: 0.5330926775932312 | Test loss: 0.5614427924156189\n",
      "Epoch: 18790 | Loss: 0.5330808758735657 | Test loss: 0.5614289045333862\n",
      "Epoch: 18800 | Loss: 0.5330691337585449 | Test loss: 0.5614150762557983\n",
      "Epoch: 18810 | Loss: 0.5330573916435242 | Test loss: 0.5614011883735657\n",
      "Epoch: 18820 | Loss: 0.5330457091331482 | Test loss: 0.5613873600959778\n",
      "Epoch: 18830 | Loss: 0.5330338478088379 | Test loss: 0.5613735318183899\n",
      "Epoch: 18840 | Loss: 0.5330221056938171 | Test loss: 0.5613596439361572\n",
      "Epoch: 18850 | Loss: 0.5330104231834412 | Test loss: 0.5613458156585693\n",
      "Epoch: 18860 | Loss: 0.5329985618591309 | Test loss: 0.5613319873809814\n",
      "Epoch: 18870 | Loss: 0.5329868197441101 | Test loss: 0.5613180994987488\n",
      "Epoch: 18880 | Loss: 0.5329751372337341 | Test loss: 0.5613042712211609\n",
      "Epoch: 18890 | Loss: 0.5329632759094238 | Test loss: 0.561290442943573\n",
      "Epoch: 18900 | Loss: 0.5329515337944031 | Test loss: 0.5612765550613403\n",
      "Epoch: 18910 | Loss: 0.5329397916793823 | Test loss: 0.5612627267837524\n",
      "Epoch: 18920 | Loss: 0.5329279899597168 | Test loss: 0.5612488985061646\n",
      "Epoch: 18930 | Loss: 0.532916247844696 | Test loss: 0.5612350702285767\n",
      "Epoch: 18940 | Loss: 0.5329045653343201 | Test loss: 0.5612212419509888\n",
      "Epoch: 18950 | Loss: 0.5328927040100098 | Test loss: 0.5612073540687561\n",
      "Epoch: 18960 | Loss: 0.532880961894989 | Test loss: 0.5611934661865234\n",
      "Epoch: 18970 | Loss: 0.532869279384613 | Test loss: 0.5611796379089355\n",
      "Epoch: 18980 | Loss: 0.5328574776649475 | Test loss: 0.5611658096313477\n",
      "Epoch: 18990 | Loss: 0.532845675945282 | Test loss: 0.5611519813537598\n",
      "Epoch: 19000 | Loss: 0.532833993434906 | Test loss: 0.5611381530761719\n",
      "Epoch: 19010 | Loss: 0.5328221321105957 | Test loss: 0.5611242651939392\n",
      "Epoch: 19020 | Loss: 0.5328104496002197 | Test loss: 0.5611104369163513\n",
      "Epoch: 19030 | Loss: 0.532798707485199 | Test loss: 0.5610966086387634\n",
      "Epoch: 19040 | Loss: 0.5327869653701782 | Test loss: 0.5610827803611755\n",
      "Epoch: 19050 | Loss: 0.5327751636505127 | Test loss: 0.5610689520835876\n",
      "Epoch: 19060 | Loss: 0.5327634215354919 | Test loss: 0.5610551238059998\n",
      "Epoch: 19070 | Loss: 0.5327516198158264 | Test loss: 0.5610411763191223\n",
      "Epoch: 19080 | Loss: 0.5327398777008057 | Test loss: 0.5610273480415344\n",
      "Epoch: 19090 | Loss: 0.5327281355857849 | Test loss: 0.5610135197639465\n",
      "Epoch: 19100 | Loss: 0.5327163934707642 | Test loss: 0.5609996318817139\n",
      "Epoch: 19110 | Loss: 0.5327045321464539 | Test loss: 0.560985803604126\n",
      "Epoch: 19120 | Loss: 0.5326928496360779 | Test loss: 0.5609719753265381\n",
      "Epoch: 19130 | Loss: 0.5326811075210571 | Test loss: 0.5609580874443054\n",
      "Epoch: 19140 | Loss: 0.5326693058013916 | Test loss: 0.5609442591667175\n",
      "Epoch: 19150 | Loss: 0.5326575636863708 | Test loss: 0.5609304308891296\n",
      "Epoch: 19160 | Loss: 0.5326458215713501 | Test loss: 0.560916543006897\n",
      "Epoch: 19170 | Loss: 0.5326340198516846 | Test loss: 0.5609027743339539\n",
      "Epoch: 19180 | Loss: 0.5326222777366638 | Test loss: 0.5608888864517212\n",
      "Epoch: 19190 | Loss: 0.5326105356216431 | Test loss: 0.5608750581741333\n",
      "Epoch: 19200 | Loss: 0.5325987935066223 | Test loss: 0.5608612298965454\n",
      "Epoch: 19210 | Loss: 0.5325869917869568 | Test loss: 0.5608474016189575\n",
      "Epoch: 19220 | Loss: 0.532575249671936 | Test loss: 0.5608334541320801\n",
      "Epoch: 19230 | Loss: 0.5325635075569153 | Test loss: 0.5608196258544922\n",
      "Epoch: 19240 | Loss: 0.5325517058372498 | Test loss: 0.5608057975769043\n",
      "Epoch: 19250 | Loss: 0.532539963722229 | Test loss: 0.5607919692993164\n",
      "Epoch: 19260 | Loss: 0.5325282216072083 | Test loss: 0.5607781410217285\n",
      "Epoch: 19270 | Loss: 0.5325164198875427 | Test loss: 0.5607642531394958\n",
      "Epoch: 19280 | Loss: 0.5325047373771667 | Test loss: 0.560750424861908\n",
      "Epoch: 19290 | Loss: 0.5324928760528564 | Test loss: 0.5607365965843201\n",
      "Epoch: 19300 | Loss: 0.5324811339378357 | Test loss: 0.5607227683067322\n",
      "Epoch: 19310 | Loss: 0.5324693918228149 | Test loss: 0.5607089400291443\n",
      "Epoch: 19320 | Loss: 0.5324575901031494 | Test loss: 0.5606951117515564\n",
      "Epoch: 19330 | Loss: 0.5324459075927734 | Test loss: 0.5606812238693237\n",
      "Epoch: 19340 | Loss: 0.5324341058731079 | Test loss: 0.5606673359870911\n",
      "Epoch: 19350 | Loss: 0.5324223637580872 | Test loss: 0.5606535077095032\n",
      "Epoch: 19360 | Loss: 0.5324105620384216 | Test loss: 0.5606396198272705\n",
      "Epoch: 19370 | Loss: 0.5323988199234009 | Test loss: 0.5606258511543274\n",
      "Epoch: 19380 | Loss: 0.5323870778083801 | Test loss: 0.5606119632720947\n",
      "Epoch: 19390 | Loss: 0.5323752760887146 | Test loss: 0.5605980753898621\n",
      "Epoch: 19400 | Loss: 0.5323635935783386 | Test loss: 0.5605842471122742\n",
      "Epoch: 19410 | Loss: 0.5323517918586731 | Test loss: 0.5605704188346863\n",
      "Epoch: 19420 | Loss: 0.5323400497436523 | Test loss: 0.5605565905570984\n",
      "Epoch: 19430 | Loss: 0.5323283076286316 | Test loss: 0.5605427622795105\n",
      "Epoch: 19440 | Loss: 0.5323165059089661 | Test loss: 0.5605288743972778\n",
      "Epoch: 19450 | Loss: 0.5323047637939453 | Test loss: 0.5605150461196899\n",
      "Epoch: 19460 | Loss: 0.5322929620742798 | Test loss: 0.560501217842102\n",
      "Epoch: 19470 | Loss: 0.532281219959259 | Test loss: 0.5604873895645142\n",
      "Epoch: 19480 | Loss: 0.5322694778442383 | Test loss: 0.5604735016822815\n",
      "Epoch: 19490 | Loss: 0.5322577357292175 | Test loss: 0.5604596734046936\n",
      "Epoch: 19500 | Loss: 0.532245934009552 | Test loss: 0.5604457855224609\n",
      "Epoch: 19510 | Loss: 0.5322341918945312 | Test loss: 0.560431957244873\n",
      "Epoch: 19520 | Loss: 0.5322223901748657 | Test loss: 0.5604181289672852\n",
      "Epoch: 19530 | Loss: 0.532210648059845 | Test loss: 0.5604043006896973\n",
      "Epoch: 19540 | Loss: 0.5321989059448242 | Test loss: 0.5603904128074646\n",
      "Epoch: 19550 | Loss: 0.5321871638298035 | Test loss: 0.5603765845298767\n",
      "Epoch: 19560 | Loss: 0.5321753621101379 | Test loss: 0.5603627562522888\n",
      "Epoch: 19570 | Loss: 0.5321636199951172 | Test loss: 0.5603489279747009\n",
      "Epoch: 19580 | Loss: 0.5321518778800964 | Test loss: 0.560335099697113\n",
      "Epoch: 19590 | Loss: 0.5321400761604309 | Test loss: 0.5603212118148804\n",
      "Epoch: 19600 | Loss: 0.5321283340454102 | Test loss: 0.5603073835372925\n",
      "Epoch: 19610 | Loss: 0.5321165919303894 | Test loss: 0.5602934956550598\n",
      "Epoch: 19620 | Loss: 0.5321049094200134 | Test loss: 0.5602796673774719\n",
      "Epoch: 19630 | Loss: 0.5320930480957031 | Test loss: 0.560265839099884\n",
      "Epoch: 19640 | Loss: 0.5320813059806824 | Test loss: 0.5602519512176514\n",
      "Epoch: 19650 | Loss: 0.5320696234703064 | Test loss: 0.5602381229400635\n",
      "Epoch: 19660 | Loss: 0.5320577621459961 | Test loss: 0.5602242946624756\n",
      "Epoch: 19670 | Loss: 0.5320460200309753 | Test loss: 0.5602104067802429\n",
      "Epoch: 19680 | Loss: 0.5320343375205994 | Test loss: 0.560196578502655\n",
      "Epoch: 19690 | Loss: 0.5320224761962891 | Test loss: 0.5601827502250671\n",
      "Epoch: 19700 | Loss: 0.5320107340812683 | Test loss: 0.5601688623428345\n",
      "Epoch: 19710 | Loss: 0.5319989919662476 | Test loss: 0.5601550340652466\n",
      "Epoch: 19720 | Loss: 0.531987190246582 | Test loss: 0.5601412057876587\n",
      "Epoch: 19730 | Loss: 0.5319754481315613 | Test loss: 0.5601273775100708\n",
      "Epoch: 19740 | Loss: 0.5319637656211853 | Test loss: 0.5601135492324829\n",
      "Epoch: 19750 | Loss: 0.531951904296875 | Test loss: 0.5600996613502502\n",
      "Epoch: 19760 | Loss: 0.5319401621818542 | Test loss: 0.5600857734680176\n",
      "Epoch: 19770 | Loss: 0.5319284796714783 | Test loss: 0.5600719451904297\n",
      "Epoch: 19780 | Loss: 0.5319166779518127 | Test loss: 0.5600581169128418\n",
      "Epoch: 19790 | Loss: 0.5319048762321472 | Test loss: 0.5600442886352539\n",
      "Epoch: 19800 | Loss: 0.5318931937217712 | Test loss: 0.560030460357666\n",
      "Epoch: 19810 | Loss: 0.5318813323974609 | Test loss: 0.5600165724754333\n",
      "Epoch: 19820 | Loss: 0.531869649887085 | Test loss: 0.5600027441978455\n",
      "Epoch: 19830 | Loss: 0.5318579077720642 | Test loss: 0.5599889159202576\n",
      "Epoch: 19840 | Loss: 0.5318461656570435 | Test loss: 0.5599750876426697\n",
      "Epoch: 19850 | Loss: 0.5318343639373779 | Test loss: 0.5599612593650818\n",
      "Epoch: 19860 | Loss: 0.5318226218223572 | Test loss: 0.5599474310874939\n",
      "Epoch: 19870 | Loss: 0.5318108201026917 | Test loss: 0.5599334836006165\n",
      "Epoch: 19880 | Loss: 0.5317990779876709 | Test loss: 0.5599196553230286\n",
      "Epoch: 19890 | Loss: 0.5317873358726501 | Test loss: 0.5599058270454407\n",
      "Epoch: 19900 | Loss: 0.5317755937576294 | Test loss: 0.559891939163208\n",
      "Epoch: 19910 | Loss: 0.5317637324333191 | Test loss: 0.5598781108856201\n",
      "Epoch: 19920 | Loss: 0.5317520499229431 | Test loss: 0.5598642826080322\n",
      "Epoch: 19930 | Loss: 0.5317403078079224 | Test loss: 0.5598503947257996\n",
      "Epoch: 19940 | Loss: 0.5317285060882568 | Test loss: 0.5598365664482117\n",
      "Epoch: 19950 | Loss: 0.5317167639732361 | Test loss: 0.5598227381706238\n",
      "Epoch: 19960 | Loss: 0.5317050218582153 | Test loss: 0.5598088502883911\n",
      "Epoch: 19970 | Loss: 0.5316932201385498 | Test loss: 0.559795081615448\n",
      "Epoch: 19980 | Loss: 0.531681478023529 | Test loss: 0.5597811937332153\n",
      "Epoch: 19990 | Loss: 0.5316697359085083 | Test loss: 0.5597673654556274\n",
      "Epoch: 20000 | Loss: 0.5316579937934875 | Test loss: 0.5597535371780396\n",
      "Epoch: 20010 | Loss: 0.531646192073822 | Test loss: 0.5597397089004517\n",
      "Epoch: 20020 | Loss: 0.5316344499588013 | Test loss: 0.5597257614135742\n",
      "Epoch: 20030 | Loss: 0.5316227078437805 | Test loss: 0.5597119331359863\n",
      "Epoch: 20040 | Loss: 0.531610906124115 | Test loss: 0.5596981048583984\n",
      "Epoch: 20050 | Loss: 0.5315991640090942 | Test loss: 0.5596842765808105\n",
      "Epoch: 20060 | Loss: 0.5315874218940735 | Test loss: 0.5596704483032227\n",
      "Epoch: 20070 | Loss: 0.531575620174408 | Test loss: 0.55965656042099\n",
      "Epoch: 20080 | Loss: 0.531563937664032 | Test loss: 0.5596427321434021\n",
      "Epoch: 20090 | Loss: 0.5315520763397217 | Test loss: 0.5596289038658142\n",
      "Epoch: 20100 | Loss: 0.5315403342247009 | Test loss: 0.5596150755882263\n",
      "Epoch: 20110 | Loss: 0.5315285921096802 | Test loss: 0.5596012473106384\n",
      "Epoch: 20120 | Loss: 0.5315167903900146 | Test loss: 0.5595874190330505\n",
      "Epoch: 20130 | Loss: 0.5315051078796387 | Test loss: 0.5595735311508179\n",
      "Epoch: 20140 | Loss: 0.5314933061599731 | Test loss: 0.5595596432685852\n",
      "Epoch: 20150 | Loss: 0.5314815640449524 | Test loss: 0.5595458149909973\n",
      "Epoch: 20160 | Loss: 0.5314697623252869 | Test loss: 0.5595319271087646\n",
      "Epoch: 20170 | Loss: 0.5314580202102661 | Test loss: 0.5595181584358215\n",
      "Epoch: 20180 | Loss: 0.5314462780952454 | Test loss: 0.5595042705535889\n",
      "Epoch: 20190 | Loss: 0.5314344763755798 | Test loss: 0.5594903826713562\n",
      "Epoch: 20200 | Loss: 0.5314227938652039 | Test loss: 0.5594765543937683\n",
      "Epoch: 20210 | Loss: 0.5314109921455383 | Test loss: 0.5594627261161804\n",
      "Epoch: 20220 | Loss: 0.5313992500305176 | Test loss: 0.5594488978385925\n",
      "Epoch: 20230 | Loss: 0.5313875079154968 | Test loss: 0.5594350695610046\n",
      "Epoch: 20240 | Loss: 0.5313757061958313 | Test loss: 0.559421181678772\n",
      "Epoch: 20250 | Loss: 0.5313639640808105 | Test loss: 0.5594073534011841\n",
      "Epoch: 20260 | Loss: 0.531352162361145 | Test loss: 0.5593935251235962\n",
      "Epoch: 20270 | Loss: 0.5313404202461243 | Test loss: 0.5593796968460083\n",
      "Epoch: 20280 | Loss: 0.5313286781311035 | Test loss: 0.5593658089637756\n",
      "Epoch: 20290 | Loss: 0.5313169360160828 | Test loss: 0.5593519806861877\n",
      "Epoch: 20300 | Loss: 0.5313051342964172 | Test loss: 0.5593380928039551\n",
      "Epoch: 20310 | Loss: 0.5312933921813965 | Test loss: 0.5593242645263672\n",
      "Epoch: 20320 | Loss: 0.531281590461731 | Test loss: 0.5593104362487793\n",
      "Epoch: 20330 | Loss: 0.5312698483467102 | Test loss: 0.5592966079711914\n",
      "Epoch: 20340 | Loss: 0.5312581062316895 | Test loss: 0.5592827200889587\n",
      "Epoch: 20350 | Loss: 0.5312463641166687 | Test loss: 0.5592688918113708\n",
      "Epoch: 20360 | Loss: 0.5312345623970032 | Test loss: 0.559255063533783\n",
      "Epoch: 20370 | Loss: 0.5312228202819824 | Test loss: 0.5592412352561951\n",
      "Epoch: 20380 | Loss: 0.5312110781669617 | Test loss: 0.5592274069786072\n",
      "Epoch: 20390 | Loss: 0.5311992764472961 | Test loss: 0.5592135190963745\n",
      "Epoch: 20400 | Loss: 0.5311875343322754 | Test loss: 0.5591996908187866\n",
      "Epoch: 20410 | Loss: 0.5311757922172546 | Test loss: 0.559185802936554\n",
      "Epoch: 20420 | Loss: 0.5311641097068787 | Test loss: 0.5591719746589661\n",
      "Epoch: 20430 | Loss: 0.5311522483825684 | Test loss: 0.5591581463813782\n",
      "Epoch: 20440 | Loss: 0.5311405062675476 | Test loss: 0.5591442584991455\n",
      "Epoch: 20450 | Loss: 0.5311288237571716 | Test loss: 0.5591304302215576\n",
      "Epoch: 20460 | Loss: 0.5311169624328613 | Test loss: 0.5591166019439697\n",
      "Epoch: 20470 | Loss: 0.5311052203178406 | Test loss: 0.5591027140617371\n",
      "Epoch: 20480 | Loss: 0.5310935378074646 | Test loss: 0.5590888857841492\n",
      "Epoch: 20490 | Loss: 0.5310816764831543 | Test loss: 0.5590750575065613\n",
      "Epoch: 20500 | Loss: 0.5310699343681335 | Test loss: 0.5590611696243286\n",
      "Epoch: 20510 | Loss: 0.5310581922531128 | Test loss: 0.5590473413467407\n",
      "Epoch: 20520 | Loss: 0.5310463905334473 | Test loss: 0.5590335130691528\n",
      "Epoch: 20530 | Loss: 0.5310346484184265 | Test loss: 0.5590196847915649\n",
      "Epoch: 20540 | Loss: 0.5310229659080505 | Test loss: 0.559005856513977\n",
      "Epoch: 20550 | Loss: 0.5310111045837402 | Test loss: 0.5589919686317444\n",
      "Epoch: 20560 | Loss: 0.5309993624687195 | Test loss: 0.5589780807495117\n",
      "Epoch: 20570 | Loss: 0.5309876799583435 | Test loss: 0.5589642524719238\n",
      "Epoch: 20580 | Loss: 0.530975878238678 | Test loss: 0.5589504241943359\n",
      "Epoch: 20590 | Loss: 0.5309640765190125 | Test loss: 0.558936595916748\n",
      "Epoch: 20600 | Loss: 0.5309523940086365 | Test loss: 0.5589227676391602\n",
      "Epoch: 20610 | Loss: 0.5309405326843262 | Test loss: 0.5589088797569275\n",
      "Epoch: 20620 | Loss: 0.5309288501739502 | Test loss: 0.5588950514793396\n",
      "Epoch: 20630 | Loss: 0.5309171080589294 | Test loss: 0.5588812232017517\n",
      "Epoch: 20640 | Loss: 0.5309053659439087 | Test loss: 0.5588673949241638\n",
      "Epoch: 20650 | Loss: 0.5308935642242432 | Test loss: 0.5588535666465759\n",
      "Epoch: 20660 | Loss: 0.5308818221092224 | Test loss: 0.558839738368988\n",
      "Epoch: 20670 | Loss: 0.5308700203895569 | Test loss: 0.5588257908821106\n",
      "Epoch: 20680 | Loss: 0.5308582782745361 | Test loss: 0.5588119626045227\n",
      "Epoch: 20690 | Loss: 0.5308465361595154 | Test loss: 0.5587981343269348\n",
      "Epoch: 20700 | Loss: 0.5308347940444946 | Test loss: 0.5587842464447021\n",
      "Epoch: 20710 | Loss: 0.5308229327201843 | Test loss: 0.5587704181671143\n",
      "Epoch: 20720 | Loss: 0.5308112502098083 | Test loss: 0.5587565898895264\n",
      "Epoch: 20730 | Loss: 0.5307995080947876 | Test loss: 0.5587427020072937\n",
      "Epoch: 20740 | Loss: 0.5307877063751221 | Test loss: 0.5587288737297058\n",
      "Epoch: 20750 | Loss: 0.5307759642601013 | Test loss: 0.5587150454521179\n",
      "Epoch: 20760 | Loss: 0.5307642221450806 | Test loss: 0.5587011575698853\n",
      "Epoch: 20770 | Loss: 0.530752420425415 | Test loss: 0.5586873888969421\n",
      "Epoch: 20780 | Loss: 0.5307406783103943 | Test loss: 0.5586735010147095\n",
      "Epoch: 20790 | Loss: 0.5307289361953735 | Test loss: 0.5586596727371216\n",
      "Epoch: 20800 | Loss: 0.5307171940803528 | Test loss: 0.5586458444595337\n",
      "Epoch: 20810 | Loss: 0.5307053923606873 | Test loss: 0.5586320161819458\n",
      "Epoch: 20820 | Loss: 0.5306936502456665 | Test loss: 0.5586180686950684\n",
      "Epoch: 20830 | Loss: 0.5306819081306458 | Test loss: 0.5586042404174805\n",
      "Epoch: 20840 | Loss: 0.5306701064109802 | Test loss: 0.5585904121398926\n",
      "Epoch: 20850 | Loss: 0.5306583642959595 | Test loss: 0.5585765838623047\n",
      "Epoch: 20860 | Loss: 0.5306466221809387 | Test loss: 0.5585627555847168\n",
      "Epoch: 20870 | Loss: 0.5306348204612732 | Test loss: 0.5585488677024841\n",
      "Epoch: 20880 | Loss: 0.5306231379508972 | Test loss: 0.5585350394248962\n",
      "Epoch: 20890 | Loss: 0.5306112766265869 | Test loss: 0.5585212111473083\n",
      "Epoch: 20900 | Loss: 0.5305995345115662 | Test loss: 0.5585073828697205\n",
      "Epoch: 20910 | Loss: 0.5305877923965454 | Test loss: 0.5584935545921326\n",
      "Epoch: 20920 | Loss: 0.5305759906768799 | Test loss: 0.5584797263145447\n",
      "Epoch: 20930 | Loss: 0.5305643081665039 | Test loss: 0.558465838432312\n",
      "Epoch: 20940 | Loss: 0.5305525064468384 | Test loss: 0.5584519505500793\n",
      "Epoch: 20950 | Loss: 0.5305407643318176 | Test loss: 0.5584381222724915\n",
      "Epoch: 20960 | Loss: 0.5305289626121521 | Test loss: 0.5584242343902588\n",
      "Epoch: 20970 | Loss: 0.5305172204971313 | Test loss: 0.5584104657173157\n",
      "Epoch: 20980 | Loss: 0.5305054783821106 | Test loss: 0.558396577835083\n",
      "Epoch: 20990 | Loss: 0.5304936766624451 | Test loss: 0.5583826899528503\n",
      "Epoch: 21000 | Loss: 0.5304819941520691 | Test loss: 0.5583688616752625\n",
      "Epoch: 21010 | Loss: 0.5304701924324036 | Test loss: 0.5583550333976746\n",
      "Epoch: 21020 | Loss: 0.5304584503173828 | Test loss: 0.5583412051200867\n",
      "Epoch: 21030 | Loss: 0.5304467082023621 | Test loss: 0.5583273768424988\n",
      "Epoch: 21040 | Loss: 0.5304349064826965 | Test loss: 0.5583134889602661\n",
      "Epoch: 21050 | Loss: 0.5304231643676758 | Test loss: 0.5582996606826782\n",
      "Epoch: 21060 | Loss: 0.5304113626480103 | Test loss: 0.5582858324050903\n",
      "Epoch: 21070 | Loss: 0.5303996205329895 | Test loss: 0.5582720041275024\n",
      "Epoch: 21080 | Loss: 0.5303878784179688 | Test loss: 0.5582581162452698\n",
      "Epoch: 21090 | Loss: 0.530376136302948 | Test loss: 0.5582442879676819\n",
      "Epoch: 21100 | Loss: 0.5303643345832825 | Test loss: 0.5582304000854492\n",
      "Epoch: 21110 | Loss: 0.5303525924682617 | Test loss: 0.5582165718078613\n",
      "Epoch: 21120 | Loss: 0.5303407907485962 | Test loss: 0.5582027435302734\n",
      "Epoch: 21130 | Loss: 0.5303290486335754 | Test loss: 0.5581889152526855\n",
      "Epoch: 21140 | Loss: 0.5303173065185547 | Test loss: 0.5581750273704529\n",
      "Epoch: 21150 | Loss: 0.5303055644035339 | Test loss: 0.558161199092865\n",
      "Epoch: 21160 | Loss: 0.5302937626838684 | Test loss: 0.5581473708152771\n",
      "Epoch: 21170 | Loss: 0.5302820205688477 | Test loss: 0.5581335425376892\n",
      "Epoch: 21180 | Loss: 0.5302702784538269 | Test loss: 0.5581197142601013\n",
      "Epoch: 21190 | Loss: 0.5302584767341614 | Test loss: 0.5581058263778687\n",
      "Epoch: 21200 | Loss: 0.5302467346191406 | Test loss: 0.5580919981002808\n",
      "Epoch: 21210 | Loss: 0.5302349925041199 | Test loss: 0.5580781102180481\n",
      "Epoch: 21220 | Loss: 0.5302233099937439 | Test loss: 0.5580642819404602\n",
      "Epoch: 21230 | Loss: 0.5302114486694336 | Test loss: 0.5580504536628723\n",
      "Epoch: 21240 | Loss: 0.5301997065544128 | Test loss: 0.5580365657806396\n",
      "Epoch: 21250 | Loss: 0.5301880240440369 | Test loss: 0.5580227375030518\n",
      "Epoch: 21260 | Loss: 0.5301761627197266 | Test loss: 0.5580089092254639\n",
      "Epoch: 21270 | Loss: 0.5301644206047058 | Test loss: 0.5579950213432312\n",
      "Epoch: 21280 | Loss: 0.5301527380943298 | Test loss: 0.5579811930656433\n",
      "Epoch: 21290 | Loss: 0.5301408767700195 | Test loss: 0.5579673647880554\n",
      "Epoch: 21300 | Loss: 0.5301291346549988 | Test loss: 0.5579534769058228\n",
      "Epoch: 21310 | Loss: 0.530117392539978 | Test loss: 0.5579396486282349\n",
      "Epoch: 21320 | Loss: 0.5301055908203125 | Test loss: 0.557925820350647\n",
      "Epoch: 21330 | Loss: 0.5300938487052917 | Test loss: 0.5579119920730591\n",
      "Epoch: 21340 | Loss: 0.5300821661949158 | Test loss: 0.5578981637954712\n",
      "Epoch: 21350 | Loss: 0.5300703048706055 | Test loss: 0.5578842759132385\n",
      "Epoch: 21360 | Loss: 0.5300585627555847 | Test loss: 0.5578703880310059\n",
      "Epoch: 21370 | Loss: 0.5300468802452087 | Test loss: 0.557856559753418\n",
      "Epoch: 21380 | Loss: 0.5300350785255432 | Test loss: 0.5578427314758301\n",
      "Epoch: 21390 | Loss: 0.5300232768058777 | Test loss: 0.5578289031982422\n",
      "Epoch: 21400 | Loss: 0.5300115942955017 | Test loss: 0.5578150749206543\n",
      "Epoch: 21410 | Loss: 0.5299997329711914 | Test loss: 0.5578011870384216\n",
      "Epoch: 21420 | Loss: 0.5299880504608154 | Test loss: 0.5577873587608337\n",
      "Epoch: 21430 | Loss: 0.5299763083457947 | Test loss: 0.5577735304832458\n",
      "Epoch: 21440 | Loss: 0.5299645662307739 | Test loss: 0.557759702205658\n",
      "Epoch: 21450 | Loss: 0.5299527645111084 | Test loss: 0.5577458739280701\n",
      "Epoch: 21460 | Loss: 0.5299410223960876 | Test loss: 0.5577320456504822\n",
      "Epoch: 21470 | Loss: 0.5299292206764221 | Test loss: 0.5577180981636047\n",
      "Epoch: 21480 | Loss: 0.5299174785614014 | Test loss: 0.5577042698860168\n",
      "Epoch: 21490 | Loss: 0.5299057364463806 | Test loss: 0.557690441608429\n",
      "Epoch: 21500 | Loss: 0.5298939943313599 | Test loss: 0.5576765537261963\n",
      "Epoch: 21510 | Loss: 0.5298821330070496 | Test loss: 0.5576627254486084\n",
      "Epoch: 21520 | Loss: 0.5298704504966736 | Test loss: 0.5576488971710205\n",
      "Epoch: 21530 | Loss: 0.5298587083816528 | Test loss: 0.5576350092887878\n",
      "Epoch: 21540 | Loss: 0.5298469066619873 | Test loss: 0.5576211810112\n",
      "Epoch: 21550 | Loss: 0.5298351645469666 | Test loss: 0.5576073527336121\n",
      "Epoch: 21560 | Loss: 0.5298234224319458 | Test loss: 0.5575934648513794\n",
      "Epoch: 21570 | Loss: 0.5298116207122803 | Test loss: 0.5575796961784363\n",
      "Epoch: 21580 | Loss: 0.5297998785972595 | Test loss: 0.5575658082962036\n",
      "Epoch: 21590 | Loss: 0.5297881364822388 | Test loss: 0.5575519800186157\n",
      "Epoch: 21600 | Loss: 0.529776394367218 | Test loss: 0.5575381517410278\n",
      "Epoch: 21610 | Loss: 0.5297645926475525 | Test loss: 0.5575243234634399\n",
      "Epoch: 21620 | Loss: 0.5297528505325317 | Test loss: 0.5575103759765625\n",
      "Epoch: 21630 | Loss: 0.529741108417511 | Test loss: 0.5574965476989746\n",
      "Epoch: 21640 | Loss: 0.5297293066978455 | Test loss: 0.5574827194213867\n",
      "Epoch: 21650 | Loss: 0.5297175645828247 | Test loss: 0.5574688911437988\n",
      "Epoch: 21660 | Loss: 0.529705822467804 | Test loss: 0.5574550628662109\n",
      "Epoch: 21670 | Loss: 0.5296940207481384 | Test loss: 0.5574411749839783\n",
      "Epoch: 21680 | Loss: 0.5296823382377625 | Test loss: 0.5574273467063904\n",
      "Epoch: 21690 | Loss: 0.5296704769134521 | Test loss: 0.5574135184288025\n",
      "Epoch: 21700 | Loss: 0.5296587347984314 | Test loss: 0.5573996901512146\n",
      "Epoch: 21710 | Loss: 0.5296469926834106 | Test loss: 0.5573858618736267\n",
      "Epoch: 21720 | Loss: 0.5296351909637451 | Test loss: 0.5573720335960388\n",
      "Epoch: 21730 | Loss: 0.5296235084533691 | Test loss: 0.5573581457138062\n",
      "Epoch: 21740 | Loss: 0.5296117067337036 | Test loss: 0.5573442578315735\n",
      "Epoch: 21750 | Loss: 0.5295999646186829 | Test loss: 0.5573304295539856\n",
      "Epoch: 21760 | Loss: 0.5295881628990173 | Test loss: 0.5573165416717529\n",
      "Epoch: 21770 | Loss: 0.5295764207839966 | Test loss: 0.5573027729988098\n",
      "Epoch: 21780 | Loss: 0.5295646786689758 | Test loss: 0.5572888851165771\n",
      "Epoch: 21790 | Loss: 0.5295528769493103 | Test loss: 0.5572749972343445\n",
      "Epoch: 21800 | Loss: 0.5295411944389343 | Test loss: 0.5572611689567566\n",
      "Epoch: 21810 | Loss: 0.5295293927192688 | Test loss: 0.5572473406791687\n",
      "Epoch: 21820 | Loss: 0.529517650604248 | Test loss: 0.5572335124015808\n",
      "Epoch: 21830 | Loss: 0.5295059084892273 | Test loss: 0.5572196841239929\n",
      "Epoch: 21840 | Loss: 0.5294941067695618 | Test loss: 0.5572057962417603\n",
      "Epoch: 21850 | Loss: 0.529482364654541 | Test loss: 0.5571919679641724\n",
      "Epoch: 21860 | Loss: 0.5294705629348755 | Test loss: 0.5571781396865845\n",
      "Epoch: 21870 | Loss: 0.5294588208198547 | Test loss: 0.5571643114089966\n",
      "Epoch: 21880 | Loss: 0.529447078704834 | Test loss: 0.5571504235267639\n",
      "Epoch: 21890 | Loss: 0.5294353365898132 | Test loss: 0.557136595249176\n",
      "Epoch: 21900 | Loss: 0.5294235348701477 | Test loss: 0.5571227073669434\n",
      "Epoch: 21910 | Loss: 0.529411792755127 | Test loss: 0.5571088790893555\n",
      "Epoch: 21920 | Loss: 0.5293999910354614 | Test loss: 0.5570950508117676\n",
      "Epoch: 21930 | Loss: 0.5293882489204407 | Test loss: 0.5570812225341797\n",
      "Epoch: 21940 | Loss: 0.5293765068054199 | Test loss: 0.557067334651947\n",
      "Epoch: 21950 | Loss: 0.5293647646903992 | Test loss: 0.5570535063743591\n",
      "Epoch: 21960 | Loss: 0.5293529629707336 | Test loss: 0.5570396780967712\n",
      "Epoch: 21970 | Loss: 0.5293412208557129 | Test loss: 0.5570258498191833\n",
      "Epoch: 21980 | Loss: 0.5293294787406921 | Test loss: 0.5570120215415955\n",
      "Epoch: 21990 | Loss: 0.5293176770210266 | Test loss: 0.5569981336593628\n",
      "Epoch: 22000 | Loss: 0.5293059349060059 | Test loss: 0.5569843053817749\n",
      "Epoch: 22010 | Loss: 0.5292941927909851 | Test loss: 0.5569704174995422\n",
      "Epoch: 22020 | Loss: 0.5292825102806091 | Test loss: 0.5569565892219543\n",
      "Epoch: 22030 | Loss: 0.5292706489562988 | Test loss: 0.5569427609443665\n",
      "Epoch: 22040 | Loss: 0.5292589068412781 | Test loss: 0.5569288730621338\n",
      "Epoch: 22050 | Loss: 0.5292472243309021 | Test loss: 0.5569150447845459\n",
      "Epoch: 22060 | Loss: 0.5292353630065918 | Test loss: 0.556901216506958\n",
      "Epoch: 22070 | Loss: 0.529223620891571 | Test loss: 0.5568873286247253\n",
      "Epoch: 22080 | Loss: 0.5292119383811951 | Test loss: 0.5568735003471375\n",
      "Epoch: 22090 | Loss: 0.5292000770568848 | Test loss: 0.5568596720695496\n",
      "Epoch: 22100 | Loss: 0.529188334941864 | Test loss: 0.5568457841873169\n",
      "Epoch: 22110 | Loss: 0.5291765928268433 | Test loss: 0.556831955909729\n",
      "Epoch: 22120 | Loss: 0.5291647911071777 | Test loss: 0.5568181276321411\n",
      "Epoch: 22130 | Loss: 0.529153048992157 | Test loss: 0.5568042993545532\n",
      "Epoch: 22140 | Loss: 0.529141366481781 | Test loss: 0.5567904710769653\n",
      "Epoch: 22150 | Loss: 0.5291295051574707 | Test loss: 0.5567765831947327\n",
      "Epoch: 22160 | Loss: 0.52911776304245 | Test loss: 0.5567626953125\n",
      "Epoch: 22170 | Loss: 0.529106080532074 | Test loss: 0.5567488670349121\n",
      "Epoch: 22180 | Loss: 0.5290942788124084 | Test loss: 0.5567350387573242\n",
      "Epoch: 22190 | Loss: 0.5290824770927429 | Test loss: 0.5567212104797363\n",
      "Epoch: 22200 | Loss: 0.5290707945823669 | Test loss: 0.5567073822021484\n",
      "Epoch: 22210 | Loss: 0.5290589332580566 | Test loss: 0.5566934943199158\n",
      "Epoch: 22220 | Loss: 0.5290472507476807 | Test loss: 0.5566796660423279\n",
      "Epoch: 22230 | Loss: 0.5290355086326599 | Test loss: 0.55666583776474\n",
      "Epoch: 22240 | Loss: 0.5290237665176392 | Test loss: 0.5566520094871521\n",
      "Epoch: 22250 | Loss: 0.5290119647979736 | Test loss: 0.5566381812095642\n",
      "Epoch: 22260 | Loss: 0.5290002226829529 | Test loss: 0.5566243529319763\n",
      "Epoch: 22270 | Loss: 0.5289884209632874 | Test loss: 0.5566104054450989\n",
      "Epoch: 22280 | Loss: 0.5289766788482666 | Test loss: 0.556596577167511\n",
      "Epoch: 22290 | Loss: 0.5289649367332458 | Test loss: 0.5565827488899231\n",
      "Epoch: 22300 | Loss: 0.5289531946182251 | Test loss: 0.5565688610076904\n",
      "Epoch: 22310 | Loss: 0.5289413332939148 | Test loss: 0.5565550327301025\n",
      "Epoch: 22320 | Loss: 0.5289296507835388 | Test loss: 0.5565412044525146\n",
      "Epoch: 22330 | Loss: 0.5289179086685181 | Test loss: 0.556527316570282\n",
      "Epoch: 22340 | Loss: 0.5289061069488525 | Test loss: 0.5565134882926941\n",
      "Epoch: 22350 | Loss: 0.5288943648338318 | Test loss: 0.5564996600151062\n",
      "Epoch: 22360 | Loss: 0.528882622718811 | Test loss: 0.5564857721328735\n",
      "Epoch: 22370 | Loss: 0.5288708209991455 | Test loss: 0.5564720034599304\n",
      "Epoch: 22380 | Loss: 0.5288590788841248 | Test loss: 0.5564581155776978\n",
      "Epoch: 22390 | Loss: 0.528847336769104 | Test loss: 0.5564442873001099\n",
      "Epoch: 22400 | Loss: 0.5288355946540833 | Test loss: 0.556430459022522\n",
      "Epoch: 22410 | Loss: 0.5288237929344177 | Test loss: 0.5564166307449341\n",
      "Epoch: 22420 | Loss: 0.528812050819397 | Test loss: 0.5564026832580566\n",
      "Epoch: 22430 | Loss: 0.5288003087043762 | Test loss: 0.5563888549804688\n",
      "Epoch: 22440 | Loss: 0.5287885069847107 | Test loss: 0.5563750267028809\n",
      "Epoch: 22450 | Loss: 0.5287767648696899 | Test loss: 0.556361198425293\n",
      "Epoch: 22460 | Loss: 0.5287650227546692 | Test loss: 0.5563473701477051\n",
      "Epoch: 22470 | Loss: 0.5287532210350037 | Test loss: 0.5563334822654724\n",
      "Epoch: 22480 | Loss: 0.5287415385246277 | Test loss: 0.5563196539878845\n",
      "Epoch: 22490 | Loss: 0.5287296772003174 | Test loss: 0.5563058257102966\n",
      "Epoch: 22500 | Loss: 0.5287179350852966 | Test loss: 0.5562919974327087\n",
      "Epoch: 22510 | Loss: 0.5287061929702759 | Test loss: 0.5562781691551208\n",
      "Epoch: 22520 | Loss: 0.5286943912506104 | Test loss: 0.556264340877533\n",
      "Epoch: 22530 | Loss: 0.5286827087402344 | Test loss: 0.5562504529953003\n",
      "Epoch: 22540 | Loss: 0.5286709070205688 | Test loss: 0.5562365651130676\n",
      "Epoch: 22550 | Loss: 0.5286591649055481 | Test loss: 0.5562227368354797\n",
      "Epoch: 22560 | Loss: 0.5286473631858826 | Test loss: 0.5562088489532471\n",
      "Epoch: 22570 | Loss: 0.5286356210708618 | Test loss: 0.556195080280304\n",
      "Epoch: 22580 | Loss: 0.5286238789558411 | Test loss: 0.5561811923980713\n",
      "Epoch: 22590 | Loss: 0.5286120772361755 | Test loss: 0.5561673045158386\n",
      "Epoch: 22600 | Loss: 0.5286003947257996 | Test loss: 0.5561534762382507\n",
      "Epoch: 22610 | Loss: 0.528588593006134 | Test loss: 0.5561396479606628\n",
      "Epoch: 22620 | Loss: 0.5285768508911133 | Test loss: 0.556125819683075\n",
      "Epoch: 22630 | Loss: 0.5285651087760925 | Test loss: 0.5561119914054871\n",
      "Epoch: 22640 | Loss: 0.528553307056427 | Test loss: 0.5560981035232544\n",
      "Epoch: 22650 | Loss: 0.5285415649414062 | Test loss: 0.5560842752456665\n",
      "Epoch: 22660 | Loss: 0.5285297632217407 | Test loss: 0.5560704469680786\n",
      "Epoch: 22670 | Loss: 0.52851802110672 | Test loss: 0.5560566186904907\n",
      "Epoch: 22680 | Loss: 0.5285062789916992 | Test loss: 0.5560427308082581\n",
      "Epoch: 22690 | Loss: 0.5284945368766785 | Test loss: 0.5560289025306702\n",
      "Epoch: 22700 | Loss: 0.5284827351570129 | Test loss: 0.5560150146484375\n",
      "Epoch: 22710 | Loss: 0.5284709930419922 | Test loss: 0.5560011863708496\n",
      "Epoch: 22720 | Loss: 0.5284591913223267 | Test loss: 0.5559873580932617\n",
      "Epoch: 22730 | Loss: 0.5284474492073059 | Test loss: 0.5559735298156738\n",
      "Epoch: 22740 | Loss: 0.5284357070922852 | Test loss: 0.5559596419334412\n",
      "Epoch: 22750 | Loss: 0.5284239649772644 | Test loss: 0.5559458136558533\n",
      "Epoch: 22760 | Loss: 0.5284121632575989 | Test loss: 0.5559319853782654\n",
      "Epoch: 22770 | Loss: 0.5284004211425781 | Test loss: 0.5559181571006775\n",
      "Epoch: 22780 | Loss: 0.5283886790275574 | Test loss: 0.5559043288230896\n",
      "Epoch: 22790 | Loss: 0.5283768773078918 | Test loss: 0.5558904409408569\n",
      "Epoch: 22800 | Loss: 0.5283651351928711 | Test loss: 0.555876612663269\n",
      "Epoch: 22810 | Loss: 0.5283533930778503 | Test loss: 0.5558627247810364\n",
      "Epoch: 22820 | Loss: 0.5283417105674744 | Test loss: 0.5558488965034485\n",
      "Epoch: 22830 | Loss: 0.5283298492431641 | Test loss: 0.5558350682258606\n",
      "Epoch: 22840 | Loss: 0.5283181071281433 | Test loss: 0.5558211803436279\n",
      "Epoch: 22850 | Loss: 0.5283064246177673 | Test loss: 0.55580735206604\n",
      "Epoch: 22860 | Loss: 0.528294563293457 | Test loss: 0.5557935237884521\n",
      "Epoch: 22870 | Loss: 0.5282828211784363 | Test loss: 0.5557796359062195\n",
      "Epoch: 22880 | Loss: 0.5282711386680603 | Test loss: 0.5557658076286316\n",
      "Epoch: 22890 | Loss: 0.52825927734375 | Test loss: 0.5557519793510437\n",
      "Epoch: 22900 | Loss: 0.5282475352287292 | Test loss: 0.555738091468811\n",
      "Epoch: 22910 | Loss: 0.5282357931137085 | Test loss: 0.5557242631912231\n",
      "Epoch: 22920 | Loss: 0.528223991394043 | Test loss: 0.5557104349136353\n",
      "Epoch: 22930 | Loss: 0.5282122492790222 | Test loss: 0.5556966066360474\n",
      "Epoch: 22940 | Loss: 0.5282005667686462 | Test loss: 0.5556827783584595\n",
      "Epoch: 22950 | Loss: 0.5281887054443359 | Test loss: 0.5556688904762268\n",
      "Epoch: 22960 | Loss: 0.5281769633293152 | Test loss: 0.5556550025939941\n",
      "Epoch: 22970 | Loss: 0.5281652808189392 | Test loss: 0.5556411743164062\n",
      "Epoch: 22980 | Loss: 0.5281534790992737 | Test loss: 0.5556273460388184\n",
      "Epoch: 22990 | Loss: 0.5281416773796082 | Test loss: 0.5556135177612305\n",
      "Epoch: 23000 | Loss: 0.5281299948692322 | Test loss: 0.5555996894836426\n",
      "Epoch: 23010 | Loss: 0.5281181335449219 | Test loss: 0.5555858016014099\n",
      "Epoch: 23020 | Loss: 0.5281064510345459 | Test loss: 0.555571973323822\n",
      "Epoch: 23030 | Loss: 0.5280947089195251 | Test loss: 0.5555581450462341\n",
      "Epoch: 23040 | Loss: 0.5280829668045044 | Test loss: 0.5555443167686462\n",
      "Epoch: 23050 | Loss: 0.5280711650848389 | Test loss: 0.5555304884910583\n",
      "Epoch: 23060 | Loss: 0.5280594229698181 | Test loss: 0.5555166602134705\n",
      "Epoch: 23070 | Loss: 0.5280476212501526 | Test loss: 0.555502712726593\n",
      "Epoch: 23080 | Loss: 0.5280358791351318 | Test loss: 0.5554888844490051\n",
      "Epoch: 23090 | Loss: 0.5280241370201111 | Test loss: 0.5554750561714172\n",
      "Epoch: 23100 | Loss: 0.5280123949050903 | Test loss: 0.5554611682891846\n",
      "Epoch: 23110 | Loss: 0.52800053358078 | Test loss: 0.5554473400115967\n",
      "Epoch: 23120 | Loss: 0.527988851070404 | Test loss: 0.5554335117340088\n",
      "Epoch: 23130 | Loss: 0.5279771089553833 | Test loss: 0.5554196238517761\n",
      "Epoch: 23140 | Loss: 0.5279653072357178 | Test loss: 0.5554057955741882\n",
      "Epoch: 23150 | Loss: 0.527953565120697 | Test loss: 0.5553919672966003\n",
      "Epoch: 23160 | Loss: 0.5279418230056763 | Test loss: 0.5553780794143677\n",
      "Epoch: 23170 | Loss: 0.5279300212860107 | Test loss: 0.5553643107414246\n",
      "Epoch: 23180 | Loss: 0.52791827917099 | Test loss: 0.5553504228591919\n",
      "Epoch: 23190 | Loss: 0.5279065370559692 | Test loss: 0.555336594581604\n",
      "Epoch: 23200 | Loss: 0.5278947949409485 | Test loss: 0.5553227663040161\n",
      "Epoch: 23210 | Loss: 0.527882993221283 | Test loss: 0.5553089380264282\n",
      "Epoch: 23220 | Loss: 0.5278712511062622 | Test loss: 0.5552949905395508\n",
      "Epoch: 23230 | Loss: 0.5278595089912415 | Test loss: 0.5552811622619629\n",
      "Epoch: 23240 | Loss: 0.5278477072715759 | Test loss: 0.555267333984375\n",
      "Epoch: 23250 | Loss: 0.5278359651565552 | Test loss: 0.5552535057067871\n",
      "Epoch: 23260 | Loss: 0.5278242230415344 | Test loss: 0.5552396774291992\n",
      "Epoch: 23270 | Loss: 0.5278124213218689 | Test loss: 0.5552257895469666\n",
      "Epoch: 23280 | Loss: 0.5278007388114929 | Test loss: 0.5552119612693787\n",
      "Epoch: 23290 | Loss: 0.5277888774871826 | Test loss: 0.5551981329917908\n",
      "Epoch: 23300 | Loss: 0.5277771353721619 | Test loss: 0.5551843047142029\n",
      "Epoch: 23310 | Loss: 0.5277653932571411 | Test loss: 0.555170476436615\n",
      "Epoch: 23320 | Loss: 0.5277535915374756 | Test loss: 0.5551566481590271\n",
      "Epoch: 23330 | Loss: 0.5277419090270996 | Test loss: 0.5551427602767944\n",
      "Epoch: 23340 | Loss: 0.5277301073074341 | Test loss: 0.5551288723945618\n",
      "Epoch: 23350 | Loss: 0.5277183651924133 | Test loss: 0.5551150441169739\n",
      "Epoch: 23360 | Loss: 0.5277065634727478 | Test loss: 0.5551011562347412\n",
      "Epoch: 23370 | Loss: 0.527694821357727 | Test loss: 0.5550873875617981\n",
      "Epoch: 23380 | Loss: 0.5276830792427063 | Test loss: 0.5550734996795654\n",
      "Epoch: 23390 | Loss: 0.5276712775230408 | Test loss: 0.5550596117973328\n",
      "Epoch: 23400 | Loss: 0.5276595950126648 | Test loss: 0.5550457835197449\n",
      "Epoch: 23410 | Loss: 0.5276477932929993 | Test loss: 0.555031955242157\n",
      "Epoch: 23420 | Loss: 0.5276360511779785 | Test loss: 0.5550181269645691\n",
      "Epoch: 23430 | Loss: 0.5276243090629578 | Test loss: 0.5550042986869812\n",
      "Epoch: 23440 | Loss: 0.5276125073432922 | Test loss: 0.5549904108047485\n",
      "Epoch: 23450 | Loss: 0.5276007652282715 | Test loss: 0.5549765825271606\n",
      "Epoch: 23460 | Loss: 0.527588963508606 | Test loss: 0.5549627542495728\n",
      "Epoch: 23470 | Loss: 0.5275772213935852 | Test loss: 0.5549489259719849\n",
      "Epoch: 23480 | Loss: 0.5275654792785645 | Test loss: 0.5549350380897522\n",
      "Epoch: 23490 | Loss: 0.5275537371635437 | Test loss: 0.5549212098121643\n",
      "Epoch: 23500 | Loss: 0.5275419354438782 | Test loss: 0.5549073219299316\n",
      "Epoch: 23510 | Loss: 0.5275301933288574 | Test loss: 0.5548934936523438\n",
      "Epoch: 23520 | Loss: 0.5275183916091919 | Test loss: 0.5548796653747559\n",
      "Epoch: 23530 | Loss: 0.5275066494941711 | Test loss: 0.554865837097168\n",
      "Epoch: 23540 | Loss: 0.5274949073791504 | Test loss: 0.5548519492149353\n",
      "Epoch: 23550 | Loss: 0.5274831652641296 | Test loss: 0.5548381209373474\n",
      "Epoch: 23560 | Loss: 0.5274713635444641 | Test loss: 0.5548242926597595\n",
      "Epoch: 23570 | Loss: 0.5274596214294434 | Test loss: 0.5548104643821716\n",
      "Epoch: 23580 | Loss: 0.5274478793144226 | Test loss: 0.5547966361045837\n",
      "Epoch: 23590 | Loss: 0.5274360775947571 | Test loss: 0.5547827482223511\n",
      "Epoch: 23600 | Loss: 0.5274243354797363 | Test loss: 0.5547689199447632\n",
      "Epoch: 23610 | Loss: 0.5274125933647156 | Test loss: 0.5547550320625305\n",
      "Epoch: 23620 | Loss: 0.5274009108543396 | Test loss: 0.5547412037849426\n",
      "Epoch: 23630 | Loss: 0.5273890495300293 | Test loss: 0.5547273755073547\n",
      "Epoch: 23640 | Loss: 0.5273773074150085 | Test loss: 0.5547134876251221\n",
      "Epoch: 23650 | Loss: 0.5273656249046326 | Test loss: 0.5546996593475342\n",
      "Epoch: 23660 | Loss: 0.5273537635803223 | Test loss: 0.5546858310699463\n",
      "Epoch: 23670 | Loss: 0.5273420214653015 | Test loss: 0.5546719431877136\n",
      "Epoch: 23680 | Loss: 0.5273303389549255 | Test loss: 0.5546581149101257\n",
      "Epoch: 23690 | Loss: 0.5273184776306152 | Test loss: 0.5546442866325378\n",
      "Epoch: 23700 | Loss: 0.5273067355155945 | Test loss: 0.5546303987503052\n",
      "Epoch: 23710 | Loss: 0.5272949934005737 | Test loss: 0.5546165704727173\n",
      "Epoch: 23720 | Loss: 0.5272831916809082 | Test loss: 0.5546027421951294\n",
      "Epoch: 23730 | Loss: 0.5272714495658875 | Test loss: 0.5545889139175415\n",
      "Epoch: 23740 | Loss: 0.5272597670555115 | Test loss: 0.5545750856399536\n",
      "Epoch: 23750 | Loss: 0.5272479057312012 | Test loss: 0.554561197757721\n",
      "Epoch: 23760 | Loss: 0.5272361636161804 | Test loss: 0.5545473098754883\n",
      "Epoch: 23770 | Loss: 0.5272244811058044 | Test loss: 0.5545334815979004\n",
      "Epoch: 23780 | Loss: 0.5272126793861389 | Test loss: 0.5545196533203125\n",
      "Epoch: 23790 | Loss: 0.5272008776664734 | Test loss: 0.5545058250427246\n",
      "Epoch: 23800 | Loss: 0.5271891951560974 | Test loss: 0.5544919967651367\n",
      "Epoch: 23810 | Loss: 0.5271773338317871 | Test loss: 0.554478108882904\n",
      "Epoch: 23820 | Loss: 0.5271656513214111 | Test loss: 0.5544642806053162\n",
      "Epoch: 23830 | Loss: 0.5271539092063904 | Test loss: 0.5544504523277283\n",
      "Epoch: 23840 | Loss: 0.5271421670913696 | Test loss: 0.5544366240501404\n",
      "Epoch: 23850 | Loss: 0.5271303653717041 | Test loss: 0.5544227957725525\n",
      "Epoch: 23860 | Loss: 0.5271186232566833 | Test loss: 0.5544089674949646\n",
      "Epoch: 23870 | Loss: 0.5271068215370178 | Test loss: 0.5543950200080872\n",
      "Epoch: 23880 | Loss: 0.5270950794219971 | Test loss: 0.5543811917304993\n",
      "Epoch: 23890 | Loss: 0.5270833373069763 | Test loss: 0.5543673634529114\n",
      "Epoch: 23900 | Loss: 0.5270715951919556 | Test loss: 0.5543534755706787\n",
      "Epoch: 23910 | Loss: 0.5270597338676453 | Test loss: 0.5543396472930908\n",
      "Epoch: 23920 | Loss: 0.5270480513572693 | Test loss: 0.5543258190155029\n",
      "Epoch: 23930 | Loss: 0.5270363092422485 | Test loss: 0.5543119311332703\n",
      "Epoch: 23940 | Loss: 0.527024507522583 | Test loss: 0.5542981028556824\n",
      "Epoch: 23950 | Loss: 0.5270127654075623 | Test loss: 0.5542842745780945\n",
      "Epoch: 23960 | Loss: 0.5270010232925415 | Test loss: 0.5542703866958618\n",
      "Epoch: 23970 | Loss: 0.526989221572876 | Test loss: 0.5542566180229187\n",
      "Epoch: 23980 | Loss: 0.5269774794578552 | Test loss: 0.554242730140686\n",
      "Epoch: 23990 | Loss: 0.5269657373428345 | Test loss: 0.5542289018630981\n",
      "Epoch: 24000 | Loss: 0.5269539952278137 | Test loss: 0.5542150735855103\n",
      "Epoch: 24010 | Loss: 0.5269421935081482 | Test loss: 0.5542012453079224\n",
      "Epoch: 24020 | Loss: 0.5269304513931274 | Test loss: 0.5541872978210449\n",
      "Epoch: 24030 | Loss: 0.5269187092781067 | Test loss: 0.554173469543457\n",
      "Epoch: 24040 | Loss: 0.5269069075584412 | Test loss: 0.5541596412658691\n",
      "Epoch: 24050 | Loss: 0.5268951654434204 | Test loss: 0.5541458129882812\n",
      "Epoch: 24060 | Loss: 0.5268834233283997 | Test loss: 0.5541319847106934\n",
      "Epoch: 24070 | Loss: 0.5268716216087341 | Test loss: 0.5541180968284607\n",
      "Epoch: 24080 | Loss: 0.5268599390983582 | Test loss: 0.5541042685508728\n",
      "Epoch: 24090 | Loss: 0.5268480777740479 | Test loss: 0.5540904402732849\n",
      "Epoch: 24100 | Loss: 0.5268363356590271 | Test loss: 0.554076611995697\n",
      "Epoch: 24110 | Loss: 0.5268245935440063 | Test loss: 0.5540627837181091\n",
      "Epoch: 24120 | Loss: 0.5268127918243408 | Test loss: 0.5540489554405212\n",
      "Epoch: 24130 | Loss: 0.5268011093139648 | Test loss: 0.5540350675582886\n",
      "Epoch: 24140 | Loss: 0.5267893075942993 | Test loss: 0.5540211796760559\n",
      "Epoch: 24150 | Loss: 0.5267775654792786 | Test loss: 0.554007351398468\n",
      "Epoch: 24160 | Loss: 0.526765763759613 | Test loss: 0.5539934635162354\n",
      "Epoch: 24170 | Loss: 0.5267540216445923 | Test loss: 0.5539796948432922\n",
      "Epoch: 24180 | Loss: 0.5267422795295715 | Test loss: 0.5539658069610596\n",
      "Epoch: 24190 | Loss: 0.526730477809906 | Test loss: 0.5539519190788269\n",
      "Epoch: 24200 | Loss: 0.52671879529953 | Test loss: 0.553938090801239\n",
      "Epoch: 24210 | Loss: 0.5267069935798645 | Test loss: 0.5539242625236511\n",
      "Epoch: 24220 | Loss: 0.5266952514648438 | Test loss: 0.5539104342460632\n",
      "Epoch: 24230 | Loss: 0.526683509349823 | Test loss: 0.5538966059684753\n",
      "Epoch: 24240 | Loss: 0.5266717076301575 | Test loss: 0.5538827180862427\n",
      "Epoch: 24250 | Loss: 0.5266599655151367 | Test loss: 0.5538688898086548\n",
      "Epoch: 24260 | Loss: 0.5266481637954712 | Test loss: 0.5538550615310669\n",
      "Epoch: 24270 | Loss: 0.5266364216804504 | Test loss: 0.553841233253479\n",
      "Epoch: 24280 | Loss: 0.5266246795654297 | Test loss: 0.5538273453712463\n",
      "Epoch: 24290 | Loss: 0.5266129374504089 | Test loss: 0.5538135170936584\n",
      "Epoch: 24300 | Loss: 0.5266011357307434 | Test loss: 0.5537996292114258\n",
      "Epoch: 24310 | Loss: 0.5265893936157227 | Test loss: 0.5537858009338379\n",
      "Epoch: 24320 | Loss: 0.5265775918960571 | Test loss: 0.55377197265625\n",
      "Epoch: 24330 | Loss: 0.5265658497810364 | Test loss: 0.5537581443786621\n",
      "Epoch: 24340 | Loss: 0.5265541076660156 | Test loss: 0.5537442564964294\n",
      "Epoch: 24350 | Loss: 0.5265423655509949 | Test loss: 0.5537304282188416\n",
      "Epoch: 24360 | Loss: 0.5265305638313293 | Test loss: 0.5537165999412537\n",
      "Epoch: 24370 | Loss: 0.5265188217163086 | Test loss: 0.5537027716636658\n",
      "Epoch: 24380 | Loss: 0.5265070796012878 | Test loss: 0.5536889433860779\n",
      "Epoch: 24390 | Loss: 0.5264952778816223 | Test loss: 0.5536750555038452\n",
      "Epoch: 24400 | Loss: 0.5264835357666016 | Test loss: 0.5536612272262573\n",
      "Epoch: 24410 | Loss: 0.5264717936515808 | Test loss: 0.5536473393440247\n",
      "Epoch: 24420 | Loss: 0.5264601111412048 | Test loss: 0.5536335110664368\n",
      "Epoch: 24430 | Loss: 0.5264482498168945 | Test loss: 0.5536196827888489\n",
      "Epoch: 24440 | Loss: 0.5264365077018738 | Test loss: 0.5536057949066162\n",
      "Epoch: 24450 | Loss: 0.5264248251914978 | Test loss: 0.5535919666290283\n",
      "Epoch: 24460 | Loss: 0.5264129638671875 | Test loss: 0.5535781383514404\n",
      "Epoch: 24470 | Loss: 0.5264012217521667 | Test loss: 0.5535642504692078\n",
      "Epoch: 24480 | Loss: 0.5263895392417908 | Test loss: 0.5535504221916199\n",
      "Epoch: 24490 | Loss: 0.5263776779174805 | Test loss: 0.553536593914032\n",
      "Epoch: 24500 | Loss: 0.5263659358024597 | Test loss: 0.5535227060317993\n",
      "Epoch: 24510 | Loss: 0.526354193687439 | Test loss: 0.5535088777542114\n",
      "Epoch: 24520 | Loss: 0.5263423919677734 | Test loss: 0.5534950494766235\n",
      "Epoch: 24530 | Loss: 0.5263306498527527 | Test loss: 0.5534812211990356\n",
      "Epoch: 24540 | Loss: 0.5263189673423767 | Test loss: 0.5534673929214478\n",
      "Epoch: 24550 | Loss: 0.5263071060180664 | Test loss: 0.5534535050392151\n",
      "Epoch: 24560 | Loss: 0.5262953639030457 | Test loss: 0.5534396171569824\n",
      "Epoch: 24570 | Loss: 0.5262836813926697 | Test loss: 0.5534257888793945\n",
      "Epoch: 24580 | Loss: 0.5262718796730042 | Test loss: 0.5534119606018066\n",
      "Epoch: 24590 | Loss: 0.5262600779533386 | Test loss: 0.5533981323242188\n",
      "Epoch: 24600 | Loss: 0.5262483954429626 | Test loss: 0.5533843040466309\n",
      "Epoch: 24610 | Loss: 0.5262365341186523 | Test loss: 0.5533704161643982\n",
      "Epoch: 24620 | Loss: 0.5262248516082764 | Test loss: 0.5533565878868103\n",
      "Epoch: 24630 | Loss: 0.5262131094932556 | Test loss: 0.5533427596092224\n",
      "Epoch: 24640 | Loss: 0.5262013673782349 | Test loss: 0.5533289313316345\n",
      "Epoch: 24650 | Loss: 0.5261895656585693 | Test loss: 0.5533151030540466\n",
      "Epoch: 24660 | Loss: 0.5261778235435486 | Test loss: 0.5533012747764587\n",
      "Epoch: 24670 | Loss: 0.5261660218238831 | Test loss: 0.5532873272895813\n",
      "Epoch: 24680 | Loss: 0.5261542797088623 | Test loss: 0.5532734990119934\n",
      "Epoch: 24690 | Loss: 0.5261425375938416 | Test loss: 0.5532596707344055\n",
      "Epoch: 24700 | Loss: 0.5261307954788208 | Test loss: 0.5532457828521729\n",
      "Epoch: 24710 | Loss: 0.5261189341545105 | Test loss: 0.553231954574585\n",
      "Epoch: 24720 | Loss: 0.5261072516441345 | Test loss: 0.5532181262969971\n",
      "Epoch: 24730 | Loss: 0.5260955095291138 | Test loss: 0.5532042384147644\n",
      "Epoch: 24740 | Loss: 0.5260837078094482 | Test loss: 0.5531904101371765\n",
      "Epoch: 24750 | Loss: 0.5260719656944275 | Test loss: 0.5531765818595886\n",
      "Epoch: 24760 | Loss: 0.5260602235794067 | Test loss: 0.553162693977356\n",
      "Epoch: 24770 | Loss: 0.5260484218597412 | Test loss: 0.5531489253044128\n",
      "Epoch: 24780 | Loss: 0.5260366797447205 | Test loss: 0.5531350374221802\n",
      "Epoch: 24790 | Loss: 0.5260249376296997 | Test loss: 0.5531212091445923\n",
      "Epoch: 24800 | Loss: 0.526013195514679 | Test loss: 0.5531073808670044\n",
      "Epoch: 24810 | Loss: 0.5260013937950134 | Test loss: 0.5530935525894165\n",
      "Epoch: 24820 | Loss: 0.5259896516799927 | Test loss: 0.5530796051025391\n",
      "Epoch: 24830 | Loss: 0.5259779095649719 | Test loss: 0.5530657768249512\n",
      "Epoch: 24840 | Loss: 0.5259661078453064 | Test loss: 0.5530519485473633\n",
      "Epoch: 24850 | Loss: 0.5259543657302856 | Test loss: 0.5530381202697754\n",
      "Epoch: 24860 | Loss: 0.5259426236152649 | Test loss: 0.5530242919921875\n",
      "Epoch: 24870 | Loss: 0.5259308218955994 | Test loss: 0.5530104041099548\n",
      "Epoch: 24880 | Loss: 0.5259191393852234 | Test loss: 0.5529965758323669\n",
      "Epoch: 24890 | Loss: 0.5259072780609131 | Test loss: 0.552982747554779\n",
      "Epoch: 24900 | Loss: 0.5258955359458923 | Test loss: 0.5529689192771912\n",
      "Epoch: 24910 | Loss: 0.5258837938308716 | Test loss: 0.5529550909996033\n",
      "Epoch: 24920 | Loss: 0.525871992111206 | Test loss: 0.5529412627220154\n",
      "Epoch: 24930 | Loss: 0.5258603096008301 | Test loss: 0.5529273748397827\n",
      "Epoch: 24940 | Loss: 0.5258485078811646 | Test loss: 0.55291348695755\n",
      "Epoch: 24950 | Loss: 0.5258367657661438 | Test loss: 0.5528996586799622\n",
      "Epoch: 24960 | Loss: 0.5258249640464783 | Test loss: 0.5528857707977295\n",
      "Epoch: 24970 | Loss: 0.5258132219314575 | Test loss: 0.5528720021247864\n",
      "Epoch: 24980 | Loss: 0.5258014798164368 | Test loss: 0.5528581142425537\n",
      "Epoch: 24990 | Loss: 0.5257896780967712 | Test loss: 0.552844226360321\n",
      "Epoch: 25000 | Loss: 0.5257779955863953 | Test loss: 0.5528303980827332\n",
      "Epoch: 25010 | Loss: 0.5257661938667297 | Test loss: 0.5528165698051453\n",
      "Epoch: 25020 | Loss: 0.525754451751709 | Test loss: 0.5528027415275574\n",
      "Epoch: 25030 | Loss: 0.5257427096366882 | Test loss: 0.5527889132499695\n",
      "Epoch: 25040 | Loss: 0.5257309079170227 | Test loss: 0.5527750253677368\n",
      "Epoch: 25050 | Loss: 0.525719165802002 | Test loss: 0.5527611970901489\n",
      "Epoch: 25060 | Loss: 0.5257073640823364 | Test loss: 0.552747368812561\n",
      "Epoch: 25070 | Loss: 0.5256956219673157 | Test loss: 0.5527335405349731\n",
      "Epoch: 25080 | Loss: 0.5256838798522949 | Test loss: 0.5527196526527405\n",
      "Epoch: 25090 | Loss: 0.5256721377372742 | Test loss: 0.5527058243751526\n",
      "Epoch: 25100 | Loss: 0.5256603360176086 | Test loss: 0.5526919364929199\n",
      "Epoch: 25110 | Loss: 0.5256485939025879 | Test loss: 0.552678108215332\n",
      "Epoch: 25120 | Loss: 0.5256367921829224 | Test loss: 0.5526642799377441\n",
      "Epoch: 25130 | Loss: 0.5256250500679016 | Test loss: 0.5526504516601562\n",
      "Epoch: 25140 | Loss: 0.5256133079528809 | Test loss: 0.5526365637779236\n",
      "Epoch: 25150 | Loss: 0.5256015658378601 | Test loss: 0.5526227355003357\n",
      "Epoch: 25160 | Loss: 0.5255897641181946 | Test loss: 0.5526089072227478\n",
      "Epoch: 25170 | Loss: 0.5255780220031738 | Test loss: 0.5525950789451599\n",
      "Epoch: 25180 | Loss: 0.5255662798881531 | Test loss: 0.552581250667572\n",
      "Epoch: 25190 | Loss: 0.5255544781684875 | Test loss: 0.5525673627853394\n",
      "Epoch: 25200 | Loss: 0.5255427360534668 | Test loss: 0.5525535345077515\n",
      "Epoch: 25210 | Loss: 0.525530993938446 | Test loss: 0.5525396466255188\n",
      "Epoch: 25220 | Loss: 0.5255193114280701 | Test loss: 0.5525258183479309\n",
      "Epoch: 25230 | Loss: 0.5255074501037598 | Test loss: 0.552511990070343\n",
      "Epoch: 25240 | Loss: 0.525495707988739 | Test loss: 0.5524981021881104\n",
      "Epoch: 25250 | Loss: 0.525484025478363 | Test loss: 0.5524842739105225\n",
      "Epoch: 25260 | Loss: 0.5254721641540527 | Test loss: 0.5524704456329346\n",
      "Epoch: 25270 | Loss: 0.525460422039032 | Test loss: 0.5524565577507019\n",
      "Epoch: 25280 | Loss: 0.525448739528656 | Test loss: 0.552442729473114\n",
      "Epoch: 25290 | Loss: 0.5254368782043457 | Test loss: 0.5524289011955261\n",
      "Epoch: 25300 | Loss: 0.525425136089325 | Test loss: 0.5524150133132935\n",
      "Epoch: 25310 | Loss: 0.5254133939743042 | Test loss: 0.5524011850357056\n",
      "Epoch: 25320 | Loss: 0.5254015922546387 | Test loss: 0.5523873567581177\n",
      "Epoch: 25330 | Loss: 0.5253898501396179 | Test loss: 0.5523735284805298\n",
      "Epoch: 25340 | Loss: 0.5253781676292419 | Test loss: 0.5523597002029419\n",
      "Epoch: 25350 | Loss: 0.5253663063049316 | Test loss: 0.5523458123207092\n",
      "Epoch: 25360 | Loss: 0.5253545641899109 | Test loss: 0.5523319244384766\n",
      "Epoch: 25370 | Loss: 0.5253428816795349 | Test loss: 0.5523180961608887\n",
      "Epoch: 25380 | Loss: 0.5253310799598694 | Test loss: 0.5523042678833008\n",
      "Epoch: 25390 | Loss: 0.5253192782402039 | Test loss: 0.5522904396057129\n",
      "Epoch: 25400 | Loss: 0.5253075957298279 | Test loss: 0.552276611328125\n",
      "Epoch: 25410 | Loss: 0.5252957344055176 | Test loss: 0.5522627234458923\n",
      "Epoch: 25420 | Loss: 0.5252840518951416 | Test loss: 0.5522488951683044\n",
      "Epoch: 25430 | Loss: 0.5252723097801208 | Test loss: 0.5522350668907166\n",
      "Epoch: 25440 | Loss: 0.5252605676651001 | Test loss: 0.5522212386131287\n",
      "Epoch: 25450 | Loss: 0.5252487659454346 | Test loss: 0.5522074103355408\n",
      "Epoch: 25460 | Loss: 0.5252370238304138 | Test loss: 0.5521935820579529\n",
      "Epoch: 25470 | Loss: 0.5252252221107483 | Test loss: 0.5521796345710754\n",
      "Epoch: 25480 | Loss: 0.5252134799957275 | Test loss: 0.5521658062934875\n",
      "Epoch: 25490 | Loss: 0.5252017378807068 | Test loss: 0.5521519780158997\n",
      "Epoch: 25500 | Loss: 0.525189995765686 | Test loss: 0.552138090133667\n",
      "Epoch: 25510 | Loss: 0.5251781344413757 | Test loss: 0.5521242618560791\n",
      "Epoch: 25520 | Loss: 0.5251664519309998 | Test loss: 0.5521104335784912\n",
      "Epoch: 25530 | Loss: 0.525154709815979 | Test loss: 0.5520965456962585\n",
      "Epoch: 25540 | Loss: 0.5251429080963135 | Test loss: 0.5520827174186707\n",
      "Epoch: 25550 | Loss: 0.5251311659812927 | Test loss: 0.5520688891410828\n",
      "Epoch: 25560 | Loss: 0.525119423866272 | Test loss: 0.5520550012588501\n",
      "Epoch: 25570 | Loss: 0.5251076221466064 | Test loss: 0.552041232585907\n",
      "Epoch: 25580 | Loss: 0.5250958800315857 | Test loss: 0.5520273447036743\n",
      "Epoch: 25590 | Loss: 0.5250841379165649 | Test loss: 0.5520135164260864\n",
      "Epoch: 25600 | Loss: 0.5250723958015442 | Test loss: 0.5519996881484985\n",
      "Epoch: 25610 | Loss: 0.5250605940818787 | Test loss: 0.5519858598709106\n",
      "Epoch: 25620 | Loss: 0.5250488519668579 | Test loss: 0.5519719123840332\n",
      "Epoch: 25630 | Loss: 0.5250371098518372 | Test loss: 0.5519580841064453\n",
      "Epoch: 25640 | Loss: 0.5250253081321716 | Test loss: 0.5519442558288574\n",
      "Epoch: 25650 | Loss: 0.5250135660171509 | Test loss: 0.5519304275512695\n",
      "Epoch: 25660 | Loss: 0.5250018239021301 | Test loss: 0.5519165992736816\n",
      "Epoch: 25670 | Loss: 0.5249900221824646 | Test loss: 0.551902711391449\n",
      "Epoch: 25680 | Loss: 0.5249783396720886 | Test loss: 0.5518888831138611\n",
      "Epoch: 25690 | Loss: 0.5249664783477783 | Test loss: 0.5518750548362732\n",
      "Epoch: 25700 | Loss: 0.5249547362327576 | Test loss: 0.5518612265586853\n",
      "Epoch: 25710 | Loss: 0.5249429941177368 | Test loss: 0.5518473982810974\n",
      "Epoch: 25720 | Loss: 0.5249311923980713 | Test loss: 0.5518335700035095\n",
      "Epoch: 25730 | Loss: 0.5249195098876953 | Test loss: 0.5518196821212769\n",
      "Epoch: 25740 | Loss: 0.5249077081680298 | Test loss: 0.5518057942390442\n",
      "Epoch: 25750 | Loss: 0.524895966053009 | Test loss: 0.5517919659614563\n",
      "Epoch: 25760 | Loss: 0.5248841643333435 | Test loss: 0.5517780780792236\n",
      "Epoch: 25770 | Loss: 0.5248724222183228 | Test loss: 0.5517643094062805\n",
      "Epoch: 25780 | Loss: 0.524860680103302 | Test loss: 0.5517504215240479\n",
      "Epoch: 25790 | Loss: 0.5248488783836365 | Test loss: 0.5517365336418152\n",
      "Epoch: 25800 | Loss: 0.5248371958732605 | Test loss: 0.5517227053642273\n",
      "Epoch: 25810 | Loss: 0.524825394153595 | Test loss: 0.5517088770866394\n",
      "Epoch: 25820 | Loss: 0.5248136520385742 | Test loss: 0.5516950488090515\n",
      "Epoch: 25830 | Loss: 0.5248019099235535 | Test loss: 0.5516812205314636\n",
      "Epoch: 25840 | Loss: 0.5247901082038879 | Test loss: 0.551667332649231\n",
      "Epoch: 25850 | Loss: 0.5247783660888672 | Test loss: 0.5516535043716431\n",
      "Epoch: 25860 | Loss: 0.5247665643692017 | Test loss: 0.5516396760940552\n",
      "Epoch: 25870 | Loss: 0.5247548222541809 | Test loss: 0.5516258478164673\n",
      "Epoch: 25880 | Loss: 0.5247430801391602 | Test loss: 0.5516119599342346\n",
      "Epoch: 25890 | Loss: 0.5247313380241394 | Test loss: 0.5515981316566467\n",
      "Epoch: 25900 | Loss: 0.5247195363044739 | Test loss: 0.5515842437744141\n",
      "Epoch: 25910 | Loss: 0.5247077941894531 | Test loss: 0.5515704154968262\n",
      "Epoch: 25920 | Loss: 0.5246959924697876 | Test loss: 0.5515565872192383\n",
      "Epoch: 25930 | Loss: 0.5246842503547668 | Test loss: 0.5515427589416504\n",
      "Epoch: 25940 | Loss: 0.5246725082397461 | Test loss: 0.5515288710594177\n",
      "Epoch: 25950 | Loss: 0.5246607661247253 | Test loss: 0.5515150427818298\n",
      "Epoch: 25960 | Loss: 0.5246489644050598 | Test loss: 0.5515012145042419\n",
      "Epoch: 25970 | Loss: 0.5246372222900391 | Test loss: 0.551487386226654\n",
      "Epoch: 25980 | Loss: 0.5246254801750183 | Test loss: 0.5514735579490662\n",
      "Epoch: 25990 | Loss: 0.5246136784553528 | Test loss: 0.5514596700668335\n",
      "Epoch: 26000 | Loss: 0.524601936340332 | Test loss: 0.5514458417892456\n",
      "Epoch: 26010 | Loss: 0.5245901942253113 | Test loss: 0.5514319539070129\n",
      "Epoch: 26020 | Loss: 0.5245785117149353 | Test loss: 0.551418125629425\n",
      "Epoch: 26030 | Loss: 0.524566650390625 | Test loss: 0.5514042973518372\n",
      "Epoch: 26040 | Loss: 0.5245549082756042 | Test loss: 0.5513904094696045\n",
      "Epoch: 26050 | Loss: 0.5245432257652283 | Test loss: 0.5513765811920166\n",
      "Epoch: 26060 | Loss: 0.524531364440918 | Test loss: 0.5513627529144287\n",
      "Epoch: 26070 | Loss: 0.5245196223258972 | Test loss: 0.551348865032196\n",
      "Epoch: 26080 | Loss: 0.5245079398155212 | Test loss: 0.5513350367546082\n",
      "Epoch: 26090 | Loss: 0.5244960784912109 | Test loss: 0.5513212084770203\n",
      "Epoch: 26100 | Loss: 0.5244843363761902 | Test loss: 0.5513073205947876\n",
      "Epoch: 26110 | Loss: 0.5244725942611694 | Test loss: 0.5512934923171997\n",
      "Epoch: 26120 | Loss: 0.5244607925415039 | Test loss: 0.5512796640396118\n",
      "Epoch: 26130 | Loss: 0.5244490504264832 | Test loss: 0.5512658357620239\n",
      "Epoch: 26140 | Loss: 0.5244373679161072 | Test loss: 0.551252007484436\n",
      "Epoch: 26150 | Loss: 0.5244255065917969 | Test loss: 0.5512381196022034\n",
      "Epoch: 26160 | Loss: 0.5244137644767761 | Test loss: 0.5512242317199707\n",
      "Epoch: 26170 | Loss: 0.5244020819664001 | Test loss: 0.5512104034423828\n",
      "Epoch: 26180 | Loss: 0.5243902802467346 | Test loss: 0.5511965751647949\n",
      "Epoch: 26190 | Loss: 0.5243784785270691 | Test loss: 0.551182746887207\n",
      "Epoch: 26200 | Loss: 0.5243667960166931 | Test loss: 0.5511689186096191\n",
      "Epoch: 26210 | Loss: 0.5243549346923828 | Test loss: 0.5511550307273865\n",
      "Epoch: 26220 | Loss: 0.5243432521820068 | Test loss: 0.5511412024497986\n",
      "Epoch: 26230 | Loss: 0.5243315100669861 | Test loss: 0.5511273741722107\n",
      "Epoch: 26240 | Loss: 0.5243197679519653 | Test loss: 0.5511135458946228\n",
      "Epoch: 26250 | Loss: 0.5243079662322998 | Test loss: 0.5510997176170349\n",
      "Epoch: 26260 | Loss: 0.524296224117279 | Test loss: 0.551085889339447\n",
      "Epoch: 26270 | Loss: 0.5242844223976135 | Test loss: 0.5510719418525696\n",
      "Epoch: 26280 | Loss: 0.5242726802825928 | Test loss: 0.5510581135749817\n",
      "Epoch: 26290 | Loss: 0.524260938167572 | Test loss: 0.5510442852973938\n",
      "Epoch: 26300 | Loss: 0.5242491960525513 | Test loss: 0.5510303974151611\n",
      "Epoch: 26310 | Loss: 0.5242373943328857 | Test loss: 0.5510165691375732\n",
      "Epoch: 26320 | Loss: 0.524225652217865 | Test loss: 0.5510027408599854\n",
      "Epoch: 26330 | Loss: 0.5242139101028442 | Test loss: 0.5509888529777527\n",
      "Epoch: 26340 | Loss: 0.5242021083831787 | Test loss: 0.5509750247001648\n",
      "Epoch: 26350 | Loss: 0.524190366268158 | Test loss: 0.5509611964225769\n",
      "Epoch: 26360 | Loss: 0.5241786241531372 | Test loss: 0.5509473085403442\n",
      "Epoch: 26370 | Loss: 0.5241668224334717 | Test loss: 0.5509335398674011\n",
      "Epoch: 26380 | Loss: 0.5241550803184509 | Test loss: 0.5509196519851685\n",
      "Epoch: 26390 | Loss: 0.5241433382034302 | Test loss: 0.5509058237075806\n",
      "Epoch: 26400 | Loss: 0.5241315960884094 | Test loss: 0.5508919954299927\n",
      "Epoch: 26410 | Loss: 0.5241197943687439 | Test loss: 0.5508781671524048\n",
      "Epoch: 26420 | Loss: 0.5241080522537231 | Test loss: 0.5508642196655273\n",
      "Epoch: 26430 | Loss: 0.5240963101387024 | Test loss: 0.5508503913879395\n",
      "Epoch: 26440 | Loss: 0.5240845084190369 | Test loss: 0.5508365631103516\n",
      "Epoch: 26450 | Loss: 0.5240727663040161 | Test loss: 0.5508227348327637\n",
      "Epoch: 26460 | Loss: 0.5240610241889954 | Test loss: 0.5508089065551758\n",
      "Epoch: 26470 | Loss: 0.5240492224693298 | Test loss: 0.5507950186729431\n",
      "Epoch: 26480 | Loss: 0.5240375399589539 | Test loss: 0.5507811903953552\n",
      "Epoch: 26490 | Loss: 0.5240256786346436 | Test loss: 0.5507673621177673\n",
      "Epoch: 26500 | Loss: 0.5240139365196228 | Test loss: 0.5507535338401794\n",
      "Epoch: 26510 | Loss: 0.524002194404602 | Test loss: 0.5507397055625916\n",
      "Epoch: 26520 | Loss: 0.5239903926849365 | Test loss: 0.5507258772850037\n",
      "Epoch: 26530 | Loss: 0.5239787101745605 | Test loss: 0.550711989402771\n",
      "Epoch: 26540 | Loss: 0.523966908454895 | Test loss: 0.5506981015205383\n",
      "Epoch: 26550 | Loss: 0.5239551663398743 | Test loss: 0.5506842732429504\n",
      "Epoch: 26560 | Loss: 0.5239433646202087 | Test loss: 0.5506703853607178\n",
      "Epoch: 26570 | Loss: 0.523931622505188 | Test loss: 0.5506566166877747\n",
      "Epoch: 26580 | Loss: 0.5239198803901672 | Test loss: 0.550642728805542\n",
      "Epoch: 26590 | Loss: 0.5239080786705017 | Test loss: 0.5506288409233093\n",
      "Epoch: 26600 | Loss: 0.5238963961601257 | Test loss: 0.5506150126457214\n",
      "Epoch: 26610 | Loss: 0.5238845944404602 | Test loss: 0.5506011843681335\n",
      "Epoch: 26620 | Loss: 0.5238728523254395 | Test loss: 0.5505873560905457\n",
      "Epoch: 26630 | Loss: 0.5238611102104187 | Test loss: 0.5505735278129578\n",
      "Epoch: 26640 | Loss: 0.5238493084907532 | Test loss: 0.5505596399307251\n",
      "Epoch: 26650 | Loss: 0.5238375663757324 | Test loss: 0.5505458116531372\n",
      "Epoch: 26660 | Loss: 0.5238257646560669 | Test loss: 0.5505319833755493\n",
      "Epoch: 26670 | Loss: 0.5238140225410461 | Test loss: 0.5505181550979614\n",
      "Epoch: 26680 | Loss: 0.5238022804260254 | Test loss: 0.5505042672157288\n",
      "Epoch: 26690 | Loss: 0.5237905383110046 | Test loss: 0.5504904389381409\n",
      "Epoch: 26700 | Loss: 0.5237787365913391 | Test loss: 0.5504765510559082\n",
      "Epoch: 26710 | Loss: 0.5237669944763184 | Test loss: 0.5504627227783203\n",
      "Epoch: 26720 | Loss: 0.5237551927566528 | Test loss: 0.5504488945007324\n",
      "Epoch: 26730 | Loss: 0.5237434506416321 | Test loss: 0.5504350662231445\n",
      "Epoch: 26740 | Loss: 0.5237317085266113 | Test loss: 0.5504211783409119\n",
      "Epoch: 26750 | Loss: 0.5237199664115906 | Test loss: 0.550407350063324\n",
      "Epoch: 26760 | Loss: 0.523708164691925 | Test loss: 0.5503935217857361\n",
      "Epoch: 26770 | Loss: 0.5236964225769043 | Test loss: 0.5503796935081482\n",
      "Epoch: 26780 | Loss: 0.5236846804618835 | Test loss: 0.5503658652305603\n",
      "Epoch: 26790 | Loss: 0.523672878742218 | Test loss: 0.5503519773483276\n",
      "Epoch: 26800 | Loss: 0.5236611366271973 | Test loss: 0.5503381490707397\n",
      "Epoch: 26810 | Loss: 0.5236493945121765 | Test loss: 0.5503242611885071\n",
      "Epoch: 26820 | Loss: 0.5236377120018005 | Test loss: 0.5503104329109192\n",
      "Epoch: 26830 | Loss: 0.5236258506774902 | Test loss: 0.5502966046333313\n",
      "Epoch: 26840 | Loss: 0.5236141085624695 | Test loss: 0.5502827167510986\n",
      "Epoch: 26850 | Loss: 0.5236024260520935 | Test loss: 0.5502688884735107\n",
      "Epoch: 26860 | Loss: 0.5235905647277832 | Test loss: 0.5502550601959229\n",
      "Epoch: 26870 | Loss: 0.5235788226127625 | Test loss: 0.5502411723136902\n",
      "Epoch: 26880 | Loss: 0.5235671401023865 | Test loss: 0.5502273440361023\n",
      "Epoch: 26890 | Loss: 0.5235552787780762 | Test loss: 0.5502135157585144\n",
      "Epoch: 26900 | Loss: 0.5235435366630554 | Test loss: 0.5501996278762817\n",
      "Epoch: 26910 | Loss: 0.5235317945480347 | Test loss: 0.5501857995986938\n",
      "Epoch: 26920 | Loss: 0.5235199928283691 | Test loss: 0.550171971321106\n",
      "Epoch: 26930 | Loss: 0.5235082507133484 | Test loss: 0.5501581430435181\n",
      "Epoch: 26940 | Loss: 0.5234965682029724 | Test loss: 0.5501443147659302\n",
      "Epoch: 26950 | Loss: 0.5234847068786621 | Test loss: 0.5501304268836975\n",
      "Epoch: 26960 | Loss: 0.5234729647636414 | Test loss: 0.5501165390014648\n",
      "Epoch: 26970 | Loss: 0.5234612822532654 | Test loss: 0.550102710723877\n",
      "Epoch: 26980 | Loss: 0.5234494805335999 | Test loss: 0.5500888824462891\n",
      "Epoch: 26990 | Loss: 0.5234376788139343 | Test loss: 0.5500750541687012\n",
      "Epoch: 27000 | Loss: 0.5234259963035583 | Test loss: 0.5500612258911133\n",
      "Epoch: 27010 | Loss: 0.523414134979248 | Test loss: 0.5500473380088806\n",
      "Epoch: 27020 | Loss: 0.5234024524688721 | Test loss: 0.5500335097312927\n",
      "Epoch: 27030 | Loss: 0.5233907103538513 | Test loss: 0.5500196814537048\n",
      "Epoch: 27040 | Loss: 0.5233789682388306 | Test loss: 0.5500058531761169\n",
      "Epoch: 27050 | Loss: 0.523367166519165 | Test loss: 0.549992024898529\n",
      "Epoch: 27060 | Loss: 0.5233554244041443 | Test loss: 0.5499781966209412\n",
      "Epoch: 27070 | Loss: 0.5233436226844788 | Test loss: 0.5499642491340637\n",
      "Epoch: 27080 | Loss: 0.523331880569458 | Test loss: 0.5499504208564758\n",
      "Epoch: 27090 | Loss: 0.5233201384544373 | Test loss: 0.5499365925788879\n",
      "Epoch: 27100 | Loss: 0.5233083963394165 | Test loss: 0.5499227046966553\n",
      "Epoch: 27110 | Loss: 0.523296594619751 | Test loss: 0.5499088764190674\n",
      "Epoch: 27120 | Loss: 0.5232848525047302 | Test loss: 0.5498950481414795\n",
      "Epoch: 27130 | Loss: 0.5232731103897095 | Test loss: 0.5498811602592468\n",
      "Epoch: 27140 | Loss: 0.523261308670044 | Test loss: 0.5498673319816589\n",
      "Epoch: 27150 | Loss: 0.5232495665550232 | Test loss: 0.549853503704071\n",
      "Epoch: 27160 | Loss: 0.5232378244400024 | Test loss: 0.5498396158218384\n",
      "Epoch: 27170 | Loss: 0.5232260227203369 | Test loss: 0.5498258471488953\n",
      "Epoch: 27180 | Loss: 0.5232142806053162 | Test loss: 0.5498119592666626\n",
      "Epoch: 27190 | Loss: 0.5232025384902954 | Test loss: 0.5497981309890747\n",
      "Epoch: 27200 | Loss: 0.5231907963752747 | Test loss: 0.5497843027114868\n",
      "Epoch: 27210 | Loss: 0.5231789946556091 | Test loss: 0.5497704744338989\n",
      "Epoch: 27220 | Loss: 0.5231672525405884 | Test loss: 0.5497565269470215\n",
      "Epoch: 27230 | Loss: 0.5231555104255676 | Test loss: 0.5497426986694336\n",
      "Epoch: 27240 | Loss: 0.5231437087059021 | Test loss: 0.5497288703918457\n",
      "Epoch: 27250 | Loss: 0.5231319665908813 | Test loss: 0.5497150421142578\n",
      "Epoch: 27260 | Loss: 0.5231202244758606 | Test loss: 0.5497012138366699\n",
      "Epoch: 27270 | Loss: 0.5231084227561951 | Test loss: 0.5496873259544373\n",
      "Epoch: 27280 | Loss: 0.5230967402458191 | Test loss: 0.5496734976768494\n",
      "Epoch: 27290 | Loss: 0.5230848789215088 | Test loss: 0.5496596693992615\n",
      "Epoch: 27300 | Loss: 0.523073136806488 | Test loss: 0.5496458411216736\n",
      "Epoch: 27310 | Loss: 0.5230613946914673 | Test loss: 0.5496320128440857\n",
      "Epoch: 27320 | Loss: 0.5230495929718018 | Test loss: 0.5496181845664978\n",
      "Epoch: 27330 | Loss: 0.5230379104614258 | Test loss: 0.5496042966842651\n",
      "Epoch: 27340 | Loss: 0.5230261087417603 | Test loss: 0.5495904088020325\n",
      "Epoch: 27350 | Loss: 0.5230143666267395 | Test loss: 0.5495765805244446\n",
      "Epoch: 27360 | Loss: 0.523002564907074 | Test loss: 0.5495626926422119\n",
      "Epoch: 27370 | Loss: 0.5229908227920532 | Test loss: 0.5495489239692688\n",
      "Epoch: 27380 | Loss: 0.5229790806770325 | Test loss: 0.5495350360870361\n",
      "Epoch: 27390 | Loss: 0.5229672789573669 | Test loss: 0.5495211482048035\n",
      "Epoch: 27400 | Loss: 0.522955596446991 | Test loss: 0.5495073199272156\n",
      "Epoch: 27410 | Loss: 0.5229437947273254 | Test loss: 0.5494934916496277\n",
      "Epoch: 27420 | Loss: 0.5229320526123047 | Test loss: 0.5494796633720398\n",
      "Epoch: 27430 | Loss: 0.5229203104972839 | Test loss: 0.5494658350944519\n",
      "Epoch: 27440 | Loss: 0.5229085087776184 | Test loss: 0.5494519472122192\n",
      "Epoch: 27450 | Loss: 0.5228967666625977 | Test loss: 0.5494381189346313\n",
      "Epoch: 27460 | Loss: 0.5228849649429321 | Test loss: 0.5494242906570435\n",
      "Epoch: 27470 | Loss: 0.5228732228279114 | Test loss: 0.5494104623794556\n",
      "Epoch: 27480 | Loss: 0.5228614807128906 | Test loss: 0.5493965744972229\n",
      "Epoch: 27490 | Loss: 0.5228497385978699 | Test loss: 0.549382746219635\n",
      "Epoch: 27500 | Loss: 0.5228379368782043 | Test loss: 0.5493688583374023\n",
      "Epoch: 27510 | Loss: 0.5228261947631836 | Test loss: 0.5493550300598145\n",
      "Epoch: 27520 | Loss: 0.5228143930435181 | Test loss: 0.5493412017822266\n",
      "Epoch: 27530 | Loss: 0.5228026509284973 | Test loss: 0.5493273735046387\n",
      "Epoch: 27540 | Loss: 0.5227909088134766 | Test loss: 0.549313485622406\n",
      "Epoch: 27550 | Loss: 0.5227791666984558 | Test loss: 0.5492996573448181\n",
      "Epoch: 27560 | Loss: 0.5227673649787903 | Test loss: 0.5492858290672302\n",
      "Epoch: 27570 | Loss: 0.5227556228637695 | Test loss: 0.5492720007896423\n",
      "Epoch: 27580 | Loss: 0.5227438807487488 | Test loss: 0.5492581725120544\n",
      "Epoch: 27590 | Loss: 0.5227320790290833 | Test loss: 0.5492442846298218\n",
      "Epoch: 27600 | Loss: 0.5227203369140625 | Test loss: 0.5492304563522339\n",
      "Epoch: 27610 | Loss: 0.5227085947990417 | Test loss: 0.5492165684700012\n",
      "Epoch: 27620 | Loss: 0.5226969122886658 | Test loss: 0.5492027401924133\n",
      "Epoch: 27630 | Loss: 0.5226850509643555 | Test loss: 0.5491889119148254\n",
      "Epoch: 27640 | Loss: 0.5226733088493347 | Test loss: 0.5491750240325928\n",
      "Epoch: 27650 | Loss: 0.5226616263389587 | Test loss: 0.5491611957550049\n",
      "Epoch: 27660 | Loss: 0.5226497650146484 | Test loss: 0.549147367477417\n",
      "Epoch: 27670 | Loss: 0.5226380228996277 | Test loss: 0.5491334795951843\n",
      "Epoch: 27680 | Loss: 0.5226263403892517 | Test loss: 0.5491196513175964\n",
      "Epoch: 27690 | Loss: 0.5226144790649414 | Test loss: 0.5491058230400085\n",
      "Epoch: 27700 | Loss: 0.5226027369499207 | Test loss: 0.5490919351577759\n",
      "Epoch: 27710 | Loss: 0.5225909948348999 | Test loss: 0.549078106880188\n",
      "Epoch: 27720 | Loss: 0.5225791931152344 | Test loss: 0.5490642786026001\n",
      "Epoch: 27730 | Loss: 0.5225674510002136 | Test loss: 0.5490504503250122\n",
      "Epoch: 27740 | Loss: 0.5225557684898376 | Test loss: 0.5490366220474243\n",
      "Epoch: 27750 | Loss: 0.5225439071655273 | Test loss: 0.5490227341651917\n",
      "Epoch: 27760 | Loss: 0.5225321650505066 | Test loss: 0.549008846282959\n",
      "Epoch: 27770 | Loss: 0.5225204825401306 | Test loss: 0.5489950180053711\n",
      "Epoch: 27780 | Loss: 0.5225086808204651 | Test loss: 0.5489811897277832\n",
      "Epoch: 27790 | Loss: 0.5224968791007996 | Test loss: 0.5489673614501953\n",
      "Epoch: 27800 | Loss: 0.5224851965904236 | Test loss: 0.5489535331726074\n",
      "Epoch: 27810 | Loss: 0.5224733352661133 | Test loss: 0.5489396452903748\n",
      "Epoch: 27820 | Loss: 0.5224616527557373 | Test loss: 0.5489258170127869\n",
      "Epoch: 27830 | Loss: 0.5224499106407166 | Test loss: 0.548911988735199\n",
      "Epoch: 27840 | Loss: 0.5224381685256958 | Test loss: 0.5488981604576111\n",
      "Epoch: 27850 | Loss: 0.5224263668060303 | Test loss: 0.5488843321800232\n",
      "Epoch: 27860 | Loss: 0.5224146246910095 | Test loss: 0.5488705039024353\n",
      "Epoch: 27870 | Loss: 0.522402822971344 | Test loss: 0.5488565564155579\n",
      "Epoch: 27880 | Loss: 0.5223910808563232 | Test loss: 0.54884272813797\n",
      "Epoch: 27890 | Loss: 0.5223793387413025 | Test loss: 0.5488288998603821\n",
      "Epoch: 27900 | Loss: 0.5223675966262817 | Test loss: 0.5488150119781494\n",
      "Epoch: 27910 | Loss: 0.5223557949066162 | Test loss: 0.5488011837005615\n",
      "Epoch: 27920 | Loss: 0.5223440527915955 | Test loss: 0.5487873554229736\n",
      "Epoch: 27930 | Loss: 0.5223323106765747 | Test loss: 0.548773467540741\n",
      "Epoch: 27940 | Loss: 0.5223205089569092 | Test loss: 0.5487596392631531\n",
      "Epoch: 27950 | Loss: 0.5223087668418884 | Test loss: 0.5487458109855652\n",
      "Epoch: 27960 | Loss: 0.5222970247268677 | Test loss: 0.5487319231033325\n",
      "Epoch: 27970 | Loss: 0.5222852230072021 | Test loss: 0.5487181544303894\n",
      "Epoch: 27980 | Loss: 0.5222734808921814 | Test loss: 0.5487042665481567\n",
      "Epoch: 27990 | Loss: 0.5222617387771606 | Test loss: 0.5486904382705688\n",
      "Epoch: 28000 | Loss: 0.5222499966621399 | Test loss: 0.548676609992981\n",
      "Epoch: 28010 | Loss: 0.5222381949424744 | Test loss: 0.5486627817153931\n",
      "Epoch: 28020 | Loss: 0.5222264528274536 | Test loss: 0.5486488342285156\n",
      "Epoch: 28030 | Loss: 0.5222147107124329 | Test loss: 0.5486350059509277\n",
      "Epoch: 28040 | Loss: 0.5222029089927673 | Test loss: 0.5486211776733398\n",
      "Epoch: 28050 | Loss: 0.5221911668777466 | Test loss: 0.548607349395752\n",
      "Epoch: 28060 | Loss: 0.5221794247627258 | Test loss: 0.5485935211181641\n",
      "Epoch: 28070 | Loss: 0.5221676230430603 | Test loss: 0.5485796332359314\n",
      "Epoch: 28080 | Loss: 0.5221559405326843 | Test loss: 0.5485658049583435\n",
      "Epoch: 28090 | Loss: 0.522144079208374 | Test loss: 0.5485519766807556\n",
      "Epoch: 28100 | Loss: 0.5221323370933533 | Test loss: 0.5485381484031677\n",
      "Epoch: 28110 | Loss: 0.5221205949783325 | Test loss: 0.5485243201255798\n",
      "Epoch: 28120 | Loss: 0.522108793258667 | Test loss: 0.5485104918479919\n",
      "Epoch: 28130 | Loss: 0.522097110748291 | Test loss: 0.5484966039657593\n",
      "Epoch: 28140 | Loss: 0.5220853090286255 | Test loss: 0.5484827160835266\n",
      "Epoch: 28150 | Loss: 0.5220735669136047 | Test loss: 0.5484688878059387\n",
      "Epoch: 28160 | Loss: 0.5220617651939392 | Test loss: 0.548454999923706\n",
      "Epoch: 28170 | Loss: 0.5220500230789185 | Test loss: 0.5484412312507629\n",
      "Epoch: 28180 | Loss: 0.5220382809638977 | Test loss: 0.5484273433685303\n",
      "Epoch: 28190 | Loss: 0.5220264792442322 | Test loss: 0.5484134554862976\n",
      "Epoch: 28200 | Loss: 0.5220147967338562 | Test loss: 0.5483996272087097\n",
      "Epoch: 28210 | Loss: 0.5220029950141907 | Test loss: 0.5483857989311218\n",
      "Epoch: 28220 | Loss: 0.5219912528991699 | Test loss: 0.5483719706535339\n",
      "Epoch: 28230 | Loss: 0.5219795107841492 | Test loss: 0.548358142375946\n",
      "Epoch: 28240 | Loss: 0.5219677090644836 | Test loss: 0.5483442544937134\n",
      "Epoch: 28250 | Loss: 0.5219559669494629 | Test loss: 0.5483304262161255\n",
      "Epoch: 28260 | Loss: 0.5219441652297974 | Test loss: 0.5483165979385376\n",
      "Epoch: 28270 | Loss: 0.5219324231147766 | Test loss: 0.5483027696609497\n",
      "Epoch: 28280 | Loss: 0.5219206809997559 | Test loss: 0.548288881778717\n",
      "Epoch: 28290 | Loss: 0.5219089388847351 | Test loss: 0.5482750535011292\n",
      "Epoch: 28300 | Loss: 0.5218971371650696 | Test loss: 0.5482611656188965\n",
      "Epoch: 28310 | Loss: 0.5218853950500488 | Test loss: 0.5482473373413086\n",
      "Epoch: 28320 | Loss: 0.5218735933303833 | Test loss: 0.5482335090637207\n",
      "Epoch: 28330 | Loss: 0.5218618512153625 | Test loss: 0.5482196807861328\n",
      "Epoch: 28340 | Loss: 0.5218501091003418 | Test loss: 0.5482057929039001\n",
      "Epoch: 28350 | Loss: 0.521838366985321 | Test loss: 0.5481919646263123\n",
      "Epoch: 28360 | Loss: 0.5218265652656555 | Test loss: 0.5481781363487244\n",
      "Epoch: 28370 | Loss: 0.5218148231506348 | Test loss: 0.5481643080711365\n",
      "Epoch: 28380 | Loss: 0.521803081035614 | Test loss: 0.5481504797935486\n",
      "Epoch: 28390 | Loss: 0.5217912793159485 | Test loss: 0.5481365919113159\n",
      "Epoch: 28400 | Loss: 0.5217795372009277 | Test loss: 0.548122763633728\n",
      "Epoch: 28410 | Loss: 0.521767795085907 | Test loss: 0.5481088757514954\n",
      "Epoch: 28420 | Loss: 0.521756112575531 | Test loss: 0.5480950474739075\n",
      "Epoch: 28430 | Loss: 0.5217442512512207 | Test loss: 0.5480812191963196\n",
      "Epoch: 28440 | Loss: 0.5217325091362 | Test loss: 0.5480673313140869\n",
      "Epoch: 28450 | Loss: 0.521720826625824 | Test loss: 0.548053503036499\n",
      "Epoch: 28460 | Loss: 0.5217089653015137 | Test loss: 0.5480396747589111\n",
      "Epoch: 28470 | Loss: 0.5216972231864929 | Test loss: 0.5480257868766785\n",
      "Epoch: 28480 | Loss: 0.5216855406761169 | Test loss: 0.5480119585990906\n",
      "Epoch: 28490 | Loss: 0.5216736793518066 | Test loss: 0.5479981303215027\n",
      "Epoch: 28500 | Loss: 0.5216619372367859 | Test loss: 0.54798424243927\n",
      "Epoch: 28510 | Loss: 0.5216501951217651 | Test loss: 0.5479704141616821\n",
      "Epoch: 28520 | Loss: 0.5216383934020996 | Test loss: 0.5479565858840942\n",
      "Epoch: 28530 | Loss: 0.5216266512870789 | Test loss: 0.5479427576065063\n",
      "Epoch: 28540 | Loss: 0.5216149687767029 | Test loss: 0.5479289293289185\n",
      "Epoch: 28550 | Loss: 0.5216031074523926 | Test loss: 0.5479150414466858\n",
      "Epoch: 28560 | Loss: 0.5215913653373718 | Test loss: 0.5479011535644531\n",
      "Epoch: 28570 | Loss: 0.5215796828269958 | Test loss: 0.5478873252868652\n",
      "Epoch: 28580 | Loss: 0.5215678811073303 | Test loss: 0.5478734970092773\n",
      "Epoch: 28590 | Loss: 0.5215560793876648 | Test loss: 0.5478596687316895\n",
      "Epoch: 28600 | Loss: 0.5215443968772888 | Test loss: 0.5478458404541016\n",
      "Epoch: 28610 | Loss: 0.5215325355529785 | Test loss: 0.5478319525718689\n",
      "Epoch: 28620 | Loss: 0.5215208530426025 | Test loss: 0.547818124294281\n",
      "Epoch: 28630 | Loss: 0.5215091109275818 | Test loss: 0.5478042960166931\n",
      "Epoch: 28640 | Loss: 0.521497368812561 | Test loss: 0.5477904677391052\n",
      "Epoch: 28650 | Loss: 0.5214855670928955 | Test loss: 0.5477766394615173\n",
      "Epoch: 28660 | Loss: 0.5214738249778748 | Test loss: 0.5477628111839294\n",
      "Epoch: 28670 | Loss: 0.5214620232582092 | Test loss: 0.547748863697052\n",
      "Epoch: 28680 | Loss: 0.5214502811431885 | Test loss: 0.5477350354194641\n",
      "Epoch: 28690 | Loss: 0.5214385390281677 | Test loss: 0.5477212071418762\n",
      "Epoch: 28700 | Loss: 0.521426796913147 | Test loss: 0.5477073192596436\n",
      "Epoch: 28710 | Loss: 0.5214149951934814 | Test loss: 0.5476934909820557\n",
      "Epoch: 28720 | Loss: 0.5214032530784607 | Test loss: 0.5476796627044678\n",
      "Epoch: 28730 | Loss: 0.5213915109634399 | Test loss: 0.5476657748222351\n",
      "Epoch: 28740 | Loss: 0.5213797092437744 | Test loss: 0.5476519465446472\n",
      "Epoch: 28750 | Loss: 0.5213679671287537 | Test loss: 0.5476381182670593\n",
      "Epoch: 28760 | Loss: 0.5213562250137329 | Test loss: 0.5476242303848267\n",
      "Epoch: 28770 | Loss: 0.5213444232940674 | Test loss: 0.5476104617118835\n",
      "Epoch: 28780 | Loss: 0.5213326811790466 | Test loss: 0.5475965738296509\n",
      "Epoch: 28790 | Loss: 0.5213209390640259 | Test loss: 0.547582745552063\n",
      "Epoch: 28800 | Loss: 0.5213091969490051 | Test loss: 0.5475689172744751\n",
      "Epoch: 28810 | Loss: 0.5212973952293396 | Test loss: 0.5475550889968872\n",
      "Epoch: 28820 | Loss: 0.5212856531143188 | Test loss: 0.5475411415100098\n",
      "Epoch: 28830 | Loss: 0.5212739109992981 | Test loss: 0.5475273132324219\n",
      "Epoch: 28840 | Loss: 0.5212621092796326 | Test loss: 0.547513484954834\n",
      "Epoch: 28850 | Loss: 0.5212503671646118 | Test loss: 0.5474996566772461\n",
      "Epoch: 28860 | Loss: 0.5212386250495911 | Test loss: 0.5474858283996582\n",
      "Epoch: 28870 | Loss: 0.5212268233299255 | Test loss: 0.5474719405174255\n",
      "Epoch: 28880 | Loss: 0.5212151408195496 | Test loss: 0.5474581122398376\n",
      "Epoch: 28890 | Loss: 0.5212032794952393 | Test loss: 0.5474442839622498\n",
      "Epoch: 28900 | Loss: 0.5211915373802185 | Test loss: 0.5474304556846619\n",
      "Epoch: 28910 | Loss: 0.5211797952651978 | Test loss: 0.547416627407074\n",
      "Epoch: 28920 | Loss: 0.5211679935455322 | Test loss: 0.5474027991294861\n",
      "Epoch: 28930 | Loss: 0.5211563110351562 | Test loss: 0.5473889112472534\n",
      "Epoch: 28940 | Loss: 0.5211445093154907 | Test loss: 0.5473750233650208\n",
      "Epoch: 28950 | Loss: 0.52113276720047 | Test loss: 0.5473611950874329\n",
      "Epoch: 28960 | Loss: 0.5211209654808044 | Test loss: 0.5473473072052002\n",
      "Epoch: 28970 | Loss: 0.5211092233657837 | Test loss: 0.5473335385322571\n",
      "Epoch: 28980 | Loss: 0.5210974812507629 | Test loss: 0.5473196506500244\n",
      "Epoch: 28990 | Loss: 0.5210856795310974 | Test loss: 0.5473057627677917\n",
      "Epoch: 29000 | Loss: 0.5210739970207214 | Test loss: 0.5472919344902039\n",
      "Epoch: 29010 | Loss: 0.5210621953010559 | Test loss: 0.547278106212616\n",
      "Epoch: 29020 | Loss: 0.5210504531860352 | Test loss: 0.5472642779350281\n",
      "Epoch: 29030 | Loss: 0.5210387110710144 | Test loss: 0.5472504496574402\n",
      "Epoch: 29040 | Loss: 0.5210269093513489 | Test loss: 0.5472365617752075\n",
      "Epoch: 29050 | Loss: 0.5210151672363281 | Test loss: 0.5472227334976196\n",
      "Epoch: 29060 | Loss: 0.5210033655166626 | Test loss: 0.5472089052200317\n",
      "Epoch: 29070 | Loss: 0.5209916234016418 | Test loss: 0.5471950769424438\n",
      "Epoch: 29080 | Loss: 0.5209798812866211 | Test loss: 0.5471811890602112\n",
      "Epoch: 29090 | Loss: 0.5209681391716003 | Test loss: 0.5471673607826233\n",
      "Epoch: 29100 | Loss: 0.5209563374519348 | Test loss: 0.5471534729003906\n",
      "Epoch: 29110 | Loss: 0.5209445953369141 | Test loss: 0.5471396446228027\n",
      "Epoch: 29120 | Loss: 0.5209327936172485 | Test loss: 0.5471258163452148\n",
      "Epoch: 29130 | Loss: 0.5209210515022278 | Test loss: 0.547111988067627\n",
      "Epoch: 29140 | Loss: 0.520909309387207 | Test loss: 0.5470981001853943\n",
      "Epoch: 29150 | Loss: 0.5208975672721863 | Test loss: 0.5470842719078064\n",
      "Epoch: 29160 | Loss: 0.5208857655525208 | Test loss: 0.5470704436302185\n",
      "Epoch: 29170 | Loss: 0.5208740234375 | Test loss: 0.5470566153526306\n",
      "Epoch: 29180 | Loss: 0.5208622813224792 | Test loss: 0.5470427870750427\n",
      "Epoch: 29190 | Loss: 0.5208504796028137 | Test loss: 0.5470288991928101\n",
      "Epoch: 29200 | Loss: 0.520838737487793 | Test loss: 0.5470150709152222\n",
      "Epoch: 29210 | Loss: 0.5208269953727722 | Test loss: 0.5470011830329895\n",
      "Epoch: 29220 | Loss: 0.5208153128623962 | Test loss: 0.5469873547554016\n",
      "Epoch: 29230 | Loss: 0.5208034515380859 | Test loss: 0.5469735264778137\n",
      "Epoch: 29240 | Loss: 0.5207917094230652 | Test loss: 0.546959638595581\n",
      "Epoch: 29250 | Loss: 0.5207800269126892 | Test loss: 0.5469458103179932\n",
      "Epoch: 29260 | Loss: 0.5207681655883789 | Test loss: 0.5469319820404053\n",
      "Epoch: 29270 | Loss: 0.5207564234733582 | Test loss: 0.5469180941581726\n",
      "Epoch: 29280 | Loss: 0.5207447409629822 | Test loss: 0.5469042658805847\n",
      "Epoch: 29290 | Loss: 0.5207328796386719 | Test loss: 0.5468904376029968\n",
      "Epoch: 29300 | Loss: 0.5207211375236511 | Test loss: 0.5468765497207642\n",
      "Epoch: 29310 | Loss: 0.5207093954086304 | Test loss: 0.5468627214431763\n",
      "Epoch: 29320 | Loss: 0.5206975936889648 | Test loss: 0.5468488931655884\n",
      "Epoch: 29330 | Loss: 0.5206858515739441 | Test loss: 0.5468350648880005\n",
      "Epoch: 29340 | Loss: 0.5206741690635681 | Test loss: 0.5468212366104126\n",
      "Epoch: 29350 | Loss: 0.5206623077392578 | Test loss: 0.5468073487281799\n",
      "Epoch: 29360 | Loss: 0.5206505656242371 | Test loss: 0.5467934608459473\n",
      "Epoch: 29370 | Loss: 0.5206388831138611 | Test loss: 0.5467796325683594\n",
      "Epoch: 29380 | Loss: 0.5206270813941956 | Test loss: 0.5467658042907715\n",
      "Epoch: 29390 | Loss: 0.52061527967453 | Test loss: 0.5467519760131836\n",
      "Epoch: 29400 | Loss: 0.520603597164154 | Test loss: 0.5467381477355957\n",
      "Epoch: 29410 | Loss: 0.5205917358398438 | Test loss: 0.546724259853363\n",
      "Epoch: 29420 | Loss: 0.5205800533294678 | Test loss: 0.5467104315757751\n",
      "Epoch: 29430 | Loss: 0.520568311214447 | Test loss: 0.5466966032981873\n",
      "Epoch: 29440 | Loss: 0.5205565690994263 | Test loss: 0.5466827750205994\n",
      "Epoch: 29450 | Loss: 0.5205447673797607 | Test loss: 0.5466689467430115\n",
      "Epoch: 29460 | Loss: 0.52053302526474 | Test loss: 0.5466551184654236\n",
      "Epoch: 29470 | Loss: 0.5205212235450745 | Test loss: 0.5466411709785461\n",
      "Epoch: 29480 | Loss: 0.5205094814300537 | Test loss: 0.5466273427009583\n",
      "Epoch: 29490 | Loss: 0.520497739315033 | Test loss: 0.5466135144233704\n",
      "Epoch: 29500 | Loss: 0.5204859972000122 | Test loss: 0.5465996265411377\n",
      "Epoch: 29510 | Loss: 0.5204741954803467 | Test loss: 0.5465857982635498\n",
      "Epoch: 29520 | Loss: 0.5204624533653259 | Test loss: 0.5465719699859619\n",
      "Epoch: 29530 | Loss: 0.5204507112503052 | Test loss: 0.5465580821037292\n",
      "Epoch: 29540 | Loss: 0.5204389095306396 | Test loss: 0.5465442538261414\n",
      "Epoch: 29550 | Loss: 0.5204271674156189 | Test loss: 0.5465304255485535\n",
      "Epoch: 29560 | Loss: 0.5204154253005981 | Test loss: 0.5465165376663208\n",
      "Epoch: 29570 | Loss: 0.5204036235809326 | Test loss: 0.5465027689933777\n",
      "Epoch: 29580 | Loss: 0.5203918814659119 | Test loss: 0.546488881111145\n",
      "Epoch: 29590 | Loss: 0.5203801393508911 | Test loss: 0.5464750528335571\n",
      "Epoch: 29600 | Loss: 0.5203683972358704 | Test loss: 0.5464612245559692\n",
      "Epoch: 29610 | Loss: 0.5203565955162048 | Test loss: 0.5464473962783813\n",
      "Epoch: 29620 | Loss: 0.5203448534011841 | Test loss: 0.5464334487915039\n",
      "Epoch: 29630 | Loss: 0.5203331112861633 | Test loss: 0.546419620513916\n",
      "Epoch: 29640 | Loss: 0.5203213095664978 | Test loss: 0.5464057922363281\n",
      "Epoch: 29650 | Loss: 0.520309567451477 | Test loss: 0.5463919639587402\n",
      "Epoch: 29660 | Loss: 0.5202978253364563 | Test loss: 0.5463781356811523\n",
      "Epoch: 29670 | Loss: 0.5202860236167908 | Test loss: 0.5463642477989197\n",
      "Epoch: 29680 | Loss: 0.5202743411064148 | Test loss: 0.5463504195213318\n",
      "Epoch: 29690 | Loss: 0.5202624797821045 | Test loss: 0.5463365912437439\n",
      "Epoch: 29700 | Loss: 0.5202507376670837 | Test loss: 0.546322762966156\n",
      "Epoch: 29710 | Loss: 0.520238995552063 | Test loss: 0.5463089346885681\n",
      "Epoch: 29720 | Loss: 0.5202271938323975 | Test loss: 0.5462951064109802\n",
      "Epoch: 29730 | Loss: 0.5202155113220215 | Test loss: 0.5462812185287476\n",
      "Epoch: 29740 | Loss: 0.520203709602356 | Test loss: 0.5462673306465149\n",
      "Epoch: 29750 | Loss: 0.5201919674873352 | Test loss: 0.546253502368927\n",
      "Epoch: 29760 | Loss: 0.5201801657676697 | Test loss: 0.5462396144866943\n",
      "Epoch: 29770 | Loss: 0.5201684236526489 | Test loss: 0.5462258458137512\n",
      "Epoch: 29780 | Loss: 0.5201566815376282 | Test loss: 0.5462119579315186\n",
      "Epoch: 29790 | Loss: 0.5201448798179626 | Test loss: 0.5461980700492859\n",
      "Epoch: 29800 | Loss: 0.5201331973075867 | Test loss: 0.546184241771698\n",
      "Epoch: 29810 | Loss: 0.5201213955879211 | Test loss: 0.5461704134941101\n",
      "Epoch: 29820 | Loss: 0.5201096534729004 | Test loss: 0.5461565852165222\n",
      "Epoch: 29830 | Loss: 0.5200979113578796 | Test loss: 0.5461427569389343\n",
      "Epoch: 29840 | Loss: 0.5200861096382141 | Test loss: 0.5461288690567017\n",
      "Epoch: 29850 | Loss: 0.5200743675231934 | Test loss: 0.5461150407791138\n",
      "Epoch: 29860 | Loss: 0.5200625658035278 | Test loss: 0.5461012125015259\n",
      "Epoch: 29870 | Loss: 0.5200508236885071 | Test loss: 0.546087384223938\n",
      "Epoch: 29880 | Loss: 0.5200390815734863 | Test loss: 0.5460734963417053\n",
      "Epoch: 29890 | Loss: 0.5200273394584656 | Test loss: 0.5460596680641174\n",
      "Epoch: 29900 | Loss: 0.5200155377388 | Test loss: 0.5460457801818848\n",
      "Epoch: 29910 | Loss: 0.5200037956237793 | Test loss: 0.5460319519042969\n",
      "Epoch: 29920 | Loss: 0.5199919939041138 | Test loss: 0.546018123626709\n",
      "Epoch: 29930 | Loss: 0.519980251789093 | Test loss: 0.5460042953491211\n",
      "Epoch: 29940 | Loss: 0.5199685096740723 | Test loss: 0.5459904074668884\n",
      "Epoch: 29950 | Loss: 0.5199567675590515 | Test loss: 0.5459765791893005\n",
      "Epoch: 29960 | Loss: 0.519944965839386 | Test loss: 0.5459627509117126\n",
      "Epoch: 29970 | Loss: 0.5199332237243652 | Test loss: 0.5459489226341248\n",
      "Epoch: 29980 | Loss: 0.5199214816093445 | Test loss: 0.5459350943565369\n",
      "Epoch: 29990 | Loss: 0.519909679889679 | Test loss: 0.5459212064743042\n",
      "Epoch: 30000 | Loss: 0.5198979377746582 | Test loss: 0.5459073781967163\n",
      "Epoch: 30010 | Loss: 0.5198861956596375 | Test loss: 0.5458934903144836\n",
      "Epoch: 30020 | Loss: 0.5198745131492615 | Test loss: 0.5458796620368958\n",
      "Epoch: 30030 | Loss: 0.5198626518249512 | Test loss: 0.5458658337593079\n",
      "Epoch: 30040 | Loss: 0.5198509097099304 | Test loss: 0.5458519458770752\n",
      "Epoch: 30050 | Loss: 0.5198392271995544 | Test loss: 0.5458381175994873\n",
      "Epoch: 30060 | Loss: 0.5198273658752441 | Test loss: 0.5458242893218994\n",
      "Epoch: 30070 | Loss: 0.5198156237602234 | Test loss: 0.5458104014396667\n",
      "Epoch: 30080 | Loss: 0.5198039412498474 | Test loss: 0.5457965731620789\n",
      "Epoch: 30090 | Loss: 0.5197920799255371 | Test loss: 0.545782744884491\n",
      "Epoch: 30100 | Loss: 0.5197803378105164 | Test loss: 0.5457688570022583\n",
      "Epoch: 30110 | Loss: 0.5197685956954956 | Test loss: 0.5457550287246704\n",
      "Epoch: 30120 | Loss: 0.5197567939758301 | Test loss: 0.5457412004470825\n",
      "Epoch: 30130 | Loss: 0.5197450518608093 | Test loss: 0.5457273721694946\n",
      "Epoch: 30140 | Loss: 0.5197333693504333 | Test loss: 0.5457135438919067\n",
      "Epoch: 30150 | Loss: 0.519721508026123 | Test loss: 0.5456996560096741\n",
      "Epoch: 30160 | Loss: 0.5197097659111023 | Test loss: 0.5456857681274414\n",
      "Epoch: 30170 | Loss: 0.5196980834007263 | Test loss: 0.5456719398498535\n",
      "Epoch: 30180 | Loss: 0.5196862816810608 | Test loss: 0.5456581115722656\n",
      "Epoch: 30190 | Loss: 0.5196744799613953 | Test loss: 0.5456442832946777\n",
      "Epoch: 30200 | Loss: 0.5196627974510193 | Test loss: 0.5456304550170898\n",
      "Epoch: 30210 | Loss: 0.519650936126709 | Test loss: 0.5456165671348572\n",
      "Epoch: 30220 | Loss: 0.519639253616333 | Test loss: 0.5456027388572693\n",
      "Epoch: 30230 | Loss: 0.5196275115013123 | Test loss: 0.5455889105796814\n",
      "Epoch: 30240 | Loss: 0.5196157693862915 | Test loss: 0.5455750823020935\n",
      "Epoch: 30250 | Loss: 0.519603967666626 | Test loss: 0.5455612540245056\n",
      "Epoch: 30260 | Loss: 0.5195922255516052 | Test loss: 0.5455474257469177\n",
      "Epoch: 30270 | Loss: 0.5195804238319397 | Test loss: 0.5455334782600403\n",
      "Epoch: 30280 | Loss: 0.519568681716919 | Test loss: 0.5455196499824524\n",
      "Epoch: 30290 | Loss: 0.5195569396018982 | Test loss: 0.5455058217048645\n",
      "Epoch: 30300 | Loss: 0.5195451974868774 | Test loss: 0.5454919338226318\n",
      "Epoch: 30310 | Loss: 0.5195333957672119 | Test loss: 0.545478105545044\n",
      "Epoch: 30320 | Loss: 0.5195216536521912 | Test loss: 0.545464277267456\n",
      "Epoch: 30330 | Loss: 0.5195099115371704 | Test loss: 0.5454503893852234\n",
      "Epoch: 30340 | Loss: 0.5194981098175049 | Test loss: 0.5454365611076355\n",
      "Epoch: 30350 | Loss: 0.5194863677024841 | Test loss: 0.5454227328300476\n",
      "Epoch: 30360 | Loss: 0.5194746255874634 | Test loss: 0.5454088449478149\n",
      "Epoch: 30370 | Loss: 0.5194628238677979 | Test loss: 0.5453950762748718\n",
      "Epoch: 30380 | Loss: 0.5194510817527771 | Test loss: 0.5453811883926392\n",
      "Epoch: 30390 | Loss: 0.5194393396377563 | Test loss: 0.5453673601150513\n",
      "Epoch: 30400 | Loss: 0.5194275975227356 | Test loss: 0.5453535318374634\n",
      "Epoch: 30410 | Loss: 0.5194157958030701 | Test loss: 0.5453397035598755\n",
      "Epoch: 30420 | Loss: 0.5194040536880493 | Test loss: 0.545325756072998\n",
      "Epoch: 30430 | Loss: 0.5193923115730286 | Test loss: 0.5453119277954102\n",
      "Epoch: 30440 | Loss: 0.519380509853363 | Test loss: 0.5452980995178223\n",
      "Epoch: 30450 | Loss: 0.5193687677383423 | Test loss: 0.5452842712402344\n",
      "Epoch: 30460 | Loss: 0.5193570256233215 | Test loss: 0.5452704429626465\n",
      "Epoch: 30470 | Loss: 0.519345223903656 | Test loss: 0.5452565550804138\n",
      "Epoch: 30480 | Loss: 0.51933354139328 | Test loss: 0.5452427268028259\n",
      "Epoch: 30490 | Loss: 0.5193216800689697 | Test loss: 0.545228898525238\n",
      "Epoch: 30500 | Loss: 0.519309937953949 | Test loss: 0.5452150702476501\n",
      "Epoch: 30510 | Loss: 0.5192981958389282 | Test loss: 0.5452012419700623\n",
      "Epoch: 30520 | Loss: 0.5192863941192627 | Test loss: 0.5451874136924744\n",
      "Epoch: 30530 | Loss: 0.5192747116088867 | Test loss: 0.5451735258102417\n",
      "Epoch: 30540 | Loss: 0.5192629098892212 | Test loss: 0.545159637928009\n",
      "Epoch: 30550 | Loss: 0.5192511677742004 | Test loss: 0.5451458096504211\n",
      "Epoch: 30560 | Loss: 0.5192393660545349 | Test loss: 0.5451319217681885\n",
      "Epoch: 30570 | Loss: 0.5192276239395142 | Test loss: 0.5451181530952454\n",
      "Epoch: 30580 | Loss: 0.5192158818244934 | Test loss: 0.5451042652130127\n",
      "Epoch: 30590 | Loss: 0.5192040801048279 | Test loss: 0.54509037733078\n",
      "Epoch: 30600 | Loss: 0.5191923975944519 | Test loss: 0.5450765490531921\n",
      "Epoch: 30610 | Loss: 0.5191805958747864 | Test loss: 0.5450627207756042\n",
      "Epoch: 30620 | Loss: 0.5191688537597656 | Test loss: 0.5450488924980164\n",
      "Epoch: 30630 | Loss: 0.5191571116447449 | Test loss: 0.5450350642204285\n",
      "Epoch: 30640 | Loss: 0.5191453099250793 | Test loss: 0.5450211763381958\n",
      "Epoch: 30650 | Loss: 0.5191335678100586 | Test loss: 0.5450073480606079\n",
      "Epoch: 30660 | Loss: 0.5191217660903931 | Test loss: 0.54499351978302\n",
      "Epoch: 30670 | Loss: 0.5191100239753723 | Test loss: 0.5449796915054321\n",
      "Epoch: 30680 | Loss: 0.5190982818603516 | Test loss: 0.5449658036231995\n",
      "Epoch: 30690 | Loss: 0.5190865397453308 | Test loss: 0.5449519753456116\n",
      "Epoch: 30700 | Loss: 0.5190747380256653 | Test loss: 0.5449380874633789\n",
      "Epoch: 30710 | Loss: 0.5190629959106445 | Test loss: 0.544924259185791\n",
      "Epoch: 30720 | Loss: 0.519051194190979 | Test loss: 0.5449104309082031\n",
      "Epoch: 30730 | Loss: 0.5190394520759583 | Test loss: 0.5448966026306152\n",
      "Epoch: 30740 | Loss: 0.5190277099609375 | Test loss: 0.5448827147483826\n",
      "Epoch: 30750 | Loss: 0.5190159678459167 | Test loss: 0.5448688864707947\n",
      "Epoch: 30760 | Loss: 0.5190041661262512 | Test loss: 0.5448550581932068\n",
      "Epoch: 30770 | Loss: 0.5189924240112305 | Test loss: 0.5448412299156189\n",
      "Epoch: 30780 | Loss: 0.5189806818962097 | Test loss: 0.544827401638031\n",
      "Epoch: 30790 | Loss: 0.5189688801765442 | Test loss: 0.5448135137557983\n",
      "Epoch: 30800 | Loss: 0.5189571380615234 | Test loss: 0.5447996854782104\n",
      "Epoch: 30810 | Loss: 0.5189453959465027 | Test loss: 0.5447857975959778\n",
      "Epoch: 30820 | Loss: 0.5189337134361267 | Test loss: 0.5447719693183899\n",
      "Epoch: 30830 | Loss: 0.5189218521118164 | Test loss: 0.544758141040802\n",
      "Epoch: 30840 | Loss: 0.5189101099967957 | Test loss: 0.5447442531585693\n",
      "Epoch: 30850 | Loss: 0.5188984274864197 | Test loss: 0.5447304248809814\n",
      "Epoch: 30860 | Loss: 0.5188865661621094 | Test loss: 0.5447165966033936\n",
      "Epoch: 30870 | Loss: 0.5188748240470886 | Test loss: 0.5447027087211609\n",
      "Epoch: 30880 | Loss: 0.5188631415367126 | Test loss: 0.544688880443573\n",
      "Epoch: 30890 | Loss: 0.5188512802124023 | Test loss: 0.5446750521659851\n",
      "Epoch: 30900 | Loss: 0.5188395380973816 | Test loss: 0.5446611642837524\n",
      "Epoch: 30910 | Loss: 0.5188277959823608 | Test loss: 0.5446473360061646\n",
      "Epoch: 30920 | Loss: 0.5188159942626953 | Test loss: 0.5446335077285767\n",
      "Epoch: 30930 | Loss: 0.5188042521476746 | Test loss: 0.5446196794509888\n",
      "Epoch: 30940 | Loss: 0.5187925696372986 | Test loss: 0.5446058511734009\n",
      "Epoch: 30950 | Loss: 0.5187807083129883 | Test loss: 0.5445919632911682\n",
      "Epoch: 30960 | Loss: 0.5187689661979675 | Test loss: 0.5445780754089355\n",
      "Epoch: 30970 | Loss: 0.5187572836875916 | Test loss: 0.5445642471313477\n",
      "Epoch: 30980 | Loss: 0.518745481967926 | Test loss: 0.5445504188537598\n",
      "Epoch: 30990 | Loss: 0.5187336802482605 | Test loss: 0.5445365905761719\n",
      "Epoch: 31000 | Loss: 0.5187219977378845 | Test loss: 0.544522762298584\n",
      "Epoch: 31010 | Loss: 0.5187101364135742 | Test loss: 0.5445088744163513\n",
      "Epoch: 31020 | Loss: 0.5186984539031982 | Test loss: 0.5444950461387634\n",
      "Epoch: 31030 | Loss: 0.5186867117881775 | Test loss: 0.5444812178611755\n",
      "Epoch: 31040 | Loss: 0.5186749696731567 | Test loss: 0.5444673895835876\n",
      "Epoch: 31050 | Loss: 0.5186631679534912 | Test loss: 0.5444535613059998\n",
      "Epoch: 31060 | Loss: 0.5186514258384705 | Test loss: 0.5444397330284119\n",
      "Epoch: 31070 | Loss: 0.5186396241188049 | Test loss: 0.5444257855415344\n",
      "Epoch: 31080 | Loss: 0.5186278820037842 | Test loss: 0.5444119572639465\n",
      "Epoch: 31090 | Loss: 0.5186161398887634 | Test loss: 0.5443981289863586\n",
      "Epoch: 31100 | Loss: 0.5186043977737427 | Test loss: 0.544384241104126\n",
      "Epoch: 31110 | Loss: 0.5185925960540771 | Test loss: 0.5443704128265381\n",
      "Epoch: 31120 | Loss: 0.5185808539390564 | Test loss: 0.5443565845489502\n",
      "Epoch: 31130 | Loss: 0.5185691118240356 | Test loss: 0.5443426966667175\n",
      "Epoch: 31140 | Loss: 0.5185573101043701 | Test loss: 0.5443288683891296\n",
      "Epoch: 31150 | Loss: 0.5185455679893494 | Test loss: 0.5443150401115417\n",
      "Epoch: 31160 | Loss: 0.5185338258743286 | Test loss: 0.5443011522293091\n",
      "Epoch: 31170 | Loss: 0.5185220241546631 | Test loss: 0.544287383556366\n",
      "Epoch: 31180 | Loss: 0.5185102820396423 | Test loss: 0.5442734956741333\n",
      "Epoch: 31190 | Loss: 0.5184985399246216 | Test loss: 0.5442596673965454\n",
      "Epoch: 31200 | Loss: 0.5184867978096008 | Test loss: 0.5442458391189575\n",
      "Epoch: 31210 | Loss: 0.5184749960899353 | Test loss: 0.5442320108413696\n",
      "Epoch: 31220 | Loss: 0.5184632539749146 | Test loss: 0.5442180633544922\n",
      "Epoch: 31230 | Loss: 0.5184515118598938 | Test loss: 0.5442042350769043\n",
      "Epoch: 31240 | Loss: 0.5184397101402283 | Test loss: 0.5441904067993164\n",
      "Epoch: 31250 | Loss: 0.5184279680252075 | Test loss: 0.5441765785217285\n",
      "Epoch: 31260 | Loss: 0.5184162259101868 | Test loss: 0.5441627502441406\n",
      "Epoch: 31270 | Loss: 0.5184044241905212 | Test loss: 0.544148862361908\n",
      "Epoch: 31280 | Loss: 0.5183927416801453 | Test loss: 0.5441350340843201\n",
      "Epoch: 31290 | Loss: 0.518380880355835 | Test loss: 0.5441212058067322\n",
      "Epoch: 31300 | Loss: 0.5183691382408142 | Test loss: 0.5441073775291443\n",
      "Epoch: 31310 | Loss: 0.5183573961257935 | Test loss: 0.5440935492515564\n",
      "Epoch: 31320 | Loss: 0.5183455944061279 | Test loss: 0.5440797209739685\n",
      "Epoch: 31330 | Loss: 0.518333911895752 | Test loss: 0.5440658330917358\n",
      "Epoch: 31340 | Loss: 0.5183221101760864 | Test loss: 0.5440519452095032\n",
      "Epoch: 31350 | Loss: 0.5183103680610657 | Test loss: 0.5440381169319153\n",
      "Epoch: 31360 | Loss: 0.5182985663414001 | Test loss: 0.5440242290496826\n",
      "Epoch: 31370 | Loss: 0.5182868242263794 | Test loss: 0.5440104603767395\n",
      "Epoch: 31380 | Loss: 0.5182750821113586 | Test loss: 0.5439965724945068\n",
      "Epoch: 31390 | Loss: 0.5182632803916931 | Test loss: 0.5439826846122742\n",
      "Epoch: 31400 | Loss: 0.5182515978813171 | Test loss: 0.5439688563346863\n",
      "Epoch: 31410 | Loss: 0.5182397961616516 | Test loss: 0.5439550280570984\n",
      "Epoch: 31420 | Loss: 0.5182280540466309 | Test loss: 0.5439411997795105\n",
      "Epoch: 31430 | Loss: 0.5182163119316101 | Test loss: 0.5439273715019226\n",
      "Epoch: 31440 | Loss: 0.5182045102119446 | Test loss: 0.5439134836196899\n",
      "Epoch: 31450 | Loss: 0.5181927680969238 | Test loss: 0.543899655342102\n",
      "Epoch: 31460 | Loss: 0.5181809663772583 | Test loss: 0.5438858270645142\n",
      "Epoch: 31470 | Loss: 0.5181692242622375 | Test loss: 0.5438719987869263\n",
      "Epoch: 31480 | Loss: 0.5181574821472168 | Test loss: 0.5438581109046936\n",
      "Epoch: 31490 | Loss: 0.518145740032196 | Test loss: 0.5438442826271057\n",
      "Epoch: 31500 | Loss: 0.5181339383125305 | Test loss: 0.543830394744873\n",
      "Epoch: 31510 | Loss: 0.5181221961975098 | Test loss: 0.5438165664672852\n",
      "Epoch: 31520 | Loss: 0.5181103944778442 | Test loss: 0.5438027381896973\n",
      "Epoch: 31530 | Loss: 0.5180986523628235 | Test loss: 0.5437889099121094\n",
      "Epoch: 31540 | Loss: 0.5180869102478027 | Test loss: 0.5437750220298767\n",
      "Epoch: 31550 | Loss: 0.518075168132782 | Test loss: 0.5437611937522888\n",
      "Epoch: 31560 | Loss: 0.5180633664131165 | Test loss: 0.5437473654747009\n",
      "Epoch: 31570 | Loss: 0.5180516242980957 | Test loss: 0.543733537197113\n",
      "Epoch: 31580 | Loss: 0.518039882183075 | Test loss: 0.5437197089195251\n",
      "Epoch: 31590 | Loss: 0.5180280804634094 | Test loss: 0.5437058210372925\n",
      "Epoch: 31600 | Loss: 0.5180163383483887 | Test loss: 0.5436919927597046\n",
      "Epoch: 31610 | Loss: 0.5180045962333679 | Test loss: 0.5436781048774719\n",
      "Epoch: 31620 | Loss: 0.5179929137229919 | Test loss: 0.543664276599884\n",
      "Epoch: 31630 | Loss: 0.5179810523986816 | Test loss: 0.5436504483222961\n",
      "Epoch: 31640 | Loss: 0.5179693102836609 | Test loss: 0.5436365604400635\n",
      "Epoch: 31650 | Loss: 0.5179576277732849 | Test loss: 0.5436227321624756\n",
      "Epoch: 31660 | Loss: 0.5179457664489746 | Test loss: 0.5436089038848877\n",
      "Epoch: 31670 | Loss: 0.5179340243339539 | Test loss: 0.543595016002655\n",
      "Epoch: 31680 | Loss: 0.5179223418235779 | Test loss: 0.5435811877250671\n",
      "Epoch: 31690 | Loss: 0.5179104804992676 | Test loss: 0.5435673594474792\n",
      "Epoch: 31700 | Loss: 0.5178987383842468 | Test loss: 0.5435534715652466\n",
      "Epoch: 31710 | Loss: 0.5178869962692261 | Test loss: 0.5435396432876587\n",
      "Epoch: 31720 | Loss: 0.5178751945495605 | Test loss: 0.5435258150100708\n",
      "Epoch: 31730 | Loss: 0.5178634524345398 | Test loss: 0.5435119867324829\n",
      "Epoch: 31740 | Loss: 0.5178517699241638 | Test loss: 0.543498158454895\n",
      "Epoch: 31750 | Loss: 0.5178399085998535 | Test loss: 0.5434842705726624\n",
      "Epoch: 31760 | Loss: 0.5178281664848328 | Test loss: 0.5434703826904297\n",
      "Epoch: 31770 | Loss: 0.5178164839744568 | Test loss: 0.5434565544128418\n",
      "Epoch: 31780 | Loss: 0.5178046822547913 | Test loss: 0.5434427261352539\n",
      "Epoch: 31790 | Loss: 0.5177928805351257 | Test loss: 0.543428897857666\n",
      "Epoch: 31800 | Loss: 0.5177811980247498 | Test loss: 0.5434150695800781\n",
      "Epoch: 31810 | Loss: 0.5177693367004395 | Test loss: 0.5434011816978455\n",
      "Epoch: 31820 | Loss: 0.5177576541900635 | Test loss: 0.5433873534202576\n",
      "Epoch: 31830 | Loss: 0.5177459120750427 | Test loss: 0.5433735251426697\n",
      "Epoch: 31840 | Loss: 0.517734169960022 | Test loss: 0.5433596968650818\n",
      "Epoch: 31850 | Loss: 0.5177223682403564 | Test loss: 0.5433458685874939\n",
      "Epoch: 31860 | Loss: 0.5177106261253357 | Test loss: 0.543332040309906\n",
      "Epoch: 31870 | Loss: 0.5176988244056702 | Test loss: 0.5433180928230286\n",
      "Epoch: 31880 | Loss: 0.5176870822906494 | Test loss: 0.5433042645454407\n",
      "Epoch: 31890 | Loss: 0.5176753401756287 | Test loss: 0.5432904362678528\n",
      "Epoch: 31900 | Loss: 0.5176635980606079 | Test loss: 0.5432765483856201\n",
      "Epoch: 31910 | Loss: 0.5176517963409424 | Test loss: 0.5432627201080322\n",
      "Epoch: 31920 | Loss: 0.5176400542259216 | Test loss: 0.5432488918304443\n",
      "Epoch: 31930 | Loss: 0.5176283121109009 | Test loss: 0.5432350039482117\n",
      "Epoch: 31940 | Loss: 0.5176165103912354 | Test loss: 0.5432211756706238\n",
      "Epoch: 31950 | Loss: 0.5176047682762146 | Test loss: 0.5432073473930359\n",
      "Epoch: 31960 | Loss: 0.5175930261611938 | Test loss: 0.5431934595108032\n",
      "Epoch: 31970 | Loss: 0.5175812244415283 | Test loss: 0.5431796908378601\n",
      "Epoch: 31980 | Loss: 0.5175694823265076 | Test loss: 0.5431658029556274\n",
      "Epoch: 31990 | Loss: 0.5175577402114868 | Test loss: 0.5431519746780396\n",
      "Epoch: 32000 | Loss: 0.5175459980964661 | Test loss: 0.5431381464004517\n",
      "Epoch: 32010 | Loss: 0.5175341963768005 | Test loss: 0.5431243181228638\n",
      "Epoch: 32020 | Loss: 0.5175224542617798 | Test loss: 0.5431103706359863\n",
      "Epoch: 32030 | Loss: 0.517510712146759 | Test loss: 0.5430965423583984\n",
      "Epoch: 32040 | Loss: 0.5174989104270935 | Test loss: 0.5430827140808105\n",
      "Epoch: 32050 | Loss: 0.5174871683120728 | Test loss: 0.5430688858032227\n",
      "Epoch: 32060 | Loss: 0.517475426197052 | Test loss: 0.5430550575256348\n",
      "Epoch: 32070 | Loss: 0.5174636244773865 | Test loss: 0.5430411696434021\n",
      "Epoch: 32080 | Loss: 0.5174519419670105 | Test loss: 0.5430273413658142\n",
      "Epoch: 32090 | Loss: 0.5174400806427002 | Test loss: 0.5430135130882263\n",
      "Epoch: 32100 | Loss: 0.5174283385276794 | Test loss: 0.5429996848106384\n",
      "Epoch: 32110 | Loss: 0.5174165964126587 | Test loss: 0.5429858565330505\n",
      "Epoch: 32120 | Loss: 0.5174047946929932 | Test loss: 0.5429720282554626\n",
      "Epoch: 32130 | Loss: 0.5173931121826172 | Test loss: 0.54295814037323\n",
      "Epoch: 32140 | Loss: 0.5173813104629517 | Test loss: 0.5429442524909973\n",
      "Epoch: 32150 | Loss: 0.5173695683479309 | Test loss: 0.5429304242134094\n",
      "Epoch: 32160 | Loss: 0.5173577666282654 | Test loss: 0.5429165363311768\n",
      "Epoch: 32170 | Loss: 0.5173460245132446 | Test loss: 0.5429027676582336\n",
      "Epoch: 32180 | Loss: 0.5173342823982239 | Test loss: 0.542888879776001\n",
      "Epoch: 32190 | Loss: 0.5173224806785583 | Test loss: 0.5428749918937683\n",
      "Epoch: 32200 | Loss: 0.5173107981681824 | Test loss: 0.5428611636161804\n",
      "Epoch: 32210 | Loss: 0.5172989964485168 | Test loss: 0.5428473353385925\n",
      "Epoch: 32220 | Loss: 0.5172872543334961 | Test loss: 0.5428335070610046\n",
      "Epoch: 32230 | Loss: 0.5172755122184753 | Test loss: 0.5428196787834167\n",
      "Epoch: 32240 | Loss: 0.5172637104988098 | Test loss: 0.5428057909011841\n",
      "Epoch: 32250 | Loss: 0.5172519683837891 | Test loss: 0.5427919626235962\n",
      "Epoch: 32260 | Loss: 0.5172401666641235 | Test loss: 0.5427781343460083\n",
      "Epoch: 32270 | Loss: 0.5172284245491028 | Test loss: 0.5427643060684204\n",
      "Epoch: 32280 | Loss: 0.517216682434082 | Test loss: 0.5427504181861877\n",
      "Epoch: 32290 | Loss: 0.5172049403190613 | Test loss: 0.5427365899085999\n",
      "Epoch: 32300 | Loss: 0.5171931385993958 | Test loss: 0.5427227020263672\n",
      "Epoch: 32310 | Loss: 0.517181396484375 | Test loss: 0.5427088737487793\n",
      "Epoch: 32320 | Loss: 0.5171695947647095 | Test loss: 0.5426950454711914\n",
      "Epoch: 32330 | Loss: 0.5171578526496887 | Test loss: 0.5426812171936035\n",
      "Epoch: 32340 | Loss: 0.517146110534668 | Test loss: 0.5426673293113708\n",
      "Epoch: 32350 | Loss: 0.5171343684196472 | Test loss: 0.542653501033783\n",
      "Epoch: 32360 | Loss: 0.5171225666999817 | Test loss: 0.5426396727561951\n",
      "Epoch: 32370 | Loss: 0.5171108245849609 | Test loss: 0.5426258444786072\n",
      "Epoch: 32380 | Loss: 0.5170990824699402 | Test loss: 0.5426120162010193\n",
      "Epoch: 32390 | Loss: 0.5170872807502747 | Test loss: 0.5425981283187866\n",
      "Epoch: 32400 | Loss: 0.5170755386352539 | Test loss: 0.5425843000411987\n",
      "Epoch: 32410 | Loss: 0.5170637965202332 | Test loss: 0.5425704121589661\n",
      "Epoch: 32420 | Loss: 0.5170521140098572 | Test loss: 0.5425565838813782\n",
      "Epoch: 32430 | Loss: 0.5170402526855469 | Test loss: 0.5425427556037903\n",
      "Epoch: 32440 | Loss: 0.5170285105705261 | Test loss: 0.5425288677215576\n",
      "Epoch: 32450 | Loss: 0.5170168280601501 | Test loss: 0.5425150394439697\n",
      "Epoch: 32460 | Loss: 0.5170049667358398 | Test loss: 0.5425012111663818\n",
      "Epoch: 32470 | Loss: 0.5169932246208191 | Test loss: 0.5424873232841492\n",
      "Epoch: 32480 | Loss: 0.5169815421104431 | Test loss: 0.5424734950065613\n",
      "Epoch: 32490 | Loss: 0.5169696807861328 | Test loss: 0.5424596667289734\n",
      "Epoch: 32500 | Loss: 0.5169579386711121 | Test loss: 0.5424457788467407\n",
      "Epoch: 32510 | Loss: 0.5169461965560913 | Test loss: 0.5424319505691528\n",
      "Epoch: 32520 | Loss: 0.5169343948364258 | Test loss: 0.5424181222915649\n",
      "Epoch: 32530 | Loss: 0.516922652721405 | Test loss: 0.542404294013977\n",
      "Epoch: 32540 | Loss: 0.516910970211029 | Test loss: 0.5423904657363892\n",
      "Epoch: 32550 | Loss: 0.5168991088867188 | Test loss: 0.5423765778541565\n",
      "Epoch: 32560 | Loss: 0.516887366771698 | Test loss: 0.5423626899719238\n",
      "Epoch: 32570 | Loss: 0.516875684261322 | Test loss: 0.5423488616943359\n",
      "Epoch: 32580 | Loss: 0.5168638825416565 | Test loss: 0.542335033416748\n",
      "Epoch: 32590 | Loss: 0.516852080821991 | Test loss: 0.5423212051391602\n",
      "Epoch: 32600 | Loss: 0.516840398311615 | Test loss: 0.5423073768615723\n",
      "Epoch: 32610 | Loss: 0.5168285369873047 | Test loss: 0.5422934889793396\n",
      "Epoch: 32620 | Loss: 0.5168168544769287 | Test loss: 0.5422796607017517\n",
      "Epoch: 32630 | Loss: 0.516805112361908 | Test loss: 0.5422658324241638\n",
      "Epoch: 32640 | Loss: 0.5167933702468872 | Test loss: 0.5422520041465759\n",
      "Epoch: 32650 | Loss: 0.5167815685272217 | Test loss: 0.542238175868988\n",
      "Epoch: 32660 | Loss: 0.5167698264122009 | Test loss: 0.5422243475914001\n",
      "Epoch: 32670 | Loss: 0.5167580246925354 | Test loss: 0.5422104001045227\n",
      "Epoch: 32680 | Loss: 0.5167462825775146 | Test loss: 0.5421965718269348\n",
      "Epoch: 32690 | Loss: 0.5167345404624939 | Test loss: 0.5421827435493469\n",
      "Epoch: 32700 | Loss: 0.5167227983474731 | Test loss: 0.5421688556671143\n",
      "Epoch: 32710 | Loss: 0.5167109966278076 | Test loss: 0.5421550273895264\n",
      "Epoch: 32720 | Loss: 0.5166992545127869 | Test loss: 0.5421411991119385\n",
      "Epoch: 32730 | Loss: 0.5166875123977661 | Test loss: 0.5421273112297058\n",
      "Epoch: 32740 | Loss: 0.5166757106781006 | Test loss: 0.5421134829521179\n",
      "Epoch: 32750 | Loss: 0.5166639685630798 | Test loss: 0.54209965467453\n",
      "Epoch: 32760 | Loss: 0.5166522264480591 | Test loss: 0.5420857667922974\n",
      "Epoch: 32770 | Loss: 0.5166404247283936 | Test loss: 0.5420719981193542\n",
      "Epoch: 32780 | Loss: 0.5166286826133728 | Test loss: 0.5420581102371216\n",
      "Epoch: 32790 | Loss: 0.516616940498352 | Test loss: 0.5420442819595337\n",
      "Epoch: 32800 | Loss: 0.5166051983833313 | Test loss: 0.5420304536819458\n",
      "Epoch: 32810 | Loss: 0.5165933966636658 | Test loss: 0.5420166254043579\n",
      "Epoch: 32820 | Loss: 0.516581654548645 | Test loss: 0.5420026779174805\n",
      "Epoch: 32830 | Loss: 0.5165699124336243 | Test loss: 0.5419888496398926\n",
      "Epoch: 32840 | Loss: 0.5165581107139587 | Test loss: 0.5419750213623047\n",
      "Epoch: 32850 | Loss: 0.516546368598938 | Test loss: 0.5419611930847168\n",
      "Epoch: 32860 | Loss: 0.5165346264839172 | Test loss: 0.5419473648071289\n",
      "Epoch: 32870 | Loss: 0.5165228247642517 | Test loss: 0.5419334769248962\n",
      "Epoch: 32880 | Loss: 0.5165111422538757 | Test loss: 0.5419196486473083\n",
      "Epoch: 32890 | Loss: 0.5164992809295654 | Test loss: 0.5419058203697205\n",
      "Epoch: 32900 | Loss: 0.5164875388145447 | Test loss: 0.5418919920921326\n",
      "Epoch: 32910 | Loss: 0.5164757966995239 | Test loss: 0.5418781638145447\n",
      "Epoch: 32920 | Loss: 0.5164639949798584 | Test loss: 0.5418643355369568\n",
      "Epoch: 32930 | Loss: 0.5164523124694824 | Test loss: 0.5418504476547241\n",
      "Epoch: 32940 | Loss: 0.5164405107498169 | Test loss: 0.5418365597724915\n",
      "Epoch: 32950 | Loss: 0.5164287686347961 | Test loss: 0.5418227314949036\n",
      "Epoch: 32960 | Loss: 0.5164169669151306 | Test loss: 0.5418088436126709\n",
      "Epoch: 32970 | Loss: 0.5164052248001099 | Test loss: 0.5417950749397278\n",
      "Epoch: 32980 | Loss: 0.5163934826850891 | Test loss: 0.5417811870574951\n",
      "Epoch: 32990 | Loss: 0.5163816809654236 | Test loss: 0.5417672991752625\n",
      "Epoch: 33000 | Loss: 0.5163699984550476 | Test loss: 0.5417534708976746\n",
      "Epoch: 33010 | Loss: 0.5163581967353821 | Test loss: 0.5417396426200867\n",
      "Epoch: 33020 | Loss: 0.5163464546203613 | Test loss: 0.5417258143424988\n",
      "Epoch: 33030 | Loss: 0.5163347125053406 | Test loss: 0.5417119860649109\n",
      "Epoch: 33040 | Loss: 0.516322910785675 | Test loss: 0.5416980981826782\n",
      "Epoch: 33050 | Loss: 0.5163111686706543 | Test loss: 0.5416842699050903\n",
      "Epoch: 33060 | Loss: 0.5162993669509888 | Test loss: 0.5416704416275024\n",
      "Epoch: 33070 | Loss: 0.516287624835968 | Test loss: 0.5416566133499146\n",
      "Epoch: 33080 | Loss: 0.5162758827209473 | Test loss: 0.5416427254676819\n",
      "Epoch: 33090 | Loss: 0.5162641406059265 | Test loss: 0.541628897190094\n",
      "Epoch: 33100 | Loss: 0.516252338886261 | Test loss: 0.5416150093078613\n",
      "Epoch: 33110 | Loss: 0.5162405967712402 | Test loss: 0.5416011810302734\n",
      "Epoch: 33120 | Loss: 0.5162287950515747 | Test loss: 0.5415873527526855\n",
      "Epoch: 33130 | Loss: 0.516217052936554 | Test loss: 0.5415735244750977\n",
      "Epoch: 33140 | Loss: 0.5162053108215332 | Test loss: 0.541559636592865\n",
      "Epoch: 33150 | Loss: 0.5161935687065125 | Test loss: 0.5415458083152771\n",
      "Epoch: 33160 | Loss: 0.5161817669868469 | Test loss: 0.5415319800376892\n",
      "Epoch: 33170 | Loss: 0.5161700248718262 | Test loss: 0.5415181517601013\n",
      "Epoch: 33180 | Loss: 0.5161582827568054 | Test loss: 0.5415043234825134\n",
      "Epoch: 33190 | Loss: 0.5161464810371399 | Test loss: 0.5414904356002808\n",
      "Epoch: 33200 | Loss: 0.5161347389221191 | Test loss: 0.5414766073226929\n",
      "Epoch: 33210 | Loss: 0.5161229968070984 | Test loss: 0.5414627194404602\n",
      "Epoch: 33220 | Loss: 0.5161113142967224 | Test loss: 0.5414488911628723\n",
      "Epoch: 33230 | Loss: 0.5160994529724121 | Test loss: 0.5414350628852844\n",
      "Epoch: 33240 | Loss: 0.5160877108573914 | Test loss: 0.5414211750030518\n",
      "Epoch: 33250 | Loss: 0.5160760283470154 | Test loss: 0.5414073467254639\n",
      "Epoch: 33260 | Loss: 0.5160641670227051 | Test loss: 0.541393518447876\n",
      "Epoch: 33270 | Loss: 0.5160524249076843 | Test loss: 0.5413796305656433\n",
      "Epoch: 33280 | Loss: 0.5160407423973083 | Test loss: 0.5413658022880554\n",
      "Epoch: 33290 | Loss: 0.516028881072998 | Test loss: 0.5413519740104675\n",
      "Epoch: 33300 | Loss: 0.5160171389579773 | Test loss: 0.5413380861282349\n",
      "Epoch: 33310 | Loss: 0.5160053968429565 | Test loss: 0.541324257850647\n",
      "Epoch: 33320 | Loss: 0.515993595123291 | Test loss: 0.5413104295730591\n",
      "Epoch: 33330 | Loss: 0.5159818530082703 | Test loss: 0.5412966012954712\n",
      "Epoch: 33340 | Loss: 0.5159701704978943 | Test loss: 0.5412827730178833\n",
      "Epoch: 33350 | Loss: 0.515958309173584 | Test loss: 0.5412688851356506\n",
      "Epoch: 33360 | Loss: 0.5159465670585632 | Test loss: 0.541254997253418\n",
      "Epoch: 33370 | Loss: 0.5159348845481873 | Test loss: 0.5412411689758301\n",
      "Epoch: 33380 | Loss: 0.5159230828285217 | Test loss: 0.5412273406982422\n",
      "Epoch: 33390 | Loss: 0.5159112811088562 | Test loss: 0.5412135124206543\n",
      "Epoch: 33400 | Loss: 0.5158995985984802 | Test loss: 0.5411996841430664\n",
      "Epoch: 33410 | Loss: 0.5158877372741699 | Test loss: 0.5411857962608337\n",
      "Epoch: 33420 | Loss: 0.515876054763794 | Test loss: 0.5411719679832458\n",
      "Epoch: 33430 | Loss: 0.5158643126487732 | Test loss: 0.541158139705658\n",
      "Epoch: 33440 | Loss: 0.5158525705337524 | Test loss: 0.5411443114280701\n",
      "Epoch: 33450 | Loss: 0.5158407688140869 | Test loss: 0.5411304831504822\n",
      "Epoch: 33460 | Loss: 0.5158290266990662 | Test loss: 0.5411166548728943\n",
      "Epoch: 33470 | Loss: 0.5158172249794006 | Test loss: 0.5411027073860168\n",
      "Epoch: 33480 | Loss: 0.5158054828643799 | Test loss: 0.541088879108429\n",
      "Epoch: 33490 | Loss: 0.5157937407493591 | Test loss: 0.5410750508308411\n",
      "Epoch: 33500 | Loss: 0.5157819986343384 | Test loss: 0.5410611629486084\n",
      "Epoch: 33510 | Loss: 0.5157701969146729 | Test loss: 0.5410473346710205\n",
      "Epoch: 33520 | Loss: 0.5157584547996521 | Test loss: 0.5410335063934326\n",
      "Epoch: 33530 | Loss: 0.5157467126846313 | Test loss: 0.5410196185112\n",
      "Epoch: 33540 | Loss: 0.5157349109649658 | Test loss: 0.5410057902336121\n",
      "Epoch: 33550 | Loss: 0.5157231688499451 | Test loss: 0.5409919619560242\n",
      "Epoch: 33560 | Loss: 0.5157114267349243 | Test loss: 0.5409780740737915\n",
      "Epoch: 33570 | Loss: 0.5156996250152588 | Test loss: 0.5409643054008484\n",
      "Epoch: 33580 | Loss: 0.515687882900238 | Test loss: 0.5409504175186157\n",
      "Epoch: 33590 | Loss: 0.5156761407852173 | Test loss: 0.5409365892410278\n",
      "Epoch: 33600 | Loss: 0.5156643986701965 | Test loss: 0.5409227609634399\n",
      "Epoch: 33610 | Loss: 0.515652596950531 | Test loss: 0.540908932685852\n",
      "Epoch: 33620 | Loss: 0.5156408548355103 | Test loss: 0.5408949851989746\n",
      "Epoch: 33630 | Loss: 0.5156291127204895 | Test loss: 0.5408811569213867\n",
      "Epoch: 33640 | Loss: 0.515617311000824 | Test loss: 0.5408673286437988\n",
      "Epoch: 33650 | Loss: 0.5156055688858032 | Test loss: 0.5408535003662109\n",
      "Epoch: 33660 | Loss: 0.5155938267707825 | Test loss: 0.540839672088623\n",
      "Epoch: 33670 | Loss: 0.5155820250511169 | Test loss: 0.5408257842063904\n",
      "Epoch: 33680 | Loss: 0.515570342540741 | Test loss: 0.5408119559288025\n",
      "Epoch: 33690 | Loss: 0.5155584812164307 | Test loss: 0.5407981276512146\n",
      "Epoch: 33700 | Loss: 0.5155467391014099 | Test loss: 0.5407842993736267\n",
      "Epoch: 33710 | Loss: 0.5155349969863892 | Test loss: 0.5407704710960388\n",
      "Epoch: 33720 | Loss: 0.5155231952667236 | Test loss: 0.5407566428184509\n",
      "Epoch: 33730 | Loss: 0.5155115127563477 | Test loss: 0.5407427549362183\n",
      "Epoch: 33740 | Loss: 0.5154997110366821 | Test loss: 0.5407288670539856\n",
      "Epoch: 33750 | Loss: 0.5154879689216614 | Test loss: 0.5407150387763977\n",
      "Epoch: 33760 | Loss: 0.5154761672019958 | Test loss: 0.540701150894165\n",
      "Epoch: 33770 | Loss: 0.5154644250869751 | Test loss: 0.5406873822212219\n",
      "Epoch: 33780 | Loss: 0.5154526829719543 | Test loss: 0.5406734943389893\n",
      "Epoch: 33790 | Loss: 0.5154408812522888 | Test loss: 0.5406596064567566\n",
      "Epoch: 33800 | Loss: 0.5154291987419128 | Test loss: 0.5406457781791687\n",
      "Epoch: 33810 | Loss: 0.5154173970222473 | Test loss: 0.5406319499015808\n",
      "Epoch: 33820 | Loss: 0.5154056549072266 | Test loss: 0.5406181216239929\n",
      "Epoch: 33830 | Loss: 0.5153939127922058 | Test loss: 0.540604293346405\n",
      "Epoch: 33840 | Loss: 0.5153821110725403 | Test loss: 0.5405904054641724\n",
      "Epoch: 33850 | Loss: 0.5153703689575195 | Test loss: 0.5405765771865845\n",
      "Epoch: 33860 | Loss: 0.515358567237854 | Test loss: 0.5405627489089966\n",
      "Epoch: 33870 | Loss: 0.5153468251228333 | Test loss: 0.5405489206314087\n",
      "Epoch: 33880 | Loss: 0.5153350830078125 | Test loss: 0.540535032749176\n",
      "Epoch: 33890 | Loss: 0.5153233408927917 | Test loss: 0.5405212044715881\n",
      "Epoch: 33900 | Loss: 0.5153115391731262 | Test loss: 0.5405073165893555\n",
      "Epoch: 33910 | Loss: 0.5152997970581055 | Test loss: 0.5404934883117676\n",
      "Epoch: 33920 | Loss: 0.5152879953384399 | Test loss: 0.5404796600341797\n",
      "Epoch: 33930 | Loss: 0.5152762532234192 | Test loss: 0.5404658317565918\n",
      "Epoch: 33940 | Loss: 0.5152645111083984 | Test loss: 0.5404519438743591\n",
      "Epoch: 33950 | Loss: 0.5152527689933777 | Test loss: 0.5404381155967712\n",
      "Epoch: 33960 | Loss: 0.5152409672737122 | Test loss: 0.5404242873191833\n",
      "Epoch: 33970 | Loss: 0.5152292251586914 | Test loss: 0.5404104590415955\n",
      "Epoch: 33980 | Loss: 0.5152174830436707 | Test loss: 0.5403966307640076\n",
      "Epoch: 33990 | Loss: 0.5152056813240051 | Test loss: 0.5403827428817749\n",
      "Epoch: 34000 | Loss: 0.5151939392089844 | Test loss: 0.540368914604187\n",
      "Epoch: 34010 | Loss: 0.5151821970939636 | Test loss: 0.5403550267219543\n",
      "Epoch: 34020 | Loss: 0.5151705145835876 | Test loss: 0.5403411984443665\n",
      "Epoch: 34030 | Loss: 0.5151586532592773 | Test loss: 0.5403273701667786\n",
      "Epoch: 34040 | Loss: 0.5151469111442566 | Test loss: 0.5403134822845459\n",
      "Epoch: 34050 | Loss: 0.5151352286338806 | Test loss: 0.540299654006958\n",
      "Epoch: 34060 | Loss: 0.5151233673095703 | Test loss: 0.5402858257293701\n",
      "Epoch: 34070 | Loss: 0.5151116251945496 | Test loss: 0.5402719378471375\n",
      "Epoch: 34080 | Loss: 0.5150999426841736 | Test loss: 0.5402581095695496\n",
      "Epoch: 34090 | Loss: 0.5150880813598633 | Test loss: 0.5402442812919617\n",
      "Epoch: 34100 | Loss: 0.5150763392448425 | Test loss: 0.540230393409729\n",
      "Epoch: 34110 | Loss: 0.5150645971298218 | Test loss: 0.5402165651321411\n",
      "Epoch: 34120 | Loss: 0.5150527954101562 | Test loss: 0.5402027368545532\n",
      "Epoch: 34130 | Loss: 0.5150410532951355 | Test loss: 0.5401889085769653\n",
      "Epoch: 34140 | Loss: 0.5150293707847595 | Test loss: 0.5401750802993774\n",
      "Epoch: 34150 | Loss: 0.5150175094604492 | Test loss: 0.5401611924171448\n",
      "Epoch: 34160 | Loss: 0.5150057673454285 | Test loss: 0.5401473045349121\n",
      "Epoch: 34170 | Loss: 0.5149940848350525 | Test loss: 0.5401334762573242\n",
      "Epoch: 34180 | Loss: 0.514982283115387 | Test loss: 0.5401196479797363\n",
      "Epoch: 34190 | Loss: 0.5149704813957214 | Test loss: 0.5401058197021484\n",
      "Epoch: 34200 | Loss: 0.5149587988853455 | Test loss: 0.5400919914245605\n",
      "Epoch: 34210 | Loss: 0.5149469375610352 | Test loss: 0.5400781035423279\n",
      "Epoch: 34220 | Loss: 0.5149352550506592 | Test loss: 0.54006427526474\n",
      "Epoch: 34230 | Loss: 0.5149235129356384 | Test loss: 0.5400504469871521\n",
      "Epoch: 34240 | Loss: 0.5149117708206177 | Test loss: 0.5400366187095642\n",
      "Epoch: 34250 | Loss: 0.5148999691009521 | Test loss: 0.5400227904319763\n",
      "Epoch: 34260 | Loss: 0.5148882269859314 | Test loss: 0.5400089621543884\n",
      "Epoch: 34270 | Loss: 0.5148764252662659 | Test loss: 0.539995014667511\n",
      "Epoch: 34280 | Loss: 0.5148646831512451 | Test loss: 0.5399811863899231\n",
      "Epoch: 34290 | Loss: 0.5148529410362244 | Test loss: 0.5399673581123352\n",
      "Epoch: 34300 | Loss: 0.5148411989212036 | Test loss: 0.5399534702301025\n",
      "Epoch: 34310 | Loss: 0.5148293972015381 | Test loss: 0.5399396419525146\n",
      "Epoch: 34320 | Loss: 0.5148176550865173 | Test loss: 0.5399258136749268\n",
      "Epoch: 34330 | Loss: 0.5148059129714966 | Test loss: 0.5399119257926941\n",
      "Epoch: 34340 | Loss: 0.514794111251831 | Test loss: 0.5398980975151062\n",
      "Epoch: 34350 | Loss: 0.5147823691368103 | Test loss: 0.5398842692375183\n",
      "Epoch: 34360 | Loss: 0.5147706270217896 | Test loss: 0.5398703813552856\n",
      "Epoch: 34370 | Loss: 0.514758825302124 | Test loss: 0.5398566126823425\n",
      "Epoch: 34380 | Loss: 0.5147470831871033 | Test loss: 0.5398427248001099\n",
      "Epoch: 34390 | Loss: 0.5147353410720825 | Test loss: 0.539828896522522\n",
      "Epoch: 34400 | Loss: 0.5147235989570618 | Test loss: 0.5398150682449341\n",
      "Epoch: 34410 | Loss: 0.5147117972373962 | Test loss: 0.5398012399673462\n",
      "Epoch: 34420 | Loss: 0.5147000551223755 | Test loss: 0.5397872924804688\n",
      "Epoch: 34430 | Loss: 0.5146883130073547 | Test loss: 0.5397734642028809\n",
      "Epoch: 34440 | Loss: 0.5146765112876892 | Test loss: 0.539759635925293\n",
      "Epoch: 34450 | Loss: 0.5146647691726685 | Test loss: 0.5397458076477051\n",
      "Epoch: 34460 | Loss: 0.5146530270576477 | Test loss: 0.5397319793701172\n",
      "Epoch: 34470 | Loss: 0.5146412253379822 | Test loss: 0.5397180914878845\n",
      "Epoch: 34480 | Loss: 0.5146295428276062 | Test loss: 0.5397042632102966\n",
      "Epoch: 34490 | Loss: 0.5146176815032959 | Test loss: 0.5396904349327087\n",
      "Epoch: 34500 | Loss: 0.5146059393882751 | Test loss: 0.5396766066551208\n",
      "Epoch: 34510 | Loss: 0.5145941972732544 | Test loss: 0.539662778377533\n",
      "Epoch: 34520 | Loss: 0.5145823955535889 | Test loss: 0.5396489500999451\n",
      "Epoch: 34530 | Loss: 0.5145707130432129 | Test loss: 0.5396350622177124\n",
      "Epoch: 34540 | Loss: 0.5145589113235474 | Test loss: 0.5396211743354797\n",
      "Epoch: 34550 | Loss: 0.5145471692085266 | Test loss: 0.5396073460578918\n",
      "Epoch: 34560 | Loss: 0.5145353674888611 | Test loss: 0.5395934581756592\n",
      "Epoch: 34570 | Loss: 0.5145236253738403 | Test loss: 0.5395796895027161\n",
      "Epoch: 34580 | Loss: 0.5145118832588196 | Test loss: 0.5395658016204834\n",
      "Epoch: 34590 | Loss: 0.514500081539154 | Test loss: 0.5395519137382507\n",
      "Epoch: 34600 | Loss: 0.5144883990287781 | Test loss: 0.5395380854606628\n",
      "Epoch: 34610 | Loss: 0.5144765973091125 | Test loss: 0.539524257183075\n",
      "Epoch: 34620 | Loss: 0.5144648551940918 | Test loss: 0.5395104289054871\n",
      "Epoch: 34630 | Loss: 0.514453113079071 | Test loss: 0.5394966006278992\n",
      "Epoch: 34640 | Loss: 0.5144413113594055 | Test loss: 0.5394827127456665\n",
      "Epoch: 34650 | Loss: 0.5144295692443848 | Test loss: 0.5394688844680786\n",
      "Epoch: 34660 | Loss: 0.5144177675247192 | Test loss: 0.5394550561904907\n",
      "Epoch: 34670 | Loss: 0.5144060254096985 | Test loss: 0.5394412279129028\n",
      "Epoch: 34680 | Loss: 0.5143942832946777 | Test loss: 0.5394273400306702\n",
      "Epoch: 34690 | Loss: 0.514382541179657 | Test loss: 0.5394135117530823\n",
      "Epoch: 34700 | Loss: 0.5143707394599915 | Test loss: 0.5393996238708496\n",
      "Epoch: 34710 | Loss: 0.5143589973449707 | Test loss: 0.5393857955932617\n",
      "Epoch: 34720 | Loss: 0.5143471956253052 | Test loss: 0.5393719673156738\n",
      "Epoch: 34730 | Loss: 0.5143354535102844 | Test loss: 0.5393581390380859\n",
      "Epoch: 34740 | Loss: 0.5143237113952637 | Test loss: 0.5393442511558533\n",
      "Epoch: 34750 | Loss: 0.5143119692802429 | Test loss: 0.5393304228782654\n",
      "Epoch: 34760 | Loss: 0.5143001675605774 | Test loss: 0.5393165946006775\n",
      "Epoch: 34770 | Loss: 0.5142884254455566 | Test loss: 0.5393027663230896\n",
      "Epoch: 34780 | Loss: 0.5142766833305359 | Test loss: 0.5392889380455017\n",
      "Epoch: 34790 | Loss: 0.5142648816108704 | Test loss: 0.539275050163269\n",
      "Epoch: 34800 | Loss: 0.5142531394958496 | Test loss: 0.5392612218856812\n",
      "Epoch: 34810 | Loss: 0.5142413973808289 | Test loss: 0.5392473340034485\n",
      "Epoch: 34820 | Loss: 0.5142297148704529 | Test loss: 0.5392335057258606\n",
      "Epoch: 34830 | Loss: 0.5142178535461426 | Test loss: 0.5392196774482727\n",
      "Epoch: 34840 | Loss: 0.5142061114311218 | Test loss: 0.53920578956604\n",
      "Epoch: 34850 | Loss: 0.5141944289207458 | Test loss: 0.5391919612884521\n",
      "Epoch: 34860 | Loss: 0.5141825675964355 | Test loss: 0.5391781330108643\n",
      "Epoch: 34870 | Loss: 0.5141708254814148 | Test loss: 0.5391642451286316\n",
      "Epoch: 34880 | Loss: 0.5141591429710388 | Test loss: 0.5391504168510437\n",
      "Epoch: 34890 | Loss: 0.5141472816467285 | Test loss: 0.5391365885734558\n",
      "Epoch: 34900 | Loss: 0.5141355395317078 | Test loss: 0.5391227006912231\n",
      "Epoch: 34910 | Loss: 0.514123797416687 | Test loss: 0.5391088724136353\n",
      "Epoch: 34920 | Loss: 0.5141119956970215 | Test loss: 0.5390950441360474\n",
      "Epoch: 34930 | Loss: 0.5141002535820007 | Test loss: 0.5390812158584595\n",
      "Epoch: 34940 | Loss: 0.5140885710716248 | Test loss: 0.5390673875808716\n",
      "Epoch: 34950 | Loss: 0.5140767097473145 | Test loss: 0.5390534996986389\n",
      "Epoch: 34960 | Loss: 0.5140649676322937 | Test loss: 0.5390396118164062\n",
      "Epoch: 34970 | Loss: 0.5140532851219177 | Test loss: 0.5390257835388184\n",
      "Epoch: 34980 | Loss: 0.5140414834022522 | Test loss: 0.5390119552612305\n",
      "Epoch: 34990 | Loss: 0.5140296816825867 | Test loss: 0.5389981269836426\n",
      "Epoch: 35000 | Loss: 0.5140179991722107 | Test loss: 0.5389842987060547\n",
      "Epoch: 35010 | Loss: 0.5140061378479004 | Test loss: 0.538970410823822\n",
      "Epoch: 35020 | Loss: 0.5139944553375244 | Test loss: 0.5389565825462341\n",
      "Epoch: 35030 | Loss: 0.5139827132225037 | Test loss: 0.5389427542686462\n",
      "Epoch: 35040 | Loss: 0.5139709711074829 | Test loss: 0.5389289259910583\n",
      "Epoch: 35050 | Loss: 0.5139591693878174 | Test loss: 0.5389150977134705\n",
      "Epoch: 35060 | Loss: 0.5139474272727966 | Test loss: 0.5389012694358826\n",
      "Epoch: 35070 | Loss: 0.5139356255531311 | Test loss: 0.5388873219490051\n",
      "Epoch: 35080 | Loss: 0.5139238834381104 | Test loss: 0.5388734936714172\n",
      "Epoch: 35090 | Loss: 0.5139121413230896 | Test loss: 0.5388596653938293\n",
      "Epoch: 35100 | Loss: 0.5139003992080688 | Test loss: 0.5388457775115967\n",
      "Epoch: 35110 | Loss: 0.5138885974884033 | Test loss: 0.5388319492340088\n",
      "Epoch: 35120 | Loss: 0.5138768553733826 | Test loss: 0.5388181209564209\n",
      "Epoch: 35130 | Loss: 0.5138651132583618 | Test loss: 0.5388042330741882\n",
      "Epoch: 35140 | Loss: 0.5138533115386963 | Test loss: 0.5387904047966003\n",
      "Epoch: 35150 | Loss: 0.5138415694236755 | Test loss: 0.5387765765190125\n",
      "Epoch: 35160 | Loss: 0.5138298273086548 | Test loss: 0.5387626886367798\n",
      "Epoch: 35170 | Loss: 0.5138180255889893 | Test loss: 0.5387489199638367\n",
      "Epoch: 35180 | Loss: 0.5138062834739685 | Test loss: 0.538735032081604\n",
      "Epoch: 35190 | Loss: 0.5137945413589478 | Test loss: 0.5387212038040161\n",
      "Epoch: 35200 | Loss: 0.513782799243927 | Test loss: 0.5387073755264282\n",
      "Epoch: 35210 | Loss: 0.5137709975242615 | Test loss: 0.5386935472488403\n",
      "Epoch: 35220 | Loss: 0.5137592554092407 | Test loss: 0.5386795997619629\n",
      "Epoch: 35230 | Loss: 0.51374751329422 | Test loss: 0.538665771484375\n",
      "Epoch: 35240 | Loss: 0.5137357115745544 | Test loss: 0.5386519432067871\n",
      "Epoch: 35250 | Loss: 0.5137239694595337 | Test loss: 0.5386381149291992\n",
      "Epoch: 35260 | Loss: 0.5137122273445129 | Test loss: 0.5386242866516113\n",
      "Epoch: 35270 | Loss: 0.5137004256248474 | Test loss: 0.5386103987693787\n",
      "Epoch: 35280 | Loss: 0.5136887431144714 | Test loss: 0.5385965704917908\n",
      "Epoch: 35290 | Loss: 0.5136768817901611 | Test loss: 0.5385827422142029\n",
      "Epoch: 35300 | Loss: 0.5136651396751404 | Test loss: 0.538568913936615\n",
      "Epoch: 35310 | Loss: 0.5136533975601196 | Test loss: 0.5385550856590271\n",
      "Epoch: 35320 | Loss: 0.5136415958404541 | Test loss: 0.5385412573814392\n",
      "Epoch: 35330 | Loss: 0.5136299133300781 | Test loss: 0.5385273694992065\n",
      "Epoch: 35340 | Loss: 0.5136181116104126 | Test loss: 0.5385134816169739\n",
      "Epoch: 35350 | Loss: 0.5136063694953918 | Test loss: 0.538499653339386\n",
      "Epoch: 35360 | Loss: 0.5135945677757263 | Test loss: 0.5384857654571533\n",
      "Epoch: 35370 | Loss: 0.5135828256607056 | Test loss: 0.5384719967842102\n",
      "Epoch: 35380 | Loss: 0.5135710835456848 | Test loss: 0.5384581089019775\n",
      "Epoch: 35390 | Loss: 0.5135592818260193 | Test loss: 0.5384442210197449\n",
      "Epoch: 35400 | Loss: 0.5135475993156433 | Test loss: 0.538430392742157\n",
      "Epoch: 35410 | Loss: 0.5135357975959778 | Test loss: 0.5384165644645691\n",
      "Epoch: 35420 | Loss: 0.513524055480957 | Test loss: 0.5384027361869812\n",
      "Epoch: 35430 | Loss: 0.5135123133659363 | Test loss: 0.5383889079093933\n",
      "Epoch: 35440 | Loss: 0.5135005116462708 | Test loss: 0.5383750200271606\n",
      "Epoch: 35450 | Loss: 0.51348876953125 | Test loss: 0.5383611917495728\n",
      "Epoch: 35460 | Loss: 0.5134769678115845 | Test loss: 0.5383473634719849\n",
      "Epoch: 35470 | Loss: 0.5134652256965637 | Test loss: 0.538333535194397\n",
      "Epoch: 35480 | Loss: 0.513453483581543 | Test loss: 0.5383196473121643\n",
      "Epoch: 35490 | Loss: 0.5134417414665222 | Test loss: 0.5383058190345764\n",
      "Epoch: 35500 | Loss: 0.5134299397468567 | Test loss: 0.5382919311523438\n",
      "Epoch: 35510 | Loss: 0.5134181976318359 | Test loss: 0.5382781028747559\n",
      "Epoch: 35520 | Loss: 0.5134063959121704 | Test loss: 0.538264274597168\n",
      "Epoch: 35530 | Loss: 0.5133946537971497 | Test loss: 0.5382504463195801\n",
      "Epoch: 35540 | Loss: 0.5133829116821289 | Test loss: 0.5382365584373474\n",
      "Epoch: 35550 | Loss: 0.5133711695671082 | Test loss: 0.5382227301597595\n",
      "Epoch: 35560 | Loss: 0.5133593678474426 | Test loss: 0.5382089018821716\n",
      "Epoch: 35570 | Loss: 0.5133476257324219 | Test loss: 0.5381950736045837\n",
      "Epoch: 35580 | Loss: 0.5133358836174011 | Test loss: 0.5381812453269958\n",
      "Epoch: 35590 | Loss: 0.5133240818977356 | Test loss: 0.5381673574447632\n",
      "Epoch: 35600 | Loss: 0.5133123397827148 | Test loss: 0.5381535291671753\n",
      "Epoch: 35610 | Loss: 0.5133005976676941 | Test loss: 0.5381396412849426\n",
      "Epoch: 35620 | Loss: 0.5132889151573181 | Test loss: 0.5381258130073547\n",
      "Epoch: 35630 | Loss: 0.5132770538330078 | Test loss: 0.5381119847297668\n",
      "Epoch: 35640 | Loss: 0.5132653117179871 | Test loss: 0.5380980968475342\n",
      "Epoch: 35650 | Loss: 0.5132536292076111 | Test loss: 0.5380842685699463\n",
      "Epoch: 35660 | Loss: 0.5132417678833008 | Test loss: 0.5380704402923584\n",
      "Epoch: 35670 | Loss: 0.51323002576828 | Test loss: 0.5380565524101257\n",
      "Epoch: 35680 | Loss: 0.513218343257904 | Test loss: 0.5380427241325378\n",
      "Epoch: 35690 | Loss: 0.5132064819335938 | Test loss: 0.53802889585495\n",
      "Epoch: 35700 | Loss: 0.513194739818573 | Test loss: 0.5380150079727173\n",
      "Epoch: 35710 | Loss: 0.5131829977035522 | Test loss: 0.5380011796951294\n",
      "Epoch: 35720 | Loss: 0.5131711959838867 | Test loss: 0.5379873514175415\n",
      "Epoch: 35730 | Loss: 0.513159453868866 | Test loss: 0.5379735231399536\n",
      "Epoch: 35740 | Loss: 0.51314777135849 | Test loss: 0.5379596948623657\n",
      "Epoch: 35750 | Loss: 0.5131359100341797 | Test loss: 0.5379458069801331\n",
      "Epoch: 35760 | Loss: 0.5131241679191589 | Test loss: 0.5379319190979004\n",
      "Epoch: 35770 | Loss: 0.513112485408783 | Test loss: 0.5379180908203125\n",
      "Epoch: 35780 | Loss: 0.5131006836891174 | Test loss: 0.5379042625427246\n",
      "Epoch: 35790 | Loss: 0.5130888819694519 | Test loss: 0.5378904342651367\n",
      "Epoch: 35800 | Loss: 0.5130771994590759 | Test loss: 0.5378766059875488\n",
      "Epoch: 35810 | Loss: 0.5130653381347656 | Test loss: 0.5378627181053162\n",
      "Epoch: 35820 | Loss: 0.5130536556243896 | Test loss: 0.5378488898277283\n",
      "Epoch: 35830 | Loss: 0.5130419135093689 | Test loss: 0.5378350615501404\n",
      "Epoch: 35840 | Loss: 0.5130301713943481 | Test loss: 0.5378212332725525\n",
      "Epoch: 35850 | Loss: 0.5130183696746826 | Test loss: 0.5378074049949646\n",
      "Epoch: 35860 | Loss: 0.5130066275596619 | Test loss: 0.5377935767173767\n",
      "Epoch: 35870 | Loss: 0.5129948258399963 | Test loss: 0.5377796292304993\n",
      "Epoch: 35880 | Loss: 0.5129830837249756 | Test loss: 0.5377658009529114\n",
      "Epoch: 35890 | Loss: 0.5129713416099548 | Test loss: 0.5377519726753235\n",
      "Epoch: 35900 | Loss: 0.5129595994949341 | Test loss: 0.5377380847930908\n",
      "Epoch: 35910 | Loss: 0.5129477977752686 | Test loss: 0.5377242565155029\n",
      "Epoch: 35920 | Loss: 0.5129360556602478 | Test loss: 0.537710428237915\n",
      "Epoch: 35930 | Loss: 0.512924313545227 | Test loss: 0.5376965403556824\n",
      "Epoch: 35940 | Loss: 0.5129125118255615 | Test loss: 0.5376827120780945\n",
      "Epoch: 35950 | Loss: 0.5129007697105408 | Test loss: 0.5376688838005066\n",
      "Epoch: 35960 | Loss: 0.51288902759552 | Test loss: 0.5376549959182739\n",
      "Epoch: 35970 | Loss: 0.5128772258758545 | Test loss: 0.5376412272453308\n",
      "Epoch: 35980 | Loss: 0.5128654837608337 | Test loss: 0.5376273393630981\n",
      "Epoch: 35990 | Loss: 0.512853741645813 | Test loss: 0.5376135110855103\n",
      "Epoch: 36000 | Loss: 0.5128419995307922 | Test loss: 0.5375996828079224\n",
      "Epoch: 36010 | Loss: 0.5128301978111267 | Test loss: 0.5375858545303345\n",
      "Epoch: 36020 | Loss: 0.512818455696106 | Test loss: 0.537571907043457\n",
      "Epoch: 36030 | Loss: 0.5128067135810852 | Test loss: 0.5375580787658691\n",
      "Epoch: 36040 | Loss: 0.5127949118614197 | Test loss: 0.5375442504882812\n",
      "Epoch: 36050 | Loss: 0.5127831697463989 | Test loss: 0.5375304222106934\n",
      "Epoch: 36060 | Loss: 0.5127714276313782 | Test loss: 0.5375165939331055\n",
      "Epoch: 36070 | Loss: 0.5127596259117126 | Test loss: 0.5375027060508728\n",
      "Epoch: 36080 | Loss: 0.5127479434013367 | Test loss: 0.5374888777732849\n",
      "Epoch: 36090 | Loss: 0.5127360820770264 | Test loss: 0.537475049495697\n",
      "Epoch: 36100 | Loss: 0.5127243399620056 | Test loss: 0.5374612212181091\n",
      "Epoch: 36110 | Loss: 0.5127125978469849 | Test loss: 0.5374473929405212\n",
      "Epoch: 36120 | Loss: 0.5127007961273193 | Test loss: 0.5374335646629333\n",
      "Epoch: 36130 | Loss: 0.5126891136169434 | Test loss: 0.5374196767807007\n",
      "Epoch: 36140 | Loss: 0.5126773118972778 | Test loss: 0.537405788898468\n",
      "Epoch: 36150 | Loss: 0.5126655697822571 | Test loss: 0.5373919606208801\n",
      "Epoch: 36160 | Loss: 0.5126537680625916 | Test loss: 0.5373780727386475\n",
      "Epoch: 36170 | Loss: 0.5126420259475708 | Test loss: 0.5373643040657043\n",
      "Epoch: 36180 | Loss: 0.51263028383255 | Test loss: 0.5373504161834717\n",
      "Epoch: 36190 | Loss: 0.5126184821128845 | Test loss: 0.537336528301239\n",
      "Epoch: 36200 | Loss: 0.5126067996025085 | Test loss: 0.5373227000236511\n",
      "Epoch: 36210 | Loss: 0.512594997882843 | Test loss: 0.5373088717460632\n",
      "Epoch: 36220 | Loss: 0.5125832557678223 | Test loss: 0.5372950434684753\n",
      "Epoch: 36230 | Loss: 0.5125715136528015 | Test loss: 0.5372812151908875\n",
      "Epoch: 36240 | Loss: 0.512559711933136 | Test loss: 0.5372673273086548\n",
      "Epoch: 36250 | Loss: 0.5125479698181152 | Test loss: 0.5372534990310669\n",
      "Epoch: 36260 | Loss: 0.5125361680984497 | Test loss: 0.537239670753479\n",
      "Epoch: 36270 | Loss: 0.512524425983429 | Test loss: 0.5372258424758911\n",
      "Epoch: 36280 | Loss: 0.5125126838684082 | Test loss: 0.5372119545936584\n",
      "Epoch: 36290 | Loss: 0.5125009417533875 | Test loss: 0.5371981263160706\n",
      "Epoch: 36300 | Loss: 0.5124891400337219 | Test loss: 0.5371842384338379\n",
      "Epoch: 36310 | Loss: 0.5124773979187012 | Test loss: 0.53717041015625\n",
      "Epoch: 36320 | Loss: 0.5124655961990356 | Test loss: 0.5371565818786621\n",
      "Epoch: 36330 | Loss: 0.5124538540840149 | Test loss: 0.5371427536010742\n",
      "Epoch: 36340 | Loss: 0.5124421119689941 | Test loss: 0.5371288657188416\n",
      "Epoch: 36350 | Loss: 0.5124303698539734 | Test loss: 0.5371150374412537\n",
      "Epoch: 36360 | Loss: 0.5124185681343079 | Test loss: 0.5371012091636658\n",
      "Epoch: 36370 | Loss: 0.5124068260192871 | Test loss: 0.5370873808860779\n",
      "Epoch: 36380 | Loss: 0.5123950839042664 | Test loss: 0.53707355260849\n",
      "Epoch: 36390 | Loss: 0.5123832821846008 | Test loss: 0.5370596647262573\n",
      "Epoch: 36400 | Loss: 0.5123715400695801 | Test loss: 0.5370458364486694\n",
      "Epoch: 36410 | Loss: 0.5123597979545593 | Test loss: 0.5370319485664368\n",
      "Epoch: 36420 | Loss: 0.5123481154441833 | Test loss: 0.5370181202888489\n",
      "Epoch: 36430 | Loss: 0.512336254119873 | Test loss: 0.537004292011261\n",
      "Epoch: 36440 | Loss: 0.5123245120048523 | Test loss: 0.5369904041290283\n",
      "Epoch: 36450 | Loss: 0.5123128294944763 | Test loss: 0.5369765758514404\n",
      "Epoch: 36460 | Loss: 0.512300968170166 | Test loss: 0.5369627475738525\n",
      "Epoch: 36470 | Loss: 0.5122892260551453 | Test loss: 0.5369488596916199\n",
      "Epoch: 36480 | Loss: 0.5122775435447693 | Test loss: 0.536935031414032\n",
      "Epoch: 36490 | Loss: 0.512265682220459 | Test loss: 0.5369212031364441\n",
      "Epoch: 36500 | Loss: 0.5122539401054382 | Test loss: 0.5369073152542114\n",
      "Epoch: 36510 | Loss: 0.5122421979904175 | Test loss: 0.5368934869766235\n",
      "Epoch: 36520 | Loss: 0.512230396270752 | Test loss: 0.5368796586990356\n",
      "Epoch: 36530 | Loss: 0.5122186541557312 | Test loss: 0.5368658304214478\n",
      "Epoch: 36540 | Loss: 0.5122069716453552 | Test loss: 0.5368520021438599\n",
      "Epoch: 36550 | Loss: 0.5121951103210449 | Test loss: 0.5368381142616272\n",
      "Epoch: 36560 | Loss: 0.5121833682060242 | Test loss: 0.5368242263793945\n",
      "Epoch: 36570 | Loss: 0.5121716856956482 | Test loss: 0.5368103981018066\n",
      "Epoch: 36580 | Loss: 0.5121598839759827 | Test loss: 0.5367965698242188\n",
      "Epoch: 36590 | Loss: 0.5121480822563171 | Test loss: 0.5367827415466309\n",
      "Epoch: 36600 | Loss: 0.5121363997459412 | Test loss: 0.536768913269043\n",
      "Epoch: 36610 | Loss: 0.5121245384216309 | Test loss: 0.5367550253868103\n",
      "Epoch: 36620 | Loss: 0.5121128559112549 | Test loss: 0.5367411971092224\n",
      "Epoch: 36630 | Loss: 0.5121011137962341 | Test loss: 0.5367273688316345\n",
      "Epoch: 36640 | Loss: 0.5120893716812134 | Test loss: 0.5367135405540466\n",
      "Epoch: 36650 | Loss: 0.5120775699615479 | Test loss: 0.5366997122764587\n",
      "Epoch: 36660 | Loss: 0.5120658278465271 | Test loss: 0.5366858839988708\n",
      "Epoch: 36670 | Loss: 0.5120540261268616 | Test loss: 0.5366719365119934\n",
      "Epoch: 36680 | Loss: 0.5120422840118408 | Test loss: 0.5366581082344055\n",
      "Epoch: 36690 | Loss: 0.5120305418968201 | Test loss: 0.5366442799568176\n",
      "Epoch: 36700 | Loss: 0.5120187997817993 | Test loss: 0.536630392074585\n",
      "Epoch: 36710 | Loss: 0.5120069980621338 | Test loss: 0.5366165637969971\n",
      "Epoch: 36720 | Loss: 0.511995255947113 | Test loss: 0.5366027355194092\n",
      "Epoch: 36730 | Loss: 0.5119835138320923 | Test loss: 0.5365888476371765\n",
      "Epoch: 36740 | Loss: 0.5119717121124268 | Test loss: 0.5365750193595886\n",
      "Epoch: 36750 | Loss: 0.511959969997406 | Test loss: 0.5365611910820007\n",
      "Epoch: 36760 | Loss: 0.5119482278823853 | Test loss: 0.5365473031997681\n",
      "Epoch: 36770 | Loss: 0.5119364261627197 | Test loss: 0.536533534526825\n",
      "Epoch: 36780 | Loss: 0.511924684047699 | Test loss: 0.5365196466445923\n",
      "Epoch: 36790 | Loss: 0.5119129419326782 | Test loss: 0.5365058183670044\n",
      "Epoch: 36800 | Loss: 0.5119011998176575 | Test loss: 0.5364919900894165\n",
      "Epoch: 36810 | Loss: 0.5118893980979919 | Test loss: 0.5364781618118286\n",
      "Epoch: 36820 | Loss: 0.5118776559829712 | Test loss: 0.5364642143249512\n",
      "Epoch: 36830 | Loss: 0.5118659138679504 | Test loss: 0.5364503860473633\n",
      "Epoch: 36840 | Loss: 0.5118541121482849 | Test loss: 0.5364365577697754\n",
      "Epoch: 36850 | Loss: 0.5118423700332642 | Test loss: 0.5364227294921875\n",
      "Epoch: 36860 | Loss: 0.5118306279182434 | Test loss: 0.5364089012145996\n",
      "Epoch: 36870 | Loss: 0.5118188261985779 | Test loss: 0.5363950133323669\n",
      "Epoch: 36880 | Loss: 0.5118071436882019 | Test loss: 0.536381185054779\n",
      "Epoch: 36890 | Loss: 0.5117952823638916 | Test loss: 0.5363673567771912\n",
      "Epoch: 36900 | Loss: 0.5117835402488708 | Test loss: 0.5363535284996033\n",
      "Epoch: 36910 | Loss: 0.5117717981338501 | Test loss: 0.5363397002220154\n",
      "Epoch: 36920 | Loss: 0.5117599964141846 | Test loss: 0.5363258719444275\n",
      "Epoch: 36930 | Loss: 0.5117483139038086 | Test loss: 0.5363119840621948\n",
      "Epoch: 36940 | Loss: 0.5117365121841431 | Test loss: 0.5362980961799622\n",
      "Epoch: 36950 | Loss: 0.5117247700691223 | Test loss: 0.5362842679023743\n",
      "Epoch: 36960 | Loss: 0.5117129683494568 | Test loss: 0.5362703800201416\n",
      "Epoch: 36970 | Loss: 0.511701226234436 | Test loss: 0.5362566113471985\n",
      "Epoch: 36980 | Loss: 0.5116894841194153 | Test loss: 0.5362427234649658\n",
      "Epoch: 36990 | Loss: 0.5116776823997498 | Test loss: 0.5362288355827332\n",
      "Epoch: 37000 | Loss: 0.5116659998893738 | Test loss: 0.5362150073051453\n",
      "Epoch: 37010 | Loss: 0.5116541981697083 | Test loss: 0.5362011790275574\n",
      "Epoch: 37020 | Loss: 0.5116424560546875 | Test loss: 0.5361873507499695\n",
      "Epoch: 37030 | Loss: 0.5116307139396667 | Test loss: 0.5361735224723816\n",
      "Epoch: 37040 | Loss: 0.5116189122200012 | Test loss: 0.5361596345901489\n",
      "Epoch: 37050 | Loss: 0.5116071701049805 | Test loss: 0.536145806312561\n",
      "Epoch: 37060 | Loss: 0.5115953683853149 | Test loss: 0.5361319780349731\n",
      "Epoch: 37070 | Loss: 0.5115836262702942 | Test loss: 0.5361181497573853\n",
      "Epoch: 37080 | Loss: 0.5115718841552734 | Test loss: 0.5361042618751526\n",
      "Epoch: 37090 | Loss: 0.5115601420402527 | Test loss: 0.5360904335975647\n",
      "Epoch: 37100 | Loss: 0.5115483403205872 | Test loss: 0.536076545715332\n",
      "Epoch: 37110 | Loss: 0.5115365982055664 | Test loss: 0.5360627174377441\n",
      "Epoch: 37120 | Loss: 0.5115247964859009 | Test loss: 0.5360488891601562\n",
      "Epoch: 37130 | Loss: 0.5115130543708801 | Test loss: 0.5360350608825684\n",
      "Epoch: 37140 | Loss: 0.5115013122558594 | Test loss: 0.5360211730003357\n",
      "Epoch: 37150 | Loss: 0.5114895701408386 | Test loss: 0.5360073447227478\n",
      "Epoch: 37160 | Loss: 0.5114777684211731 | Test loss: 0.5359935164451599\n",
      "Epoch: 37170 | Loss: 0.5114660263061523 | Test loss: 0.535979688167572\n",
      "Epoch: 37180 | Loss: 0.5114542841911316 | Test loss: 0.5359658598899841\n",
      "Epoch: 37190 | Loss: 0.5114424824714661 | Test loss: 0.5359519720077515\n",
      "Epoch: 37200 | Loss: 0.5114307403564453 | Test loss: 0.5359381437301636\n",
      "Epoch: 37210 | Loss: 0.5114189982414246 | Test loss: 0.5359242558479309\n",
      "Epoch: 37220 | Loss: 0.5114073157310486 | Test loss: 0.535910427570343\n",
      "Epoch: 37230 | Loss: 0.5113954544067383 | Test loss: 0.5358965992927551\n",
      "Epoch: 37240 | Loss: 0.5113837122917175 | Test loss: 0.5358827114105225\n",
      "Epoch: 37250 | Loss: 0.5113720297813416 | Test loss: 0.5358688831329346\n",
      "Epoch: 37260 | Loss: 0.5113601684570312 | Test loss: 0.5358550548553467\n",
      "Epoch: 37270 | Loss: 0.5113484263420105 | Test loss: 0.535841166973114\n",
      "Epoch: 37280 | Loss: 0.5113367438316345 | Test loss: 0.5358273386955261\n",
      "Epoch: 37290 | Loss: 0.5113248825073242 | Test loss: 0.5358135104179382\n",
      "Epoch: 37300 | Loss: 0.5113131403923035 | Test loss: 0.5357996225357056\n",
      "Epoch: 37310 | Loss: 0.5113013982772827 | Test loss: 0.5357857942581177\n",
      "Epoch: 37320 | Loss: 0.5112895965576172 | Test loss: 0.5357719659805298\n",
      "Epoch: 37330 | Loss: 0.5112778544425964 | Test loss: 0.5357581377029419\n",
      "Epoch: 37340 | Loss: 0.5112661719322205 | Test loss: 0.535744309425354\n",
      "Epoch: 37350 | Loss: 0.5112543106079102 | Test loss: 0.5357304215431213\n",
      "Epoch: 37360 | Loss: 0.5112425684928894 | Test loss: 0.5357165336608887\n",
      "Epoch: 37370 | Loss: 0.5112308859825134 | Test loss: 0.5357027053833008\n",
      "Epoch: 37380 | Loss: 0.5112190842628479 | Test loss: 0.5356888771057129\n",
      "Epoch: 37390 | Loss: 0.5112072825431824 | Test loss: 0.535675048828125\n",
      "Epoch: 37400 | Loss: 0.5111956000328064 | Test loss: 0.5356612205505371\n",
      "Epoch: 37410 | Loss: 0.5111837387084961 | Test loss: 0.5356473326683044\n",
      "Epoch: 37420 | Loss: 0.5111720561981201 | Test loss: 0.5356335043907166\n",
      "Epoch: 37430 | Loss: 0.5111603140830994 | Test loss: 0.5356196761131287\n",
      "Epoch: 37440 | Loss: 0.5111485719680786 | Test loss: 0.5356058478355408\n",
      "Epoch: 37450 | Loss: 0.5111367702484131 | Test loss: 0.5355920195579529\n",
      "Epoch: 37460 | Loss: 0.5111250281333923 | Test loss: 0.535578191280365\n",
      "Epoch: 37470 | Loss: 0.5111132264137268 | Test loss: 0.5355642437934875\n",
      "Epoch: 37480 | Loss: 0.511101484298706 | Test loss: 0.5355504155158997\n",
      "Epoch: 37490 | Loss: 0.5110897421836853 | Test loss: 0.5355365872383118\n",
      "Epoch: 37500 | Loss: 0.5110780000686646 | Test loss: 0.5355226993560791\n",
      "Epoch: 37510 | Loss: 0.511066198348999 | Test loss: 0.5355088710784912\n",
      "Epoch: 37520 | Loss: 0.5110544562339783 | Test loss: 0.5354950428009033\n",
      "Epoch: 37530 | Loss: 0.5110427141189575 | Test loss: 0.5354811549186707\n",
      "Epoch: 37540 | Loss: 0.511030912399292 | Test loss: 0.5354673266410828\n",
      "Epoch: 37550 | Loss: 0.5110191702842712 | Test loss: 0.5354534983634949\n",
      "Epoch: 37560 | Loss: 0.5110074281692505 | Test loss: 0.5354396104812622\n",
      "Epoch: 37570 | Loss: 0.510995626449585 | Test loss: 0.5354258418083191\n",
      "Epoch: 37580 | Loss: 0.5109838843345642 | Test loss: 0.5354119539260864\n",
      "Epoch: 37590 | Loss: 0.5109721422195435 | Test loss: 0.5353981256484985\n",
      "Epoch: 37600 | Loss: 0.5109604001045227 | Test loss: 0.5353842973709106\n",
      "Epoch: 37610 | Loss: 0.5109485983848572 | Test loss: 0.5353704690933228\n",
      "Epoch: 37620 | Loss: 0.5109368562698364 | Test loss: 0.5353565216064453\n",
      "Epoch: 37630 | Loss: 0.5109251141548157 | Test loss: 0.5353426933288574\n",
      "Epoch: 37640 | Loss: 0.5109133124351501 | Test loss: 0.5353288650512695\n",
      "Epoch: 37650 | Loss: 0.5109015703201294 | Test loss: 0.5353150367736816\n",
      "Epoch: 37660 | Loss: 0.5108898282051086 | Test loss: 0.5353012084960938\n",
      "Epoch: 37670 | Loss: 0.5108780264854431 | Test loss: 0.5352873206138611\n",
      "Epoch: 37680 | Loss: 0.5108663439750671 | Test loss: 0.5352734923362732\n",
      "Epoch: 37690 | Loss: 0.5108544826507568 | Test loss: 0.5352596640586853\n",
      "Epoch: 37700 | Loss: 0.5108427405357361 | Test loss: 0.5352458357810974\n",
      "Epoch: 37710 | Loss: 0.5108309984207153 | Test loss: 0.5352320075035095\n",
      "Epoch: 37720 | Loss: 0.5108191967010498 | Test loss: 0.5352181792259216\n",
      "Epoch: 37730 | Loss: 0.5108075141906738 | Test loss: 0.535204291343689\n",
      "Epoch: 37740 | Loss: 0.5107957124710083 | Test loss: 0.5351904034614563\n",
      "Epoch: 37750 | Loss: 0.5107839703559875 | Test loss: 0.5351765751838684\n",
      "Epoch: 37760 | Loss: 0.510772168636322 | Test loss: 0.5351626873016357\n",
      "Epoch: 37770 | Loss: 0.5107604265213013 | Test loss: 0.5351489186286926\n",
      "Epoch: 37780 | Loss: 0.5107486844062805 | Test loss: 0.53513503074646\n",
      "Epoch: 37790 | Loss: 0.510736882686615 | Test loss: 0.5351211428642273\n",
      "Epoch: 37800 | Loss: 0.510725200176239 | Test loss: 0.5351073145866394\n",
      "Epoch: 37810 | Loss: 0.5107133984565735 | Test loss: 0.5350934863090515\n",
      "Epoch: 37820 | Loss: 0.5107016563415527 | Test loss: 0.5350796580314636\n",
      "Epoch: 37830 | Loss: 0.510689914226532 | Test loss: 0.5350658297538757\n",
      "Epoch: 37840 | Loss: 0.5106781125068665 | Test loss: 0.5350519418716431\n",
      "Epoch: 37850 | Loss: 0.5106663703918457 | Test loss: 0.5350381135940552\n",
      "Epoch: 37860 | Loss: 0.5106545686721802 | Test loss: 0.5350242853164673\n",
      "Epoch: 37870 | Loss: 0.5106428265571594 | Test loss: 0.5350104570388794\n",
      "Epoch: 37880 | Loss: 0.5106310844421387 | Test loss: 0.5349965691566467\n",
      "Epoch: 37890 | Loss: 0.5106193423271179 | Test loss: 0.5349827408790588\n",
      "Epoch: 37900 | Loss: 0.5106075406074524 | Test loss: 0.5349688529968262\n",
      "Epoch: 37910 | Loss: 0.5105957984924316 | Test loss: 0.5349550247192383\n",
      "Epoch: 37920 | Loss: 0.5105839967727661 | Test loss: 0.5349411964416504\n",
      "Epoch: 37930 | Loss: 0.5105722546577454 | Test loss: 0.5349273681640625\n",
      "Epoch: 37940 | Loss: 0.5105605125427246 | Test loss: 0.5349134802818298\n",
      "Epoch: 37950 | Loss: 0.5105487704277039 | Test loss: 0.5348996520042419\n",
      "Epoch: 37960 | Loss: 0.5105369687080383 | Test loss: 0.534885823726654\n",
      "Epoch: 37970 | Loss: 0.5105252265930176 | Test loss: 0.5348719954490662\n",
      "Epoch: 37980 | Loss: 0.5105134844779968 | Test loss: 0.5348581671714783\n",
      "Epoch: 37990 | Loss: 0.5105016827583313 | Test loss: 0.5348442792892456\n",
      "Epoch: 38000 | Loss: 0.5104899406433105 | Test loss: 0.5348304510116577\n",
      "Epoch: 38010 | Loss: 0.5104781985282898 | Test loss: 0.534816563129425\n",
      "Epoch: 38020 | Loss: 0.5104665160179138 | Test loss: 0.5348027348518372\n",
      "Epoch: 38030 | Loss: 0.5104546546936035 | Test loss: 0.5347889065742493\n",
      "Epoch: 38040 | Loss: 0.5104429125785828 | Test loss: 0.5347750186920166\n",
      "Epoch: 38050 | Loss: 0.5104312300682068 | Test loss: 0.5347611904144287\n",
      "Epoch: 38060 | Loss: 0.5104193687438965 | Test loss: 0.5347473621368408\n",
      "Epoch: 38070 | Loss: 0.5104076266288757 | Test loss: 0.5347334742546082\n",
      "Epoch: 38080 | Loss: 0.5103959441184998 | Test loss: 0.5347196459770203\n",
      "Epoch: 38090 | Loss: 0.5103840827941895 | Test loss: 0.5347058176994324\n",
      "Epoch: 38100 | Loss: 0.5103723406791687 | Test loss: 0.5346919298171997\n",
      "Epoch: 38110 | Loss: 0.510360598564148 | Test loss: 0.5346781015396118\n",
      "Epoch: 38120 | Loss: 0.5103487968444824 | Test loss: 0.5346642732620239\n",
      "Epoch: 38130 | Loss: 0.5103370547294617 | Test loss: 0.534650444984436\n",
      "Epoch: 38140 | Loss: 0.5103253722190857 | Test loss: 0.5346366167068481\n",
      "Epoch: 38150 | Loss: 0.5103135108947754 | Test loss: 0.5346227288246155\n",
      "Epoch: 38160 | Loss: 0.5103017687797546 | Test loss: 0.5346088409423828\n",
      "Epoch: 38170 | Loss: 0.5102900862693787 | Test loss: 0.5345950126647949\n",
      "Epoch: 38180 | Loss: 0.5102782845497131 | Test loss: 0.534581184387207\n",
      "Epoch: 38190 | Loss: 0.5102664828300476 | Test loss: 0.5345673561096191\n",
      "Epoch: 38200 | Loss: 0.5102548003196716 | Test loss: 0.5345535278320312\n",
      "Epoch: 38210 | Loss: 0.5102429389953613 | Test loss: 0.5345396399497986\n",
      "Epoch: 38220 | Loss: 0.5102312564849854 | Test loss: 0.5345258116722107\n",
      "Epoch: 38230 | Loss: 0.5102195143699646 | Test loss: 0.5345119833946228\n",
      "Epoch: 38240 | Loss: 0.5102077722549438 | Test loss: 0.5344981551170349\n",
      "Epoch: 38250 | Loss: 0.5101959705352783 | Test loss: 0.534484326839447\n",
      "Epoch: 38260 | Loss: 0.5101842284202576 | Test loss: 0.5344704985618591\n",
      "Epoch: 38270 | Loss: 0.510172426700592 | Test loss: 0.5344565510749817\n",
      "Epoch: 38280 | Loss: 0.5101606845855713 | Test loss: 0.5344427227973938\n",
      "Epoch: 38290 | Loss: 0.5101489424705505 | Test loss: 0.5344288945198059\n",
      "Epoch: 38300 | Loss: 0.5101372003555298 | Test loss: 0.5344150066375732\n",
      "Epoch: 38310 | Loss: 0.5101253986358643 | Test loss: 0.5344011783599854\n",
      "Epoch: 38320 | Loss: 0.5101136565208435 | Test loss: 0.5343873500823975\n",
      "Epoch: 38330 | Loss: 0.5101019144058228 | Test loss: 0.5343734622001648\n",
      "Epoch: 38340 | Loss: 0.5100901126861572 | Test loss: 0.5343596339225769\n",
      "Epoch: 38350 | Loss: 0.5100783705711365 | Test loss: 0.534345805644989\n",
      "Epoch: 38360 | Loss: 0.5100666284561157 | Test loss: 0.5343319177627563\n",
      "Epoch: 38370 | Loss: 0.5100548267364502 | Test loss: 0.5343181490898132\n",
      "Epoch: 38380 | Loss: 0.5100430846214294 | Test loss: 0.5343042612075806\n",
      "Epoch: 38390 | Loss: 0.5100313425064087 | Test loss: 0.5342904329299927\n",
      "Epoch: 38400 | Loss: 0.5100196003913879 | Test loss: 0.5342766046524048\n",
      "Epoch: 38410 | Loss: 0.5100077986717224 | Test loss: 0.5342627763748169\n",
      "Epoch: 38420 | Loss: 0.5099960565567017 | Test loss: 0.5342488288879395\n",
      "Epoch: 38430 | Loss: 0.5099843144416809 | Test loss: 0.5342350006103516\n",
      "Epoch: 38440 | Loss: 0.5099725127220154 | Test loss: 0.5342211723327637\n",
      "Epoch: 38450 | Loss: 0.5099607706069946 | Test loss: 0.5342073440551758\n",
      "Epoch: 38460 | Loss: 0.5099490284919739 | Test loss: 0.5341935157775879\n",
      "Epoch: 38470 | Loss: 0.5099372267723083 | Test loss: 0.5341796278953552\n",
      "Epoch: 38480 | Loss: 0.5099255442619324 | Test loss: 0.5341657996177673\n",
      "Epoch: 38490 | Loss: 0.5099136829376221 | Test loss: 0.5341519713401794\n",
      "Epoch: 38500 | Loss: 0.5099019408226013 | Test loss: 0.5341381430625916\n",
      "Epoch: 38510 | Loss: 0.5098901987075806 | Test loss: 0.5341243147850037\n",
      "Epoch: 38520 | Loss: 0.509878396987915 | Test loss: 0.5341104865074158\n",
      "Epoch: 38530 | Loss: 0.5098667144775391 | Test loss: 0.5340965986251831\n",
      "Epoch: 38540 | Loss: 0.5098549127578735 | Test loss: 0.5340827107429504\n",
      "Epoch: 38550 | Loss: 0.5098431706428528 | Test loss: 0.5340688824653625\n",
      "Epoch: 38560 | Loss: 0.5098313689231873 | Test loss: 0.5340549945831299\n",
      "Epoch: 38570 | Loss: 0.5098196268081665 | Test loss: 0.5340412259101868\n",
      "Epoch: 38580 | Loss: 0.5098078846931458 | Test loss: 0.5340273380279541\n",
      "Epoch: 38590 | Loss: 0.5097960829734802 | Test loss: 0.5340134501457214\n",
      "Epoch: 38600 | Loss: 0.5097844004631042 | Test loss: 0.5339996218681335\n",
      "Epoch: 38610 | Loss: 0.5097725987434387 | Test loss: 0.5339857935905457\n",
      "Epoch: 38620 | Loss: 0.509760856628418 | Test loss: 0.5339719653129578\n",
      "Epoch: 38630 | Loss: 0.5097491145133972 | Test loss: 0.5339581370353699\n",
      "Epoch: 38640 | Loss: 0.5097373127937317 | Test loss: 0.5339442491531372\n",
      "Epoch: 38650 | Loss: 0.5097255706787109 | Test loss: 0.5339304208755493\n",
      "Epoch: 38660 | Loss: 0.5097137689590454 | Test loss: 0.5339165925979614\n",
      "Epoch: 38670 | Loss: 0.5097020268440247 | Test loss: 0.5339027643203735\n",
      "Epoch: 38680 | Loss: 0.5096902847290039 | Test loss: 0.5338888764381409\n",
      "Epoch: 38690 | Loss: 0.5096785426139832 | Test loss: 0.533875048160553\n",
      "Epoch: 38700 | Loss: 0.5096667408943176 | Test loss: 0.5338611602783203\n",
      "Epoch: 38710 | Loss: 0.5096549987792969 | Test loss: 0.5338473320007324\n",
      "Epoch: 38720 | Loss: 0.5096431970596313 | Test loss: 0.5338335037231445\n",
      "Epoch: 38730 | Loss: 0.5096314549446106 | Test loss: 0.5338196754455566\n",
      "Epoch: 38740 | Loss: 0.5096197128295898 | Test loss: 0.533805787563324\n",
      "Epoch: 38750 | Loss: 0.5096079707145691 | Test loss: 0.5337919592857361\n",
      "Epoch: 38760 | Loss: 0.5095961689949036 | Test loss: 0.5337781310081482\n",
      "Epoch: 38770 | Loss: 0.5095844268798828 | Test loss: 0.5337643027305603\n",
      "Epoch: 38780 | Loss: 0.5095726847648621 | Test loss: 0.5337504744529724\n",
      "Epoch: 38790 | Loss: 0.5095608830451965 | Test loss: 0.5337365865707397\n",
      "Epoch: 38800 | Loss: 0.5095491409301758 | Test loss: 0.5337227582931519\n",
      "Epoch: 38810 | Loss: 0.509537398815155 | Test loss: 0.5337088704109192\n",
      "Epoch: 38820 | Loss: 0.509525716304779 | Test loss: 0.5336950421333313\n",
      "Epoch: 38830 | Loss: 0.5095138549804688 | Test loss: 0.5336812138557434\n",
      "Epoch: 38840 | Loss: 0.509502112865448 | Test loss: 0.5336673259735107\n",
      "Epoch: 38850 | Loss: 0.509490430355072 | Test loss: 0.5336534976959229\n",
      "Epoch: 38860 | Loss: 0.5094785690307617 | Test loss: 0.533639669418335\n",
      "Epoch: 38870 | Loss: 0.509466826915741 | Test loss: 0.5336257815361023\n",
      "Epoch: 38880 | Loss: 0.509455144405365 | Test loss: 0.5336119532585144\n",
      "Epoch: 38890 | Loss: 0.5094432830810547 | Test loss: 0.5335981249809265\n",
      "Epoch: 38900 | Loss: 0.5094315409660339 | Test loss: 0.5335842370986938\n",
      "Epoch: 38910 | Loss: 0.5094197988510132 | Test loss: 0.533570408821106\n",
      "Epoch: 38920 | Loss: 0.5094079971313477 | Test loss: 0.5335565805435181\n",
      "Epoch: 38930 | Loss: 0.5093962550163269 | Test loss: 0.5335427522659302\n",
      "Epoch: 38940 | Loss: 0.5093845725059509 | Test loss: 0.5335289239883423\n",
      "Epoch: 38950 | Loss: 0.5093727111816406 | Test loss: 0.5335150361061096\n",
      "Epoch: 38960 | Loss: 0.5093609690666199 | Test loss: 0.533501148223877\n",
      "Epoch: 38970 | Loss: 0.5093492865562439 | Test loss: 0.5334873199462891\n",
      "Epoch: 38980 | Loss: 0.5093374848365784 | Test loss: 0.5334734916687012\n",
      "Epoch: 38990 | Loss: 0.5093256831169128 | Test loss: 0.5334596633911133\n",
      "Epoch: 39000 | Loss: 0.5093140006065369 | Test loss: 0.5334458351135254\n",
      "Epoch: 39010 | Loss: 0.5093021392822266 | Test loss: 0.5334319472312927\n",
      "Epoch: 39020 | Loss: 0.5092904567718506 | Test loss: 0.5334181189537048\n",
      "Epoch: 39030 | Loss: 0.5092787146568298 | Test loss: 0.5334042906761169\n",
      "Epoch: 39040 | Loss: 0.5092669725418091 | Test loss: 0.533390462398529\n",
      "Epoch: 39050 | Loss: 0.5092551708221436 | Test loss: 0.5333766341209412\n",
      "Epoch: 39060 | Loss: 0.5092434287071228 | Test loss: 0.5333628058433533\n",
      "Epoch: 39070 | Loss: 0.5092316269874573 | Test loss: 0.5333488583564758\n",
      "Epoch: 39080 | Loss: 0.5092198848724365 | Test loss: 0.5333350300788879\n",
      "Epoch: 39090 | Loss: 0.5092081427574158 | Test loss: 0.5333212018013\n",
      "Epoch: 39100 | Loss: 0.509196400642395 | Test loss: 0.5333073139190674\n",
      "Epoch: 39110 | Loss: 0.5091845989227295 | Test loss: 0.5332934856414795\n",
      "Epoch: 39120 | Loss: 0.5091728568077087 | Test loss: 0.5332796573638916\n",
      "Epoch: 39130 | Loss: 0.509161114692688 | Test loss: 0.5332657694816589\n",
      "Epoch: 39140 | Loss: 0.5091493129730225 | Test loss: 0.533251941204071\n",
      "Epoch: 39150 | Loss: 0.5091375708580017 | Test loss: 0.5332381129264832\n",
      "Epoch: 39160 | Loss: 0.509125828742981 | Test loss: 0.5332242250442505\n",
      "Epoch: 39170 | Loss: 0.5091140270233154 | Test loss: 0.5332104563713074\n",
      "Epoch: 39180 | Loss: 0.5091022849082947 | Test loss: 0.5331965684890747\n",
      "Epoch: 39190 | Loss: 0.5090905427932739 | Test loss: 0.5331827402114868\n",
      "Epoch: 39200 | Loss: 0.5090788006782532 | Test loss: 0.5331689119338989\n",
      "Epoch: 39210 | Loss: 0.5090669989585876 | Test loss: 0.533155083656311\n",
      "Epoch: 39220 | Loss: 0.5090552568435669 | Test loss: 0.5331411361694336\n",
      "Epoch: 39230 | Loss: 0.5090435147285461 | Test loss: 0.5331273078918457\n",
      "Epoch: 39240 | Loss: 0.5090317130088806 | Test loss: 0.5331134796142578\n",
      "Epoch: 39250 | Loss: 0.5090199708938599 | Test loss: 0.5330996513366699\n",
      "Epoch: 39260 | Loss: 0.5090082287788391 | Test loss: 0.533085823059082\n",
      "Epoch: 39270 | Loss: 0.5089964270591736 | Test loss: 0.5330719351768494\n",
      "Epoch: 39280 | Loss: 0.5089847445487976 | Test loss: 0.5330581068992615\n",
      "Epoch: 39290 | Loss: 0.5089728832244873 | Test loss: 0.5330442786216736\n",
      "Epoch: 39300 | Loss: 0.5089611411094666 | Test loss: 0.5330304503440857\n",
      "Epoch: 39310 | Loss: 0.5089493989944458 | Test loss: 0.5330166220664978\n",
      "Epoch: 39320 | Loss: 0.5089375972747803 | Test loss: 0.5330027937889099\n",
      "Epoch: 39330 | Loss: 0.5089259147644043 | Test loss: 0.5329889059066772\n",
      "Epoch: 39340 | Loss: 0.5089141130447388 | Test loss: 0.5329750180244446\n",
      "Epoch: 39350 | Loss: 0.508902370929718 | Test loss: 0.5329611897468567\n",
      "Epoch: 39360 | Loss: 0.5088905692100525 | Test loss: 0.532947301864624\n",
      "Epoch: 39370 | Loss: 0.5088788270950317 | Test loss: 0.5329335331916809\n",
      "Epoch: 39380 | Loss: 0.508867084980011 | Test loss: 0.5329196453094482\n",
      "Epoch: 39390 | Loss: 0.5088552832603455 | Test loss: 0.5329057574272156\n",
      "Epoch: 39400 | Loss: 0.5088436007499695 | Test loss: 0.5328919291496277\n",
      "Epoch: 39410 | Loss: 0.508831799030304 | Test loss: 0.5328781008720398\n",
      "Epoch: 39420 | Loss: 0.5088200569152832 | Test loss: 0.5328642725944519\n",
      "Epoch: 39430 | Loss: 0.5088083148002625 | Test loss: 0.532850444316864\n",
      "Epoch: 39440 | Loss: 0.5087965130805969 | Test loss: 0.5328365564346313\n",
      "Epoch: 39450 | Loss: 0.5087847709655762 | Test loss: 0.5328227281570435\n",
      "Epoch: 39460 | Loss: 0.5087729692459106 | Test loss: 0.5328088998794556\n",
      "Epoch: 39470 | Loss: 0.5087612271308899 | Test loss: 0.5327950716018677\n",
      "Epoch: 39480 | Loss: 0.5087494850158691 | Test loss: 0.532781183719635\n",
      "Epoch: 39490 | Loss: 0.5087377429008484 | Test loss: 0.5327673554420471\n",
      "Epoch: 39500 | Loss: 0.5087259411811829 | Test loss: 0.5327534675598145\n",
      "Epoch: 39510 | Loss: 0.5087141990661621 | Test loss: 0.5327396392822266\n",
      "Epoch: 39520 | Loss: 0.5087023973464966 | Test loss: 0.5327258110046387\n",
      "Epoch: 39530 | Loss: 0.5086906552314758 | Test loss: 0.5327119827270508\n",
      "Epoch: 39540 | Loss: 0.5086789131164551 | Test loss: 0.5326980948448181\n",
      "Epoch: 39550 | Loss: 0.5086671710014343 | Test loss: 0.5326842665672302\n",
      "Epoch: 39560 | Loss: 0.5086553692817688 | Test loss: 0.5326704382896423\n",
      "Epoch: 39570 | Loss: 0.508643627166748 | Test loss: 0.5326566100120544\n",
      "Epoch: 39580 | Loss: 0.5086318850517273 | Test loss: 0.5326427817344666\n",
      "Epoch: 39590 | Loss: 0.5086200833320618 | Test loss: 0.5326288938522339\n",
      "Epoch: 39600 | Loss: 0.508608341217041 | Test loss: 0.532615065574646\n",
      "Epoch: 39610 | Loss: 0.5085965991020203 | Test loss: 0.5326011776924133\n",
      "Epoch: 39620 | Loss: 0.5085849165916443 | Test loss: 0.5325873494148254\n",
      "Epoch: 39630 | Loss: 0.508573055267334 | Test loss: 0.5325735211372375\n",
      "Epoch: 39640 | Loss: 0.5085613131523132 | Test loss: 0.5325596332550049\n",
      "Epoch: 39650 | Loss: 0.5085496306419373 | Test loss: 0.532545804977417\n",
      "Epoch: 39660 | Loss: 0.508537769317627 | Test loss: 0.5325319766998291\n",
      "Epoch: 39670 | Loss: 0.5085260272026062 | Test loss: 0.5325180888175964\n",
      "Epoch: 39680 | Loss: 0.5085143446922302 | Test loss: 0.5325042605400085\n",
      "Epoch: 39690 | Loss: 0.5085024833679199 | Test loss: 0.5324904322624207\n",
      "Epoch: 39700 | Loss: 0.5084907412528992 | Test loss: 0.532476544380188\n",
      "Epoch: 39710 | Loss: 0.5084789991378784 | Test loss: 0.5324627161026001\n",
      "Epoch: 39720 | Loss: 0.5084671974182129 | Test loss: 0.5324488878250122\n",
      "Epoch: 39730 | Loss: 0.5084554553031921 | Test loss: 0.5324350595474243\n",
      "Epoch: 39740 | Loss: 0.5084437727928162 | Test loss: 0.5324212312698364\n",
      "Epoch: 39750 | Loss: 0.5084319114685059 | Test loss: 0.5324073433876038\n",
      "Epoch: 39760 | Loss: 0.5084201693534851 | Test loss: 0.5323934555053711\n",
      "Epoch: 39770 | Loss: 0.5084084868431091 | Test loss: 0.5323796272277832\n",
      "Epoch: 39780 | Loss: 0.5083966851234436 | Test loss: 0.5323657989501953\n",
      "Epoch: 39790 | Loss: 0.5083848834037781 | Test loss: 0.5323519706726074\n",
      "Epoch: 39800 | Loss: 0.5083732008934021 | Test loss: 0.5323381423950195\n",
      "Epoch: 39810 | Loss: 0.5083613395690918 | Test loss: 0.5323242545127869\n",
      "Epoch: 39820 | Loss: 0.5083496570587158 | Test loss: 0.532310426235199\n",
      "Epoch: 39830 | Loss: 0.5083379149436951 | Test loss: 0.5322965979576111\n",
      "Epoch: 39840 | Loss: 0.5083261728286743 | Test loss: 0.5322827696800232\n",
      "Epoch: 39850 | Loss: 0.5083143711090088 | Test loss: 0.5322689414024353\n",
      "Epoch: 39860 | Loss: 0.508302628993988 | Test loss: 0.5322551131248474\n",
      "Epoch: 39870 | Loss: 0.5082908272743225 | Test loss: 0.53224116563797\n",
      "Epoch: 39880 | Loss: 0.5082790851593018 | Test loss: 0.5322273373603821\n",
      "Epoch: 39890 | Loss: 0.508267343044281 | Test loss: 0.5322135090827942\n",
      "Epoch: 39900 | Loss: 0.5082556009292603 | Test loss: 0.5321996212005615\n",
      "Epoch: 39910 | Loss: 0.5082437992095947 | Test loss: 0.5321857929229736\n",
      "Epoch: 39920 | Loss: 0.508232057094574 | Test loss: 0.5321719646453857\n",
      "Epoch: 39930 | Loss: 0.5082203149795532 | Test loss: 0.5321580767631531\n",
      "Epoch: 39940 | Loss: 0.5082085132598877 | Test loss: 0.5321442484855652\n",
      "Epoch: 39950 | Loss: 0.5081967711448669 | Test loss: 0.5321304202079773\n",
      "Epoch: 39960 | Loss: 0.5081850290298462 | Test loss: 0.5321165323257446\n",
      "Epoch: 39970 | Loss: 0.5081732273101807 | Test loss: 0.5321027636528015\n",
      "Epoch: 39980 | Loss: 0.5081614851951599 | Test loss: 0.5320888757705688\n",
      "Epoch: 39990 | Loss: 0.5081497430801392 | Test loss: 0.532075047492981\n",
      "Epoch: 40000 | Loss: 0.5081380009651184 | Test loss: 0.5320612192153931\n",
      "Epoch: 40010 | Loss: 0.5081261992454529 | Test loss: 0.5320473909378052\n",
      "Epoch: 40020 | Loss: 0.5081144571304321 | Test loss: 0.5320334434509277\n",
      "Epoch: 40030 | Loss: 0.5081027150154114 | Test loss: 0.5320196151733398\n",
      "Epoch: 40040 | Loss: 0.5080909132957458 | Test loss: 0.532005786895752\n",
      "Epoch: 40050 | Loss: 0.5080791711807251 | Test loss: 0.5319919586181641\n",
      "Epoch: 40060 | Loss: 0.5080674290657043 | Test loss: 0.5319781303405762\n",
      "Epoch: 40070 | Loss: 0.5080556273460388 | Test loss: 0.5319642424583435\n",
      "Epoch: 40080 | Loss: 0.5080439448356628 | Test loss: 0.5319504141807556\n",
      "Epoch: 40090 | Loss: 0.5080320835113525 | Test loss: 0.5319365859031677\n",
      "Epoch: 40100 | Loss: 0.5080203413963318 | Test loss: 0.5319227576255798\n",
      "Epoch: 40110 | Loss: 0.508008599281311 | Test loss: 0.5319089293479919\n",
      "Epoch: 40120 | Loss: 0.5079967975616455 | Test loss: 0.531895101070404\n",
      "Epoch: 40130 | Loss: 0.5079851150512695 | Test loss: 0.5318812131881714\n",
      "Epoch: 40140 | Loss: 0.507973313331604 | Test loss: 0.5318673253059387\n",
      "Epoch: 40150 | Loss: 0.5079615712165833 | Test loss: 0.5318534970283508\n",
      "Epoch: 40160 | Loss: 0.5079497694969177 | Test loss: 0.5318396091461182\n",
      "Epoch: 40170 | Loss: 0.507938027381897 | Test loss: 0.531825840473175\n",
      "Epoch: 40180 | Loss: 0.5079262852668762 | Test loss: 0.5318119525909424\n",
      "Epoch: 40190 | Loss: 0.5079144835472107 | Test loss: 0.5317980647087097\n",
      "Epoch: 40200 | Loss: 0.5079028010368347 | Test loss: 0.5317842364311218\n",
      "Epoch: 40210 | Loss: 0.5078909993171692 | Test loss: 0.5317704081535339\n",
      "Epoch: 40220 | Loss: 0.5078792572021484 | Test loss: 0.531756579875946\n",
      "Epoch: 40230 | Loss: 0.5078675150871277 | Test loss: 0.5317427515983582\n",
      "Epoch: 40240 | Loss: 0.5078557133674622 | Test loss: 0.5317288637161255\n",
      "Epoch: 40250 | Loss: 0.5078439712524414 | Test loss: 0.5317150354385376\n",
      "Epoch: 40260 | Loss: 0.5078321695327759 | Test loss: 0.5317012071609497\n",
      "Epoch: 40270 | Loss: 0.5078204274177551 | Test loss: 0.5316873788833618\n",
      "Epoch: 40280 | Loss: 0.5078086853027344 | Test loss: 0.5316734910011292\n",
      "Epoch: 40290 | Loss: 0.5077969431877136 | Test loss: 0.5316596627235413\n",
      "Epoch: 40300 | Loss: 0.5077851414680481 | Test loss: 0.5316457748413086\n",
      "Epoch: 40310 | Loss: 0.5077733993530273 | Test loss: 0.5316319465637207\n",
      "Epoch: 40320 | Loss: 0.5077615976333618 | Test loss: 0.5316181182861328\n",
      "Epoch: 40330 | Loss: 0.5077498555183411 | Test loss: 0.5316042900085449\n",
      "Epoch: 40340 | Loss: 0.5077381134033203 | Test loss: 0.5315904021263123\n",
      "Epoch: 40350 | Loss: 0.5077263712882996 | Test loss: 0.5315765738487244\n",
      "Epoch: 40360 | Loss: 0.507714569568634 | Test loss: 0.5315627455711365\n",
      "Epoch: 40370 | Loss: 0.5077028274536133 | Test loss: 0.5315489172935486\n",
      "Epoch: 40380 | Loss: 0.5076910853385925 | Test loss: 0.5315350890159607\n",
      "Epoch: 40390 | Loss: 0.507679283618927 | Test loss: 0.531521201133728\n",
      "Epoch: 40400 | Loss: 0.5076675415039062 | Test loss: 0.5315073728561401\n",
      "Epoch: 40410 | Loss: 0.5076557993888855 | Test loss: 0.5314934849739075\n",
      "Epoch: 40420 | Loss: 0.5076441168785095 | Test loss: 0.5314796566963196\n",
      "Epoch: 40430 | Loss: 0.5076322555541992 | Test loss: 0.5314658284187317\n",
      "Epoch: 40440 | Loss: 0.5076205134391785 | Test loss: 0.531451940536499\n",
      "Epoch: 40450 | Loss: 0.5076088309288025 | Test loss: 0.5314381122589111\n",
      "Epoch: 40460 | Loss: 0.5075969696044922 | Test loss: 0.5314242839813232\n",
      "Epoch: 40470 | Loss: 0.5075852274894714 | Test loss: 0.5314103960990906\n",
      "Epoch: 40480 | Loss: 0.5075735449790955 | Test loss: 0.5313965678215027\n",
      "Epoch: 40490 | Loss: 0.5075616836547852 | Test loss: 0.5313827395439148\n",
      "Epoch: 40500 | Loss: 0.5075499415397644 | Test loss: 0.5313688516616821\n",
      "Epoch: 40510 | Loss: 0.5075381994247437 | Test loss: 0.5313550233840942\n",
      "Epoch: 40520 | Loss: 0.5075263977050781 | Test loss: 0.5313411951065063\n",
      "Epoch: 40530 | Loss: 0.5075146555900574 | Test loss: 0.5313273668289185\n",
      "Epoch: 40540 | Loss: 0.5075029730796814 | Test loss: 0.5313135385513306\n",
      "Epoch: 40550 | Loss: 0.5074911117553711 | Test loss: 0.5312996506690979\n",
      "Epoch: 40560 | Loss: 0.5074793696403503 | Test loss: 0.5312857627868652\n",
      "Epoch: 40570 | Loss: 0.5074676871299744 | Test loss: 0.5312719345092773\n",
      "Epoch: 40580 | Loss: 0.5074558854103088 | Test loss: 0.5312581062316895\n",
      "Epoch: 40590 | Loss: 0.5074440836906433 | Test loss: 0.5312442779541016\n",
      "Epoch: 40600 | Loss: 0.5074324011802673 | Test loss: 0.5312304496765137\n",
      "Epoch: 40610 | Loss: 0.507420539855957 | Test loss: 0.531216561794281\n",
      "Epoch: 40620 | Loss: 0.507408857345581 | Test loss: 0.5312027335166931\n",
      "Epoch: 40630 | Loss: 0.5073971152305603 | Test loss: 0.5311889052391052\n",
      "Epoch: 40640 | Loss: 0.5073853731155396 | Test loss: 0.5311750769615173\n",
      "Epoch: 40650 | Loss: 0.507373571395874 | Test loss: 0.5311612486839294\n",
      "Epoch: 40660 | Loss: 0.5073618292808533 | Test loss: 0.5311474204063416\n",
      "Epoch: 40670 | Loss: 0.5073500275611877 | Test loss: 0.5311334729194641\n",
      "Epoch: 40680 | Loss: 0.507338285446167 | Test loss: 0.5311196446418762\n",
      "Epoch: 40690 | Loss: 0.5073265433311462 | Test loss: 0.5311058163642883\n",
      "Epoch: 40700 | Loss: 0.5073148012161255 | Test loss: 0.5310919284820557\n",
      "Epoch: 40710 | Loss: 0.50730299949646 | Test loss: 0.5310781002044678\n",
      "Epoch: 40720 | Loss: 0.5072912573814392 | Test loss: 0.5310642719268799\n",
      "Epoch: 40730 | Loss: 0.5072795152664185 | Test loss: 0.5310503840446472\n",
      "Epoch: 40740 | Loss: 0.5072677135467529 | Test loss: 0.5310365557670593\n",
      "Epoch: 40750 | Loss: 0.5072559714317322 | Test loss: 0.5310227274894714\n",
      "Epoch: 40760 | Loss: 0.5072442293167114 | Test loss: 0.5310088396072388\n",
      "Epoch: 40770 | Loss: 0.5072324275970459 | Test loss: 0.5309950709342957\n",
      "Epoch: 40780 | Loss: 0.5072206854820251 | Test loss: 0.530981183052063\n",
      "Epoch: 40790 | Loss: 0.5072089433670044 | Test loss: 0.5309673547744751\n",
      "Epoch: 40800 | Loss: 0.5071972012519836 | Test loss: 0.5309535264968872\n",
      "Epoch: 40810 | Loss: 0.5071853995323181 | Test loss: 0.5309396982192993\n",
      "Epoch: 40820 | Loss: 0.5071736574172974 | Test loss: 0.5309257507324219\n",
      "Epoch: 40830 | Loss: 0.5071619153022766 | Test loss: 0.530911922454834\n",
      "Epoch: 40840 | Loss: 0.5071501135826111 | Test loss: 0.5308980941772461\n",
      "Epoch: 40850 | Loss: 0.5071383714675903 | Test loss: 0.5308842658996582\n",
      "Epoch: 40860 | Loss: 0.5071266293525696 | Test loss: 0.5308704376220703\n",
      "Epoch: 40870 | Loss: 0.507114827632904 | Test loss: 0.5308565497398376\n",
      "Epoch: 40880 | Loss: 0.5071031451225281 | Test loss: 0.5308427214622498\n",
      "Epoch: 40890 | Loss: 0.5070912837982178 | Test loss: 0.5308288931846619\n",
      "Epoch: 40900 | Loss: 0.507079541683197 | Test loss: 0.530815064907074\n",
      "Epoch: 40910 | Loss: 0.5070677995681763 | Test loss: 0.5308012366294861\n",
      "Epoch: 40920 | Loss: 0.5070559978485107 | Test loss: 0.5307874083518982\n",
      "Epoch: 40930 | Loss: 0.5070443153381348 | Test loss: 0.5307735204696655\n",
      "Epoch: 40940 | Loss: 0.5070325136184692 | Test loss: 0.5307596325874329\n",
      "Epoch: 40950 | Loss: 0.5070207715034485 | Test loss: 0.530745804309845\n",
      "Epoch: 40960 | Loss: 0.507008969783783 | Test loss: 0.5307319164276123\n",
      "Epoch: 40970 | Loss: 0.5069972276687622 | Test loss: 0.5307181477546692\n",
      "Epoch: 40980 | Loss: 0.5069854855537415 | Test loss: 0.5307042598724365\n",
      "Epoch: 40990 | Loss: 0.5069736838340759 | Test loss: 0.5306903719902039\n",
      "Epoch: 41000 | Loss: 0.5069620013237 | Test loss: 0.530676543712616\n",
      "Epoch: 41010 | Loss: 0.5069501996040344 | Test loss: 0.5306627154350281\n",
      "Epoch: 41020 | Loss: 0.5069384574890137 | Test loss: 0.5306488871574402\n",
      "Epoch: 41030 | Loss: 0.5069267153739929 | Test loss: 0.5306350588798523\n",
      "Epoch: 41040 | Loss: 0.5069149136543274 | Test loss: 0.5306211709976196\n",
      "Epoch: 41050 | Loss: 0.5069031715393066 | Test loss: 0.5306073427200317\n",
      "Epoch: 41060 | Loss: 0.5068913698196411 | Test loss: 0.5305935144424438\n",
      "Epoch: 41070 | Loss: 0.5068796277046204 | Test loss: 0.530579686164856\n",
      "Epoch: 41080 | Loss: 0.5068678855895996 | Test loss: 0.5305657982826233\n",
      "Epoch: 41090 | Loss: 0.5068561434745789 | Test loss: 0.5305519700050354\n",
      "Epoch: 41100 | Loss: 0.5068443417549133 | Test loss: 0.5305380821228027\n",
      "Epoch: 41110 | Loss: 0.5068325996398926 | Test loss: 0.5305242538452148\n",
      "Epoch: 41120 | Loss: 0.506820797920227 | Test loss: 0.530510425567627\n",
      "Epoch: 41130 | Loss: 0.5068090558052063 | Test loss: 0.5304965972900391\n",
      "Epoch: 41140 | Loss: 0.5067973136901855 | Test loss: 0.5304827094078064\n",
      "Epoch: 41150 | Loss: 0.5067855715751648 | Test loss: 0.5304688811302185\n",
      "Epoch: 41160 | Loss: 0.5067737698554993 | Test loss: 0.5304550528526306\n",
      "Epoch: 41170 | Loss: 0.5067620277404785 | Test loss: 0.5304412245750427\n",
      "Epoch: 41180 | Loss: 0.5067502856254578 | Test loss: 0.5304273962974548\n",
      "Epoch: 41190 | Loss: 0.5067384839057922 | Test loss: 0.5304135084152222\n",
      "Epoch: 41200 | Loss: 0.5067267417907715 | Test loss: 0.5303996801376343\n",
      "Epoch: 41210 | Loss: 0.5067149996757507 | Test loss: 0.5303857922554016\n",
      "Epoch: 41220 | Loss: 0.5067033171653748 | Test loss: 0.5303719639778137\n",
      "Epoch: 41230 | Loss: 0.5066914558410645 | Test loss: 0.5303581357002258\n",
      "Epoch: 41240 | Loss: 0.5066797137260437 | Test loss: 0.5303442478179932\n",
      "Epoch: 41250 | Loss: 0.5066680312156677 | Test loss: 0.5303304195404053\n",
      "Epoch: 41260 | Loss: 0.5066561698913574 | Test loss: 0.5303165912628174\n",
      "Epoch: 41270 | Loss: 0.5066444277763367 | Test loss: 0.5303027033805847\n",
      "Epoch: 41280 | Loss: 0.5066327452659607 | Test loss: 0.5302888751029968\n",
      "Epoch: 41290 | Loss: 0.5066208839416504 | Test loss: 0.5302750468254089\n",
      "Epoch: 41300 | Loss: 0.5066091418266296 | Test loss: 0.5302611589431763\n",
      "Epoch: 41310 | Loss: 0.5065973997116089 | Test loss: 0.5302473306655884\n",
      "Epoch: 41320 | Loss: 0.5065855979919434 | Test loss: 0.5302335023880005\n",
      "Epoch: 41330 | Loss: 0.5065738558769226 | Test loss: 0.5302196741104126\n",
      "Epoch: 41340 | Loss: 0.5065621733665466 | Test loss: 0.5302058458328247\n",
      "Epoch: 41350 | Loss: 0.5065503120422363 | Test loss: 0.530191957950592\n",
      "Epoch: 41360 | Loss: 0.5065385699272156 | Test loss: 0.5301780700683594\n",
      "Epoch: 41370 | Loss: 0.5065268874168396 | Test loss: 0.5301642417907715\n",
      "Epoch: 41380 | Loss: 0.5065150856971741 | Test loss: 0.5301504135131836\n",
      "Epoch: 41390 | Loss: 0.5065032839775085 | Test loss: 0.5301365852355957\n",
      "Epoch: 41400 | Loss: 0.5064916014671326 | Test loss: 0.5301227569580078\n",
      "Epoch: 41410 | Loss: 0.5064797401428223 | Test loss: 0.5301088690757751\n",
      "Epoch: 41420 | Loss: 0.5064680576324463 | Test loss: 0.5300950407981873\n",
      "Epoch: 41430 | Loss: 0.5064563155174255 | Test loss: 0.5300812125205994\n",
      "Epoch: 41440 | Loss: 0.5064445734024048 | Test loss: 0.5300673842430115\n",
      "Epoch: 41450 | Loss: 0.5064327716827393 | Test loss: 0.5300535559654236\n",
      "Epoch: 41460 | Loss: 0.5064210295677185 | Test loss: 0.5300397276878357\n",
      "Epoch: 41470 | Loss: 0.506409227848053 | Test loss: 0.5300257802009583\n",
      "Epoch: 41480 | Loss: 0.5063974857330322 | Test loss: 0.5300119519233704\n",
      "Epoch: 41490 | Loss: 0.5063857436180115 | Test loss: 0.5299981236457825\n",
      "Epoch: 41500 | Loss: 0.5063740015029907 | Test loss: 0.5299842357635498\n",
      "Epoch: 41510 | Loss: 0.5063621997833252 | Test loss: 0.5299704074859619\n",
      "Epoch: 41520 | Loss: 0.5063504576683044 | Test loss: 0.529956579208374\n",
      "Epoch: 41530 | Loss: 0.5063387155532837 | Test loss: 0.5299426913261414\n",
      "Epoch: 41540 | Loss: 0.5063269138336182 | Test loss: 0.5299288630485535\n",
      "Epoch: 41550 | Loss: 0.5063151717185974 | Test loss: 0.5299150347709656\n",
      "Epoch: 41560 | Loss: 0.5063034296035767 | Test loss: 0.5299011468887329\n",
      "Epoch: 41570 | Loss: 0.5062916278839111 | Test loss: 0.5298873782157898\n",
      "Epoch: 41580 | Loss: 0.5062798857688904 | Test loss: 0.5298734903335571\n",
      "Epoch: 41590 | Loss: 0.5062681436538696 | Test loss: 0.5298596620559692\n",
      "Epoch: 41600 | Loss: 0.5062564015388489 | Test loss: 0.5298458337783813\n",
      "Epoch: 41610 | Loss: 0.5062445998191833 | Test loss: 0.5298320055007935\n",
      "Epoch: 41620 | Loss: 0.5062328577041626 | Test loss: 0.529818058013916\n",
      "Epoch: 41630 | Loss: 0.5062211155891418 | Test loss: 0.5298042297363281\n",
      "Epoch: 41640 | Loss: 0.5062093138694763 | Test loss: 0.5297904014587402\n",
      "Epoch: 41650 | Loss: 0.5061975717544556 | Test loss: 0.5297765731811523\n",
      "Epoch: 41660 | Loss: 0.5061858296394348 | Test loss: 0.5297627449035645\n",
      "Epoch: 41670 | Loss: 0.5061740279197693 | Test loss: 0.5297488570213318\n",
      "Epoch: 41680 | Loss: 0.5061623454093933 | Test loss: 0.5297350287437439\n",
      "Epoch: 41690 | Loss: 0.506150484085083 | Test loss: 0.529721200466156\n",
      "Epoch: 41700 | Loss: 0.5061387419700623 | Test loss: 0.5297073721885681\n",
      "Epoch: 41710 | Loss: 0.5061269998550415 | Test loss: 0.5296935439109802\n",
      "Epoch: 41720 | Loss: 0.506115198135376 | Test loss: 0.5296797156333923\n",
      "Epoch: 41730 | Loss: 0.506103515625 | Test loss: 0.5296658277511597\n",
      "Epoch: 41740 | Loss: 0.5060917139053345 | Test loss: 0.529651939868927\n",
      "Epoch: 41750 | Loss: 0.5060799717903137 | Test loss: 0.5296381115913391\n",
      "Epoch: 41760 | Loss: 0.5060681700706482 | Test loss: 0.5296242237091064\n",
      "Epoch: 41770 | Loss: 0.5060564279556274 | Test loss: 0.5296104550361633\n",
      "Epoch: 41780 | Loss: 0.5060446858406067 | Test loss: 0.5295965671539307\n",
      "Epoch: 41790 | Loss: 0.5060328841209412 | Test loss: 0.529582679271698\n",
      "Epoch: 41800 | Loss: 0.5060212016105652 | Test loss: 0.5295688509941101\n",
      "Epoch: 41810 | Loss: 0.5060093998908997 | Test loss: 0.5295550227165222\n",
      "Epoch: 41820 | Loss: 0.5059976577758789 | Test loss: 0.5295411944389343\n",
      "Epoch: 41830 | Loss: 0.5059859156608582 | Test loss: 0.5295273661613464\n",
      "Epoch: 41840 | Loss: 0.5059741139411926 | Test loss: 0.5295134782791138\n",
      "Epoch: 41850 | Loss: 0.5059623718261719 | Test loss: 0.5294996500015259\n",
      "Epoch: 41860 | Loss: 0.5059505701065063 | Test loss: 0.529485821723938\n",
      "Epoch: 41870 | Loss: 0.5059388279914856 | Test loss: 0.5294719934463501\n",
      "Epoch: 41880 | Loss: 0.5059270858764648 | Test loss: 0.5294581055641174\n",
      "Epoch: 41890 | Loss: 0.5059153437614441 | Test loss: 0.5294442772865295\n",
      "Epoch: 41900 | Loss: 0.5059035420417786 | Test loss: 0.5294303894042969\n",
      "Epoch: 41910 | Loss: 0.5058917999267578 | Test loss: 0.529416561126709\n",
      "Epoch: 41920 | Loss: 0.5058799982070923 | Test loss: 0.5294027328491211\n",
      "Epoch: 41930 | Loss: 0.5058682560920715 | Test loss: 0.5293889045715332\n",
      "Epoch: 41940 | Loss: 0.5058565139770508 | Test loss: 0.5293750166893005\n",
      "Epoch: 41950 | Loss: 0.50584477186203 | Test loss: 0.5293611884117126\n",
      "Epoch: 41960 | Loss: 0.5058329701423645 | Test loss: 0.5293473601341248\n",
      "Epoch: 41970 | Loss: 0.5058212280273438 | Test loss: 0.5293335318565369\n",
      "Epoch: 41980 | Loss: 0.505809485912323 | Test loss: 0.529319703578949\n",
      "Epoch: 41990 | Loss: 0.5057976841926575 | Test loss: 0.5293058156967163\n",
      "Epoch: 42000 | Loss: 0.5057859420776367 | Test loss: 0.5292919874191284\n",
      "Epoch: 42010 | Loss: 0.505774199962616 | Test loss: 0.5292780995368958\n",
      "Epoch: 42020 | Loss: 0.50576251745224 | Test loss: 0.5292642712593079\n",
      "Epoch: 42030 | Loss: 0.5057506561279297 | Test loss: 0.52925044298172\n",
      "Epoch: 42040 | Loss: 0.5057389140129089 | Test loss: 0.5292365550994873\n",
      "Epoch: 42050 | Loss: 0.505727231502533 | Test loss: 0.5292227268218994\n",
      "Epoch: 42060 | Loss: 0.5057153701782227 | Test loss: 0.5292088985443115\n",
      "Epoch: 42070 | Loss: 0.5057036280632019 | Test loss: 0.5291950106620789\n",
      "Epoch: 42080 | Loss: 0.5056919455528259 | Test loss: 0.529181182384491\n",
      "Epoch: 42090 | Loss: 0.5056800842285156 | Test loss: 0.5291673541069031\n",
      "Epoch: 42100 | Loss: 0.5056683421134949 | Test loss: 0.5291534662246704\n",
      "Epoch: 42110 | Loss: 0.5056565999984741 | Test loss: 0.5291396379470825\n",
      "Epoch: 42120 | Loss: 0.5056447982788086 | Test loss: 0.5291258096694946\n",
      "Epoch: 42130 | Loss: 0.5056330561637878 | Test loss: 0.5291119813919067\n",
      "Epoch: 42140 | Loss: 0.5056213736534119 | Test loss: 0.5290981531143188\n",
      "Epoch: 42150 | Loss: 0.5056095123291016 | Test loss: 0.5290842652320862\n",
      "Epoch: 42160 | Loss: 0.5055977702140808 | Test loss: 0.5290703773498535\n",
      "Epoch: 42170 | Loss: 0.5055860877037048 | Test loss: 0.5290565490722656\n",
      "Epoch: 42180 | Loss: 0.5055742859840393 | Test loss: 0.5290427207946777\n",
      "Epoch: 42190 | Loss: 0.5055624842643738 | Test loss: 0.5290288925170898\n",
      "Epoch: 42200 | Loss: 0.5055508017539978 | Test loss: 0.529015064239502\n",
      "Epoch: 42210 | Loss: 0.5055389404296875 | Test loss: 0.5290011763572693\n",
      "Epoch: 42220 | Loss: 0.5055272579193115 | Test loss: 0.5289873480796814\n",
      "Epoch: 42230 | Loss: 0.5055155158042908 | Test loss: 0.5289735198020935\n",
      "Epoch: 42240 | Loss: 0.50550377368927 | Test loss: 0.5289596915245056\n",
      "Epoch: 42250 | Loss: 0.5054919719696045 | Test loss: 0.5289458632469177\n",
      "Epoch: 42260 | Loss: 0.5054802298545837 | Test loss: 0.5289320349693298\n",
      "Epoch: 42270 | Loss: 0.5054684281349182 | Test loss: 0.5289180874824524\n",
      "Epoch: 42280 | Loss: 0.5054566860198975 | Test loss: 0.5289042592048645\n",
      "Epoch: 42290 | Loss: 0.5054449439048767 | Test loss: 0.5288904309272766\n",
      "Epoch: 42300 | Loss: 0.505433201789856 | Test loss: 0.528876543045044\n",
      "Epoch: 42310 | Loss: 0.5054214000701904 | Test loss: 0.528862714767456\n",
      "Epoch: 42320 | Loss: 0.5054096579551697 | Test loss: 0.5288488864898682\n",
      "Epoch: 42330 | Loss: 0.5053979158401489 | Test loss: 0.5288349986076355\n",
      "Epoch: 42340 | Loss: 0.5053861141204834 | Test loss: 0.5288211703300476\n",
      "Epoch: 42350 | Loss: 0.5053743720054626 | Test loss: 0.5288073420524597\n",
      "Epoch: 42360 | Loss: 0.5053626298904419 | Test loss: 0.528793454170227\n",
      "Epoch: 42370 | Loss: 0.5053508281707764 | Test loss: 0.5287796854972839\n",
      "Epoch: 42380 | Loss: 0.5053390860557556 | Test loss: 0.5287657976150513\n",
      "Epoch: 42390 | Loss: 0.5053273439407349 | Test loss: 0.5287519693374634\n",
      "Epoch: 42400 | Loss: 0.5053156018257141 | Test loss: 0.5287381410598755\n",
      "Epoch: 42410 | Loss: 0.5053038001060486 | Test loss: 0.5287243127822876\n",
      "Epoch: 42420 | Loss: 0.5052920579910278 | Test loss: 0.5287103652954102\n",
      "Epoch: 42430 | Loss: 0.5052803158760071 | Test loss: 0.5286965370178223\n",
      "Epoch: 42440 | Loss: 0.5052685141563416 | Test loss: 0.5286827087402344\n",
      "Epoch: 42450 | Loss: 0.5052567720413208 | Test loss: 0.5286688804626465\n",
      "Epoch: 42460 | Loss: 0.5052450299263 | Test loss: 0.5286550521850586\n",
      "Epoch: 42470 | Loss: 0.5052332282066345 | Test loss: 0.5286411643028259\n",
      "Epoch: 42480 | Loss: 0.5052215456962585 | Test loss: 0.528627336025238\n",
      "Epoch: 42490 | Loss: 0.5052096843719482 | Test loss: 0.5286135077476501\n",
      "Epoch: 42500 | Loss: 0.5051979422569275 | Test loss: 0.5285996794700623\n",
      "Epoch: 42510 | Loss: 0.5051862001419067 | Test loss: 0.5285858511924744\n",
      "Epoch: 42520 | Loss: 0.5051743984222412 | Test loss: 0.5285720229148865\n",
      "Epoch: 42530 | Loss: 0.5051627159118652 | Test loss: 0.5285581350326538\n",
      "Epoch: 42540 | Loss: 0.5051509141921997 | Test loss: 0.5285442471504211\n",
      "Epoch: 42550 | Loss: 0.505139172077179 | Test loss: 0.5285304188728333\n",
      "Epoch: 42560 | Loss: 0.5051273703575134 | Test loss: 0.5285165309906006\n",
      "Epoch: 42570 | Loss: 0.5051156282424927 | Test loss: 0.5285027623176575\n",
      "Epoch: 42580 | Loss: 0.5051038861274719 | Test loss: 0.5284888744354248\n",
      "Epoch: 42590 | Loss: 0.5050920844078064 | Test loss: 0.5284749865531921\n",
      "Epoch: 42600 | Loss: 0.5050804018974304 | Test loss: 0.5284611582756042\n",
      "Epoch: 42610 | Loss: 0.5050686001777649 | Test loss: 0.5284473299980164\n",
      "Epoch: 42620 | Loss: 0.5050568580627441 | Test loss: 0.5284335017204285\n",
      "Epoch: 42630 | Loss: 0.5050451159477234 | Test loss: 0.5284196734428406\n",
      "Epoch: 42640 | Loss: 0.5050333142280579 | Test loss: 0.5284057855606079\n",
      "Epoch: 42650 | Loss: 0.5050215721130371 | Test loss: 0.52839195728302\n",
      "Epoch: 42660 | Loss: 0.5050097703933716 | Test loss: 0.5283781290054321\n",
      "Epoch: 42670 | Loss: 0.5049980282783508 | Test loss: 0.5283643007278442\n",
      "Epoch: 42680 | Loss: 0.5049862861633301 | Test loss: 0.5283504128456116\n",
      "Epoch: 42690 | Loss: 0.5049745440483093 | Test loss: 0.5283365845680237\n",
      "Epoch: 42700 | Loss: 0.5049627423286438 | Test loss: 0.528322696685791\n",
      "Epoch: 42710 | Loss: 0.504951000213623 | Test loss: 0.5283088684082031\n",
      "Epoch: 42720 | Loss: 0.5049391984939575 | Test loss: 0.5282950401306152\n",
      "Epoch: 42730 | Loss: 0.5049274563789368 | Test loss: 0.5282812118530273\n",
      "Epoch: 42740 | Loss: 0.504915714263916 | Test loss: 0.5282673239707947\n",
      "Epoch: 42750 | Loss: 0.5049039721488953 | Test loss: 0.5282534956932068\n",
      "Epoch: 42760 | Loss: 0.5048921704292297 | Test loss: 0.5282396674156189\n",
      "Epoch: 42770 | Loss: 0.504880428314209 | Test loss: 0.528225839138031\n",
      "Epoch: 42780 | Loss: 0.5048686861991882 | Test loss: 0.5282120108604431\n",
      "Epoch: 42790 | Loss: 0.5048568844795227 | Test loss: 0.5281981229782104\n",
      "Epoch: 42800 | Loss: 0.504845142364502 | Test loss: 0.5281842947006226\n",
      "Epoch: 42810 | Loss: 0.5048334002494812 | Test loss: 0.5281704068183899\n",
      "Epoch: 42820 | Loss: 0.5048217177391052 | Test loss: 0.528156578540802\n",
      "Epoch: 42830 | Loss: 0.5048098564147949 | Test loss: 0.5281427502632141\n",
      "Epoch: 42840 | Loss: 0.5047981142997742 | Test loss: 0.5281288623809814\n",
      "Epoch: 42850 | Loss: 0.5047864317893982 | Test loss: 0.5281150341033936\n",
      "Epoch: 42860 | Loss: 0.5047745704650879 | Test loss: 0.5281012058258057\n",
      "Epoch: 42870 | Loss: 0.5047628283500671 | Test loss: 0.528087317943573\n",
      "Epoch: 42880 | Loss: 0.5047511458396912 | Test loss: 0.5280734896659851\n",
      "Epoch: 42890 | Loss: 0.5047392845153809 | Test loss: 0.5280596613883972\n",
      "Epoch: 42900 | Loss: 0.5047275424003601 | Test loss: 0.5280457735061646\n",
      "Epoch: 42910 | Loss: 0.5047158002853394 | Test loss: 0.5280319452285767\n",
      "Epoch: 42920 | Loss: 0.5047039985656738 | Test loss: 0.5280181169509888\n",
      "Epoch: 42930 | Loss: 0.5046922564506531 | Test loss: 0.5280042886734009\n",
      "Epoch: 42940 | Loss: 0.5046805739402771 | Test loss: 0.527990460395813\n",
      "Epoch: 42950 | Loss: 0.5046687126159668 | Test loss: 0.5279765725135803\n",
      "Epoch: 42960 | Loss: 0.504656970500946 | Test loss: 0.5279626846313477\n",
      "Epoch: 42970 | Loss: 0.5046452879905701 | Test loss: 0.5279488563537598\n",
      "Epoch: 42980 | Loss: 0.5046334862709045 | Test loss: 0.5279350280761719\n",
      "Epoch: 42990 | Loss: 0.504621684551239 | Test loss: 0.527921199798584\n",
      "Epoch: 43000 | Loss: 0.504610002040863 | Test loss: 0.5279073715209961\n",
      "Epoch: 43010 | Loss: 0.5045981407165527 | Test loss: 0.5278934836387634\n",
      "Epoch: 43020 | Loss: 0.5045864582061768 | Test loss: 0.5278796553611755\n",
      "Epoch: 43030 | Loss: 0.504574716091156 | Test loss: 0.5278658270835876\n",
      "Epoch: 43040 | Loss: 0.5045629739761353 | Test loss: 0.5278519988059998\n",
      "Epoch: 43050 | Loss: 0.5045511722564697 | Test loss: 0.5278381705284119\n",
      "Epoch: 43060 | Loss: 0.504539430141449 | Test loss: 0.527824342250824\n",
      "Epoch: 43070 | Loss: 0.5045276284217834 | Test loss: 0.5278103947639465\n",
      "Epoch: 43080 | Loss: 0.5045158863067627 | Test loss: 0.5277965664863586\n",
      "Epoch: 43090 | Loss: 0.5045041441917419 | Test loss: 0.5277827382087708\n",
      "Epoch: 43100 | Loss: 0.5044924020767212 | Test loss: 0.5277688503265381\n",
      "Epoch: 43110 | Loss: 0.5044806003570557 | Test loss: 0.5277550220489502\n",
      "Epoch: 43120 | Loss: 0.5044688582420349 | Test loss: 0.5277411937713623\n",
      "Epoch: 43130 | Loss: 0.5044571161270142 | Test loss: 0.5277273058891296\n",
      "Epoch: 43140 | Loss: 0.5044453144073486 | Test loss: 0.5277134776115417\n",
      "Epoch: 43150 | Loss: 0.5044335722923279 | Test loss: 0.5276996493339539\n",
      "Epoch: 43160 | Loss: 0.5044218301773071 | Test loss: 0.5276857614517212\n",
      "Epoch: 43170 | Loss: 0.5044100284576416 | Test loss: 0.5276719927787781\n",
      "Epoch: 43180 | Loss: 0.5043982863426208 | Test loss: 0.5276581048965454\n",
      "Epoch: 43190 | Loss: 0.5043865442276001 | Test loss: 0.5276442766189575\n",
      "Epoch: 43200 | Loss: 0.5043748021125793 | Test loss: 0.5276304483413696\n",
      "Epoch: 43210 | Loss: 0.5043630003929138 | Test loss: 0.5276166200637817\n",
      "Epoch: 43220 | Loss: 0.5043512582778931 | Test loss: 0.5276026725769043\n",
      "Epoch: 43230 | Loss: 0.5043395161628723 | Test loss: 0.5275888442993164\n",
      "Epoch: 43240 | Loss: 0.5043277144432068 | Test loss: 0.5275750160217285\n",
      "Epoch: 43250 | Loss: 0.504315972328186 | Test loss: 0.5275611877441406\n",
      "Epoch: 43260 | Loss: 0.5043042302131653 | Test loss: 0.5275473594665527\n",
      "Epoch: 43270 | Loss: 0.5042924284934998 | Test loss: 0.5275334715843201\n",
      "Epoch: 43280 | Loss: 0.5042807459831238 | Test loss: 0.5275196433067322\n",
      "Epoch: 43290 | Loss: 0.5042688846588135 | Test loss: 0.5275058150291443\n",
      "Epoch: 43300 | Loss: 0.5042571425437927 | Test loss: 0.5274919867515564\n",
      "Epoch: 43310 | Loss: 0.504245400428772 | Test loss: 0.5274781584739685\n",
      "Epoch: 43320 | Loss: 0.5042335987091064 | Test loss: 0.5274643301963806\n",
      "Epoch: 43330 | Loss: 0.5042219161987305 | Test loss: 0.527450442314148\n",
      "Epoch: 43340 | Loss: 0.5042101144790649 | Test loss: 0.5274365544319153\n",
      "Epoch: 43350 | Loss: 0.5041983723640442 | Test loss: 0.5274227261543274\n",
      "Epoch: 43360 | Loss: 0.5041865706443787 | Test loss: 0.5274088382720947\n",
      "Epoch: 43370 | Loss: 0.5041748285293579 | Test loss: 0.5273950695991516\n",
      "Epoch: 43380 | Loss: 0.5041630864143372 | Test loss: 0.527381181716919\n",
      "Epoch: 43390 | Loss: 0.5041512846946716 | Test loss: 0.5273672938346863\n",
      "Epoch: 43400 | Loss: 0.5041396021842957 | Test loss: 0.5273534655570984\n",
      "Epoch: 43410 | Loss: 0.5041278004646301 | Test loss: 0.5273396372795105\n",
      "Epoch: 43420 | Loss: 0.5041160583496094 | Test loss: 0.5273258090019226\n",
      "Epoch: 43430 | Loss: 0.5041043162345886 | Test loss: 0.5273119807243347\n",
      "Epoch: 43440 | Loss: 0.5040925145149231 | Test loss: 0.527298092842102\n",
      "Epoch: 43450 | Loss: 0.5040807723999023 | Test loss: 0.5272842645645142\n",
      "Epoch: 43460 | Loss: 0.5040689706802368 | Test loss: 0.5272704362869263\n",
      "Epoch: 43470 | Loss: 0.5040572285652161 | Test loss: 0.5272566080093384\n",
      "Epoch: 43480 | Loss: 0.5040454864501953 | Test loss: 0.5272427201271057\n",
      "Epoch: 43490 | Loss: 0.5040337443351746 | Test loss: 0.5272288918495178\n",
      "Epoch: 43500 | Loss: 0.504021942615509 | Test loss: 0.5272150039672852\n",
      "Epoch: 43510 | Loss: 0.5040102005004883 | Test loss: 0.5272011756896973\n",
      "Epoch: 43520 | Loss: 0.5039983987808228 | Test loss: 0.5271873474121094\n",
      "Epoch: 43530 | Loss: 0.503986656665802 | Test loss: 0.5271735191345215\n",
      "Epoch: 43540 | Loss: 0.5039749145507812 | Test loss: 0.5271596312522888\n",
      "Epoch: 43550 | Loss: 0.5039631724357605 | Test loss: 0.5271458029747009\n",
      "Epoch: 43560 | Loss: 0.503951370716095 | Test loss: 0.527131974697113\n",
      "Epoch: 43570 | Loss: 0.5039396286010742 | Test loss: 0.5271181464195251\n",
      "Epoch: 43580 | Loss: 0.5039278864860535 | Test loss: 0.5271043181419373\n",
      "Epoch: 43590 | Loss: 0.5039160847663879 | Test loss: 0.5270904302597046\n",
      "Epoch: 43600 | Loss: 0.5039043426513672 | Test loss: 0.5270766019821167\n",
      "Epoch: 43610 | Loss: 0.5038926005363464 | Test loss: 0.527062714099884\n",
      "Epoch: 43620 | Loss: 0.5038809180259705 | Test loss: 0.5270488858222961\n",
      "Epoch: 43630 | Loss: 0.5038690567016602 | Test loss: 0.5270350575447083\n",
      "Epoch: 43640 | Loss: 0.5038573145866394 | Test loss: 0.5270211696624756\n",
      "Epoch: 43650 | Loss: 0.5038456320762634 | Test loss: 0.5270073413848877\n",
      "Epoch: 43660 | Loss: 0.5038337707519531 | Test loss: 0.5269935131072998\n",
      "Epoch: 43670 | Loss: 0.5038220286369324 | Test loss: 0.5269796252250671\n",
      "Epoch: 43680 | Loss: 0.5038103461265564 | Test loss: 0.5269657969474792\n",
      "Epoch: 43690 | Loss: 0.5037984848022461 | Test loss: 0.5269519686698914\n",
      "Epoch: 43700 | Loss: 0.5037867426872253 | Test loss: 0.5269380807876587\n",
      "Epoch: 43710 | Loss: 0.5037750005722046 | Test loss: 0.5269242525100708\n",
      "Epoch: 43720 | Loss: 0.5037631988525391 | Test loss: 0.5269104242324829\n",
      "Epoch: 43730 | Loss: 0.5037514567375183 | Test loss: 0.526896595954895\n",
      "Epoch: 43740 | Loss: 0.5037397742271423 | Test loss: 0.5268827676773071\n",
      "Epoch: 43750 | Loss: 0.503727912902832 | Test loss: 0.5268688797950745\n",
      "Epoch: 43760 | Loss: 0.5037161707878113 | Test loss: 0.5268549919128418\n",
      "Epoch: 43770 | Loss: 0.5037044882774353 | Test loss: 0.5268411636352539\n",
      "Epoch: 43780 | Loss: 0.5036926865577698 | Test loss: 0.526827335357666\n",
      "Epoch: 43790 | Loss: 0.5036808848381042 | Test loss: 0.5268135070800781\n",
      "Epoch: 43800 | Loss: 0.5036692023277283 | Test loss: 0.5267996788024902\n",
      "Epoch: 43810 | Loss: 0.503657341003418 | Test loss: 0.5267857909202576\n",
      "Epoch: 43820 | Loss: 0.503645658493042 | Test loss: 0.5267719626426697\n",
      "Epoch: 43830 | Loss: 0.5036339163780212 | Test loss: 0.5267581343650818\n",
      "Epoch: 43840 | Loss: 0.5036221742630005 | Test loss: 0.5267443060874939\n",
      "Epoch: 43850 | Loss: 0.503610372543335 | Test loss: 0.526730477809906\n",
      "Epoch: 43860 | Loss: 0.5035986304283142 | Test loss: 0.5267166495323181\n",
      "Epoch: 43870 | Loss: 0.5035868287086487 | Test loss: 0.5267027020454407\n",
      "Epoch: 43880 | Loss: 0.5035750865936279 | Test loss: 0.5266888737678528\n",
      "Epoch: 43890 | Loss: 0.5035633444786072 | Test loss: 0.5266750454902649\n",
      "Epoch: 43900 | Loss: 0.5035516023635864 | Test loss: 0.5266611576080322\n",
      "Epoch: 43910 | Loss: 0.5035398006439209 | Test loss: 0.5266473293304443\n",
      "Epoch: 43920 | Loss: 0.5035280585289001 | Test loss: 0.5266335010528564\n",
      "Epoch: 43930 | Loss: 0.5035163164138794 | Test loss: 0.5266196131706238\n",
      "Epoch: 43940 | Loss: 0.5035045146942139 | Test loss: 0.5266057848930359\n",
      "Epoch: 43950 | Loss: 0.5034927725791931 | Test loss: 0.526591956615448\n",
      "Epoch: 43960 | Loss: 0.5034810304641724 | Test loss: 0.5265780687332153\n",
      "Epoch: 43970 | Loss: 0.5034692287445068 | Test loss: 0.5265643000602722\n",
      "Epoch: 43980 | Loss: 0.5034574866294861 | Test loss: 0.5265504121780396\n",
      "Epoch: 43990 | Loss: 0.5034457445144653 | Test loss: 0.5265365839004517\n",
      "Epoch: 44000 | Loss: 0.5034340023994446 | Test loss: 0.5265227556228638\n",
      "Epoch: 44010 | Loss: 0.503422200679779 | Test loss: 0.5265089273452759\n",
      "Epoch: 44020 | Loss: 0.5034104585647583 | Test loss: 0.5264949798583984\n",
      "Epoch: 44030 | Loss: 0.5033987164497375 | Test loss: 0.5264811515808105\n",
      "Epoch: 44040 | Loss: 0.503386914730072 | Test loss: 0.5264673233032227\n",
      "Epoch: 44050 | Loss: 0.5033751726150513 | Test loss: 0.5264534950256348\n",
      "Epoch: 44060 | Loss: 0.5033634305000305 | Test loss: 0.5264396667480469\n",
      "Epoch: 44070 | Loss: 0.503351628780365 | Test loss: 0.5264257788658142\n",
      "Epoch: 44080 | Loss: 0.503339946269989 | Test loss: 0.5264119505882263\n",
      "Epoch: 44090 | Loss: 0.5033280849456787 | Test loss: 0.5263981223106384\n",
      "Epoch: 44100 | Loss: 0.503316342830658 | Test loss: 0.5263842940330505\n",
      "Epoch: 44110 | Loss: 0.5033046007156372 | Test loss: 0.5263704657554626\n",
      "Epoch: 44120 | Loss: 0.5032927989959717 | Test loss: 0.5263566374778748\n",
      "Epoch: 44130 | Loss: 0.5032811164855957 | Test loss: 0.5263427495956421\n",
      "Epoch: 44140 | Loss: 0.5032693147659302 | Test loss: 0.5263288617134094\n",
      "Epoch: 44150 | Loss: 0.5032575726509094 | Test loss: 0.5263150334358215\n",
      "Epoch: 44160 | Loss: 0.5032457709312439 | Test loss: 0.5263011455535889\n",
      "Epoch: 44170 | Loss: 0.5032340288162231 | Test loss: 0.5262873768806458\n",
      "Epoch: 44180 | Loss: 0.5032222867012024 | Test loss: 0.5262734889984131\n",
      "Epoch: 44190 | Loss: 0.5032104849815369 | Test loss: 0.5262596011161804\n",
      "Epoch: 44200 | Loss: 0.5031988024711609 | Test loss: 0.5262457728385925\n",
      "Epoch: 44210 | Loss: 0.5031870007514954 | Test loss: 0.5262319445610046\n",
      "Epoch: 44220 | Loss: 0.5031752586364746 | Test loss: 0.5262181162834167\n",
      "Epoch: 44230 | Loss: 0.5031635165214539 | Test loss: 0.5262042880058289\n",
      "Epoch: 44240 | Loss: 0.5031517148017883 | Test loss: 0.5261904001235962\n",
      "Epoch: 44250 | Loss: 0.5031399726867676 | Test loss: 0.5261765718460083\n",
      "Epoch: 44260 | Loss: 0.503128170967102 | Test loss: 0.5261627435684204\n",
      "Epoch: 44270 | Loss: 0.5031164288520813 | Test loss: 0.5261489152908325\n",
      "Epoch: 44280 | Loss: 0.5031046867370605 | Test loss: 0.5261350274085999\n",
      "Epoch: 44290 | Loss: 0.5030929446220398 | Test loss: 0.526121199131012\n",
      "Epoch: 44300 | Loss: 0.5030811429023743 | Test loss: 0.5261073112487793\n",
      "Epoch: 44310 | Loss: 0.5030694007873535 | Test loss: 0.5260934829711914\n",
      "Epoch: 44320 | Loss: 0.503057599067688 | Test loss: 0.5260796546936035\n",
      "Epoch: 44330 | Loss: 0.5030458569526672 | Test loss: 0.5260658264160156\n",
      "Epoch: 44340 | Loss: 0.5030341148376465 | Test loss: 0.526051938533783\n",
      "Epoch: 44350 | Loss: 0.5030223727226257 | Test loss: 0.5260381102561951\n",
      "Epoch: 44360 | Loss: 0.5030105710029602 | Test loss: 0.5260242819786072\n",
      "Epoch: 44370 | Loss: 0.5029988288879395 | Test loss: 0.5260104537010193\n",
      "Epoch: 44380 | Loss: 0.5029870867729187 | Test loss: 0.5259966254234314\n",
      "Epoch: 44390 | Loss: 0.5029752850532532 | Test loss: 0.5259827375411987\n",
      "Epoch: 44400 | Loss: 0.5029635429382324 | Test loss: 0.5259689092636108\n",
      "Epoch: 44410 | Loss: 0.5029518008232117 | Test loss: 0.5259550213813782\n",
      "Epoch: 44420 | Loss: 0.5029401183128357 | Test loss: 0.5259411931037903\n",
      "Epoch: 44430 | Loss: 0.5029282569885254 | Test loss: 0.5259273648262024\n",
      "Epoch: 44440 | Loss: 0.5029165148735046 | Test loss: 0.5259134769439697\n",
      "Epoch: 44450 | Loss: 0.5029048323631287 | Test loss: 0.5258996486663818\n",
      "Epoch: 44460 | Loss: 0.5028929710388184 | Test loss: 0.525885820388794\n",
      "Epoch: 44470 | Loss: 0.5028812289237976 | Test loss: 0.5258719325065613\n",
      "Epoch: 44480 | Loss: 0.5028695464134216 | Test loss: 0.5258581042289734\n",
      "Epoch: 44490 | Loss: 0.5028576850891113 | Test loss: 0.5258442759513855\n",
      "Epoch: 44500 | Loss: 0.5028459429740906 | Test loss: 0.5258303880691528\n",
      "Epoch: 44510 | Loss: 0.5028342008590698 | Test loss: 0.5258165597915649\n",
      "Epoch: 44520 | Loss: 0.5028223991394043 | Test loss: 0.525802731513977\n",
      "Epoch: 44530 | Loss: 0.5028106570243835 | Test loss: 0.5257889032363892\n",
      "Epoch: 44540 | Loss: 0.5027989745140076 | Test loss: 0.5257750749588013\n",
      "Epoch: 44550 | Loss: 0.5027871131896973 | Test loss: 0.5257611870765686\n",
      "Epoch: 44560 | Loss: 0.5027753710746765 | Test loss: 0.5257472991943359\n",
      "Epoch: 44570 | Loss: 0.5027636885643005 | Test loss: 0.525733470916748\n",
      "Epoch: 44580 | Loss: 0.502751886844635 | Test loss: 0.5257196426391602\n",
      "Epoch: 44590 | Loss: 0.5027400851249695 | Test loss: 0.5257058143615723\n",
      "Epoch: 44600 | Loss: 0.5027284026145935 | Test loss: 0.5256919860839844\n",
      "Epoch: 44610 | Loss: 0.5027165412902832 | Test loss: 0.5256780982017517\n",
      "Epoch: 44620 | Loss: 0.5027048587799072 | Test loss: 0.5256642699241638\n",
      "Epoch: 44630 | Loss: 0.5026931166648865 | Test loss: 0.5256504416465759\n",
      "Epoch: 44640 | Loss: 0.5026813745498657 | Test loss: 0.525636613368988\n",
      "Epoch: 44650 | Loss: 0.5026695728302002 | Test loss: 0.5256227850914001\n",
      "Epoch: 44660 | Loss: 0.5026578307151794 | Test loss: 0.5256089568138123\n",
      "Epoch: 44670 | Loss: 0.5026460289955139 | Test loss: 0.5255950093269348\n",
      "Epoch: 44680 | Loss: 0.5026342868804932 | Test loss: 0.5255811810493469\n",
      "Epoch: 44690 | Loss: 0.5026225447654724 | Test loss: 0.525567352771759\n",
      "Epoch: 44700 | Loss: 0.5026108026504517 | Test loss: 0.5255534648895264\n",
      "Epoch: 44710 | Loss: 0.5025990009307861 | Test loss: 0.5255396366119385\n",
      "Epoch: 44720 | Loss: 0.5025872588157654 | Test loss: 0.5255258083343506\n",
      "Epoch: 44730 | Loss: 0.5025755167007446 | Test loss: 0.5255119204521179\n",
      "Epoch: 44740 | Loss: 0.5025637149810791 | Test loss: 0.52549809217453\n",
      "Epoch: 44750 | Loss: 0.5025519728660583 | Test loss: 0.5254842638969421\n",
      "Epoch: 44760 | Loss: 0.5025402307510376 | Test loss: 0.5254703760147095\n",
      "Epoch: 44770 | Loss: 0.5025284290313721 | Test loss: 0.5254566073417664\n",
      "Epoch: 44780 | Loss: 0.5025166869163513 | Test loss: 0.5254427194595337\n",
      "Epoch: 44790 | Loss: 0.5025049448013306 | Test loss: 0.5254288911819458\n",
      "Epoch: 44800 | Loss: 0.5024932026863098 | Test loss: 0.5254150629043579\n",
      "Epoch: 44810 | Loss: 0.5024814009666443 | Test loss: 0.52540123462677\n",
      "Epoch: 44820 | Loss: 0.5024696588516235 | Test loss: 0.5253872871398926\n",
      "Epoch: 44830 | Loss: 0.5024579167366028 | Test loss: 0.5253734588623047\n",
      "Epoch: 44840 | Loss: 0.5024461150169373 | Test loss: 0.5253596305847168\n",
      "Epoch: 44850 | Loss: 0.5024343729019165 | Test loss: 0.5253458023071289\n",
      "Epoch: 44860 | Loss: 0.5024226307868958 | Test loss: 0.525331974029541\n",
      "Epoch: 44870 | Loss: 0.5024108290672302 | Test loss: 0.5253180861473083\n",
      "Epoch: 44880 | Loss: 0.5023991465568542 | Test loss: 0.5253042578697205\n",
      "Epoch: 44890 | Loss: 0.502387285232544 | Test loss: 0.5252904295921326\n",
      "Epoch: 44900 | Loss: 0.5023755431175232 | Test loss: 0.5252766013145447\n",
      "Epoch: 44910 | Loss: 0.5023638010025024 | Test loss: 0.5252627730369568\n",
      "Epoch: 44920 | Loss: 0.5023519992828369 | Test loss: 0.5252489447593689\n",
      "Epoch: 44930 | Loss: 0.5023403167724609 | Test loss: 0.5252350568771362\n",
      "Epoch: 44940 | Loss: 0.5023285150527954 | Test loss: 0.5252211689949036\n",
      "Epoch: 44950 | Loss: 0.5023167729377747 | Test loss: 0.5252073407173157\n",
      "Epoch: 44960 | Loss: 0.5023049712181091 | Test loss: 0.525193452835083\n",
      "Epoch: 44970 | Loss: 0.5022932291030884 | Test loss: 0.5251796841621399\n",
      "Epoch: 44980 | Loss: 0.5022814869880676 | Test loss: 0.5251657962799072\n",
      "Epoch: 44990 | Loss: 0.5022696852684021 | Test loss: 0.5251519083976746\n",
      "Epoch: 45000 | Loss: 0.5022579431533813 | Test loss: 0.5251380801200867\n",
      "Epoch: 45010 | Loss: 0.5022462010383606 | Test loss: 0.5251242518424988\n",
      "Epoch: 45020 | Loss: 0.5022344589233398 | Test loss: 0.5251104235649109\n",
      "Epoch: 45030 | Loss: 0.5022227168083191 | Test loss: 0.525096595287323\n",
      "Epoch: 45040 | Loss: 0.5022109150886536 | Test loss: 0.5250827074050903\n",
      "Epoch: 45050 | Loss: 0.5021991729736328 | Test loss: 0.5250688791275024\n",
      "Epoch: 45060 | Loss: 0.5021873712539673 | Test loss: 0.5250550508499146\n",
      "Epoch: 45070 | Loss: 0.5021756291389465 | Test loss: 0.5250412225723267\n",
      "Epoch: 45080 | Loss: 0.5021638870239258 | Test loss: 0.525027334690094\n",
      "Epoch: 45090 | Loss: 0.502152144908905 | Test loss: 0.5250135064125061\n",
      "Epoch: 45100 | Loss: 0.5021403431892395 | Test loss: 0.5249996185302734\n",
      "Epoch: 45110 | Loss: 0.5021286010742188 | Test loss: 0.5249857902526855\n",
      "Epoch: 45120 | Loss: 0.5021167993545532 | Test loss: 0.5249719619750977\n",
      "Epoch: 45130 | Loss: 0.5021050572395325 | Test loss: 0.5249581336975098\n",
      "Epoch: 45140 | Loss: 0.5020933151245117 | Test loss: 0.5249442458152771\n",
      "Epoch: 45150 | Loss: 0.502081573009491 | Test loss: 0.5249304175376892\n",
      "Epoch: 45160 | Loss: 0.5020697712898254 | Test loss: 0.5249165892601013\n",
      "Epoch: 45170 | Loss: 0.5020580291748047 | Test loss: 0.5249027609825134\n",
      "Epoch: 45180 | Loss: 0.5020462870597839 | Test loss: 0.5248889327049255\n",
      "Epoch: 45190 | Loss: 0.5020344853401184 | Test loss: 0.5248750448226929\n",
      "Epoch: 45200 | Loss: 0.5020227432250977 | Test loss: 0.524861216545105\n",
      "Epoch: 45210 | Loss: 0.5020110011100769 | Test loss: 0.5248473286628723\n",
      "Epoch: 45220 | Loss: 0.5019993185997009 | Test loss: 0.5248335003852844\n",
      "Epoch: 45230 | Loss: 0.5019874572753906 | Test loss: 0.5248196721076965\n",
      "Epoch: 45240 | Loss: 0.5019757151603699 | Test loss: 0.5248057842254639\n",
      "Epoch: 45250 | Loss: 0.5019640326499939 | Test loss: 0.524791955947876\n",
      "Epoch: 45260 | Loss: 0.5019521713256836 | Test loss: 0.5247781276702881\n",
      "Epoch: 45270 | Loss: 0.5019404292106628 | Test loss: 0.5247642397880554\n",
      "Epoch: 45280 | Loss: 0.5019287467002869 | Test loss: 0.5247504115104675\n",
      "Epoch: 45290 | Loss: 0.5019168853759766 | Test loss: 0.5247365832328796\n",
      "Epoch: 45300 | Loss: 0.5019051432609558 | Test loss: 0.524722695350647\n",
      "Epoch: 45310 | Loss: 0.5018934011459351 | Test loss: 0.5247088670730591\n",
      "Epoch: 45320 | Loss: 0.5018815994262695 | Test loss: 0.5246950387954712\n",
      "Epoch: 45330 | Loss: 0.5018698573112488 | Test loss: 0.5246812105178833\n",
      "Epoch: 45340 | Loss: 0.5018581748008728 | Test loss: 0.5246673822402954\n",
      "Epoch: 45350 | Loss: 0.5018463134765625 | Test loss: 0.5246534943580627\n",
      "Epoch: 45360 | Loss: 0.5018345713615417 | Test loss: 0.5246396064758301\n",
      "Epoch: 45370 | Loss: 0.5018228888511658 | Test loss: 0.5246257781982422\n",
      "Epoch: 45380 | Loss: 0.5018110871315002 | Test loss: 0.5246119499206543\n",
      "Epoch: 45390 | Loss: 0.5017992854118347 | Test loss: 0.5245981216430664\n",
      "Epoch: 45400 | Loss: 0.5017876029014587 | Test loss: 0.5245842933654785\n",
      "Epoch: 45410 | Loss: 0.5017758011817932 | Test loss: 0.5245704054832458\n",
      "Epoch: 45420 | Loss: 0.5017640590667725 | Test loss: 0.524556577205658\n",
      "Epoch: 45430 | Loss: 0.5017523169517517 | Test loss: 0.5245427489280701\n",
      "Epoch: 45440 | Loss: 0.501740574836731 | Test loss: 0.5245289206504822\n",
      "Epoch: 45450 | Loss: 0.5017287731170654 | Test loss: 0.5245150923728943\n",
      "Epoch: 45460 | Loss: 0.5017170310020447 | Test loss: 0.5245012640953064\n",
      "Epoch: 45470 | Loss: 0.5017052292823792 | Test loss: 0.524487316608429\n",
      "Epoch: 45480 | Loss: 0.5016934871673584 | Test loss: 0.5244734883308411\n",
      "Epoch: 45490 | Loss: 0.5016817450523376 | Test loss: 0.5244596600532532\n",
      "Epoch: 45500 | Loss: 0.5016700029373169 | Test loss: 0.5244457721710205\n",
      "Epoch: 45510 | Loss: 0.5016582012176514 | Test loss: 0.5244319438934326\n",
      "Epoch: 45520 | Loss: 0.5016464591026306 | Test loss: 0.5244181156158447\n",
      "Epoch: 45530 | Loss: 0.5016347169876099 | Test loss: 0.5244042277336121\n",
      "Epoch: 45540 | Loss: 0.5016229152679443 | Test loss: 0.5243903994560242\n",
      "Epoch: 45550 | Loss: 0.5016111731529236 | Test loss: 0.5243765711784363\n",
      "Epoch: 45560 | Loss: 0.5015994310379028 | Test loss: 0.5243626832962036\n",
      "Epoch: 45570 | Loss: 0.5015876293182373 | Test loss: 0.5243489146232605\n",
      "Epoch: 45580 | Loss: 0.5015758872032166 | Test loss: 0.5243350267410278\n",
      "Epoch: 45590 | Loss: 0.5015641450881958 | Test loss: 0.5243211984634399\n",
      "Epoch: 45600 | Loss: 0.5015523433685303 | Test loss: 0.524307370185852\n",
      "Epoch: 45610 | Loss: 0.5015406012535095 | Test loss: 0.5242935419082642\n",
      "Epoch: 45620 | Loss: 0.5015288591384888 | Test loss: 0.5242795944213867\n",
      "Epoch: 45630 | Loss: 0.501517117023468 | Test loss: 0.5242657661437988\n",
      "Epoch: 45640 | Loss: 0.5015053153038025 | Test loss: 0.5242519378662109\n",
      "Epoch: 45650 | Loss: 0.5014935731887817 | Test loss: 0.524238109588623\n",
      "Epoch: 45660 | Loss: 0.501481831073761 | Test loss: 0.5242242813110352\n",
      "Epoch: 45670 | Loss: 0.5014700293540955 | Test loss: 0.5242103934288025\n",
      "Epoch: 45680 | Loss: 0.5014583468437195 | Test loss: 0.5241965651512146\n",
      "Epoch: 45690 | Loss: 0.5014464855194092 | Test loss: 0.5241827368736267\n",
      "Epoch: 45700 | Loss: 0.5014347434043884 | Test loss: 0.5241689085960388\n",
      "Epoch: 45710 | Loss: 0.5014230012893677 | Test loss: 0.5241550803184509\n",
      "Epoch: 45720 | Loss: 0.5014111995697021 | Test loss: 0.524141252040863\n",
      "Epoch: 45730 | Loss: 0.5013995170593262 | Test loss: 0.5241273641586304\n",
      "Epoch: 45740 | Loss: 0.5013877153396606 | Test loss: 0.5241134762763977\n",
      "Epoch: 45750 | Loss: 0.5013759732246399 | Test loss: 0.5240996479988098\n",
      "Epoch: 45760 | Loss: 0.5013641715049744 | Test loss: 0.5240857601165771\n",
      "Epoch: 45770 | Loss: 0.5013524293899536 | Test loss: 0.524071991443634\n",
      "Epoch: 45780 | Loss: 0.5013406872749329 | Test loss: 0.5240581035614014\n",
      "Epoch: 45790 | Loss: 0.5013288855552673 | Test loss: 0.5240442156791687\n",
      "Epoch: 45800 | Loss: 0.5013171434402466 | Test loss: 0.5240303874015808\n",
      "Epoch: 45810 | Loss: 0.5013054013252258 | Test loss: 0.5240165591239929\n",
      "Epoch: 45820 | Loss: 0.5012936592102051 | Test loss: 0.524002730846405\n",
      "Epoch: 45830 | Loss: 0.5012819170951843 | Test loss: 0.5239889025688171\n",
      "Epoch: 45840 | Loss: 0.5012701153755188 | Test loss: 0.5239750146865845\n",
      "Epoch: 45850 | Loss: 0.501258373260498 | Test loss: 0.5239611864089966\n",
      "Epoch: 45860 | Loss: 0.5012465715408325 | Test loss: 0.5239473581314087\n",
      "Epoch: 45870 | Loss: 0.5012348294258118 | Test loss: 0.5239335298538208\n",
      "Epoch: 45880 | Loss: 0.501223087310791 | Test loss: 0.5239196419715881\n",
      "Epoch: 45890 | Loss: 0.5012113451957703 | Test loss: 0.5239058136940002\n",
      "Epoch: 45900 | Loss: 0.5011995434761047 | Test loss: 0.5238919258117676\n",
      "Epoch: 45910 | Loss: 0.501187801361084 | Test loss: 0.5238780975341797\n",
      "Epoch: 45920 | Loss: 0.5011759996414185 | Test loss: 0.5238642692565918\n",
      "Epoch: 45930 | Loss: 0.5011642575263977 | Test loss: 0.5238504409790039\n",
      "Epoch: 45940 | Loss: 0.501152515411377 | Test loss: 0.5238365530967712\n",
      "Epoch: 45950 | Loss: 0.5011407732963562 | Test loss: 0.5238227248191833\n",
      "Epoch: 45960 | Loss: 0.5011289715766907 | Test loss: 0.5238088965415955\n",
      "Epoch: 45970 | Loss: 0.5011172294616699 | Test loss: 0.5237950682640076\n",
      "Epoch: 45980 | Loss: 0.5011054873466492 | Test loss: 0.5237812399864197\n",
      "Epoch: 45990 | Loss: 0.5010936856269836 | Test loss: 0.523767352104187\n",
      "Epoch: 46000 | Loss: 0.5010819435119629 | Test loss: 0.5237535238265991\n",
      "Epoch: 46010 | Loss: 0.5010702013969421 | Test loss: 0.5237396359443665\n",
      "Epoch: 46020 | Loss: 0.5010585188865662 | Test loss: 0.5237258076667786\n",
      "Epoch: 46030 | Loss: 0.5010466575622559 | Test loss: 0.5237119793891907\n",
      "Epoch: 46040 | Loss: 0.5010349154472351 | Test loss: 0.523698091506958\n",
      "Epoch: 46050 | Loss: 0.5010232329368591 | Test loss: 0.5236842632293701\n",
      "Epoch: 46060 | Loss: 0.5010113716125488 | Test loss: 0.5236704349517822\n",
      "Epoch: 46070 | Loss: 0.5009996294975281 | Test loss: 0.5236565470695496\n",
      "Epoch: 46080 | Loss: 0.5009879469871521 | Test loss: 0.5236427187919617\n",
      "Epoch: 46090 | Loss: 0.5009760856628418 | Test loss: 0.5236288905143738\n",
      "Epoch: 46100 | Loss: 0.500964343547821 | Test loss: 0.5236150026321411\n",
      "Epoch: 46110 | Loss: 0.5009526014328003 | Test loss: 0.5236011743545532\n",
      "Epoch: 46120 | Loss: 0.5009407997131348 | Test loss: 0.5235873460769653\n",
      "Epoch: 46130 | Loss: 0.500929057598114 | Test loss: 0.5235735177993774\n",
      "Epoch: 46140 | Loss: 0.500917375087738 | Test loss: 0.5235596895217896\n",
      "Epoch: 46150 | Loss: 0.5009055733680725 | Test loss: 0.5235458016395569\n",
      "Epoch: 46160 | Loss: 0.500893771648407 | Test loss: 0.5235319137573242\n",
      "Epoch: 46170 | Loss: 0.500882089138031 | Test loss: 0.5235180854797363\n",
      "Epoch: 46180 | Loss: 0.5008702874183655 | Test loss: 0.5235042572021484\n",
      "Epoch: 46190 | Loss: 0.5008584856987 | Test loss: 0.5234904289245605\n",
      "Epoch: 46200 | Loss: 0.500846803188324 | Test loss: 0.5234766006469727\n",
      "Epoch: 46210 | Loss: 0.5008350610733032 | Test loss: 0.52346271276474\n",
      "Epoch: 46220 | Loss: 0.5008231997489929 | Test loss: 0.5234488844871521\n",
      "Epoch: 46230 | Loss: 0.5008115172386169 | Test loss: 0.5234350562095642\n",
      "Epoch: 46240 | Loss: 0.5007997155189514 | Test loss: 0.5234212279319763\n",
      "Epoch: 46250 | Loss: 0.5007879734039307 | Test loss: 0.5234073996543884\n",
      "Epoch: 46260 | Loss: 0.5007762312889099 | Test loss: 0.5233935713768005\n",
      "Epoch: 46270 | Loss: 0.5007644295692444 | Test loss: 0.5233796238899231\n",
      "Epoch: 46280 | Loss: 0.5007526874542236 | Test loss: 0.5233657956123352\n",
      "Epoch: 46290 | Loss: 0.5007409453392029 | Test loss: 0.5233519673347473\n",
      "Epoch: 46300 | Loss: 0.5007292032241821 | Test loss: 0.5233380794525146\n",
      "Epoch: 46310 | Loss: 0.5007174015045166 | Test loss: 0.5233242511749268\n",
      "Epoch: 46320 | Loss: 0.5007056593894958 | Test loss: 0.5233104228973389\n",
      "Epoch: 46330 | Loss: 0.5006938576698303 | Test loss: 0.5232965350151062\n",
      "Epoch: 46340 | Loss: 0.5006821155548096 | Test loss: 0.5232827067375183\n",
      "Epoch: 46350 | Loss: 0.5006703734397888 | Test loss: 0.5232688784599304\n",
      "Epoch: 46360 | Loss: 0.5006586313247681 | Test loss: 0.5232549905776978\n",
      "Epoch: 46370 | Loss: 0.5006468296051025 | Test loss: 0.5232412219047546\n",
      "Epoch: 46380 | Loss: 0.5006350874900818 | Test loss: 0.523227334022522\n",
      "Epoch: 46390 | Loss: 0.500623345375061 | Test loss: 0.5232135057449341\n",
      "Epoch: 46400 | Loss: 0.5006115436553955 | Test loss: 0.5231996774673462\n",
      "Epoch: 46410 | Loss: 0.5005998015403748 | Test loss: 0.5231858491897583\n",
      "Epoch: 46420 | Loss: 0.500588059425354 | Test loss: 0.5231719017028809\n",
      "Epoch: 46430 | Loss: 0.5005763173103333 | Test loss: 0.523158073425293\n",
      "Epoch: 46440 | Loss: 0.5005645155906677 | Test loss: 0.5231442451477051\n",
      "Epoch: 46450 | Loss: 0.500552773475647 | Test loss: 0.5231304168701172\n",
      "Epoch: 46460 | Loss: 0.5005410313606262 | Test loss: 0.5231165885925293\n",
      "Epoch: 46470 | Loss: 0.5005292296409607 | Test loss: 0.5231027007102966\n",
      "Epoch: 46480 | Loss: 0.5005175471305847 | Test loss: 0.5230888724327087\n",
      "Epoch: 46490 | Loss: 0.5005056858062744 | Test loss: 0.5230750441551208\n",
      "Epoch: 46500 | Loss: 0.5004939436912537 | Test loss: 0.523061215877533\n",
      "Epoch: 46510 | Loss: 0.5004822015762329 | Test loss: 0.5230473875999451\n",
      "Epoch: 46520 | Loss: 0.5004704594612122 | Test loss: 0.5230335593223572\n",
      "Epoch: 46530 | Loss: 0.5004586577415466 | Test loss: 0.5230196714401245\n",
      "Epoch: 46540 | Loss: 0.5004469156265259 | Test loss: 0.5230057835578918\n",
      "Epoch: 46550 | Loss: 0.5004351735115051 | Test loss: 0.522991955280304\n",
      "Epoch: 46560 | Loss: 0.5004233717918396 | Test loss: 0.5229780673980713\n",
      "Epoch: 46570 | Loss: 0.5004116296768188 | Test loss: 0.5229642987251282\n",
      "Epoch: 46580 | Loss: 0.5003998875617981 | Test loss: 0.5229504108428955\n",
      "Epoch: 46590 | Loss: 0.5003880858421326 | Test loss: 0.5229365229606628\n",
      "Epoch: 46600 | Loss: 0.5003763437271118 | Test loss: 0.522922694683075\n",
      "Epoch: 46610 | Loss: 0.5003646016120911 | Test loss: 0.5229088664054871\n",
      "Epoch: 46620 | Loss: 0.5003528594970703 | Test loss: 0.5228950381278992\n",
      "Epoch: 46630 | Loss: 0.5003411173820496 | Test loss: 0.5228812098503113\n",
      "Epoch: 46640 | Loss: 0.500329315662384 | Test loss: 0.5228673219680786\n",
      "Epoch: 46650 | Loss: 0.5003175735473633 | Test loss: 0.5228534936904907\n",
      "Epoch: 46660 | Loss: 0.5003057718276978 | Test loss: 0.5228396654129028\n",
      "Epoch: 46670 | Loss: 0.500294029712677 | Test loss: 0.5228258371353149\n",
      "Epoch: 46680 | Loss: 0.5002822875976562 | Test loss: 0.5228119492530823\n",
      "Epoch: 46690 | Loss: 0.5002705454826355 | Test loss: 0.5227981209754944\n",
      "Epoch: 46700 | Loss: 0.50025874376297 | Test loss: 0.5227842330932617\n",
      "Epoch: 46710 | Loss: 0.5002470016479492 | Test loss: 0.5227704048156738\n",
      "Epoch: 46720 | Loss: 0.5002352595329285 | Test loss: 0.5227565765380859\n",
      "Epoch: 46730 | Loss: 0.5002234578132629 | Test loss: 0.522742748260498\n",
      "Epoch: 46740 | Loss: 0.5002117156982422 | Test loss: 0.5227288603782654\n",
      "Epoch: 46750 | Loss: 0.5001999735832214 | Test loss: 0.5227150321006775\n",
      "Epoch: 46760 | Loss: 0.5001881718635559 | Test loss: 0.5227012038230896\n",
      "Epoch: 46770 | Loss: 0.5001764297485352 | Test loss: 0.5226873755455017\n",
      "Epoch: 46780 | Loss: 0.5001646876335144 | Test loss: 0.5226735472679138\n",
      "Epoch: 46790 | Loss: 0.5001528859138489 | Test loss: 0.5226596593856812\n",
      "Epoch: 46800 | Loss: 0.5001411437988281 | Test loss: 0.5226458311080933\n",
      "Epoch: 46810 | Loss: 0.5001294016838074 | Test loss: 0.5226319432258606\n",
      "Epoch: 46820 | Loss: 0.5001176595687866 | Test loss: 0.5226181149482727\n",
      "Epoch: 46830 | Loss: 0.5001058578491211 | Test loss: 0.5226042866706848\n",
      "Epoch: 46840 | Loss: 0.5000941157341003 | Test loss: 0.5225903987884521\n",
      "Epoch: 46850 | Loss: 0.5000824332237244 | Test loss: 0.5225765705108643\n",
      "Epoch: 46860 | Loss: 0.5000705718994141 | Test loss: 0.5225627422332764\n",
      "Epoch: 46870 | Loss: 0.5000588297843933 | Test loss: 0.5225488543510437\n",
      "Epoch: 46880 | Loss: 0.5000471472740173 | Test loss: 0.5225350260734558\n",
      "Epoch: 46890 | Loss: 0.500035285949707 | Test loss: 0.5225211977958679\n",
      "Epoch: 46900 | Loss: 0.5000235438346863 | Test loss: 0.5225073099136353\n",
      "Epoch: 46910 | Loss: 0.5000118017196655 | Test loss: 0.5224934816360474\n",
      "Epoch: 46920 | Loss: 0.5 | Test loss: 0.5224796533584595\n",
      "Epoch: 46930 | Loss: 0.49998828768730164 | Test loss: 0.5224658250808716\n",
      "Epoch: 46940 | Loss: 0.4999765455722809 | Test loss: 0.5224519968032837\n",
      "Epoch: 46950 | Loss: 0.49996477365493774 | Test loss: 0.522438108921051\n",
      "Epoch: 46960 | Loss: 0.4999530017375946 | Test loss: 0.5224242210388184\n",
      "Epoch: 46970 | Loss: 0.49994125962257385 | Test loss: 0.5224103927612305\n",
      "Epoch: 46980 | Loss: 0.4999294877052307 | Test loss: 0.5223965644836426\n",
      "Epoch: 46990 | Loss: 0.4999177157878876 | Test loss: 0.5223827362060547\n",
      "Epoch: 47000 | Loss: 0.4999059736728668 | Test loss: 0.5223689079284668\n",
      "Epoch: 47010 | Loss: 0.4998942017555237 | Test loss: 0.5223550200462341\n",
      "Epoch: 47020 | Loss: 0.49988242983818054 | Test loss: 0.5223411917686462\n",
      "Epoch: 47030 | Loss: 0.4998706877231598 | Test loss: 0.5223273634910583\n",
      "Epoch: 47040 | Loss: 0.49985891580581665 | Test loss: 0.5223135352134705\n",
      "Epoch: 47050 | Loss: 0.4998471736907959 | Test loss: 0.5222997069358826\n",
      "Epoch: 47060 | Loss: 0.49983540177345276 | Test loss: 0.5222858786582947\n",
      "Epoch: 47070 | Loss: 0.4998236298561096 | Test loss: 0.5222719311714172\n",
      "Epoch: 47080 | Loss: 0.49981188774108887 | Test loss: 0.5222581028938293\n",
      "Epoch: 47090 | Loss: 0.4998001158237457 | Test loss: 0.5222442746162415\n",
      "Epoch: 47100 | Loss: 0.499788373708725 | Test loss: 0.5222303867340088\n",
      "Epoch: 47110 | Loss: 0.49977657198905945 | Test loss: 0.5222165584564209\n",
      "Epoch: 47120 | Loss: 0.4997648298740387 | Test loss: 0.522202730178833\n",
      "Epoch: 47130 | Loss: 0.49975305795669556 | Test loss: 0.5221888422966003\n",
      "Epoch: 47140 | Loss: 0.4997413158416748 | Test loss: 0.5221750140190125\n",
      "Epoch: 47150 | Loss: 0.49972954392433167 | Test loss: 0.5221611857414246\n",
      "Epoch: 47160 | Loss: 0.4997178018093109 | Test loss: 0.5221472978591919\n",
      "Epoch: 47170 | Loss: 0.4997060298919678 | Test loss: 0.5221335291862488\n",
      "Epoch: 47180 | Loss: 0.49969425797462463 | Test loss: 0.5221196413040161\n",
      "Epoch: 47190 | Loss: 0.4996825158596039 | Test loss: 0.5221058130264282\n",
      "Epoch: 47200 | Loss: 0.49967074394226074 | Test loss: 0.5220919847488403\n",
      "Epoch: 47210 | Loss: 0.4996590316295624 | Test loss: 0.5220781564712524\n",
      "Epoch: 47220 | Loss: 0.49964722990989685 | Test loss: 0.522064208984375\n",
      "Epoch: 47230 | Loss: 0.4996355175971985 | Test loss: 0.5220503807067871\n",
      "Epoch: 47240 | Loss: 0.49962368607521057 | Test loss: 0.5220365524291992\n",
      "Epoch: 47250 | Loss: 0.4996119439601898 | Test loss: 0.5220227241516113\n",
      "Epoch: 47260 | Loss: 0.49960023164749146 | Test loss: 0.5220088958740234\n",
      "Epoch: 47270 | Loss: 0.49958840012550354 | Test loss: 0.5219950079917908\n",
      "Epoch: 47280 | Loss: 0.4995766580104828 | Test loss: 0.5219811797142029\n",
      "Epoch: 47290 | Loss: 0.49956488609313965 | Test loss: 0.521967351436615\n",
      "Epoch: 47300 | Loss: 0.4995531141757965 | Test loss: 0.5219535231590271\n",
      "Epoch: 47310 | Loss: 0.49954137206077576 | Test loss: 0.5219396948814392\n",
      "Epoch: 47320 | Loss: 0.4995296597480774 | Test loss: 0.5219258666038513\n",
      "Epoch: 47330 | Loss: 0.49951788783073425 | Test loss: 0.5219119787216187\n",
      "Epoch: 47340 | Loss: 0.4995060861110687 | Test loss: 0.521898090839386\n",
      "Epoch: 47350 | Loss: 0.49949437379837036 | Test loss: 0.5218842625617981\n",
      "Epoch: 47360 | Loss: 0.4994826018810272 | Test loss: 0.5218703746795654\n",
      "Epoch: 47370 | Loss: 0.4994708001613617 | Test loss: 0.5218566060066223\n",
      "Epoch: 47380 | Loss: 0.49945908784866333 | Test loss: 0.5218427181243896\n",
      "Epoch: 47390 | Loss: 0.4994473159313202 | Test loss: 0.521828830242157\n",
      "Epoch: 47400 | Loss: 0.49943557381629944 | Test loss: 0.5218150019645691\n",
      "Epoch: 47410 | Loss: 0.4994238018989563 | Test loss: 0.5218011736869812\n",
      "Epoch: 47420 | Loss: 0.49941202998161316 | Test loss: 0.5217873454093933\n",
      "Epoch: 47430 | Loss: 0.4994002878665924 | Test loss: 0.5217735171318054\n",
      "Epoch: 47440 | Loss: 0.49938851594924927 | Test loss: 0.5217596292495728\n",
      "Epoch: 47450 | Loss: 0.4993767738342285 | Test loss: 0.5217458009719849\n",
      "Epoch: 47460 | Loss: 0.4993649423122406 | Test loss: 0.521731972694397\n",
      "Epoch: 47470 | Loss: 0.49935322999954224 | Test loss: 0.5217181444168091\n",
      "Epoch: 47480 | Loss: 0.4993414878845215 | Test loss: 0.5217042565345764\n",
      "Epoch: 47490 | Loss: 0.49932971596717834 | Test loss: 0.5216904282569885\n",
      "Epoch: 47500 | Loss: 0.4993179440498352 | Test loss: 0.5216765403747559\n",
      "Epoch: 47510 | Loss: 0.49930620193481445 | Test loss: 0.521662712097168\n",
      "Epoch: 47520 | Loss: 0.4992944896221161 | Test loss: 0.5216488838195801\n",
      "Epoch: 47530 | Loss: 0.4992826581001282 | Test loss: 0.5216350555419922\n",
      "Epoch: 47540 | Loss: 0.4992709159851074 | Test loss: 0.5216211676597595\n",
      "Epoch: 47550 | Loss: 0.49925920367240906 | Test loss: 0.5216073393821716\n",
      "Epoch: 47560 | Loss: 0.49924737215042114 | Test loss: 0.5215935111045837\n",
      "Epoch: 47570 | Loss: 0.4992356300354004 | Test loss: 0.5215796828269958\n",
      "Epoch: 47580 | Loss: 0.499223917722702 | Test loss: 0.521565854549408\n",
      "Epoch: 47590 | Loss: 0.4992120862007141 | Test loss: 0.5215519666671753\n",
      "Epoch: 47600 | Loss: 0.49920034408569336 | Test loss: 0.5215381383895874\n",
      "Epoch: 47610 | Loss: 0.499188631772995 | Test loss: 0.5215242505073547\n",
      "Epoch: 47620 | Loss: 0.4991768002510071 | Test loss: 0.5215104222297668\n",
      "Epoch: 47630 | Loss: 0.49916505813598633 | Test loss: 0.521496593952179\n",
      "Epoch: 47640 | Loss: 0.49915334582328796 | Test loss: 0.5214827060699463\n",
      "Epoch: 47650 | Loss: 0.4991416037082672 | Test loss: 0.5214688777923584\n",
      "Epoch: 47660 | Loss: 0.4991297721862793 | Test loss: 0.5214550495147705\n",
      "Epoch: 47670 | Loss: 0.49911805987358093 | Test loss: 0.5214411616325378\n",
      "Epoch: 47680 | Loss: 0.4991063177585602 | Test loss: 0.52142733335495\n",
      "Epoch: 47690 | Loss: 0.49909448623657227 | Test loss: 0.5214135050773621\n",
      "Epoch: 47700 | Loss: 0.4990827739238739 | Test loss: 0.5213996171951294\n",
      "Epoch: 47710 | Loss: 0.4990709722042084 | Test loss: 0.5213857889175415\n",
      "Epoch: 47720 | Loss: 0.49905925989151 | Test loss: 0.5213719606399536\n",
      "Epoch: 47730 | Loss: 0.49904748797416687 | Test loss: 0.5213581323623657\n",
      "Epoch: 47740 | Loss: 0.4990357458591461 | Test loss: 0.5213443040847778\n",
      "Epoch: 47750 | Loss: 0.499023973941803 | Test loss: 0.5213304162025452\n",
      "Epoch: 47760 | Loss: 0.49901220202445984 | Test loss: 0.5213165283203125\n",
      "Epoch: 47770 | Loss: 0.4990004599094391 | Test loss: 0.5213027000427246\n",
      "Epoch: 47780 | Loss: 0.49898868799209595 | Test loss: 0.5212888717651367\n",
      "Epoch: 47790 | Loss: 0.4989769160747528 | Test loss: 0.5212750434875488\n",
      "Epoch: 47800 | Loss: 0.49896517395973206 | Test loss: 0.5212612152099609\n",
      "Epoch: 47810 | Loss: 0.4989534020423889 | Test loss: 0.5212473273277283\n",
      "Epoch: 47820 | Loss: 0.4989416301250458 | Test loss: 0.5212334990501404\n",
      "Epoch: 47830 | Loss: 0.498929888010025 | Test loss: 0.5212196707725525\n",
      "Epoch: 47840 | Loss: 0.4989181160926819 | Test loss: 0.5212058424949646\n",
      "Epoch: 47850 | Loss: 0.49890637397766113 | Test loss: 0.5211920142173767\n",
      "Epoch: 47860 | Loss: 0.498894602060318 | Test loss: 0.5211781859397888\n",
      "Epoch: 47870 | Loss: 0.49888283014297485 | Test loss: 0.5211642384529114\n",
      "Epoch: 47880 | Loss: 0.4988710880279541 | Test loss: 0.5211504101753235\n",
      "Epoch: 47890 | Loss: 0.49885931611061096 | Test loss: 0.5211365818977356\n",
      "Epoch: 47900 | Loss: 0.4988475739955902 | Test loss: 0.5211226940155029\n",
      "Epoch: 47910 | Loss: 0.4988357722759247 | Test loss: 0.521108865737915\n",
      "Epoch: 47920 | Loss: 0.49882403016090393 | Test loss: 0.5210950374603271\n",
      "Epoch: 47930 | Loss: 0.4988122582435608 | Test loss: 0.5210811495780945\n",
      "Epoch: 47940 | Loss: 0.49880051612854004 | Test loss: 0.5210673213005066\n",
      "Epoch: 47950 | Loss: 0.4987887442111969 | Test loss: 0.5210534930229187\n",
      "Epoch: 47960 | Loss: 0.49877700209617615 | Test loss: 0.521039605140686\n",
      "Epoch: 47970 | Loss: 0.498765230178833 | Test loss: 0.5210258364677429\n",
      "Epoch: 47980 | Loss: 0.49875345826148987 | Test loss: 0.5210119485855103\n",
      "Epoch: 47990 | Loss: 0.4987417161464691 | Test loss: 0.5209981203079224\n",
      "Epoch: 48000 | Loss: 0.498729944229126 | Test loss: 0.5209842920303345\n",
      "Epoch: 48010 | Loss: 0.4987182319164276 | Test loss: 0.5209704637527466\n",
      "Epoch: 48020 | Loss: 0.4987064301967621 | Test loss: 0.5209565162658691\n",
      "Epoch: 48030 | Loss: 0.4986947178840637 | Test loss: 0.5209426879882812\n",
      "Epoch: 48040 | Loss: 0.4986828863620758 | Test loss: 0.5209288597106934\n",
      "Epoch: 48050 | Loss: 0.49867114424705505 | Test loss: 0.5209150314331055\n",
      "Epoch: 48060 | Loss: 0.4986594319343567 | Test loss: 0.5209012031555176\n",
      "Epoch: 48070 | Loss: 0.4986476004123688 | Test loss: 0.5208873152732849\n",
      "Epoch: 48080 | Loss: 0.498635858297348 | Test loss: 0.520873486995697\n",
      "Epoch: 48090 | Loss: 0.4986240863800049 | Test loss: 0.5208596587181091\n",
      "Epoch: 48100 | Loss: 0.49861231446266174 | Test loss: 0.5208458304405212\n",
      "Epoch: 48110 | Loss: 0.498600572347641 | Test loss: 0.5208320021629333\n",
      "Epoch: 48120 | Loss: 0.4985888600349426 | Test loss: 0.5208181738853455\n",
      "Epoch: 48130 | Loss: 0.4985770881175995 | Test loss: 0.5208042860031128\n",
      "Epoch: 48140 | Loss: 0.49856528639793396 | Test loss: 0.5207903981208801\n",
      "Epoch: 48150 | Loss: 0.4985535740852356 | Test loss: 0.5207765698432922\n",
      "Epoch: 48160 | Loss: 0.49854180216789246 | Test loss: 0.5207626819610596\n",
      "Epoch: 48170 | Loss: 0.49853000044822693 | Test loss: 0.5207489132881165\n",
      "Epoch: 48180 | Loss: 0.49851828813552856 | Test loss: 0.5207350254058838\n",
      "Epoch: 48190 | Loss: 0.4985065162181854 | Test loss: 0.5207211375236511\n",
      "Epoch: 48200 | Loss: 0.4984947144985199 | Test loss: 0.5207073092460632\n",
      "Epoch: 48210 | Loss: 0.49848300218582153 | Test loss: 0.5206934809684753\n",
      "Epoch: 48220 | Loss: 0.4984712302684784 | Test loss: 0.5206796526908875\n",
      "Epoch: 48230 | Loss: 0.49845948815345764 | Test loss: 0.5206658244132996\n",
      "Epoch: 48240 | Loss: 0.4984477162361145 | Test loss: 0.5206519365310669\n",
      "Epoch: 48250 | Loss: 0.49843597412109375 | Test loss: 0.520638108253479\n",
      "Epoch: 48260 | Loss: 0.49842414259910583 | Test loss: 0.5206242799758911\n",
      "Epoch: 48270 | Loss: 0.49841243028640747 | Test loss: 0.5206104516983032\n",
      "Epoch: 48280 | Loss: 0.4984006881713867 | Test loss: 0.5205965638160706\n",
      "Epoch: 48290 | Loss: 0.4983889162540436 | Test loss: 0.5205827355384827\n",
      "Epoch: 48300 | Loss: 0.49837714433670044 | Test loss: 0.52056884765625\n",
      "Epoch: 48310 | Loss: 0.4983654022216797 | Test loss: 0.5205550193786621\n",
      "Epoch: 48320 | Loss: 0.4983536899089813 | Test loss: 0.5205411911010742\n",
      "Epoch: 48330 | Loss: 0.4983418583869934 | Test loss: 0.5205273628234863\n",
      "Epoch: 48340 | Loss: 0.49833011627197266 | Test loss: 0.5205134749412537\n",
      "Epoch: 48350 | Loss: 0.4983184039592743 | Test loss: 0.5204996466636658\n",
      "Epoch: 48360 | Loss: 0.4983065724372864 | Test loss: 0.5204858183860779\n",
      "Epoch: 48370 | Loss: 0.4982948303222656 | Test loss: 0.52047199010849\n",
      "Epoch: 48380 | Loss: 0.49828311800956726 | Test loss: 0.5204581618309021\n",
      "Epoch: 48390 | Loss: 0.49827128648757935 | Test loss: 0.5204442739486694\n",
      "Epoch: 48400 | Loss: 0.4982595443725586 | Test loss: 0.5204304456710815\n",
      "Epoch: 48410 | Loss: 0.49824783205986023 | Test loss: 0.5204165577888489\n",
      "Epoch: 48420 | Loss: 0.4982360005378723 | Test loss: 0.520402729511261\n",
      "Epoch: 48430 | Loss: 0.49822425842285156 | Test loss: 0.5203889012336731\n",
      "Epoch: 48440 | Loss: 0.4982125461101532 | Test loss: 0.5203750133514404\n",
      "Epoch: 48450 | Loss: 0.49820080399513245 | Test loss: 0.5203611850738525\n",
      "Epoch: 48460 | Loss: 0.49818897247314453 | Test loss: 0.5203473567962646\n",
      "Epoch: 48470 | Loss: 0.49817726016044617 | Test loss: 0.520333468914032\n",
      "Epoch: 48480 | Loss: 0.4981655180454254 | Test loss: 0.5203196406364441\n",
      "Epoch: 48490 | Loss: 0.4981536865234375 | Test loss: 0.5203058123588562\n",
      "Epoch: 48500 | Loss: 0.49814197421073914 | Test loss: 0.5202919244766235\n",
      "Epoch: 48510 | Loss: 0.4981301724910736 | Test loss: 0.5202780961990356\n",
      "Epoch: 48520 | Loss: 0.49811840057373047 | Test loss: 0.5202642679214478\n",
      "Epoch: 48530 | Loss: 0.4981066882610321 | Test loss: 0.5202504396438599\n",
      "Epoch: 48540 | Loss: 0.49809494614601135 | Test loss: 0.520236611366272\n",
      "Epoch: 48550 | Loss: 0.4980831742286682 | Test loss: 0.5202227234840393\n",
      "Epoch: 48560 | Loss: 0.4980714023113251 | Test loss: 0.5202088356018066\n",
      "Epoch: 48570 | Loss: 0.4980596601963043 | Test loss: 0.5201950073242188\n",
      "Epoch: 48580 | Loss: 0.4980478882789612 | Test loss: 0.5201811790466309\n",
      "Epoch: 48590 | Loss: 0.49803611636161804 | Test loss: 0.520167350769043\n",
      "Epoch: 48600 | Loss: 0.4980243742465973 | Test loss: 0.5201535224914551\n",
      "Epoch: 48610 | Loss: 0.49801260232925415 | Test loss: 0.5201396346092224\n",
      "Epoch: 48620 | Loss: 0.498000830411911 | Test loss: 0.5201258063316345\n",
      "Epoch: 48630 | Loss: 0.49798908829689026 | Test loss: 0.5201119780540466\n",
      "Epoch: 48640 | Loss: 0.4979773163795471 | Test loss: 0.5200981497764587\n",
      "Epoch: 48650 | Loss: 0.49796557426452637 | Test loss: 0.5200843214988708\n",
      "Epoch: 48660 | Loss: 0.4979538023471832 | Test loss: 0.520070493221283\n",
      "Epoch: 48670 | Loss: 0.4979420304298401 | Test loss: 0.5200565457344055\n",
      "Epoch: 48680 | Loss: 0.49793028831481934 | Test loss: 0.5200427174568176\n",
      "Epoch: 48690 | Loss: 0.4979185163974762 | Test loss: 0.5200288891792297\n",
      "Epoch: 48700 | Loss: 0.49790677428245544 | Test loss: 0.5200150012969971\n",
      "Epoch: 48710 | Loss: 0.4978949725627899 | Test loss: 0.5200011730194092\n",
      "Epoch: 48720 | Loss: 0.49788323044776917 | Test loss: 0.5199873447418213\n",
      "Epoch: 48730 | Loss: 0.497871458530426 | Test loss: 0.5199734568595886\n",
      "Epoch: 48740 | Loss: 0.4978597164154053 | Test loss: 0.5199596285820007\n",
      "Epoch: 48750 | Loss: 0.49784794449806213 | Test loss: 0.5199458003044128\n",
      "Epoch: 48760 | Loss: 0.4978362023830414 | Test loss: 0.5199319124221802\n",
      "Epoch: 48770 | Loss: 0.49782443046569824 | Test loss: 0.5199181437492371\n",
      "Epoch: 48780 | Loss: 0.4978126585483551 | Test loss: 0.5199042558670044\n",
      "Epoch: 48790 | Loss: 0.49780091643333435 | Test loss: 0.5198904275894165\n",
      "Epoch: 48800 | Loss: 0.4977891445159912 | Test loss: 0.5198765993118286\n",
      "Epoch: 48810 | Loss: 0.49777743220329285 | Test loss: 0.5198627710342407\n",
      "Epoch: 48820 | Loss: 0.4977656304836273 | Test loss: 0.5198488235473633\n",
      "Epoch: 48830 | Loss: 0.49775391817092896 | Test loss: 0.5198349952697754\n",
      "Epoch: 48840 | Loss: 0.49774208664894104 | Test loss: 0.5198211669921875\n",
      "Epoch: 48850 | Loss: 0.4977303445339203 | Test loss: 0.5198073387145996\n",
      "Epoch: 48860 | Loss: 0.4977186322212219 | Test loss: 0.5197935104370117\n",
      "Epoch: 48870 | Loss: 0.497706800699234 | Test loss: 0.519779622554779\n",
      "Epoch: 48880 | Loss: 0.49769511818885803 | Test loss: 0.5197657942771912\n",
      "Epoch: 48890 | Loss: 0.4976832866668701 | Test loss: 0.5197519659996033\n",
      "Epoch: 48900 | Loss: 0.497671514749527 | Test loss: 0.5197381377220154\n",
      "Epoch: 48910 | Loss: 0.4976597726345062 | Test loss: 0.5197243094444275\n",
      "Epoch: 48920 | Loss: 0.4976480007171631 | Test loss: 0.5197104811668396\n",
      "Epoch: 48930 | Loss: 0.4976362884044647 | Test loss: 0.5196965932846069\n",
      "Epoch: 48940 | Loss: 0.4976244866847992 | Test loss: 0.5196827054023743\n",
      "Epoch: 48950 | Loss: 0.49761277437210083 | Test loss: 0.5196688771247864\n",
      "Epoch: 48960 | Loss: 0.4976010024547577 | Test loss: 0.5196549892425537\n",
      "Epoch: 48970 | Loss: 0.49758920073509216 | Test loss: 0.5196412205696106\n",
      "Epoch: 48980 | Loss: 0.4975774884223938 | Test loss: 0.5196273326873779\n",
      "Epoch: 48990 | Loss: 0.49756571650505066 | Test loss: 0.5196134448051453\n",
      "Epoch: 49000 | Loss: 0.49755391478538513 | Test loss: 0.5195996165275574\n",
      "Epoch: 49010 | Loss: 0.49754220247268677 | Test loss: 0.5195857882499695\n",
      "Epoch: 49020 | Loss: 0.497530460357666 | Test loss: 0.5195719599723816\n",
      "Epoch: 49030 | Loss: 0.4975186884403229 | Test loss: 0.5195581316947937\n",
      "Epoch: 49040 | Loss: 0.49750691652297974 | Test loss: 0.519544243812561\n",
      "Epoch: 49050 | Loss: 0.497495174407959 | Test loss: 0.5195304155349731\n",
      "Epoch: 49060 | Loss: 0.49748334288597107 | Test loss: 0.5195165872573853\n",
      "Epoch: 49070 | Loss: 0.4974716305732727 | Test loss: 0.5195027589797974\n",
      "Epoch: 49080 | Loss: 0.49745988845825195 | Test loss: 0.5194888710975647\n",
      "Epoch: 49090 | Loss: 0.4974481165409088 | Test loss: 0.5194750428199768\n",
      "Epoch: 49100 | Loss: 0.4974363446235657 | Test loss: 0.5194611549377441\n",
      "Epoch: 49110 | Loss: 0.4974246025085449 | Test loss: 0.5194473266601562\n",
      "Epoch: 49120 | Loss: 0.497412770986557 | Test loss: 0.5194334983825684\n",
      "Epoch: 49130 | Loss: 0.49740105867385864 | Test loss: 0.5194196701049805\n",
      "Epoch: 49140 | Loss: 0.4973893165588379 | Test loss: 0.5194057822227478\n",
      "Epoch: 49150 | Loss: 0.4973776042461395 | Test loss: 0.5193919539451599\n",
      "Epoch: 49160 | Loss: 0.4973657727241516 | Test loss: 0.519378125667572\n",
      "Epoch: 49170 | Loss: 0.49735403060913086 | Test loss: 0.5193642973899841\n",
      "Epoch: 49180 | Loss: 0.4973423182964325 | Test loss: 0.5193504691123962\n",
      "Epoch: 49190 | Loss: 0.4973304867744446 | Test loss: 0.5193365812301636\n",
      "Epoch: 49200 | Loss: 0.49731874465942383 | Test loss: 0.5193227529525757\n",
      "Epoch: 49210 | Loss: 0.49730703234672546 | Test loss: 0.519308865070343\n",
      "Epoch: 49220 | Loss: 0.49729523062705994 | Test loss: 0.5192950367927551\n",
      "Epoch: 49230 | Loss: 0.4972834587097168 | Test loss: 0.5192812085151672\n",
      "Epoch: 49240 | Loss: 0.49727174639701843 | Test loss: 0.5192673206329346\n",
      "Epoch: 49250 | Loss: 0.4972600042819977 | Test loss: 0.5192534923553467\n",
      "Epoch: 49260 | Loss: 0.49724817276000977 | Test loss: 0.5192396640777588\n",
      "Epoch: 49270 | Loss: 0.4972364604473114 | Test loss: 0.5192257761955261\n",
      "Epoch: 49280 | Loss: 0.49722471833229065 | Test loss: 0.5192119479179382\n",
      "Epoch: 49290 | Loss: 0.49721288681030273 | Test loss: 0.5191981196403503\n",
      "Epoch: 49300 | Loss: 0.49720117449760437 | Test loss: 0.5191842317581177\n",
      "Epoch: 49310 | Loss: 0.49718937277793884 | Test loss: 0.5191704034805298\n",
      "Epoch: 49320 | Loss: 0.4971776008605957 | Test loss: 0.5191565752029419\n",
      "Epoch: 49330 | Loss: 0.49716588854789734 | Test loss: 0.519142746925354\n",
      "Epoch: 49340 | Loss: 0.4971541464328766 | Test loss: 0.5191289186477661\n",
      "Epoch: 49350 | Loss: 0.49714237451553345 | Test loss: 0.5191150307655334\n",
      "Epoch: 49360 | Loss: 0.4971306025981903 | Test loss: 0.5191011428833008\n",
      "Epoch: 49370 | Loss: 0.49711886048316956 | Test loss: 0.5190873146057129\n",
      "Epoch: 49380 | Loss: 0.4971070885658264 | Test loss: 0.519073486328125\n",
      "Epoch: 49390 | Loss: 0.4970953166484833 | Test loss: 0.5190596580505371\n",
      "Epoch: 49400 | Loss: 0.4970835745334625 | Test loss: 0.5190458297729492\n",
      "Epoch: 49410 | Loss: 0.4970718026161194 | Test loss: 0.5190319418907166\n",
      "Epoch: 49420 | Loss: 0.49706003069877625 | Test loss: 0.5190181136131287\n",
      "Epoch: 49430 | Loss: 0.4970482885837555 | Test loss: 0.5190042853355408\n",
      "Epoch: 49440 | Loss: 0.49703651666641235 | Test loss: 0.5189904570579529\n",
      "Epoch: 49450 | Loss: 0.4970247745513916 | Test loss: 0.518976628780365\n",
      "Epoch: 49460 | Loss: 0.49701300263404846 | Test loss: 0.5189628005027771\n",
      "Epoch: 49470 | Loss: 0.4970012605190277 | Test loss: 0.5189488530158997\n",
      "Epoch: 49480 | Loss: 0.49698948860168457 | Test loss: 0.5189350247383118\n",
      "Epoch: 49490 | Loss: 0.49697771668434143 | Test loss: 0.5189211964607239\n",
      "Epoch: 49500 | Loss: 0.4969659745693207 | Test loss: 0.5189073085784912\n",
      "Epoch: 49510 | Loss: 0.49695417284965515 | Test loss: 0.5188934803009033\n",
      "Epoch: 49520 | Loss: 0.4969424307346344 | Test loss: 0.5188796520233154\n",
      "Epoch: 49530 | Loss: 0.49693065881729126 | Test loss: 0.5188657641410828\n",
      "Epoch: 49540 | Loss: 0.4969189167022705 | Test loss: 0.5188519358634949\n",
      "Epoch: 49550 | Loss: 0.49690714478492737 | Test loss: 0.518838107585907\n",
      "Epoch: 49560 | Loss: 0.4968954026699066 | Test loss: 0.5188242197036743\n",
      "Epoch: 49570 | Loss: 0.4968836307525635 | Test loss: 0.5188104510307312\n",
      "Epoch: 49580 | Loss: 0.49687185883522034 | Test loss: 0.5187965631484985\n",
      "Epoch: 49590 | Loss: 0.4968601167201996 | Test loss: 0.5187827348709106\n",
      "Epoch: 49600 | Loss: 0.49684834480285645 | Test loss: 0.5187689065933228\n",
      "Epoch: 49610 | Loss: 0.4968366324901581 | Test loss: 0.5187550783157349\n",
      "Epoch: 49620 | Loss: 0.49682483077049255 | Test loss: 0.5187411308288574\n",
      "Epoch: 49630 | Loss: 0.4968131184577942 | Test loss: 0.5187273025512695\n",
      "Epoch: 49640 | Loss: 0.4968012869358063 | Test loss: 0.5187134742736816\n",
      "Epoch: 49650 | Loss: 0.4967895448207855 | Test loss: 0.5186996459960938\n",
      "Epoch: 49660 | Loss: 0.49677783250808716 | Test loss: 0.5186858177185059\n",
      "Epoch: 49670 | Loss: 0.49676600098609924 | Test loss: 0.5186719298362732\n",
      "Epoch: 49680 | Loss: 0.49675431847572327 | Test loss: 0.5186581015586853\n",
      "Epoch: 49690 | Loss: 0.49674248695373535 | Test loss: 0.5186442732810974\n",
      "Epoch: 49700 | Loss: 0.4967307150363922 | Test loss: 0.5186304450035095\n",
      "Epoch: 49710 | Loss: 0.49671897292137146 | Test loss: 0.5186166167259216\n",
      "Epoch: 49720 | Loss: 0.4967072606086731 | Test loss: 0.5186027884483337\n",
      "Epoch: 49730 | Loss: 0.49669548869132996 | Test loss: 0.5185889005661011\n",
      "Epoch: 49740 | Loss: 0.49668368697166443 | Test loss: 0.5185750126838684\n",
      "Epoch: 49750 | Loss: 0.49667197465896606 | Test loss: 0.5185611844062805\n",
      "Epoch: 49760 | Loss: 0.4966602027416229 | Test loss: 0.5185472965240479\n",
      "Epoch: 49770 | Loss: 0.4966484010219574 | Test loss: 0.5185335278511047\n",
      "Epoch: 49780 | Loss: 0.49663668870925903 | Test loss: 0.5185196399688721\n",
      "Epoch: 49790 | Loss: 0.4966249167919159 | Test loss: 0.5185057520866394\n",
      "Epoch: 49800 | Loss: 0.49661311507225037 | Test loss: 0.5184919238090515\n",
      "Epoch: 49810 | Loss: 0.496601402759552 | Test loss: 0.5184780955314636\n",
      "Epoch: 49820 | Loss: 0.49658966064453125 | Test loss: 0.5184642672538757\n",
      "Epoch: 49830 | Loss: 0.4965778887271881 | Test loss: 0.5184504389762878\n",
      "Epoch: 49840 | Loss: 0.49656611680984497 | Test loss: 0.5184365510940552\n",
      "Epoch: 49850 | Loss: 0.4965543746948242 | Test loss: 0.5184227228164673\n",
      "Epoch: 49860 | Loss: 0.4965425431728363 | Test loss: 0.5184088945388794\n",
      "Epoch: 49870 | Loss: 0.49653083086013794 | Test loss: 0.5183950662612915\n",
      "Epoch: 49880 | Loss: 0.4965190887451172 | Test loss: 0.5183811783790588\n",
      "Epoch: 49890 | Loss: 0.49650731682777405 | Test loss: 0.518367350101471\n",
      "Epoch: 49900 | Loss: 0.4964955449104309 | Test loss: 0.5183534622192383\n",
      "Epoch: 49910 | Loss: 0.49648380279541016 | Test loss: 0.5183396339416504\n",
      "Epoch: 49920 | Loss: 0.496472030878067 | Test loss: 0.5183258056640625\n",
      "Epoch: 49930 | Loss: 0.4964602589607239 | Test loss: 0.5183119773864746\n",
      "Epoch: 49940 | Loss: 0.4964485168457031 | Test loss: 0.5182980895042419\n",
      "Epoch: 49950 | Loss: 0.49643680453300476 | Test loss: 0.518284261226654\n",
      "Epoch: 49960 | Loss: 0.49642497301101685 | Test loss: 0.5182704329490662\n",
      "Epoch: 49970 | Loss: 0.4964132308959961 | Test loss: 0.5182566046714783\n",
      "Epoch: 49980 | Loss: 0.49640151858329773 | Test loss: 0.5182427763938904\n",
      "Epoch: 49990 | Loss: 0.4963896870613098 | Test loss: 0.5182288885116577\n",
      "Epoch: 50000 | Loss: 0.49637794494628906 | Test loss: 0.5182150602340698\n",
      "Epoch: 50010 | Loss: 0.4963662326335907 | Test loss: 0.5182011723518372\n",
      "Epoch: 50020 | Loss: 0.49635443091392517 | Test loss: 0.5181873440742493\n",
      "Epoch: 50030 | Loss: 0.49634265899658203 | Test loss: 0.5181735157966614\n",
      "Epoch: 50040 | Loss: 0.49633094668388367 | Test loss: 0.5181596279144287\n",
      "Epoch: 50050 | Loss: 0.49631914496421814 | Test loss: 0.5181457996368408\n",
      "Epoch: 50060 | Loss: 0.496307373046875 | Test loss: 0.5181319713592529\n",
      "Epoch: 50070 | Loss: 0.49629560112953186 | Test loss: 0.5181180834770203\n",
      "Epoch: 50080 | Loss: 0.4962839186191559 | Test loss: 0.5181042551994324\n",
      "Epoch: 50090 | Loss: 0.49627208709716797 | Test loss: 0.5180904269218445\n",
      "Epoch: 50100 | Loss: 0.4962603747844696 | Test loss: 0.5180765390396118\n",
      "Epoch: 50110 | Loss: 0.4962485730648041 | Test loss: 0.5180627107620239\n",
      "Epoch: 50120 | Loss: 0.49623680114746094 | Test loss: 0.518048882484436\n",
      "Epoch: 50130 | Loss: 0.4962250888347626 | Test loss: 0.5180350542068481\n",
      "Epoch: 50140 | Loss: 0.4962133467197418 | Test loss: 0.5180212259292603\n",
      "Epoch: 50150 | Loss: 0.4962015748023987 | Test loss: 0.5180073380470276\n",
      "Epoch: 50160 | Loss: 0.49618980288505554 | Test loss: 0.5179934501647949\n",
      "Epoch: 50170 | Loss: 0.4961780607700348 | Test loss: 0.517979621887207\n",
      "Epoch: 50180 | Loss: 0.49616628885269165 | Test loss: 0.5179657936096191\n",
      "Epoch: 50190 | Loss: 0.4961545169353485 | Test loss: 0.5179519653320312\n",
      "Epoch: 50200 | Loss: 0.49614277482032776 | Test loss: 0.5179381370544434\n",
      "Epoch: 50210 | Loss: 0.4961310029029846 | Test loss: 0.5179242491722107\n",
      "Epoch: 50220 | Loss: 0.4961192309856415 | Test loss: 0.5179104208946228\n",
      "Epoch: 50230 | Loss: 0.4961074888706207 | Test loss: 0.5178965926170349\n",
      "Epoch: 50240 | Loss: 0.4960957169532776 | Test loss: 0.517882764339447\n",
      "Epoch: 50250 | Loss: 0.49608394503593445 | Test loss: 0.5178689360618591\n",
      "Epoch: 50260 | Loss: 0.4960722029209137 | Test loss: 0.5178551077842712\n",
      "Epoch: 50270 | Loss: 0.49606043100357056 | Test loss: 0.5178411602973938\n",
      "Epoch: 50280 | Loss: 0.4960486888885498 | Test loss: 0.5178273320198059\n",
      "Epoch: 50290 | Loss: 0.49603691697120667 | Test loss: 0.517813503742218\n",
      "Epoch: 50300 | Loss: 0.4960251748561859 | Test loss: 0.5177996158599854\n",
      "Epoch: 50310 | Loss: 0.4960134029388428 | Test loss: 0.5177857875823975\n",
      "Epoch: 50320 | Loss: 0.49600163102149963 | Test loss: 0.5177719593048096\n",
      "Epoch: 50330 | Loss: 0.4959898591041565 | Test loss: 0.5177580714225769\n",
      "Epoch: 50340 | Loss: 0.49597811698913574 | Test loss: 0.517744243144989\n",
      "Epoch: 50350 | Loss: 0.4959663450717926 | Test loss: 0.5177304148674011\n",
      "Epoch: 50360 | Loss: 0.49595457315444946 | Test loss: 0.5177165269851685\n",
      "Epoch: 50370 | Loss: 0.4959428310394287 | Test loss: 0.5177027583122253\n",
      "Epoch: 50380 | Loss: 0.49593105912208557 | Test loss: 0.5176888704299927\n",
      "Epoch: 50390 | Loss: 0.4959193170070648 | Test loss: 0.5176750421524048\n",
      "Epoch: 50400 | Loss: 0.4959075450897217 | Test loss: 0.5176612138748169\n",
      "Epoch: 50410 | Loss: 0.4958958327770233 | Test loss: 0.517647385597229\n",
      "Epoch: 50420 | Loss: 0.4958840310573578 | Test loss: 0.5176334381103516\n",
      "Epoch: 50430 | Loss: 0.49587225914001465 | Test loss: 0.5176196098327637\n",
      "Epoch: 50440 | Loss: 0.4958605468273163 | Test loss: 0.5176057815551758\n",
      "Epoch: 50450 | Loss: 0.49584874510765076 | Test loss: 0.5175919532775879\n",
      "Epoch: 50460 | Loss: 0.4958370327949524 | Test loss: 0.517578125\n",
      "Epoch: 50470 | Loss: 0.4958252012729645 | Test loss: 0.5175642371177673\n",
      "Epoch: 50480 | Loss: 0.4958135187625885 | Test loss: 0.5175504088401794\n",
      "Epoch: 50490 | Loss: 0.4958016872406006 | Test loss: 0.5175365805625916\n",
      "Epoch: 50500 | Loss: 0.49578991532325745 | Test loss: 0.5175227522850037\n",
      "Epoch: 50510 | Loss: 0.4957781732082367 | Test loss: 0.5175089240074158\n",
      "Epoch: 50520 | Loss: 0.49576646089553833 | Test loss: 0.5174950957298279\n",
      "Epoch: 50530 | Loss: 0.4957546889781952 | Test loss: 0.5174812078475952\n",
      "Epoch: 50540 | Loss: 0.49574288725852966 | Test loss: 0.5174673199653625\n",
      "Epoch: 50550 | Loss: 0.4957311749458313 | Test loss: 0.5174534916877747\n",
      "Epoch: 50560 | Loss: 0.49571940302848816 | Test loss: 0.517439603805542\n",
      "Epoch: 50570 | Loss: 0.49570760130882263 | Test loss: 0.5174258351325989\n",
      "Epoch: 50580 | Loss: 0.49569588899612427 | Test loss: 0.5174119472503662\n",
      "Epoch: 50590 | Loss: 0.49568411707878113 | Test loss: 0.5173980593681335\n",
      "Epoch: 50600 | Loss: 0.4956723153591156 | Test loss: 0.5173842310905457\n",
      "Epoch: 50610 | Loss: 0.49566060304641724 | Test loss: 0.5173704028129578\n",
      "Epoch: 50620 | Loss: 0.4956488609313965 | Test loss: 0.5173565745353699\n",
      "Epoch: 50630 | Loss: 0.49563708901405334 | Test loss: 0.517342746257782\n",
      "Epoch: 50640 | Loss: 0.4956253170967102 | Test loss: 0.5173288583755493\n",
      "Epoch: 50650 | Loss: 0.49561357498168945 | Test loss: 0.5173150300979614\n",
      "Epoch: 50660 | Loss: 0.49560174345970154 | Test loss: 0.5173012018203735\n",
      "Epoch: 50670 | Loss: 0.4955900311470032 | Test loss: 0.5172873735427856\n",
      "Epoch: 50680 | Loss: 0.4955782890319824 | Test loss: 0.517273485660553\n",
      "Epoch: 50690 | Loss: 0.4955665171146393 | Test loss: 0.5172596573829651\n",
      "Epoch: 50700 | Loss: 0.49555474519729614 | Test loss: 0.5172457695007324\n",
      "Epoch: 50710 | Loss: 0.4955430030822754 | Test loss: 0.5172319412231445\n",
      "Epoch: 50720 | Loss: 0.49553123116493225 | Test loss: 0.5172181129455566\n",
      "Epoch: 50730 | Loss: 0.4955194592475891 | Test loss: 0.5172042846679688\n",
      "Epoch: 50740 | Loss: 0.49550771713256836 | Test loss: 0.5171903967857361\n",
      "Epoch: 50750 | Loss: 0.49549600481987 | Test loss: 0.5171765685081482\n",
      "Epoch: 50760 | Loss: 0.4954841732978821 | Test loss: 0.5171627402305603\n",
      "Epoch: 50770 | Loss: 0.49547243118286133 | Test loss: 0.5171489119529724\n",
      "Epoch: 50780 | Loss: 0.49546071887016296 | Test loss: 0.5171350836753845\n",
      "Epoch: 50790 | Loss: 0.49544888734817505 | Test loss: 0.5171211957931519\n",
      "Epoch: 50800 | Loss: 0.4954371452331543 | Test loss: 0.517107367515564\n",
      "Epoch: 50810 | Loss: 0.49542543292045593 | Test loss: 0.5170934796333313\n",
      "Epoch: 50820 | Loss: 0.4954136312007904 | Test loss: 0.5170796513557434\n",
      "Epoch: 50830 | Loss: 0.49540185928344727 | Test loss: 0.5170658230781555\n",
      "Epoch: 50840 | Loss: 0.4953901469707489 | Test loss: 0.5170519351959229\n",
      "Epoch: 50850 | Loss: 0.4953783452510834 | Test loss: 0.517038106918335\n",
      "Epoch: 50860 | Loss: 0.49536657333374023 | Test loss: 0.5170242786407471\n",
      "Epoch: 50870 | Loss: 0.4953548014163971 | Test loss: 0.5170103907585144\n",
      "Epoch: 50880 | Loss: 0.4953431189060211 | Test loss: 0.5169965624809265\n",
      "Epoch: 50890 | Loss: 0.4953312873840332 | Test loss: 0.5169827342033386\n",
      "Epoch: 50900 | Loss: 0.49531957507133484 | Test loss: 0.516968846321106\n",
      "Epoch: 50910 | Loss: 0.4953077733516693 | Test loss: 0.5169550180435181\n",
      "Epoch: 50920 | Loss: 0.49529600143432617 | Test loss: 0.5169411897659302\n",
      "Epoch: 50930 | Loss: 0.4952842891216278 | Test loss: 0.5169273614883423\n",
      "Epoch: 50940 | Loss: 0.49527254700660706 | Test loss: 0.5169135332107544\n",
      "Epoch: 50950 | Loss: 0.4952607750892639 | Test loss: 0.5168996453285217\n",
      "Epoch: 50960 | Loss: 0.4952490031719208 | Test loss: 0.5168857574462891\n",
      "Epoch: 50970 | Loss: 0.4952372610569 | Test loss: 0.5168719291687012\n",
      "Epoch: 50980 | Loss: 0.4952254891395569 | Test loss: 0.5168581008911133\n",
      "Epoch: 50990 | Loss: 0.49521371722221375 | Test loss: 0.5168442726135254\n",
      "Epoch: 51000 | Loss: 0.495201975107193 | Test loss: 0.5168304443359375\n",
      "Epoch: 51010 | Loss: 0.49519020318984985 | Test loss: 0.5168165564537048\n",
      "Epoch: 51020 | Loss: 0.4951784312725067 | Test loss: 0.5168027281761169\n",
      "Epoch: 51030 | Loss: 0.49516668915748596 | Test loss: 0.516788899898529\n",
      "Epoch: 51040 | Loss: 0.4951549172401428 | Test loss: 0.5167750716209412\n",
      "Epoch: 51050 | Loss: 0.4951431453227997 | Test loss: 0.5167612433433533\n",
      "Epoch: 51060 | Loss: 0.49513140320777893 | Test loss: 0.5167474150657654\n",
      "Epoch: 51070 | Loss: 0.4951196312904358 | Test loss: 0.5167334675788879\n",
      "Epoch: 51080 | Loss: 0.49510788917541504 | Test loss: 0.5167196393013\n",
      "Epoch: 51090 | Loss: 0.4950961172580719 | Test loss: 0.5167058110237122\n",
      "Epoch: 51100 | Loss: 0.49508437514305115 | Test loss: 0.5166919231414795\n",
      "Epoch: 51110 | Loss: 0.495072603225708 | Test loss: 0.5166780948638916\n",
      "Epoch: 51120 | Loss: 0.49506083130836487 | Test loss: 0.5166642665863037\n",
      "Epoch: 51130 | Loss: 0.49504905939102173 | Test loss: 0.516650378704071\n",
      "Epoch: 51140 | Loss: 0.495037317276001 | Test loss: 0.5166365504264832\n",
      "Epoch: 51150 | Loss: 0.49502554535865784 | Test loss: 0.5166227221488953\n",
      "Epoch: 51160 | Loss: 0.4950137734413147 | Test loss: 0.5166088342666626\n",
      "Epoch: 51170 | Loss: 0.49500203132629395 | Test loss: 0.5165950655937195\n",
      "Epoch: 51180 | Loss: 0.4949902594089508 | Test loss: 0.5165811777114868\n",
      "Epoch: 51190 | Loss: 0.49497851729393005 | Test loss: 0.5165673494338989\n",
      "Epoch: 51200 | Loss: 0.4949667453765869 | Test loss: 0.516553521156311\n",
      "Epoch: 51210 | Loss: 0.49495503306388855 | Test loss: 0.5165396928787231\n",
      "Epoch: 51220 | Loss: 0.494943231344223 | Test loss: 0.5165257453918457\n",
      "Epoch: 51230 | Loss: 0.4949314594268799 | Test loss: 0.5165119171142578\n",
      "Epoch: 51240 | Loss: 0.4949197471141815 | Test loss: 0.5164980888366699\n",
      "Epoch: 51250 | Loss: 0.494907945394516 | Test loss: 0.516484260559082\n",
      "Epoch: 51260 | Loss: 0.4948962330818176 | Test loss: 0.5164704322814941\n",
      "Epoch: 51270 | Loss: 0.4948844015598297 | Test loss: 0.5164565443992615\n",
      "Epoch: 51280 | Loss: 0.49487271904945374 | Test loss: 0.5164427161216736\n",
      "Epoch: 51290 | Loss: 0.4948608875274658 | Test loss: 0.5164288878440857\n",
      "Epoch: 51300 | Loss: 0.4948491156101227 | Test loss: 0.5164150595664978\n",
      "Epoch: 51310 | Loss: 0.49483737349510193 | Test loss: 0.5164012312889099\n",
      "Epoch: 51320 | Loss: 0.49482566118240356 | Test loss: 0.516387403011322\n",
      "Epoch: 51330 | Loss: 0.4948138892650604 | Test loss: 0.5163735151290894\n",
      "Epoch: 51340 | Loss: 0.4948020875453949 | Test loss: 0.5163596272468567\n",
      "Epoch: 51350 | Loss: 0.49479037523269653 | Test loss: 0.5163457989692688\n",
      "Epoch: 51360 | Loss: 0.4947786033153534 | Test loss: 0.5163319110870361\n",
      "Epoch: 51370 | Loss: 0.49476680159568787 | Test loss: 0.516318142414093\n",
      "Epoch: 51380 | Loss: 0.4947550892829895 | Test loss: 0.5163042545318604\n",
      "Epoch: 51390 | Loss: 0.49474331736564636 | Test loss: 0.5162903666496277\n",
      "Epoch: 51400 | Loss: 0.49473151564598083 | Test loss: 0.5162765383720398\n",
      "Epoch: 51410 | Loss: 0.49471980333328247 | Test loss: 0.5162627100944519\n",
      "Epoch: 51420 | Loss: 0.4947080612182617 | Test loss: 0.516248881816864\n",
      "Epoch: 51430 | Loss: 0.4946962893009186 | Test loss: 0.5162350535392761\n",
      "Epoch: 51440 | Loss: 0.49468451738357544 | Test loss: 0.5162211656570435\n",
      "Epoch: 51450 | Loss: 0.4946727752685547 | Test loss: 0.5162073373794556\n",
      "Epoch: 51460 | Loss: 0.4946609437465668 | Test loss: 0.5161935091018677\n",
      "Epoch: 51470 | Loss: 0.4946492314338684 | Test loss: 0.5161796808242798\n",
      "Epoch: 51480 | Loss: 0.49463748931884766 | Test loss: 0.5161657929420471\n",
      "Epoch: 51490 | Loss: 0.4946257174015045 | Test loss: 0.5161519646644592\n",
      "Epoch: 51500 | Loss: 0.4946139454841614 | Test loss: 0.5161380767822266\n",
      "Epoch: 51510 | Loss: 0.4946022033691406 | Test loss: 0.5161242485046387\n",
      "Epoch: 51520 | Loss: 0.4945904314517975 | Test loss: 0.5161104202270508\n",
      "Epoch: 51530 | Loss: 0.49457865953445435 | Test loss: 0.5160965919494629\n",
      "Epoch: 51540 | Loss: 0.4945669174194336 | Test loss: 0.5160827040672302\n",
      "Epoch: 51550 | Loss: 0.49455520510673523 | Test loss: 0.5160688757896423\n",
      "Epoch: 51560 | Loss: 0.4945433735847473 | Test loss: 0.5160550475120544\n",
      "Epoch: 51570 | Loss: 0.49453163146972656 | Test loss: 0.5160412192344666\n",
      "Epoch: 51580 | Loss: 0.4945199191570282 | Test loss: 0.5160273909568787\n",
      "Epoch: 51590 | Loss: 0.4945080876350403 | Test loss: 0.516013503074646\n",
      "Epoch: 51600 | Loss: 0.49449634552001953 | Test loss: 0.5159996747970581\n",
      "Epoch: 51610 | Loss: 0.49448463320732117 | Test loss: 0.5159857869148254\n",
      "Epoch: 51620 | Loss: 0.49447283148765564 | Test loss: 0.5159719586372375\n",
      "Epoch: 51630 | Loss: 0.4944610595703125 | Test loss: 0.5159581303596497\n",
      "Epoch: 51640 | Loss: 0.49444934725761414 | Test loss: 0.515944242477417\n",
      "Epoch: 51650 | Loss: 0.4944375455379486 | Test loss: 0.5159304141998291\n",
      "Epoch: 51660 | Loss: 0.49442577362060547 | Test loss: 0.5159165859222412\n",
      "Epoch: 51670 | Loss: 0.49441400170326233 | Test loss: 0.5159026980400085\n",
      "Epoch: 51680 | Loss: 0.49440231919288635 | Test loss: 0.5158888697624207\n",
      "Epoch: 51690 | Loss: 0.49439048767089844 | Test loss: 0.5158750414848328\n",
      "Epoch: 51700 | Loss: 0.4943787753582001 | Test loss: 0.5158611536026001\n",
      "Epoch: 51710 | Loss: 0.49436697363853455 | Test loss: 0.5158473253250122\n",
      "Epoch: 51720 | Loss: 0.4943552017211914 | Test loss: 0.5158334970474243\n",
      "Epoch: 51730 | Loss: 0.49434348940849304 | Test loss: 0.5158196687698364\n",
      "Epoch: 51740 | Loss: 0.4943317472934723 | Test loss: 0.5158058404922485\n",
      "Epoch: 51750 | Loss: 0.49431997537612915 | Test loss: 0.5157919526100159\n",
      "Epoch: 51760 | Loss: 0.494308203458786 | Test loss: 0.5157780647277832\n",
      "Epoch: 51770 | Loss: 0.49429646134376526 | Test loss: 0.5157642364501953\n",
      "Epoch: 51780 | Loss: 0.4942846894264221 | Test loss: 0.5157504081726074\n",
      "Epoch: 51790 | Loss: 0.494272917509079 | Test loss: 0.5157365798950195\n",
      "Epoch: 51800 | Loss: 0.4942611753940582 | Test loss: 0.5157227516174316\n",
      "Epoch: 51810 | Loss: 0.4942494034767151 | Test loss: 0.515708863735199\n",
      "Epoch: 51820 | Loss: 0.49423763155937195 | Test loss: 0.5156950354576111\n",
      "Epoch: 51830 | Loss: 0.4942258894443512 | Test loss: 0.5156812071800232\n",
      "Epoch: 51840 | Loss: 0.49421411752700806 | Test loss: 0.5156673789024353\n",
      "Epoch: 51850 | Loss: 0.4942023456096649 | Test loss: 0.5156535506248474\n",
      "Epoch: 51860 | Loss: 0.49419060349464417 | Test loss: 0.5156397223472595\n",
      "Epoch: 51870 | Loss: 0.494178831577301 | Test loss: 0.5156257748603821\n",
      "Epoch: 51880 | Loss: 0.4941670894622803 | Test loss: 0.5156119465827942\n",
      "Epoch: 51890 | Loss: 0.49415531754493713 | Test loss: 0.5155981183052063\n",
      "Epoch: 51900 | Loss: 0.4941435754299164 | Test loss: 0.5155842304229736\n",
      "Epoch: 51910 | Loss: 0.49413180351257324 | Test loss: 0.5155704021453857\n",
      "Epoch: 51920 | Loss: 0.4941200315952301 | Test loss: 0.5155565738677979\n",
      "Epoch: 51930 | Loss: 0.49410825967788696 | Test loss: 0.5155426859855652\n",
      "Epoch: 51940 | Loss: 0.4940965175628662 | Test loss: 0.5155288577079773\n",
      "Epoch: 51950 | Loss: 0.49408474564552307 | Test loss: 0.5155150294303894\n",
      "Epoch: 51960 | Loss: 0.49407297372817993 | Test loss: 0.5155011415481567\n",
      "Epoch: 51970 | Loss: 0.4940612316131592 | Test loss: 0.5154873728752136\n",
      "Epoch: 51980 | Loss: 0.49404945969581604 | Test loss: 0.515473484992981\n",
      "Epoch: 51990 | Loss: 0.4940377175807953 | Test loss: 0.5154596567153931\n",
      "Epoch: 52000 | Loss: 0.49402594566345215 | Test loss: 0.5154458284378052\n",
      "Epoch: 52010 | Loss: 0.4940142333507538 | Test loss: 0.5154320001602173\n",
      "Epoch: 52020 | Loss: 0.49400243163108826 | Test loss: 0.5154180526733398\n",
      "Epoch: 52030 | Loss: 0.4939906597137451 | Test loss: 0.515404224395752\n",
      "Epoch: 52040 | Loss: 0.49397894740104675 | Test loss: 0.5153903961181641\n",
      "Epoch: 52050 | Loss: 0.4939671456813812 | Test loss: 0.5153765678405762\n",
      "Epoch: 52060 | Loss: 0.49395543336868286 | Test loss: 0.5153627395629883\n",
      "Epoch: 52070 | Loss: 0.49394360184669495 | Test loss: 0.5153488516807556\n",
      "Epoch: 52080 | Loss: 0.49393191933631897 | Test loss: 0.5153350234031677\n",
      "Epoch: 52090 | Loss: 0.49392008781433105 | Test loss: 0.5153211951255798\n",
      "Epoch: 52100 | Loss: 0.4939083158969879 | Test loss: 0.5153073668479919\n",
      "Epoch: 52110 | Loss: 0.49389657378196716 | Test loss: 0.515293538570404\n",
      "Epoch: 52120 | Loss: 0.4938848614692688 | Test loss: 0.5152797102928162\n",
      "Epoch: 52130 | Loss: 0.49387308955192566 | Test loss: 0.5152658224105835\n",
      "Epoch: 52140 | Loss: 0.49386128783226013 | Test loss: 0.5152519345283508\n",
      "Epoch: 52150 | Loss: 0.49384957551956177 | Test loss: 0.5152381062507629\n",
      "Epoch: 52160 | Loss: 0.49383780360221863 | Test loss: 0.5152242183685303\n",
      "Epoch: 52170 | Loss: 0.4938260018825531 | Test loss: 0.5152104496955872\n",
      "Epoch: 52180 | Loss: 0.49381428956985474 | Test loss: 0.5151965618133545\n",
      "Epoch: 52190 | Loss: 0.4938025176525116 | Test loss: 0.5151826739311218\n",
      "Epoch: 52200 | Loss: 0.49379071593284607 | Test loss: 0.5151688456535339\n",
      "Epoch: 52210 | Loss: 0.4937790036201477 | Test loss: 0.515155017375946\n",
      "Epoch: 52220 | Loss: 0.49376726150512695 | Test loss: 0.5151411890983582\n",
      "Epoch: 52230 | Loss: 0.4937554895877838 | Test loss: 0.5151273608207703\n",
      "Epoch: 52240 | Loss: 0.4937437176704407 | Test loss: 0.5151134729385376\n",
      "Epoch: 52250 | Loss: 0.4937319755554199 | Test loss: 0.5150996446609497\n",
      "Epoch: 52260 | Loss: 0.493720144033432 | Test loss: 0.5150858163833618\n",
      "Epoch: 52270 | Loss: 0.49370843172073364 | Test loss: 0.5150719881057739\n",
      "Epoch: 52280 | Loss: 0.4936966896057129 | Test loss: 0.5150581002235413\n",
      "Epoch: 52290 | Loss: 0.49368491768836975 | Test loss: 0.5150442719459534\n",
      "Epoch: 52300 | Loss: 0.4936731457710266 | Test loss: 0.5150303840637207\n",
      "Epoch: 52310 | Loss: 0.49366140365600586 | Test loss: 0.5150165557861328\n",
      "Epoch: 52320 | Loss: 0.4936496317386627 | Test loss: 0.5150027275085449\n",
      "Epoch: 52330 | Loss: 0.4936378598213196 | Test loss: 0.514988899230957\n",
      "Epoch: 52340 | Loss: 0.49362611770629883 | Test loss: 0.5149750113487244\n",
      "Epoch: 52350 | Loss: 0.49361440539360046 | Test loss: 0.5149611830711365\n",
      "Epoch: 52360 | Loss: 0.49360257387161255 | Test loss: 0.5149473547935486\n",
      "Epoch: 52370 | Loss: 0.4935908317565918 | Test loss: 0.5149335265159607\n",
      "Epoch: 52380 | Loss: 0.49357911944389343 | Test loss: 0.5149196982383728\n",
      "Epoch: 52390 | Loss: 0.4935672879219055 | Test loss: 0.5149058103561401\n",
      "Epoch: 52400 | Loss: 0.49355554580688477 | Test loss: 0.5148919820785522\n",
      "Epoch: 52410 | Loss: 0.4935438334941864 | Test loss: 0.5148780941963196\n",
      "Epoch: 52420 | Loss: 0.4935320317745209 | Test loss: 0.5148642659187317\n",
      "Epoch: 52430 | Loss: 0.49352025985717773 | Test loss: 0.5148504376411438\n",
      "Epoch: 52440 | Loss: 0.49350854754447937 | Test loss: 0.5148365497589111\n",
      "Epoch: 52450 | Loss: 0.49349674582481384 | Test loss: 0.5148227214813232\n",
      "Epoch: 52460 | Loss: 0.4934849739074707 | Test loss: 0.5148088932037354\n",
      "Epoch: 52470 | Loss: 0.49347320199012756 | Test loss: 0.5147950053215027\n",
      "Epoch: 52480 | Loss: 0.4934615194797516 | Test loss: 0.5147811770439148\n",
      "Epoch: 52490 | Loss: 0.49344968795776367 | Test loss: 0.5147673487663269\n",
      "Epoch: 52500 | Loss: 0.4934379756450653 | Test loss: 0.5147534608840942\n",
      "Epoch: 52510 | Loss: 0.4934261739253998 | Test loss: 0.5147396326065063\n",
      "Epoch: 52520 | Loss: 0.49341440200805664 | Test loss: 0.5147258043289185\n",
      "Epoch: 52530 | Loss: 0.4934026896953583 | Test loss: 0.5147119760513306\n",
      "Epoch: 52540 | Loss: 0.4933909475803375 | Test loss: 0.5146981477737427\n",
      "Epoch: 52550 | Loss: 0.4933791160583496 | Test loss: 0.51468425989151\n",
      "Epoch: 52560 | Loss: 0.49336740374565125 | Test loss: 0.5146703720092773\n",
      "Epoch: 52570 | Loss: 0.4933556616306305 | Test loss: 0.5146565437316895\n",
      "Epoch: 52580 | Loss: 0.49334388971328735 | Test loss: 0.5146427154541016\n",
      "Epoch: 52590 | Loss: 0.4933321177959442 | Test loss: 0.5146288871765137\n",
      "Epoch: 52600 | Loss: 0.49332037568092346 | Test loss: 0.5146150588989258\n",
      "Epoch: 52610 | Loss: 0.4933086037635803 | Test loss: 0.5146011710166931\n",
      "Epoch: 52620 | Loss: 0.4932968318462372 | Test loss: 0.5145873427391052\n",
      "Epoch: 52630 | Loss: 0.49328508973121643 | Test loss: 0.5145735144615173\n",
      "Epoch: 52640 | Loss: 0.4932733178138733 | Test loss: 0.5145596861839294\n",
      "Epoch: 52650 | Loss: 0.49326154589653015 | Test loss: 0.5145458579063416\n",
      "Epoch: 52660 | Loss: 0.4932498037815094 | Test loss: 0.5145320296287537\n",
      "Epoch: 52670 | Loss: 0.49323803186416626 | Test loss: 0.5145180821418762\n",
      "Epoch: 52680 | Loss: 0.4932262897491455 | Test loss: 0.5145042538642883\n",
      "Epoch: 52690 | Loss: 0.49321451783180237 | Test loss: 0.5144904255867004\n",
      "Epoch: 52700 | Loss: 0.4932027757167816 | Test loss: 0.5144765377044678\n",
      "Epoch: 52710 | Loss: 0.4931910037994385 | Test loss: 0.5144627094268799\n",
      "Epoch: 52720 | Loss: 0.49317923188209534 | Test loss: 0.514448881149292\n",
      "Epoch: 52730 | Loss: 0.4931674599647522 | Test loss: 0.5144349932670593\n",
      "Epoch: 52740 | Loss: 0.49315571784973145 | Test loss: 0.5144211649894714\n",
      "Epoch: 52750 | Loss: 0.4931439459323883 | Test loss: 0.5144073367118835\n",
      "Epoch: 52760 | Loss: 0.49313217401504517 | Test loss: 0.5143934488296509\n",
      "Epoch: 52770 | Loss: 0.4931204319000244 | Test loss: 0.5143796801567078\n",
      "Epoch: 52780 | Loss: 0.4931086599826813 | Test loss: 0.5143657922744751\n",
      "Epoch: 52790 | Loss: 0.4930969178676605 | Test loss: 0.5143519639968872\n",
      "Epoch: 52800 | Loss: 0.4930851459503174 | Test loss: 0.5143381357192993\n",
      "Epoch: 52810 | Loss: 0.493073433637619 | Test loss: 0.5143243074417114\n",
      "Epoch: 52820 | Loss: 0.4930616319179535 | Test loss: 0.514310359954834\n",
      "Epoch: 52830 | Loss: 0.49304986000061035 | Test loss: 0.5142965316772461\n",
      "Epoch: 52840 | Loss: 0.493038147687912 | Test loss: 0.5142827033996582\n",
      "Epoch: 52850 | Loss: 0.49302634596824646 | Test loss: 0.5142688751220703\n",
      "Epoch: 52860 | Loss: 0.4930146336555481 | Test loss: 0.5142550468444824\n",
      "Epoch: 52870 | Loss: 0.4930028021335602 | Test loss: 0.5142411589622498\n",
      "Epoch: 52880 | Loss: 0.4929911196231842 | Test loss: 0.5142273306846619\n",
      "Epoch: 52890 | Loss: 0.4929792881011963 | Test loss: 0.514213502407074\n",
      "Epoch: 52900 | Loss: 0.49296751618385315 | Test loss: 0.5141996741294861\n",
      "Epoch: 52910 | Loss: 0.4929557740688324 | Test loss: 0.5141858458518982\n",
      "Epoch: 52920 | Loss: 0.49294406175613403 | Test loss: 0.5141720175743103\n",
      "Epoch: 52930 | Loss: 0.4929322898387909 | Test loss: 0.5141581296920776\n",
      "Epoch: 52940 | Loss: 0.49292048811912537 | Test loss: 0.514144241809845\n",
      "Epoch: 52950 | Loss: 0.492908775806427 | Test loss: 0.5141304135322571\n",
      "Epoch: 52960 | Loss: 0.49289700388908386 | Test loss: 0.5141165256500244\n",
      "Epoch: 52970 | Loss: 0.49288520216941833 | Test loss: 0.5141027569770813\n",
      "Epoch: 52980 | Loss: 0.49287348985671997 | Test loss: 0.5140888690948486\n",
      "Epoch: 52990 | Loss: 0.49286171793937683 | Test loss: 0.514074981212616\n",
      "Epoch: 53000 | Loss: 0.4928499162197113 | Test loss: 0.5140611529350281\n",
      "Epoch: 53010 | Loss: 0.49283820390701294 | Test loss: 0.5140473246574402\n",
      "Epoch: 53020 | Loss: 0.4928264319896698 | Test loss: 0.5140334963798523\n",
      "Epoch: 53030 | Loss: 0.49281468987464905 | Test loss: 0.5140196681022644\n",
      "Epoch: 53040 | Loss: 0.4928029179573059 | Test loss: 0.5140057802200317\n",
      "Epoch: 53050 | Loss: 0.49279117584228516 | Test loss: 0.5139919519424438\n",
      "Epoch: 53060 | Loss: 0.49277934432029724 | Test loss: 0.513978123664856\n",
      "Epoch: 53070 | Loss: 0.4927676320075989 | Test loss: 0.5139642953872681\n",
      "Epoch: 53080 | Loss: 0.4927558898925781 | Test loss: 0.5139504075050354\n",
      "Epoch: 53090 | Loss: 0.492744117975235 | Test loss: 0.5139365792274475\n",
      "Epoch: 53100 | Loss: 0.49273234605789185 | Test loss: 0.5139226913452148\n",
      "Epoch: 53110 | Loss: 0.4927206039428711 | Test loss: 0.513908863067627\n",
      "Epoch: 53120 | Loss: 0.49270883202552795 | Test loss: 0.5138950347900391\n",
      "Epoch: 53130 | Loss: 0.4926970601081848 | Test loss: 0.5138812065124512\n",
      "Epoch: 53140 | Loss: 0.49268531799316406 | Test loss: 0.5138673186302185\n",
      "Epoch: 53150 | Loss: 0.4926736056804657 | Test loss: 0.5138534903526306\n",
      "Epoch: 53160 | Loss: 0.4926617741584778 | Test loss: 0.5138396620750427\n",
      "Epoch: 53170 | Loss: 0.49265003204345703 | Test loss: 0.5138258337974548\n",
      "Epoch: 53180 | Loss: 0.49263831973075867 | Test loss: 0.5138120055198669\n",
      "Epoch: 53190 | Loss: 0.49262648820877075 | Test loss: 0.5137981176376343\n",
      "Epoch: 53200 | Loss: 0.49261474609375 | Test loss: 0.5137842893600464\n",
      "Epoch: 53210 | Loss: 0.49260303378105164 | Test loss: 0.5137704014778137\n",
      "Epoch: 53220 | Loss: 0.4925912320613861 | Test loss: 0.5137565732002258\n",
      "Epoch: 53230 | Loss: 0.49257946014404297 | Test loss: 0.5137427449226379\n",
      "Epoch: 53240 | Loss: 0.4925677478313446 | Test loss: 0.5137288570404053\n",
      "Epoch: 53250 | Loss: 0.4925559461116791 | Test loss: 0.5137150287628174\n",
      "Epoch: 53260 | Loss: 0.49254417419433594 | Test loss: 0.5137012004852295\n",
      "Epoch: 53270 | Loss: 0.4925324618816376 | Test loss: 0.5136873126029968\n",
      "Epoch: 53280 | Loss: 0.4925207197666168 | Test loss: 0.5136734843254089\n",
      "Epoch: 53290 | Loss: 0.4925088882446289 | Test loss: 0.513659656047821\n",
      "Epoch: 53300 | Loss: 0.49249717593193054 | Test loss: 0.5136457681655884\n",
      "Epoch: 53310 | Loss: 0.492485374212265 | Test loss: 0.5136319398880005\n",
      "Epoch: 53320 | Loss: 0.49247366189956665 | Test loss: 0.5136181116104126\n",
      "Epoch: 53330 | Loss: 0.4924618899822235 | Test loss: 0.5136042833328247\n",
      "Epoch: 53340 | Loss: 0.49245014786720276 | Test loss: 0.5135904550552368\n",
      "Epoch: 53350 | Loss: 0.49243831634521484 | Test loss: 0.5135765671730042\n",
      "Epoch: 53360 | Loss: 0.4924266040325165 | Test loss: 0.5135626792907715\n",
      "Epoch: 53370 | Loss: 0.4924148619174957 | Test loss: 0.5135488510131836\n",
      "Epoch: 53380 | Loss: 0.4924030900001526 | Test loss: 0.5135350227355957\n",
      "Epoch: 53390 | Loss: 0.49239131808280945 | Test loss: 0.5135211944580078\n",
      "Epoch: 53400 | Loss: 0.4923795759677887 | Test loss: 0.5135073661804199\n",
      "Epoch: 53410 | Loss: 0.49236780405044556 | Test loss: 0.5134934782981873\n",
      "Epoch: 53420 | Loss: 0.4923560321331024 | Test loss: 0.5134796500205994\n",
      "Epoch: 53430 | Loss: 0.49234429001808167 | Test loss: 0.5134658217430115\n",
      "Epoch: 53440 | Loss: 0.4923325181007385 | Test loss: 0.5134519934654236\n",
      "Epoch: 53450 | Loss: 0.4923207461833954 | Test loss: 0.5134381651878357\n",
      "Epoch: 53460 | Loss: 0.49230900406837463 | Test loss: 0.5134243369102478\n",
      "Epoch: 53470 | Loss: 0.4922972321510315 | Test loss: 0.5134103894233704\n",
      "Epoch: 53480 | Loss: 0.49228549003601074 | Test loss: 0.5133965611457825\n",
      "Epoch: 53490 | Loss: 0.4922737181186676 | Test loss: 0.5133827328681946\n",
      "Epoch: 53500 | Loss: 0.49226197600364685 | Test loss: 0.5133688449859619\n",
      "Epoch: 53510 | Loss: 0.4922502040863037 | Test loss: 0.513355016708374\n",
      "Epoch: 53520 | Loss: 0.49223843216896057 | Test loss: 0.5133411884307861\n",
      "Epoch: 53530 | Loss: 0.49222666025161743 | Test loss: 0.5133273005485535\n",
      "Epoch: 53540 | Loss: 0.4922149181365967 | Test loss: 0.5133134722709656\n",
      "Epoch: 53550 | Loss: 0.49220314621925354 | Test loss: 0.5132996439933777\n",
      "Epoch: 53560 | Loss: 0.4921913743019104 | Test loss: 0.513285756111145\n",
      "Epoch: 53570 | Loss: 0.49217963218688965 | Test loss: 0.5132719874382019\n",
      "Epoch: 53580 | Loss: 0.4921678602695465 | Test loss: 0.5132580995559692\n",
      "Epoch: 53590 | Loss: 0.49215611815452576 | Test loss: 0.5132442712783813\n",
      "Epoch: 53600 | Loss: 0.4921443462371826 | Test loss: 0.5132304430007935\n",
      "Epoch: 53610 | Loss: 0.49213263392448425 | Test loss: 0.5132166147232056\n",
      "Epoch: 53620 | Loss: 0.4921208322048187 | Test loss: 0.5132026672363281\n",
      "Epoch: 53630 | Loss: 0.4921090602874756 | Test loss: 0.5131888389587402\n",
      "Epoch: 53640 | Loss: 0.4920973479747772 | Test loss: 0.5131750106811523\n",
      "Epoch: 53650 | Loss: 0.4920855462551117 | Test loss: 0.5131611824035645\n",
      "Epoch: 53660 | Loss: 0.49207383394241333 | Test loss: 0.5131473541259766\n",
      "Epoch: 53670 | Loss: 0.4920620024204254 | Test loss: 0.5131334662437439\n",
      "Epoch: 53680 | Loss: 0.49205031991004944 | Test loss: 0.513119637966156\n",
      "Epoch: 53690 | Loss: 0.4920384883880615 | Test loss: 0.5131058096885681\n",
      "Epoch: 53700 | Loss: 0.4920267164707184 | Test loss: 0.5130919814109802\n",
      "Epoch: 53710 | Loss: 0.49201497435569763 | Test loss: 0.5130781531333923\n",
      "Epoch: 53720 | Loss: 0.49200326204299927 | Test loss: 0.5130643248558044\n",
      "Epoch: 53730 | Loss: 0.49199149012565613 | Test loss: 0.5130504369735718\n",
      "Epoch: 53740 | Loss: 0.4919796884059906 | Test loss: 0.5130365490913391\n",
      "Epoch: 53750 | Loss: 0.49196797609329224 | Test loss: 0.5130227208137512\n",
      "Epoch: 53760 | Loss: 0.4919562041759491 | Test loss: 0.5130088329315186\n",
      "Epoch: 53770 | Loss: 0.49194440245628357 | Test loss: 0.5129950642585754\n",
      "Epoch: 53780 | Loss: 0.4919326901435852 | Test loss: 0.5129811763763428\n",
      "Epoch: 53790 | Loss: 0.49192091822624207 | Test loss: 0.5129672884941101\n",
      "Epoch: 53800 | Loss: 0.49190911650657654 | Test loss: 0.5129534602165222\n",
      "Epoch: 53810 | Loss: 0.4918974041938782 | Test loss: 0.5129396319389343\n",
      "Epoch: 53820 | Loss: 0.49188563227653503 | Test loss: 0.5129258036613464\n",
      "Epoch: 53830 | Loss: 0.4918738901615143 | Test loss: 0.5129119753837585\n",
      "Epoch: 53840 | Loss: 0.49186211824417114 | Test loss: 0.5128980875015259\n",
      "Epoch: 53850 | Loss: 0.4918503761291504 | Test loss: 0.512884259223938\n",
      "Epoch: 53860 | Loss: 0.4918385446071625 | Test loss: 0.5128704309463501\n",
      "Epoch: 53870 | Loss: 0.4918268322944641 | Test loss: 0.5128566026687622\n",
      "Epoch: 53880 | Loss: 0.49181509017944336 | Test loss: 0.5128427147865295\n",
      "Epoch: 53890 | Loss: 0.4918033182621002 | Test loss: 0.5128288865089417\n",
      "Epoch: 53900 | Loss: 0.4917915463447571 | Test loss: 0.512814998626709\n",
      "Epoch: 53910 | Loss: 0.49177980422973633 | Test loss: 0.5128011703491211\n",
      "Epoch: 53920 | Loss: 0.4917680323123932 | Test loss: 0.5127873420715332\n",
      "Epoch: 53930 | Loss: 0.49175626039505005 | Test loss: 0.5127735137939453\n",
      "Epoch: 53940 | Loss: 0.4917445182800293 | Test loss: 0.5127596259117126\n",
      "Epoch: 53950 | Loss: 0.49173280596733093 | Test loss: 0.5127457976341248\n",
      "Epoch: 53960 | Loss: 0.491720974445343 | Test loss: 0.5127319693565369\n",
      "Epoch: 53970 | Loss: 0.49170923233032227 | Test loss: 0.512718141078949\n",
      "Epoch: 53980 | Loss: 0.4916975200176239 | Test loss: 0.5127043128013611\n",
      "Epoch: 53990 | Loss: 0.491685688495636 | Test loss: 0.5126904249191284\n",
      "Epoch: 54000 | Loss: 0.49167394638061523 | Test loss: 0.5126765966415405\n",
      "Epoch: 54010 | Loss: 0.49166223406791687 | Test loss: 0.5126627087593079\n",
      "Epoch: 54020 | Loss: 0.49165043234825134 | Test loss: 0.51264888048172\n",
      "Epoch: 54030 | Loss: 0.4916386604309082 | Test loss: 0.5126350522041321\n",
      "Epoch: 54040 | Loss: 0.49162694811820984 | Test loss: 0.5126211643218994\n",
      "Epoch: 54050 | Loss: 0.4916151463985443 | Test loss: 0.5126073360443115\n",
      "Epoch: 54060 | Loss: 0.49160337448120117 | Test loss: 0.5125935077667236\n",
      "Epoch: 54070 | Loss: 0.4915916621685028 | Test loss: 0.512579619884491\n",
      "Epoch: 54080 | Loss: 0.49157992005348206 | Test loss: 0.5125657916069031\n",
      "Epoch: 54090 | Loss: 0.49156808853149414 | Test loss: 0.5125519633293152\n",
      "Epoch: 54100 | Loss: 0.4915563762187958 | Test loss: 0.5125380754470825\n",
      "Epoch: 54110 | Loss: 0.49154457449913025 | Test loss: 0.5125242471694946\n",
      "Epoch: 54120 | Loss: 0.4915328025817871 | Test loss: 0.5125104188919067\n",
      "Epoch: 54130 | Loss: 0.49152109026908875 | Test loss: 0.5124965906143188\n",
      "Epoch: 54140 | Loss: 0.4915092885494232 | Test loss: 0.512482762336731\n",
      "Epoch: 54150 | Loss: 0.4914975166320801 | Test loss: 0.5124688744544983\n",
      "Epoch: 54160 | Loss: 0.4914858043193817 | Test loss: 0.5124549865722656\n",
      "Epoch: 54170 | Loss: 0.49147406220436096 | Test loss: 0.5124411582946777\n",
      "Epoch: 54180 | Loss: 0.4914622902870178 | Test loss: 0.5124273300170898\n",
      "Epoch: 54190 | Loss: 0.4914505183696747 | Test loss: 0.512413501739502\n",
      "Epoch: 54200 | Loss: 0.49143877625465393 | Test loss: 0.5123996734619141\n",
      "Epoch: 54210 | Loss: 0.4914270043373108 | Test loss: 0.5123857855796814\n",
      "Epoch: 54220 | Loss: 0.49141523241996765 | Test loss: 0.5123719573020935\n",
      "Epoch: 54230 | Loss: 0.4914034903049469 | Test loss: 0.5123581290245056\n",
      "Epoch: 54240 | Loss: 0.49139171838760376 | Test loss: 0.5123443007469177\n",
      "Epoch: 54250 | Loss: 0.4913799464702606 | Test loss: 0.5123304724693298\n",
      "Epoch: 54260 | Loss: 0.49136820435523987 | Test loss: 0.5123166441917419\n",
      "Epoch: 54270 | Loss: 0.49135643243789673 | Test loss: 0.5123026967048645\n",
      "Epoch: 54280 | Loss: 0.4913446605205536 | Test loss: 0.5122888684272766\n",
      "Epoch: 54290 | Loss: 0.49133291840553284 | Test loss: 0.5122750401496887\n",
      "Epoch: 54300 | Loss: 0.4913211762905121 | Test loss: 0.512261152267456\n",
      "Epoch: 54310 | Loss: 0.49130940437316895 | Test loss: 0.5122473239898682\n",
      "Epoch: 54320 | Loss: 0.4912976324558258 | Test loss: 0.5122334957122803\n",
      "Epoch: 54330 | Loss: 0.49128586053848267 | Test loss: 0.5122196078300476\n",
      "Epoch: 54340 | Loss: 0.4912741184234619 | Test loss: 0.5122057795524597\n",
      "Epoch: 54350 | Loss: 0.4912623465061188 | Test loss: 0.5121919512748718\n",
      "Epoch: 54360 | Loss: 0.49125057458877563 | Test loss: 0.5121780633926392\n",
      "Epoch: 54370 | Loss: 0.4912388324737549 | Test loss: 0.512164294719696\n",
      "Epoch: 54380 | Loss: 0.49122706055641174 | Test loss: 0.5121504068374634\n",
      "Epoch: 54390 | Loss: 0.491215318441391 | Test loss: 0.5121365785598755\n",
      "Epoch: 54400 | Loss: 0.49120354652404785 | Test loss: 0.5121227502822876\n",
      "Epoch: 54410 | Loss: 0.4911918342113495 | Test loss: 0.5121089220046997\n",
      "Epoch: 54420 | Loss: 0.49118003249168396 | Test loss: 0.5120949745178223\n",
      "Epoch: 54430 | Loss: 0.4911682605743408 | Test loss: 0.5120811462402344\n",
      "Epoch: 54440 | Loss: 0.49115654826164246 | Test loss: 0.5120673179626465\n",
      "Epoch: 54450 | Loss: 0.49114474654197693 | Test loss: 0.5120534896850586\n",
      "Epoch: 54460 | Loss: 0.49113303422927856 | Test loss: 0.5120396614074707\n",
      "Epoch: 54470 | Loss: 0.49112120270729065 | Test loss: 0.512025773525238\n",
      "Epoch: 54480 | Loss: 0.4911094605922699 | Test loss: 0.5120119452476501\n",
      "Epoch: 54490 | Loss: 0.49109768867492676 | Test loss: 0.5119981169700623\n",
      "Epoch: 54500 | Loss: 0.4910859167575836 | Test loss: 0.5119842886924744\n",
      "Epoch: 54510 | Loss: 0.49107417464256287 | Test loss: 0.5119704604148865\n",
      "Epoch: 54520 | Loss: 0.4910624623298645 | Test loss: 0.5119566321372986\n",
      "Epoch: 54530 | Loss: 0.49105069041252136 | Test loss: 0.5119427442550659\n",
      "Epoch: 54540 | Loss: 0.49103888869285583 | Test loss: 0.5119288563728333\n",
      "Epoch: 54550 | Loss: 0.49102717638015747 | Test loss: 0.5119150280952454\n",
      "Epoch: 54560 | Loss: 0.49101540446281433 | Test loss: 0.5119011402130127\n",
      "Epoch: 54570 | Loss: 0.4910036027431488 | Test loss: 0.5118873715400696\n",
      "Epoch: 54580 | Loss: 0.49099189043045044 | Test loss: 0.5118734836578369\n",
      "Epoch: 54590 | Loss: 0.4909801185131073 | Test loss: 0.5118595957756042\n",
      "Epoch: 54600 | Loss: 0.4909683167934418 | Test loss: 0.5118457674980164\n",
      "Epoch: 54610 | Loss: 0.4909566044807434 | Test loss: 0.5118319392204285\n",
      "Epoch: 54620 | Loss: 0.49094483256340027 | Test loss: 0.5118181109428406\n",
      "Epoch: 54630 | Loss: 0.4909330904483795 | Test loss: 0.5118042826652527\n",
      "Epoch: 54640 | Loss: 0.4909213185310364 | Test loss: 0.51179039478302\n",
      "Epoch: 54650 | Loss: 0.4909095764160156 | Test loss: 0.5117765665054321\n",
      "Epoch: 54660 | Loss: 0.4908977448940277 | Test loss: 0.5117627382278442\n",
      "Epoch: 54670 | Loss: 0.49088603258132935 | Test loss: 0.5117489099502563\n",
      "Epoch: 54680 | Loss: 0.4908742904663086 | Test loss: 0.5117350220680237\n",
      "Epoch: 54690 | Loss: 0.49086251854896545 | Test loss: 0.5117211937904358\n",
      "Epoch: 54700 | Loss: 0.4908507466316223 | Test loss: 0.5117073059082031\n",
      "Epoch: 54710 | Loss: 0.49083900451660156 | Test loss: 0.5116934776306152\n",
      "Epoch: 54720 | Loss: 0.4908272325992584 | Test loss: 0.5116796493530273\n",
      "Epoch: 54730 | Loss: 0.4908154606819153 | Test loss: 0.5116658210754395\n",
      "Epoch: 54740 | Loss: 0.49080371856689453 | Test loss: 0.5116519331932068\n",
      "Epoch: 54750 | Loss: 0.49079200625419617 | Test loss: 0.5116381049156189\n",
      "Epoch: 54760 | Loss: 0.49078017473220825 | Test loss: 0.511624276638031\n",
      "Epoch: 54770 | Loss: 0.4907684326171875 | Test loss: 0.5116104483604431\n",
      "Epoch: 54780 | Loss: 0.49075672030448914 | Test loss: 0.5115966200828552\n",
      "Epoch: 54790 | Loss: 0.4907448887825012 | Test loss: 0.5115827322006226\n",
      "Epoch: 54800 | Loss: 0.49073314666748047 | Test loss: 0.5115689039230347\n",
      "Epoch: 54810 | Loss: 0.4907214343547821 | Test loss: 0.511555016040802\n",
      "Epoch: 54820 | Loss: 0.4907096326351166 | Test loss: 0.5115411877632141\n",
      "Epoch: 54830 | Loss: 0.49069786071777344 | Test loss: 0.5115273594856262\n",
      "Epoch: 54840 | Loss: 0.4906861484050751 | Test loss: 0.5115134716033936\n",
      "Epoch: 54850 | Loss: 0.49067434668540955 | Test loss: 0.5114996433258057\n",
      "Epoch: 54860 | Loss: 0.4906625747680664 | Test loss: 0.5114858150482178\n",
      "Epoch: 54870 | Loss: 0.49065086245536804 | Test loss: 0.5114719271659851\n",
      "Epoch: 54880 | Loss: 0.4906391203403473 | Test loss: 0.5114580988883972\n",
      "Epoch: 54890 | Loss: 0.4906272888183594 | Test loss: 0.5114442706108093\n",
      "Epoch: 54900 | Loss: 0.490615576505661 | Test loss: 0.5114303827285767\n",
      "Epoch: 54910 | Loss: 0.4906037747859955 | Test loss: 0.5114165544509888\n",
      "Epoch: 54920 | Loss: 0.4905920624732971 | Test loss: 0.5114027261734009\n",
      "Epoch: 54930 | Loss: 0.490580290555954 | Test loss: 0.511388897895813\n",
      "Epoch: 54940 | Loss: 0.49056848883628845 | Test loss: 0.5113750696182251\n",
      "Epoch: 54950 | Loss: 0.4905567169189453 | Test loss: 0.5113611817359924\n",
      "Epoch: 54960 | Loss: 0.49054500460624695 | Test loss: 0.5113472938537598\n",
      "Epoch: 54970 | Loss: 0.4905332624912262 | Test loss: 0.5113334655761719\n",
      "Epoch: 54980 | Loss: 0.49052149057388306 | Test loss: 0.511319637298584\n",
      "Epoch: 54990 | Loss: 0.4905097186565399 | Test loss: 0.5113058090209961\n",
      "Epoch: 55000 | Loss: 0.49049797654151917 | Test loss: 0.5112919807434082\n",
      "Epoch: 55010 | Loss: 0.490486204624176 | Test loss: 0.5112780928611755\n",
      "Epoch: 55020 | Loss: 0.4904744327068329 | Test loss: 0.5112642645835876\n",
      "Epoch: 55030 | Loss: 0.49046269059181213 | Test loss: 0.5112504363059998\n",
      "Epoch: 55040 | Loss: 0.490450918674469 | Test loss: 0.5112366080284119\n",
      "Epoch: 55050 | Loss: 0.49043914675712585 | Test loss: 0.511222779750824\n",
      "Epoch: 55060 | Loss: 0.4904274046421051 | Test loss: 0.5112089514732361\n",
      "Epoch: 55070 | Loss: 0.49041563272476196 | Test loss: 0.5111950039863586\n",
      "Epoch: 55080 | Loss: 0.4904038608074188 | Test loss: 0.5111811757087708\n",
      "Epoch: 55090 | Loss: 0.49039211869239807 | Test loss: 0.5111673474311829\n",
      "Epoch: 55100 | Loss: 0.4903803765773773 | Test loss: 0.5111534595489502\n",
      "Epoch: 55110 | Loss: 0.4903685748577118 | Test loss: 0.5111396312713623\n",
      "Epoch: 55120 | Loss: 0.49035683274269104 | Test loss: 0.5111258029937744\n",
      "Epoch: 55130 | Loss: 0.4903450608253479 | Test loss: 0.5111119151115417\n",
      "Epoch: 55140 | Loss: 0.49033331871032715 | Test loss: 0.5110980868339539\n",
      "Epoch: 55150 | Loss: 0.490321546792984 | Test loss: 0.511084258556366\n",
      "Epoch: 55160 | Loss: 0.49030980467796326 | Test loss: 0.5110703706741333\n",
      "Epoch: 55170 | Loss: 0.4902980327606201 | Test loss: 0.5110566020011902\n",
      "Epoch: 55180 | Loss: 0.490286260843277 | Test loss: 0.5110427141189575\n",
      "Epoch: 55190 | Loss: 0.4902745187282562 | Test loss: 0.5110288858413696\n",
      "Epoch: 55200 | Loss: 0.4902627468109131 | Test loss: 0.5110150575637817\n",
      "Epoch: 55210 | Loss: 0.4902510344982147 | Test loss: 0.5110012292861938\n",
      "Epoch: 55220 | Loss: 0.4902392327785492 | Test loss: 0.5109872817993164\n",
      "Epoch: 55230 | Loss: 0.49022746086120605 | Test loss: 0.5109734535217285\n",
      "Epoch: 55240 | Loss: 0.4902157485485077 | Test loss: 0.5109596252441406\n",
      "Epoch: 55250 | Loss: 0.49020394682884216 | Test loss: 0.5109457969665527\n",
      "Epoch: 55260 | Loss: 0.4901922345161438 | Test loss: 0.5109319686889648\n",
      "Epoch: 55270 | Loss: 0.4901804029941559 | Test loss: 0.5109180808067322\n",
      "Epoch: 55280 | Loss: 0.49016866087913513 | Test loss: 0.5109042525291443\n",
      "Epoch: 55290 | Loss: 0.490156888961792 | Test loss: 0.5108904242515564\n",
      "Epoch: 55300 | Loss: 0.49014511704444885 | Test loss: 0.5108765959739685\n",
      "Epoch: 55310 | Loss: 0.4901333749294281 | Test loss: 0.5108627676963806\n",
      "Epoch: 55320 | Loss: 0.49012166261672974 | Test loss: 0.5108489394187927\n",
      "Epoch: 55330 | Loss: 0.4901098906993866 | Test loss: 0.5108350515365601\n",
      "Epoch: 55340 | Loss: 0.49009808897972107 | Test loss: 0.5108211636543274\n",
      "Epoch: 55350 | Loss: 0.4900863766670227 | Test loss: 0.5108073353767395\n",
      "Epoch: 55360 | Loss: 0.49007460474967957 | Test loss: 0.5107934474945068\n",
      "Epoch: 55370 | Loss: 0.49006280303001404 | Test loss: 0.5107796788215637\n",
      "Epoch: 55380 | Loss: 0.4900510907173157 | Test loss: 0.510765790939331\n",
      "Epoch: 55390 | Loss: 0.49003931879997253 | Test loss: 0.5107519030570984\n",
      "Epoch: 55400 | Loss: 0.490027517080307 | Test loss: 0.5107380747795105\n",
      "Epoch: 55410 | Loss: 0.49001580476760864 | Test loss: 0.5107242465019226\n",
      "Epoch: 55420 | Loss: 0.4900040328502655 | Test loss: 0.5107104182243347\n",
      "Epoch: 55430 | Loss: 0.48999229073524475 | Test loss: 0.5106965899467468\n",
      "Epoch: 55440 | Loss: 0.4899805188179016 | Test loss: 0.5106827020645142\n",
      "Epoch: 55450 | Loss: 0.48996877670288086 | Test loss: 0.5106688737869263\n",
      "Epoch: 55460 | Loss: 0.48995694518089294 | Test loss: 0.5106550455093384\n",
      "Epoch: 55470 | Loss: 0.4899452328681946 | Test loss: 0.5106412172317505\n",
      "Epoch: 55480 | Loss: 0.48993349075317383 | Test loss: 0.5106273293495178\n",
      "Epoch: 55490 | Loss: 0.4899217188358307 | Test loss: 0.5106135010719299\n",
      "Epoch: 55500 | Loss: 0.48990994691848755 | Test loss: 0.5105996131896973\n",
      "Epoch: 55510 | Loss: 0.4898982048034668 | Test loss: 0.5105857849121094\n",
      "Epoch: 55520 | Loss: 0.48988643288612366 | Test loss: 0.5105719566345215\n",
      "Epoch: 55530 | Loss: 0.4898746609687805 | Test loss: 0.5105581283569336\n",
      "Epoch: 55540 | Loss: 0.48986291885375977 | Test loss: 0.5105442404747009\n",
      "Epoch: 55550 | Loss: 0.4898512065410614 | Test loss: 0.510530412197113\n",
      "Epoch: 55560 | Loss: 0.4898393750190735 | Test loss: 0.5105165839195251\n",
      "Epoch: 55570 | Loss: 0.48982763290405273 | Test loss: 0.5105027556419373\n",
      "Epoch: 55580 | Loss: 0.48981592059135437 | Test loss: 0.5104889273643494\n",
      "Epoch: 55590 | Loss: 0.48980408906936646 | Test loss: 0.5104750394821167\n",
      "Epoch: 55600 | Loss: 0.4897923469543457 | Test loss: 0.5104612112045288\n",
      "Epoch: 55610 | Loss: 0.48978063464164734 | Test loss: 0.5104473233222961\n",
      "Epoch: 55620 | Loss: 0.4897688329219818 | Test loss: 0.5104334950447083\n",
      "Epoch: 55630 | Loss: 0.48975706100463867 | Test loss: 0.5104196667671204\n",
      "Epoch: 55640 | Loss: 0.4897453486919403 | Test loss: 0.5104057788848877\n",
      "Epoch: 55650 | Loss: 0.4897335469722748 | Test loss: 0.5103919506072998\n",
      "Epoch: 55660 | Loss: 0.48972177505493164 | Test loss: 0.5103781223297119\n",
      "Epoch: 55670 | Loss: 0.4897100627422333 | Test loss: 0.5103642344474792\n",
      "Epoch: 55680 | Loss: 0.4896983206272125 | Test loss: 0.5103504061698914\n",
      "Epoch: 55690 | Loss: 0.4896864891052246 | Test loss: 0.5103365778923035\n",
      "Epoch: 55700 | Loss: 0.48967477679252625 | Test loss: 0.5103226900100708\n",
      "Epoch: 55710 | Loss: 0.4896629750728607 | Test loss: 0.5103088617324829\n",
      "Epoch: 55720 | Loss: 0.48965126276016235 | Test loss: 0.510295033454895\n",
      "Epoch: 55730 | Loss: 0.4896394908428192 | Test loss: 0.5102812051773071\n",
      "Epoch: 55740 | Loss: 0.4896276891231537 | Test loss: 0.5102673768997192\n",
      "Epoch: 55750 | Loss: 0.48961591720581055 | Test loss: 0.5102534890174866\n",
      "Epoch: 55760 | Loss: 0.4896042048931122 | Test loss: 0.5102396011352539\n",
      "Epoch: 55770 | Loss: 0.48959246277809143 | Test loss: 0.510225772857666\n",
      "Epoch: 55780 | Loss: 0.4895806908607483 | Test loss: 0.5102119445800781\n",
      "Epoch: 55790 | Loss: 0.48956891894340515 | Test loss: 0.5101981163024902\n",
      "Epoch: 55800 | Loss: 0.4895571768283844 | Test loss: 0.5101842880249023\n",
      "Epoch: 55810 | Loss: 0.48954540491104126 | Test loss: 0.5101704001426697\n",
      "Epoch: 55820 | Loss: 0.4895336329936981 | Test loss: 0.5101565718650818\n",
      "Epoch: 55830 | Loss: 0.48952189087867737 | Test loss: 0.5101427435874939\n",
      "Epoch: 55840 | Loss: 0.48951011896133423 | Test loss: 0.510128915309906\n",
      "Epoch: 55850 | Loss: 0.4894983470439911 | Test loss: 0.5101150870323181\n",
      "Epoch: 55860 | Loss: 0.48948660492897034 | Test loss: 0.5101012587547302\n",
      "Epoch: 55870 | Loss: 0.4894748330116272 | Test loss: 0.5100873112678528\n",
      "Epoch: 55880 | Loss: 0.48946306109428406 | Test loss: 0.5100734829902649\n",
      "Epoch: 55890 | Loss: 0.4894513189792633 | Test loss: 0.510059654712677\n",
      "Epoch: 55900 | Loss: 0.48943957686424255 | Test loss: 0.5100457668304443\n",
      "Epoch: 55910 | Loss: 0.489427775144577 | Test loss: 0.5100319385528564\n",
      "Epoch: 55920 | Loss: 0.4894160330295563 | Test loss: 0.5100181102752686\n",
      "Epoch: 55930 | Loss: 0.48940426111221313 | Test loss: 0.5100042223930359\n",
      "Epoch: 55940 | Loss: 0.4893925189971924 | Test loss: 0.509990394115448\n",
      "Epoch: 55950 | Loss: 0.48938074707984924 | Test loss: 0.5099765658378601\n",
      "Epoch: 55960 | Loss: 0.4893690049648285 | Test loss: 0.5099626779556274\n",
      "Epoch: 55970 | Loss: 0.48935723304748535 | Test loss: 0.5099489092826843\n",
      "Epoch: 55980 | Loss: 0.4893454611301422 | Test loss: 0.5099350214004517\n",
      "Epoch: 55990 | Loss: 0.48933371901512146 | Test loss: 0.5099211931228638\n",
      "Epoch: 56000 | Loss: 0.4893219470977783 | Test loss: 0.5099073648452759\n",
      "Epoch: 56010 | Loss: 0.48931023478507996 | Test loss: 0.509893536567688\n",
      "Epoch: 56020 | Loss: 0.48929843306541443 | Test loss: 0.5098795890808105\n",
      "Epoch: 56030 | Loss: 0.4892866611480713 | Test loss: 0.5098657608032227\n",
      "Epoch: 56040 | Loss: 0.4892749488353729 | Test loss: 0.5098519325256348\n",
      "Epoch: 56050 | Loss: 0.4892631471157074 | Test loss: 0.5098381042480469\n",
      "Epoch: 56060 | Loss: 0.48925143480300903 | Test loss: 0.509824275970459\n",
      "Epoch: 56070 | Loss: 0.4892396032810211 | Test loss: 0.5098103880882263\n",
      "Epoch: 56080 | Loss: 0.48922786116600037 | Test loss: 0.5097965598106384\n",
      "Epoch: 56090 | Loss: 0.4892160892486572 | Test loss: 0.5097827315330505\n",
      "Epoch: 56100 | Loss: 0.4892043173313141 | Test loss: 0.5097689032554626\n",
      "Epoch: 56110 | Loss: 0.48919257521629333 | Test loss: 0.5097550749778748\n",
      "Epoch: 56120 | Loss: 0.48918086290359497 | Test loss: 0.5097412467002869\n",
      "Epoch: 56130 | Loss: 0.48916909098625183 | Test loss: 0.5097273588180542\n",
      "Epoch: 56140 | Loss: 0.4891572892665863 | Test loss: 0.5097134709358215\n",
      "Epoch: 56150 | Loss: 0.48914557695388794 | Test loss: 0.5096996426582336\n",
      "Epoch: 56160 | Loss: 0.4891338050365448 | Test loss: 0.509685754776001\n",
      "Epoch: 56170 | Loss: 0.4891220033168793 | Test loss: 0.5096719861030579\n",
      "Epoch: 56180 | Loss: 0.4891102910041809 | Test loss: 0.5096580982208252\n",
      "Epoch: 56190 | Loss: 0.48909851908683777 | Test loss: 0.5096442103385925\n",
      "Epoch: 56200 | Loss: 0.48908671736717224 | Test loss: 0.5096303820610046\n",
      "Epoch: 56210 | Loss: 0.4890750050544739 | Test loss: 0.5096165537834167\n",
      "Epoch: 56220 | Loss: 0.48906323313713074 | Test loss: 0.5096027255058289\n",
      "Epoch: 56230 | Loss: 0.48905149102211 | Test loss: 0.509588897228241\n",
      "Epoch: 56240 | Loss: 0.48903971910476685 | Test loss: 0.5095750093460083\n",
      "Epoch: 56250 | Loss: 0.4890279769897461 | Test loss: 0.5095611810684204\n",
      "Epoch: 56260 | Loss: 0.4890161454677582 | Test loss: 0.5095473527908325\n",
      "Epoch: 56270 | Loss: 0.4890044331550598 | Test loss: 0.5095335245132446\n",
      "Epoch: 56280 | Loss: 0.48899269104003906 | Test loss: 0.509519636631012\n",
      "Epoch: 56290 | Loss: 0.4889809191226959 | Test loss: 0.5095058083534241\n",
      "Epoch: 56300 | Loss: 0.4889691472053528 | Test loss: 0.5094919204711914\n",
      "Epoch: 56310 | Loss: 0.48895740509033203 | Test loss: 0.5094780921936035\n",
      "Epoch: 56320 | Loss: 0.4889456331729889 | Test loss: 0.5094642639160156\n",
      "Epoch: 56330 | Loss: 0.48893386125564575 | Test loss: 0.5094504356384277\n",
      "Epoch: 56340 | Loss: 0.488922119140625 | Test loss: 0.5094365477561951\n",
      "Epoch: 56350 | Loss: 0.48891040682792664 | Test loss: 0.5094227194786072\n",
      "Epoch: 56360 | Loss: 0.4888985753059387 | Test loss: 0.5094088912010193\n",
      "Epoch: 56370 | Loss: 0.48888683319091797 | Test loss: 0.5093950629234314\n",
      "Epoch: 56380 | Loss: 0.4888751208782196 | Test loss: 0.5093812346458435\n",
      "Epoch: 56390 | Loss: 0.4888632893562317 | Test loss: 0.5093673467636108\n",
      "Epoch: 56400 | Loss: 0.48885154724121094 | Test loss: 0.509353518486023\n",
      "Epoch: 56410 | Loss: 0.4888398349285126 | Test loss: 0.5093396306037903\n",
      "Epoch: 56420 | Loss: 0.48882803320884705 | Test loss: 0.5093258023262024\n",
      "Epoch: 56430 | Loss: 0.4888162612915039 | Test loss: 0.5093119740486145\n",
      "Epoch: 56440 | Loss: 0.48880454897880554 | Test loss: 0.5092980861663818\n",
      "Epoch: 56450 | Loss: 0.48879274725914 | Test loss: 0.509284257888794\n",
      "Epoch: 56460 | Loss: 0.4887809753417969 | Test loss: 0.509270429611206\n",
      "Epoch: 56470 | Loss: 0.4887692630290985 | Test loss: 0.5092565417289734\n",
      "Epoch: 56480 | Loss: 0.48875752091407776 | Test loss: 0.5092427134513855\n",
      "Epoch: 56490 | Loss: 0.48874568939208984 | Test loss: 0.5092288851737976\n",
      "Epoch: 56500 | Loss: 0.4887339770793915 | Test loss: 0.5092149972915649\n",
      "Epoch: 56510 | Loss: 0.48872217535972595 | Test loss: 0.509201169013977\n",
      "Epoch: 56520 | Loss: 0.4887104630470276 | Test loss: 0.5091873407363892\n",
      "Epoch: 56530 | Loss: 0.48869869112968445 | Test loss: 0.5091735124588013\n",
      "Epoch: 56540 | Loss: 0.4886868894100189 | Test loss: 0.5091596841812134\n",
      "Epoch: 56550 | Loss: 0.4886751174926758 | Test loss: 0.5091457962989807\n",
      "Epoch: 56560 | Loss: 0.4886634051799774 | Test loss: 0.509131908416748\n",
      "Epoch: 56570 | Loss: 0.48865166306495667 | Test loss: 0.5091180801391602\n",
      "Epoch: 56580 | Loss: 0.4886398911476135 | Test loss: 0.5091042518615723\n",
      "Epoch: 56590 | Loss: 0.4886281192302704 | Test loss: 0.5090904235839844\n",
      "Epoch: 56600 | Loss: 0.48861637711524963 | Test loss: 0.5090765953063965\n",
      "Epoch: 56610 | Loss: 0.4886046051979065 | Test loss: 0.5090627074241638\n",
      "Epoch: 56620 | Loss: 0.48859283328056335 | Test loss: 0.5090488791465759\n",
      "Epoch: 56630 | Loss: 0.4885810911655426 | Test loss: 0.509035050868988\n",
      "Epoch: 56640 | Loss: 0.48856931924819946 | Test loss: 0.5090212225914001\n",
      "Epoch: 56650 | Loss: 0.4885575473308563 | Test loss: 0.5090073943138123\n",
      "Epoch: 56660 | Loss: 0.48854580521583557 | Test loss: 0.5089935660362244\n",
      "Epoch: 56670 | Loss: 0.48853403329849243 | Test loss: 0.5089796185493469\n",
      "Epoch: 56680 | Loss: 0.4885222613811493 | Test loss: 0.508965790271759\n",
      "Epoch: 56690 | Loss: 0.48851051926612854 | Test loss: 0.5089519619941711\n",
      "Epoch: 56700 | Loss: 0.4884987771511078 | Test loss: 0.5089380741119385\n",
      "Epoch: 56710 | Loss: 0.48848697543144226 | Test loss: 0.5089242458343506\n",
      "Epoch: 56720 | Loss: 0.4884752333164215 | Test loss: 0.5089104175567627\n",
      "Epoch: 56730 | Loss: 0.48846346139907837 | Test loss: 0.50889652967453\n",
      "Epoch: 56740 | Loss: 0.4884517192840576 | Test loss: 0.5088827013969421\n",
      "Epoch: 56750 | Loss: 0.4884399473667145 | Test loss: 0.5088688731193542\n",
      "Epoch: 56760 | Loss: 0.4884282052516937 | Test loss: 0.5088549852371216\n",
      "Epoch: 56770 | Loss: 0.4884164333343506 | Test loss: 0.5088412165641785\n",
      "Epoch: 56780 | Loss: 0.48840466141700745 | Test loss: 0.5088273286819458\n",
      "Epoch: 56790 | Loss: 0.4883929193019867 | Test loss: 0.5088135004043579\n",
      "Epoch: 56800 | Loss: 0.48838114738464355 | Test loss: 0.50879967212677\n",
      "Epoch: 56810 | Loss: 0.4883694350719452 | Test loss: 0.5087858438491821\n",
      "Epoch: 56820 | Loss: 0.48835763335227966 | Test loss: 0.5087718963623047\n",
      "Epoch: 56830 | Loss: 0.4883458614349365 | Test loss: 0.5087580680847168\n",
      "Epoch: 56840 | Loss: 0.48833414912223816 | Test loss: 0.5087442398071289\n",
      "Epoch: 56850 | Loss: 0.48832234740257263 | Test loss: 0.508730411529541\n",
      "Epoch: 56860 | Loss: 0.48831063508987427 | Test loss: 0.5087165832519531\n",
      "Epoch: 56870 | Loss: 0.48829880356788635 | Test loss: 0.5087026953697205\n",
      "Epoch: 56880 | Loss: 0.4882870614528656 | Test loss: 0.5086888670921326\n",
      "Epoch: 56890 | Loss: 0.48827528953552246 | Test loss: 0.5086750388145447\n",
      "Epoch: 56900 | Loss: 0.4882635176181793 | Test loss: 0.5086612105369568\n",
      "Epoch: 56910 | Loss: 0.48825177550315857 | Test loss: 0.5086473822593689\n",
      "Epoch: 56920 | Loss: 0.4882400631904602 | Test loss: 0.508633553981781\n",
      "Epoch: 56930 | Loss: 0.48822829127311707 | Test loss: 0.5086196660995483\n",
      "Epoch: 56940 | Loss: 0.48821648955345154 | Test loss: 0.5086057782173157\n",
      "Epoch: 56950 | Loss: 0.4882047772407532 | Test loss: 0.5085919499397278\n",
      "Epoch: 56960 | Loss: 0.48819300532341003 | Test loss: 0.5085780620574951\n",
      "Epoch: 56970 | Loss: 0.4881812036037445 | Test loss: 0.508564293384552\n",
      "Epoch: 56980 | Loss: 0.48816949129104614 | Test loss: 0.5085504055023193\n",
      "Epoch: 56990 | Loss: 0.488157719373703 | Test loss: 0.5085365176200867\n",
      "Epoch: 57000 | Loss: 0.4881459176540375 | Test loss: 0.5085226893424988\n",
      "Epoch: 57010 | Loss: 0.4881342053413391 | Test loss: 0.5085088610649109\n",
      "Epoch: 57020 | Loss: 0.48812243342399597 | Test loss: 0.508495032787323\n",
      "Epoch: 57030 | Loss: 0.4881106913089752 | Test loss: 0.5084812045097351\n",
      "Epoch: 57040 | Loss: 0.4880989193916321 | Test loss: 0.5084673166275024\n",
      "Epoch: 57050 | Loss: 0.48808717727661133 | Test loss: 0.5084534883499146\n",
      "Epoch: 57060 | Loss: 0.4880753457546234 | Test loss: 0.5084396600723267\n",
      "Epoch: 57070 | Loss: 0.48806363344192505 | Test loss: 0.5084258317947388\n",
      "Epoch: 57080 | Loss: 0.4880518913269043 | Test loss: 0.5084119439125061\n",
      "Epoch: 57090 | Loss: 0.48804011940956116 | Test loss: 0.5083981156349182\n",
      "Epoch: 57100 | Loss: 0.488028347492218 | Test loss: 0.5083842277526855\n",
      "Epoch: 57110 | Loss: 0.48801660537719727 | Test loss: 0.5083703994750977\n",
      "Epoch: 57120 | Loss: 0.4880048334598541 | Test loss: 0.5083565711975098\n",
      "Epoch: 57130 | Loss: 0.487993061542511 | Test loss: 0.5083427429199219\n",
      "Epoch: 57140 | Loss: 0.48798131942749023 | Test loss: 0.5083288550376892\n",
      "Epoch: 57150 | Loss: 0.48796960711479187 | Test loss: 0.5083150267601013\n",
      "Epoch: 57160 | Loss: 0.48795777559280396 | Test loss: 0.5083011984825134\n",
      "Epoch: 57170 | Loss: 0.4879460334777832 | Test loss: 0.5082873702049255\n",
      "Epoch: 57180 | Loss: 0.48793432116508484 | Test loss: 0.5082735419273376\n",
      "Epoch: 57190 | Loss: 0.4879224896430969 | Test loss: 0.508259654045105\n",
      "Epoch: 57200 | Loss: 0.48791074752807617 | Test loss: 0.5082458257675171\n",
      "Epoch: 57210 | Loss: 0.4878990352153778 | Test loss: 0.5082319378852844\n",
      "Epoch: 57220 | Loss: 0.4878872334957123 | Test loss: 0.5082181096076965\n",
      "Epoch: 57230 | Loss: 0.48787546157836914 | Test loss: 0.5082042813301086\n",
      "Epoch: 57240 | Loss: 0.4878637492656708 | Test loss: 0.508190393447876\n",
      "Epoch: 57250 | Loss: 0.48785194754600525 | Test loss: 0.5081765651702881\n",
      "Epoch: 57260 | Loss: 0.4878401756286621 | Test loss: 0.5081627368927002\n",
      "Epoch: 57270 | Loss: 0.48782846331596375 | Test loss: 0.5081488490104675\n",
      "Epoch: 57280 | Loss: 0.487816721200943 | Test loss: 0.5081350207328796\n",
      "Epoch: 57290 | Loss: 0.4878048896789551 | Test loss: 0.5081211924552917\n",
      "Epoch: 57300 | Loss: 0.4877931773662567 | Test loss: 0.5081073045730591\n",
      "Epoch: 57310 | Loss: 0.4877813756465912 | Test loss: 0.5080934762954712\n",
      "Epoch: 57320 | Loss: 0.4877696633338928 | Test loss: 0.5080796480178833\n",
      "Epoch: 57330 | Loss: 0.4877578914165497 | Test loss: 0.5080658197402954\n",
      "Epoch: 57340 | Loss: 0.48774608969688416 | Test loss: 0.5080519914627075\n",
      "Epoch: 57350 | Loss: 0.487734317779541 | Test loss: 0.5080381035804749\n",
      "Epoch: 57360 | Loss: 0.48772260546684265 | Test loss: 0.5080242156982422\n",
      "Epoch: 57370 | Loss: 0.4877108633518219 | Test loss: 0.5080103874206543\n",
      "Epoch: 57380 | Loss: 0.48769909143447876 | Test loss: 0.5079965591430664\n",
      "Epoch: 57390 | Loss: 0.4876873195171356 | Test loss: 0.5079827308654785\n",
      "Epoch: 57400 | Loss: 0.48767557740211487 | Test loss: 0.5079689025878906\n",
      "Epoch: 57410 | Loss: 0.48766380548477173 | Test loss: 0.507955014705658\n",
      "Epoch: 57420 | Loss: 0.4876520335674286 | Test loss: 0.5079411864280701\n",
      "Epoch: 57430 | Loss: 0.48764029145240784 | Test loss: 0.5079273581504822\n",
      "Epoch: 57440 | Loss: 0.4876285195350647 | Test loss: 0.5079135298728943\n",
      "Epoch: 57450 | Loss: 0.48761674761772156 | Test loss: 0.5078997015953064\n",
      "Epoch: 57460 | Loss: 0.4876050055027008 | Test loss: 0.5078858733177185\n",
      "Epoch: 57470 | Loss: 0.48759323358535767 | Test loss: 0.5078719258308411\n",
      "Epoch: 57480 | Loss: 0.4875814616680145 | Test loss: 0.5078580975532532\n",
      "Epoch: 57490 | Loss: 0.4875697195529938 | Test loss: 0.5078442692756653\n",
      "Epoch: 57500 | Loss: 0.487557977437973 | Test loss: 0.5078303813934326\n",
      "Epoch: 57510 | Loss: 0.4875461757183075 | Test loss: 0.5078165531158447\n",
      "Epoch: 57520 | Loss: 0.48753443360328674 | Test loss: 0.5078027248382568\n",
      "Epoch: 57530 | Loss: 0.4875226616859436 | Test loss: 0.5077888369560242\n",
      "Epoch: 57540 | Loss: 0.48751091957092285 | Test loss: 0.5077750086784363\n",
      "Epoch: 57550 | Loss: 0.4874991476535797 | Test loss: 0.5077611804008484\n",
      "Epoch: 57560 | Loss: 0.48748740553855896 | Test loss: 0.5077472925186157\n",
      "Epoch: 57570 | Loss: 0.4874756336212158 | Test loss: 0.5077335238456726\n",
      "Epoch: 57580 | Loss: 0.4874638617038727 | Test loss: 0.5077196359634399\n",
      "Epoch: 57590 | Loss: 0.48745211958885193 | Test loss: 0.507705807685852\n",
      "Epoch: 57600 | Loss: 0.4874403476715088 | Test loss: 0.5076919794082642\n",
      "Epoch: 57610 | Loss: 0.4874286353588104 | Test loss: 0.5076781511306763\n",
      "Epoch: 57620 | Loss: 0.4874168336391449 | Test loss: 0.5076642036437988\n",
      "Epoch: 57630 | Loss: 0.48740506172180176 | Test loss: 0.5076503753662109\n",
      "Epoch: 57640 | Loss: 0.4873933494091034 | Test loss: 0.507636547088623\n",
      "Epoch: 57650 | Loss: 0.48738154768943787 | Test loss: 0.5076227188110352\n",
      "Epoch: 57660 | Loss: 0.4873698353767395 | Test loss: 0.5076088905334473\n",
      "Epoch: 57670 | Loss: 0.4873580038547516 | Test loss: 0.5075950026512146\n",
      "Epoch: 57680 | Loss: 0.48734626173973083 | Test loss: 0.5075811743736267\n",
      "Epoch: 57690 | Loss: 0.4873344898223877 | Test loss: 0.5075673460960388\n",
      "Epoch: 57700 | Loss: 0.48732271790504456 | Test loss: 0.5075535178184509\n",
      "Epoch: 57710 | Loss: 0.4873109757900238 | Test loss: 0.507539689540863\n",
      "Epoch: 57720 | Loss: 0.48729926347732544 | Test loss: 0.5075258612632751\n",
      "Epoch: 57730 | Loss: 0.4872874915599823 | Test loss: 0.5075119733810425\n",
      "Epoch: 57740 | Loss: 0.4872756898403168 | Test loss: 0.5074980854988098\n",
      "Epoch: 57750 | Loss: 0.4872639775276184 | Test loss: 0.5074842572212219\n",
      "Epoch: 57760 | Loss: 0.48725220561027527 | Test loss: 0.5074703693389893\n",
      "Epoch: 57770 | Loss: 0.48724040389060974 | Test loss: 0.5074566006660461\n",
      "Epoch: 57780 | Loss: 0.4872286915779114 | Test loss: 0.5074427127838135\n",
      "Epoch: 57790 | Loss: 0.48721691966056824 | Test loss: 0.5074288249015808\n",
      "Epoch: 57800 | Loss: 0.4872051179409027 | Test loss: 0.5074149966239929\n",
      "Epoch: 57810 | Loss: 0.48719340562820435 | Test loss: 0.507401168346405\n",
      "Epoch: 57820 | Loss: 0.4871816337108612 | Test loss: 0.5073873400688171\n",
      "Epoch: 57830 | Loss: 0.48716989159584045 | Test loss: 0.5073735117912292\n",
      "Epoch: 57840 | Loss: 0.4871581196784973 | Test loss: 0.5073596239089966\n",
      "Epoch: 57850 | Loss: 0.48714637756347656 | Test loss: 0.5073457956314087\n",
      "Epoch: 57860 | Loss: 0.48713454604148865 | Test loss: 0.5073319673538208\n",
      "Epoch: 57870 | Loss: 0.4871228337287903 | Test loss: 0.5073181390762329\n",
      "Epoch: 57880 | Loss: 0.48711109161376953 | Test loss: 0.5073042511940002\n",
      "Epoch: 57890 | Loss: 0.4870993196964264 | Test loss: 0.5072904229164124\n",
      "Epoch: 57900 | Loss: 0.48708754777908325 | Test loss: 0.5072765350341797\n",
      "Epoch: 57910 | Loss: 0.4870758056640625 | Test loss: 0.5072627067565918\n",
      "Epoch: 57920 | Loss: 0.48706403374671936 | Test loss: 0.5072488784790039\n",
      "Epoch: 57930 | Loss: 0.4870522618293762 | Test loss: 0.507235050201416\n",
      "Epoch: 57940 | Loss: 0.48704051971435547 | Test loss: 0.5072211623191833\n",
      "Epoch: 57950 | Loss: 0.4870288074016571 | Test loss: 0.5072073340415955\n",
      "Epoch: 57960 | Loss: 0.4870169758796692 | Test loss: 0.5071935057640076\n",
      "Epoch: 57970 | Loss: 0.48700523376464844 | Test loss: 0.5071796774864197\n",
      "Epoch: 57980 | Loss: 0.4869935214519501 | Test loss: 0.5071658492088318\n",
      "Epoch: 57990 | Loss: 0.48698168992996216 | Test loss: 0.5071519613265991\n",
      "Epoch: 58000 | Loss: 0.4869699478149414 | Test loss: 0.5071381330490112\n",
      "Epoch: 58010 | Loss: 0.48695823550224304 | Test loss: 0.5071242451667786\n",
      "Epoch: 58020 | Loss: 0.4869464337825775 | Test loss: 0.5071104168891907\n",
      "Epoch: 58030 | Loss: 0.4869346618652344 | Test loss: 0.5070965886116028\n",
      "Epoch: 58040 | Loss: 0.486922949552536 | Test loss: 0.5070827007293701\n",
      "Epoch: 58050 | Loss: 0.4869111478328705 | Test loss: 0.5070688724517822\n",
      "Epoch: 58060 | Loss: 0.48689937591552734 | Test loss: 0.5070550441741943\n",
      "Epoch: 58070 | Loss: 0.486887663602829 | Test loss: 0.5070411562919617\n",
      "Epoch: 58080 | Loss: 0.4868759214878082 | Test loss: 0.5070273280143738\n",
      "Epoch: 58090 | Loss: 0.4868640899658203 | Test loss: 0.5070134997367859\n",
      "Epoch: 58100 | Loss: 0.48685237765312195 | Test loss: 0.5069996118545532\n",
      "Epoch: 58110 | Loss: 0.4868405759334564 | Test loss: 0.5069857835769653\n",
      "Epoch: 58120 | Loss: 0.48682886362075806 | Test loss: 0.5069719552993774\n",
      "Epoch: 58130 | Loss: 0.4868170917034149 | Test loss: 0.5069581270217896\n",
      "Epoch: 58140 | Loss: 0.4868052899837494 | Test loss: 0.5069442987442017\n",
      "Epoch: 58150 | Loss: 0.48679351806640625 | Test loss: 0.506930410861969\n",
      "Epoch: 58160 | Loss: 0.4867818057537079 | Test loss: 0.5069165229797363\n",
      "Epoch: 58170 | Loss: 0.48677006363868713 | Test loss: 0.5069026947021484\n",
      "Epoch: 58180 | Loss: 0.486758291721344 | Test loss: 0.5068888664245605\n",
      "Epoch: 58190 | Loss: 0.48674651980400085 | Test loss: 0.5068750381469727\n",
      "Epoch: 58200 | Loss: 0.4867347776889801 | Test loss: 0.5068612098693848\n",
      "Epoch: 58210 | Loss: 0.48672300577163696 | Test loss: 0.5068473219871521\n",
      "Epoch: 58220 | Loss: 0.4867112338542938 | Test loss: 0.5068334937095642\n",
      "Epoch: 58230 | Loss: 0.48669949173927307 | Test loss: 0.5068196654319763\n",
      "Epoch: 58240 | Loss: 0.48668771982192993 | Test loss: 0.5068058371543884\n",
      "Epoch: 58250 | Loss: 0.4866759479045868 | Test loss: 0.5067920088768005\n",
      "Epoch: 58260 | Loss: 0.48666420578956604 | Test loss: 0.5067781805992126\n",
      "Epoch: 58270 | Loss: 0.4866524338722229 | Test loss: 0.5067642331123352\n",
      "Epoch: 58280 | Loss: 0.48664066195487976 | Test loss: 0.5067504048347473\n",
      "Epoch: 58290 | Loss: 0.486628919839859 | Test loss: 0.5067365765571594\n",
      "Epoch: 58300 | Loss: 0.48661717772483826 | Test loss: 0.5067226886749268\n",
      "Epoch: 58310 | Loss: 0.48660537600517273 | Test loss: 0.5067088603973389\n",
      "Epoch: 58320 | Loss: 0.486593633890152 | Test loss: 0.506695032119751\n",
      "Epoch: 58330 | Loss: 0.48658186197280884 | Test loss: 0.5066811442375183\n",
      "Epoch: 58340 | Loss: 0.4865701198577881 | Test loss: 0.5066673159599304\n",
      "Epoch: 58350 | Loss: 0.48655834794044495 | Test loss: 0.5066534876823425\n",
      "Epoch: 58360 | Loss: 0.4865466058254242 | Test loss: 0.5066395998001099\n",
      "Epoch: 58370 | Loss: 0.48653483390808105 | Test loss: 0.5066258311271667\n",
      "Epoch: 58380 | Loss: 0.4865230619907379 | Test loss: 0.5066119432449341\n",
      "Epoch: 58390 | Loss: 0.48651131987571716 | Test loss: 0.5065981149673462\n",
      "Epoch: 58400 | Loss: 0.486499547958374 | Test loss: 0.5065842866897583\n",
      "Epoch: 58410 | Loss: 0.48648783564567566 | Test loss: 0.5065704584121704\n",
      "Epoch: 58420 | Loss: 0.48647603392601013 | Test loss: 0.506556510925293\n",
      "Epoch: 58430 | Loss: 0.486464262008667 | Test loss: 0.5065426826477051\n",
      "Epoch: 58440 | Loss: 0.48645254969596863 | Test loss: 0.5065288543701172\n",
      "Epoch: 58450 | Loss: 0.4864407479763031 | Test loss: 0.5065150260925293\n",
      "Epoch: 58460 | Loss: 0.48642903566360474 | Test loss: 0.5065011978149414\n",
      "Epoch: 58470 | Loss: 0.4864172041416168 | Test loss: 0.5064873099327087\n",
      "Epoch: 58480 | Loss: 0.48640546202659607 | Test loss: 0.5064734816551208\n",
      "Epoch: 58490 | Loss: 0.48639369010925293 | Test loss: 0.506459653377533\n",
      "Epoch: 58500 | Loss: 0.4863819181919098 | Test loss: 0.5064458250999451\n",
      "Epoch: 58510 | Loss: 0.48637017607688904 | Test loss: 0.5064319968223572\n",
      "Epoch: 58520 | Loss: 0.4863584637641907 | Test loss: 0.5064181685447693\n",
      "Epoch: 58530 | Loss: 0.48634669184684753 | Test loss: 0.5064042806625366\n",
      "Epoch: 58540 | Loss: 0.486334890127182 | Test loss: 0.506390392780304\n",
      "Epoch: 58550 | Loss: 0.48632317781448364 | Test loss: 0.5063765645027161\n",
      "Epoch: 58560 | Loss: 0.4863114058971405 | Test loss: 0.5063626766204834\n",
      "Epoch: 58570 | Loss: 0.486299604177475 | Test loss: 0.5063489079475403\n",
      "Epoch: 58580 | Loss: 0.4862878918647766 | Test loss: 0.5063350200653076\n",
      "Epoch: 58590 | Loss: 0.48627611994743347 | Test loss: 0.506321132183075\n",
      "Epoch: 58600 | Loss: 0.48626431822776794 | Test loss: 0.5063073039054871\n",
      "Epoch: 58610 | Loss: 0.4862526059150696 | Test loss: 0.5062934756278992\n",
      "Epoch: 58620 | Loss: 0.48624083399772644 | Test loss: 0.5062796473503113\n",
      "Epoch: 58630 | Loss: 0.4862290918827057 | Test loss: 0.5062658190727234\n",
      "Epoch: 58640 | Loss: 0.48621731996536255 | Test loss: 0.5062519311904907\n",
      "Epoch: 58650 | Loss: 0.4862055778503418 | Test loss: 0.5062381029129028\n",
      "Epoch: 58660 | Loss: 0.4861937463283539 | Test loss: 0.5062242746353149\n",
      "Epoch: 58670 | Loss: 0.4861820340156555 | Test loss: 0.506210446357727\n",
      "Epoch: 58680 | Loss: 0.48617029190063477 | Test loss: 0.5061965584754944\n",
      "Epoch: 58690 | Loss: 0.4861585199832916 | Test loss: 0.5061827301979065\n",
      "Epoch: 58700 | Loss: 0.4861467480659485 | Test loss: 0.5061688423156738\n",
      "Epoch: 58710 | Loss: 0.48613500595092773 | Test loss: 0.5061550140380859\n",
      "Epoch: 58720 | Loss: 0.4861232340335846 | Test loss: 0.506141185760498\n",
      "Epoch: 58730 | Loss: 0.48611146211624146 | Test loss: 0.5061273574829102\n",
      "Epoch: 58740 | Loss: 0.4860997200012207 | Test loss: 0.5061134696006775\n",
      "Epoch: 58750 | Loss: 0.48608800768852234 | Test loss: 0.5060996413230896\n",
      "Epoch: 58760 | Loss: 0.4860761761665344 | Test loss: 0.5060858130455017\n",
      "Epoch: 58770 | Loss: 0.48606443405151367 | Test loss: 0.5060719847679138\n",
      "Epoch: 58780 | Loss: 0.4860527217388153 | Test loss: 0.5060581564903259\n",
      "Epoch: 58790 | Loss: 0.4860408902168274 | Test loss: 0.5060442686080933\n",
      "Epoch: 58800 | Loss: 0.48602914810180664 | Test loss: 0.5060304403305054\n",
      "Epoch: 58810 | Loss: 0.4860174357891083 | Test loss: 0.5060165524482727\n",
      "Epoch: 58820 | Loss: 0.48600563406944275 | Test loss: 0.5060027241706848\n",
      "Epoch: 58830 | Loss: 0.4859938621520996 | Test loss: 0.5059888958930969\n",
      "Epoch: 58840 | Loss: 0.48598214983940125 | Test loss: 0.5059750080108643\n",
      "Epoch: 58850 | Loss: 0.4859703481197357 | Test loss: 0.5059611797332764\n",
      "Epoch: 58860 | Loss: 0.4859585762023926 | Test loss: 0.5059473514556885\n",
      "Epoch: 58870 | Loss: 0.4859468638896942 | Test loss: 0.5059334635734558\n",
      "Epoch: 58880 | Loss: 0.48593512177467346 | Test loss: 0.5059196352958679\n",
      "Epoch: 58890 | Loss: 0.48592329025268555 | Test loss: 0.50590580701828\n",
      "Epoch: 58900 | Loss: 0.4859115779399872 | Test loss: 0.5058919191360474\n",
      "Epoch: 58910 | Loss: 0.48589977622032166 | Test loss: 0.5058780908584595\n",
      "Epoch: 58920 | Loss: 0.4858880639076233 | Test loss: 0.5058642625808716\n",
      "Epoch: 58930 | Loss: 0.48587629199028015 | Test loss: 0.5058504343032837\n",
      "Epoch: 58940 | Loss: 0.4858644902706146 | Test loss: 0.5058366060256958\n",
      "Epoch: 58950 | Loss: 0.4858527183532715 | Test loss: 0.5058227181434631\n",
      "Epoch: 58960 | Loss: 0.4858410060405731 | Test loss: 0.5058088302612305\n",
      "Epoch: 58970 | Loss: 0.48582926392555237 | Test loss: 0.5057950019836426\n",
      "Epoch: 58980 | Loss: 0.48581749200820923 | Test loss: 0.5057811737060547\n",
      "Epoch: 58990 | Loss: 0.4858057200908661 | Test loss: 0.5057673454284668\n",
      "Epoch: 59000 | Loss: 0.48579397797584534 | Test loss: 0.5057535171508789\n",
      "Epoch: 59010 | Loss: 0.4857822060585022 | Test loss: 0.5057396292686462\n",
      "Epoch: 59020 | Loss: 0.48577043414115906 | Test loss: 0.5057258009910583\n",
      "Epoch: 59030 | Loss: 0.4857586920261383 | Test loss: 0.5057119727134705\n",
      "Epoch: 59040 | Loss: 0.48574692010879517 | Test loss: 0.5056981444358826\n",
      "Epoch: 59050 | Loss: 0.485735148191452 | Test loss: 0.5056843161582947\n",
      "Epoch: 59060 | Loss: 0.4857234060764313 | Test loss: 0.5056704878807068\n",
      "Epoch: 59070 | Loss: 0.48571163415908813 | Test loss: 0.5056565403938293\n",
      "Epoch: 59080 | Loss: 0.485699862241745 | Test loss: 0.5056427121162415\n",
      "Epoch: 59090 | Loss: 0.48568812012672424 | Test loss: 0.5056288838386536\n",
      "Epoch: 59100 | Loss: 0.4856763780117035 | Test loss: 0.5056149959564209\n",
      "Epoch: 59110 | Loss: 0.48566457629203796 | Test loss: 0.505601167678833\n",
      "Epoch: 59120 | Loss: 0.4856528341770172 | Test loss: 0.5055873394012451\n",
      "Epoch: 59130 | Loss: 0.4856410622596741 | Test loss: 0.5055734515190125\n",
      "Epoch: 59140 | Loss: 0.4856293201446533 | Test loss: 0.5055596232414246\n",
      "Epoch: 59150 | Loss: 0.4856175482273102 | Test loss: 0.5055457949638367\n",
      "Epoch: 59160 | Loss: 0.48560580611228943 | Test loss: 0.505531907081604\n",
      "Epoch: 59170 | Loss: 0.4855940341949463 | Test loss: 0.5055181384086609\n",
      "Epoch: 59180 | Loss: 0.48558226227760315 | Test loss: 0.5055042505264282\n",
      "Epoch: 59190 | Loss: 0.4855705201625824 | Test loss: 0.5054904222488403\n",
      "Epoch: 59200 | Loss: 0.48555874824523926 | Test loss: 0.5054765939712524\n",
      "Epoch: 59210 | Loss: 0.4855470359325409 | Test loss: 0.5054627656936646\n",
      "Epoch: 59220 | Loss: 0.48553523421287537 | Test loss: 0.5054488182067871\n",
      "Epoch: 59230 | Loss: 0.4855234622955322 | Test loss: 0.5054349899291992\n",
      "Epoch: 59240 | Loss: 0.48551174998283386 | Test loss: 0.5054211616516113\n",
      "Epoch: 59250 | Loss: 0.48549994826316833 | Test loss: 0.5054073333740234\n",
      "Epoch: 59260 | Loss: 0.48548823595046997 | Test loss: 0.5053935050964355\n",
      "Epoch: 59270 | Loss: 0.48547640442848206 | Test loss: 0.5053796172142029\n",
      "Epoch: 59280 | Loss: 0.4854646623134613 | Test loss: 0.505365788936615\n",
      "Epoch: 59290 | Loss: 0.48545289039611816 | Test loss: 0.5053519606590271\n",
      "Epoch: 59300 | Loss: 0.485441118478775 | Test loss: 0.5053381323814392\n",
      "Epoch: 59310 | Loss: 0.4854293763637543 | Test loss: 0.5053243041038513\n",
      "Epoch: 59320 | Loss: 0.4854176640510559 | Test loss: 0.5053104758262634\n",
      "Epoch: 59330 | Loss: 0.48540589213371277 | Test loss: 0.5052965879440308\n",
      "Epoch: 59340 | Loss: 0.48539409041404724 | Test loss: 0.5052827000617981\n",
      "Epoch: 59350 | Loss: 0.4853823781013489 | Test loss: 0.5052688717842102\n",
      "Epoch: 59360 | Loss: 0.48537060618400574 | Test loss: 0.5052549839019775\n",
      "Epoch: 59370 | Loss: 0.4853588044643402 | Test loss: 0.5052412152290344\n",
      "Epoch: 59380 | Loss: 0.48534709215164185 | Test loss: 0.5052273273468018\n",
      "Epoch: 59390 | Loss: 0.4853353202342987 | Test loss: 0.5052134394645691\n",
      "Epoch: 59400 | Loss: 0.4853235185146332 | Test loss: 0.5051996111869812\n",
      "Epoch: 59410 | Loss: 0.4853118062019348 | Test loss: 0.5051857829093933\n",
      "Epoch: 59420 | Loss: 0.4853000342845917 | Test loss: 0.5051719546318054\n",
      "Epoch: 59430 | Loss: 0.4852882921695709 | Test loss: 0.5051581263542175\n",
      "Epoch: 59440 | Loss: 0.4852765202522278 | Test loss: 0.5051442384719849\n",
      "Epoch: 59450 | Loss: 0.48526477813720703 | Test loss: 0.505130410194397\n",
      "Epoch: 59460 | Loss: 0.4852529466152191 | Test loss: 0.5051165819168091\n",
      "Epoch: 59470 | Loss: 0.48524123430252075 | Test loss: 0.5051027536392212\n",
      "Epoch: 59480 | Loss: 0.4852294921875 | Test loss: 0.5050888657569885\n",
      "Epoch: 59490 | Loss: 0.48521772027015686 | Test loss: 0.5050750374794006\n",
      "Epoch: 59500 | Loss: 0.4852059483528137 | Test loss: 0.505061149597168\n",
      "Epoch: 59510 | Loss: 0.48519420623779297 | Test loss: 0.5050473213195801\n",
      "Epoch: 59520 | Loss: 0.48518243432044983 | Test loss: 0.5050334930419922\n",
      "Epoch: 59530 | Loss: 0.4851706624031067 | Test loss: 0.5050196647644043\n",
      "Epoch: 59540 | Loss: 0.48515892028808594 | Test loss: 0.5050057768821716\n",
      "Epoch: 59550 | Loss: 0.4851472079753876 | Test loss: 0.5049919486045837\n",
      "Epoch: 59560 | Loss: 0.48513537645339966 | Test loss: 0.5049781203269958\n",
      "Epoch: 59570 | Loss: 0.4851236343383789 | Test loss: 0.504964292049408\n",
      "Epoch: 59580 | Loss: 0.48511192202568054 | Test loss: 0.5049504637718201\n",
      "Epoch: 59590 | Loss: 0.4851000905036926 | Test loss: 0.5049365758895874\n",
      "Epoch: 59600 | Loss: 0.4850883483886719 | Test loss: 0.5049227476119995\n",
      "Epoch: 59610 | Loss: 0.4850766360759735 | Test loss: 0.5049088597297668\n",
      "Epoch: 59620 | Loss: 0.485064834356308 | Test loss: 0.504895031452179\n",
      "Epoch: 59630 | Loss: 0.48505306243896484 | Test loss: 0.5048812031745911\n",
      "Epoch: 59640 | Loss: 0.4850413501262665 | Test loss: 0.5048673152923584\n",
      "Epoch: 59650 | Loss: 0.48502954840660095 | Test loss: 0.5048534870147705\n",
      "Epoch: 59660 | Loss: 0.4850177764892578 | Test loss: 0.5048396587371826\n",
      "Epoch: 59670 | Loss: 0.48500606417655945 | Test loss: 0.50482577085495\n",
      "Epoch: 59680 | Loss: 0.4849943220615387 | Test loss: 0.5048119425773621\n",
      "Epoch: 59690 | Loss: 0.4849824905395508 | Test loss: 0.5047981142997742\n",
      "Epoch: 59700 | Loss: 0.4849707782268524 | Test loss: 0.5047842264175415\n",
      "Epoch: 59710 | Loss: 0.4849589765071869 | Test loss: 0.5047703981399536\n",
      "Epoch: 59720 | Loss: 0.4849472641944885 | Test loss: 0.5047565698623657\n",
      "Epoch: 59730 | Loss: 0.4849354922771454 | Test loss: 0.5047427415847778\n",
      "Epoch: 59740 | Loss: 0.48492369055747986 | Test loss: 0.5047289133071899\n",
      "Epoch: 59750 | Loss: 0.4849119186401367 | Test loss: 0.5047150254249573\n",
      "Epoch: 59760 | Loss: 0.48490020632743835 | Test loss: 0.5047011375427246\n",
      "Epoch: 59770 | Loss: 0.4848884642124176 | Test loss: 0.5046873092651367\n",
      "Epoch: 59780 | Loss: 0.48487669229507446 | Test loss: 0.5046734809875488\n",
      "Epoch: 59790 | Loss: 0.4848649203777313 | Test loss: 0.5046596527099609\n",
      "Epoch: 59800 | Loss: 0.48485317826271057 | Test loss: 0.504645824432373\n",
      "Epoch: 59810 | Loss: 0.48484140634536743 | Test loss: 0.5046319365501404\n",
      "Epoch: 59820 | Loss: 0.4848296344280243 | Test loss: 0.5046181082725525\n",
      "Epoch: 59830 | Loss: 0.48481789231300354 | Test loss: 0.5046042799949646\n",
      "Epoch: 59840 | Loss: 0.4848061203956604 | Test loss: 0.5045904517173767\n",
      "Epoch: 59850 | Loss: 0.48479434847831726 | Test loss: 0.5045766234397888\n",
      "Epoch: 59860 | Loss: 0.4847826063632965 | Test loss: 0.5045627951622009\n",
      "Epoch: 59870 | Loss: 0.48477083444595337 | Test loss: 0.5045488476753235\n",
      "Epoch: 59880 | Loss: 0.48475906252861023 | Test loss: 0.5045350193977356\n",
      "Epoch: 59890 | Loss: 0.4847473204135895 | Test loss: 0.5045211911201477\n",
      "Epoch: 59900 | Loss: 0.4847355782985687 | Test loss: 0.504507303237915\n",
      "Epoch: 59910 | Loss: 0.4847237765789032 | Test loss: 0.5044934749603271\n",
      "Epoch: 59920 | Loss: 0.48471203446388245 | Test loss: 0.5044796466827393\n",
      "Epoch: 59930 | Loss: 0.4847002625465393 | Test loss: 0.5044657588005066\n",
      "Epoch: 59940 | Loss: 0.48468852043151855 | Test loss: 0.5044519305229187\n",
      "Epoch: 59950 | Loss: 0.4846767485141754 | Test loss: 0.5044381022453308\n",
      "Epoch: 59960 | Loss: 0.48466500639915466 | Test loss: 0.5044242143630981\n",
      "Epoch: 59970 | Loss: 0.4846532344818115 | Test loss: 0.504410445690155\n",
      "Epoch: 59980 | Loss: 0.4846414625644684 | Test loss: 0.5043965578079224\n",
      "Epoch: 59990 | Loss: 0.48462972044944763 | Test loss: 0.5043827295303345\n",
      "Epoch: 60000 | Loss: 0.4846179485321045 | Test loss: 0.5043689012527466\n",
      "Epoch: 60010 | Loss: 0.48460623621940613 | Test loss: 0.5043550729751587\n",
      "Epoch: 60020 | Loss: 0.4845944344997406 | Test loss: 0.5043411254882812\n",
      "Epoch: 60030 | Loss: 0.48458266258239746 | Test loss: 0.5043272972106934\n",
      "Epoch: 60040 | Loss: 0.4845709502696991 | Test loss: 0.5043134689331055\n",
      "Epoch: 60050 | Loss: 0.48455914855003357 | Test loss: 0.5042996406555176\n",
      "Epoch: 60060 | Loss: 0.4845474362373352 | Test loss: 0.5042858123779297\n",
      "Epoch: 60070 | Loss: 0.4845356047153473 | Test loss: 0.504271924495697\n",
      "Epoch: 60080 | Loss: 0.48452386260032654 | Test loss: 0.5042580962181091\n",
      "Epoch: 60090 | Loss: 0.4845120906829834 | Test loss: 0.5042442679405212\n",
      "Epoch: 60100 | Loss: 0.48450031876564026 | Test loss: 0.5042304396629333\n",
      "Epoch: 60110 | Loss: 0.4844885766506195 | Test loss: 0.5042166113853455\n",
      "Epoch: 60120 | Loss: 0.48447686433792114 | Test loss: 0.5042027831077576\n",
      "Epoch: 60130 | Loss: 0.484465092420578 | Test loss: 0.5041888952255249\n",
      "Epoch: 60140 | Loss: 0.4844532907009125 | Test loss: 0.5041750073432922\n",
      "Epoch: 60150 | Loss: 0.4844415783882141 | Test loss: 0.5041611790657043\n",
      "Epoch: 60160 | Loss: 0.48442980647087097 | Test loss: 0.5041472911834717\n",
      "Epoch: 60170 | Loss: 0.48441800475120544 | Test loss: 0.5041335225105286\n",
      "Epoch: 60180 | Loss: 0.4844062924385071 | Test loss: 0.5041196346282959\n",
      "Epoch: 60190 | Loss: 0.48439452052116394 | Test loss: 0.5041057467460632\n",
      "Epoch: 60200 | Loss: 0.4843827188014984 | Test loss: 0.5040919184684753\n",
      "Epoch: 60210 | Loss: 0.48437100648880005 | Test loss: 0.5040780901908875\n",
      "Epoch: 60220 | Loss: 0.4843592345714569 | Test loss: 0.5040642619132996\n",
      "Epoch: 60230 | Loss: 0.48434749245643616 | Test loss: 0.5040504336357117\n",
      "Epoch: 60240 | Loss: 0.484335720539093 | Test loss: 0.504036545753479\n",
      "Epoch: 60250 | Loss: 0.48432397842407227 | Test loss: 0.5040227174758911\n",
      "Epoch: 60260 | Loss: 0.48431214690208435 | Test loss: 0.5040088891983032\n",
      "Epoch: 60270 | Loss: 0.484300434589386 | Test loss: 0.5039950609207153\n",
      "Epoch: 60280 | Loss: 0.48428869247436523 | Test loss: 0.5039811730384827\n",
      "Epoch: 60290 | Loss: 0.4842769205570221 | Test loss: 0.5039673447608948\n",
      "Epoch: 60300 | Loss: 0.48426514863967896 | Test loss: 0.5039534568786621\n",
      "Epoch: 60310 | Loss: 0.4842534065246582 | Test loss: 0.5039396286010742\n",
      "Epoch: 60320 | Loss: 0.48424163460731506 | Test loss: 0.5039258003234863\n",
      "Epoch: 60330 | Loss: 0.4842298626899719 | Test loss: 0.5039119720458984\n",
      "Epoch: 60340 | Loss: 0.48421812057495117 | Test loss: 0.5038980841636658\n",
      "Epoch: 60350 | Loss: 0.4842064082622528 | Test loss: 0.5038842558860779\n",
      "Epoch: 60360 | Loss: 0.4841945767402649 | Test loss: 0.50387042760849\n",
      "Epoch: 60370 | Loss: 0.48418283462524414 | Test loss: 0.5038565993309021\n",
      "Epoch: 60380 | Loss: 0.4841711223125458 | Test loss: 0.5038427710533142\n",
      "Epoch: 60390 | Loss: 0.48415929079055786 | Test loss: 0.5038288831710815\n",
      "Epoch: 60400 | Loss: 0.4841475486755371 | Test loss: 0.5038150548934937\n",
      "Epoch: 60410 | Loss: 0.48413583636283875 | Test loss: 0.503801167011261\n",
      "Epoch: 60420 | Loss: 0.4841240346431732 | Test loss: 0.5037873387336731\n",
      "Epoch: 60430 | Loss: 0.4841122627258301 | Test loss: 0.5037735104560852\n",
      "Epoch: 60440 | Loss: 0.4841005504131317 | Test loss: 0.5037596225738525\n",
      "Epoch: 60450 | Loss: 0.4840887486934662 | Test loss: 0.5037457942962646\n",
      "Epoch: 60460 | Loss: 0.48407697677612305 | Test loss: 0.5037319660186768\n",
      "Epoch: 60470 | Loss: 0.4840652644634247 | Test loss: 0.5037180781364441\n",
      "Epoch: 60480 | Loss: 0.48405352234840393 | Test loss: 0.5037042498588562\n",
      "Epoch: 60490 | Loss: 0.484041690826416 | Test loss: 0.5036904215812683\n",
      "Epoch: 60500 | Loss: 0.48402997851371765 | Test loss: 0.5036765336990356\n",
      "Epoch: 60510 | Loss: 0.4840181767940521 | Test loss: 0.5036627054214478\n",
      "Epoch: 60520 | Loss: 0.48400646448135376 | Test loss: 0.5036488771438599\n",
      "Epoch: 60530 | Loss: 0.4839946925640106 | Test loss: 0.503635048866272\n",
      "Epoch: 60540 | Loss: 0.4839828908443451 | Test loss: 0.5036212205886841\n",
      "Epoch: 60550 | Loss: 0.48397111892700195 | Test loss: 0.5036073327064514\n",
      "Epoch: 60560 | Loss: 0.4839594066143036 | Test loss: 0.5035934448242188\n",
      "Epoch: 60570 | Loss: 0.48394766449928284 | Test loss: 0.5035796165466309\n",
      "Epoch: 60580 | Loss: 0.4839358925819397 | Test loss: 0.503565788269043\n",
      "Epoch: 60590 | Loss: 0.48392412066459656 | Test loss: 0.5035519599914551\n",
      "Epoch: 60600 | Loss: 0.4839123785495758 | Test loss: 0.5035381317138672\n",
      "Epoch: 60610 | Loss: 0.48390060663223267 | Test loss: 0.5035242438316345\n",
      "Epoch: 60620 | Loss: 0.4838888347148895 | Test loss: 0.5035104155540466\n",
      "Epoch: 60630 | Loss: 0.4838770925998688 | Test loss: 0.5034965872764587\n",
      "Epoch: 60640 | Loss: 0.48386532068252563 | Test loss: 0.5034827589988708\n",
      "Epoch: 60650 | Loss: 0.4838535487651825 | Test loss: 0.503468930721283\n",
      "Epoch: 60660 | Loss: 0.48384180665016174 | Test loss: 0.5034551024436951\n",
      "Epoch: 60670 | Loss: 0.4838300347328186 | Test loss: 0.5034411549568176\n",
      "Epoch: 60680 | Loss: 0.48381826281547546 | Test loss: 0.5034273266792297\n",
      "Epoch: 60690 | Loss: 0.4838065207004547 | Test loss: 0.5034134984016418\n",
      "Epoch: 60700 | Loss: 0.48379477858543396 | Test loss: 0.5033996105194092\n",
      "Epoch: 60710 | Loss: 0.48378297686576843 | Test loss: 0.5033857822418213\n",
      "Epoch: 60720 | Loss: 0.4837712347507477 | Test loss: 0.5033719539642334\n",
      "Epoch: 60730 | Loss: 0.48375946283340454 | Test loss: 0.5033580660820007\n",
      "Epoch: 60740 | Loss: 0.4837477207183838 | Test loss: 0.5033442378044128\n",
      "Epoch: 60750 | Loss: 0.48373594880104065 | Test loss: 0.503330409526825\n",
      "Epoch: 60760 | Loss: 0.4837242066860199 | Test loss: 0.5033165216445923\n",
      "Epoch: 60770 | Loss: 0.48371243476867676 | Test loss: 0.5033027529716492\n",
      "Epoch: 60780 | Loss: 0.4837006628513336 | Test loss: 0.5032888650894165\n",
      "Epoch: 60790 | Loss: 0.48368892073631287 | Test loss: 0.5032750368118286\n",
      "Epoch: 60800 | Loss: 0.4836771488189697 | Test loss: 0.5032612085342407\n",
      "Epoch: 60810 | Loss: 0.48366543650627136 | Test loss: 0.5032473802566528\n",
      "Epoch: 60820 | Loss: 0.48365363478660583 | Test loss: 0.5032334327697754\n",
      "Epoch: 60830 | Loss: 0.4836418628692627 | Test loss: 0.5032196044921875\n",
      "Epoch: 60840 | Loss: 0.48363015055656433 | Test loss: 0.5032057762145996\n",
      "Epoch: 60850 | Loss: 0.4836183488368988 | Test loss: 0.5031919479370117\n",
      "Epoch: 60860 | Loss: 0.48360663652420044 | Test loss: 0.5031781196594238\n",
      "Epoch: 60870 | Loss: 0.4835948050022125 | Test loss: 0.5031642317771912\n",
      "Epoch: 60880 | Loss: 0.4835830628871918 | Test loss: 0.5031504034996033\n",
      "Epoch: 60890 | Loss: 0.48357129096984863 | Test loss: 0.5031365752220154\n",
      "Epoch: 60900 | Loss: 0.4835595190525055 | Test loss: 0.5031227469444275\n",
      "Epoch: 60910 | Loss: 0.48354777693748474 | Test loss: 0.5031089186668396\n",
      "Epoch: 60920 | Loss: 0.4835360646247864 | Test loss: 0.5030950903892517\n",
      "Epoch: 60930 | Loss: 0.48352429270744324 | Test loss: 0.503081202507019\n",
      "Epoch: 60940 | Loss: 0.4835124909877777 | Test loss: 0.5030673146247864\n",
      "Epoch: 60950 | Loss: 0.48350077867507935 | Test loss: 0.5030534863471985\n",
      "Epoch: 60960 | Loss: 0.4834890067577362 | Test loss: 0.5030395984649658\n",
      "Epoch: 60970 | Loss: 0.4834772050380707 | Test loss: 0.5030258297920227\n",
      "Epoch: 60980 | Loss: 0.4834654927253723 | Test loss: 0.50301194190979\n",
      "Epoch: 60990 | Loss: 0.4834537208080292 | Test loss: 0.5029980540275574\n",
      "Epoch: 61000 | Loss: 0.48344191908836365 | Test loss: 0.5029842257499695\n",
      "Epoch: 61010 | Loss: 0.4834302067756653 | Test loss: 0.5029703974723816\n",
      "Epoch: 61020 | Loss: 0.48341843485832214 | Test loss: 0.5029565691947937\n",
      "Epoch: 61030 | Loss: 0.4834066927433014 | Test loss: 0.5029427409172058\n",
      "Epoch: 61040 | Loss: 0.48339492082595825 | Test loss: 0.5029288530349731\n",
      "Epoch: 61050 | Loss: 0.4833831787109375 | Test loss: 0.5029150247573853\n",
      "Epoch: 61060 | Loss: 0.4833713471889496 | Test loss: 0.5029011964797974\n",
      "Epoch: 61070 | Loss: 0.4833596348762512 | Test loss: 0.5028873682022095\n",
      "Epoch: 61080 | Loss: 0.48334789276123047 | Test loss: 0.5028734803199768\n",
      "Epoch: 61090 | Loss: 0.48333612084388733 | Test loss: 0.5028596520423889\n",
      "Epoch: 61100 | Loss: 0.4833243489265442 | Test loss: 0.5028457641601562\n",
      "Epoch: 61110 | Loss: 0.48331260681152344 | Test loss: 0.5028319358825684\n",
      "Epoch: 61120 | Loss: 0.4833008348941803 | Test loss: 0.5028181076049805\n",
      "Epoch: 61130 | Loss: 0.48328906297683716 | Test loss: 0.5028042793273926\n",
      "Epoch: 61140 | Loss: 0.4832773208618164 | Test loss: 0.5027903914451599\n",
      "Epoch: 61150 | Loss: 0.48326560854911804 | Test loss: 0.502776563167572\n",
      "Epoch: 61160 | Loss: 0.4832537770271301 | Test loss: 0.5027627348899841\n",
      "Epoch: 61170 | Loss: 0.4832420349121094 | Test loss: 0.5027489066123962\n",
      "Epoch: 61180 | Loss: 0.483230322599411 | Test loss: 0.5027350783348083\n",
      "Epoch: 61190 | Loss: 0.4832184910774231 | Test loss: 0.5027211904525757\n",
      "Epoch: 61200 | Loss: 0.48320674896240234 | Test loss: 0.5027073621749878\n",
      "Epoch: 61210 | Loss: 0.483195036649704 | Test loss: 0.5026934742927551\n",
      "Epoch: 61220 | Loss: 0.48318323493003845 | Test loss: 0.5026796460151672\n",
      "Epoch: 61230 | Loss: 0.4831714630126953 | Test loss: 0.5026658177375793\n",
      "Epoch: 61240 | Loss: 0.48315975069999695 | Test loss: 0.5026519298553467\n",
      "Epoch: 61250 | Loss: 0.4831479489803314 | Test loss: 0.5026381015777588\n",
      "Epoch: 61260 | Loss: 0.4831361770629883 | Test loss: 0.5026242733001709\n",
      "Epoch: 61270 | Loss: 0.4831244647502899 | Test loss: 0.5026103854179382\n",
      "Epoch: 61280 | Loss: 0.48311272263526917 | Test loss: 0.5025965571403503\n",
      "Epoch: 61290 | Loss: 0.48310089111328125 | Test loss: 0.5025827288627625\n",
      "Epoch: 61300 | Loss: 0.4830891788005829 | Test loss: 0.5025688409805298\n",
      "Epoch: 61310 | Loss: 0.48307737708091736 | Test loss: 0.5025550127029419\n",
      "Epoch: 61320 | Loss: 0.483065664768219 | Test loss: 0.502541184425354\n",
      "Epoch: 61330 | Loss: 0.48305389285087585 | Test loss: 0.5025273561477661\n",
      "Epoch: 61340 | Loss: 0.4830420911312103 | Test loss: 0.5025135278701782\n",
      "Epoch: 61350 | Loss: 0.4830303192138672 | Test loss: 0.5024996399879456\n",
      "Epoch: 61360 | Loss: 0.4830186069011688 | Test loss: 0.5024857521057129\n",
      "Epoch: 61370 | Loss: 0.48300686478614807 | Test loss: 0.502471923828125\n",
      "Epoch: 61380 | Loss: 0.48299509286880493 | Test loss: 0.5024580955505371\n",
      "Epoch: 61390 | Loss: 0.4829833209514618 | Test loss: 0.5024442672729492\n",
      "Epoch: 61400 | Loss: 0.48297157883644104 | Test loss: 0.5024304389953613\n",
      "Epoch: 61410 | Loss: 0.4829598069190979 | Test loss: 0.5024165511131287\n",
      "Epoch: 61420 | Loss: 0.48294803500175476 | Test loss: 0.5024027228355408\n",
      "Epoch: 61430 | Loss: 0.482936292886734 | Test loss: 0.5023888945579529\n",
      "Epoch: 61440 | Loss: 0.48292452096939087 | Test loss: 0.502375066280365\n",
      "Epoch: 61450 | Loss: 0.48291274905204773 | Test loss: 0.5023612380027771\n",
      "Epoch: 61460 | Loss: 0.482901006937027 | Test loss: 0.5023474097251892\n",
      "Epoch: 61470 | Loss: 0.48288923501968384 | Test loss: 0.5023334622383118\n",
      "Epoch: 61480 | Loss: 0.4828774631023407 | Test loss: 0.5023196339607239\n",
      "Epoch: 61490 | Loss: 0.48286572098731995 | Test loss: 0.502305805683136\n",
      "Epoch: 61500 | Loss: 0.4828539788722992 | Test loss: 0.5022919178009033\n",
      "Epoch: 61510 | Loss: 0.48284217715263367 | Test loss: 0.5022780895233154\n",
      "Epoch: 61520 | Loss: 0.4828304350376129 | Test loss: 0.5022642612457275\n",
      "Epoch: 61530 | Loss: 0.4828186631202698 | Test loss: 0.5022503733634949\n",
      "Epoch: 61540 | Loss: 0.482806921005249 | Test loss: 0.502236545085907\n",
      "Epoch: 61550 | Loss: 0.4827951490879059 | Test loss: 0.5022227168083191\n",
      "Epoch: 61560 | Loss: 0.48278340697288513 | Test loss: 0.5022088289260864\n",
      "Epoch: 61570 | Loss: 0.482771635055542 | Test loss: 0.5021950602531433\n",
      "Epoch: 61580 | Loss: 0.48275986313819885 | Test loss: 0.5021811723709106\n",
      "Epoch: 61590 | Loss: 0.4827481210231781 | Test loss: 0.5021673440933228\n",
      "Epoch: 61600 | Loss: 0.48273634910583496 | Test loss: 0.5021535158157349\n",
      "Epoch: 61610 | Loss: 0.4827246367931366 | Test loss: 0.502139687538147\n",
      "Epoch: 61620 | Loss: 0.48271283507347107 | Test loss: 0.5021257400512695\n",
      "Epoch: 61630 | Loss: 0.48270106315612793 | Test loss: 0.5021119117736816\n",
      "Epoch: 61640 | Loss: 0.48268935084342957 | Test loss: 0.5020980834960938\n",
      "Epoch: 61650 | Loss: 0.48267754912376404 | Test loss: 0.5020842552185059\n",
      "Epoch: 61660 | Loss: 0.4826658368110657 | Test loss: 0.502070426940918\n",
      "Epoch: 61670 | Loss: 0.48265400528907776 | Test loss: 0.5020565390586853\n",
      "Epoch: 61680 | Loss: 0.482642263174057 | Test loss: 0.5020427107810974\n",
      "Epoch: 61690 | Loss: 0.48263049125671387 | Test loss: 0.5020288825035095\n",
      "Epoch: 61700 | Loss: 0.4826187193393707 | Test loss: 0.5020150542259216\n",
      "Epoch: 61710 | Loss: 0.48260697722435 | Test loss: 0.5020012259483337\n",
      "Epoch: 61720 | Loss: 0.4825952649116516 | Test loss: 0.5019873976707458\n",
      "Epoch: 61730 | Loss: 0.48258349299430847 | Test loss: 0.5019735097885132\n",
      "Epoch: 61740 | Loss: 0.48257169127464294 | Test loss: 0.5019596219062805\n",
      "Epoch: 61750 | Loss: 0.4825599789619446 | Test loss: 0.5019457936286926\n",
      "Epoch: 61760 | Loss: 0.48254820704460144 | Test loss: 0.50193190574646\n",
      "Epoch: 61770 | Loss: 0.4825364053249359 | Test loss: 0.5019181370735168\n",
      "Epoch: 61780 | Loss: 0.48252469301223755 | Test loss: 0.5019042491912842\n",
      "Epoch: 61790 | Loss: 0.4825129210948944 | Test loss: 0.5018903613090515\n",
      "Epoch: 61800 | Loss: 0.4825011193752289 | Test loss: 0.5018765330314636\n",
      "Epoch: 61810 | Loss: 0.4824894070625305 | Test loss: 0.5018627047538757\n",
      "Epoch: 61820 | Loss: 0.4824776351451874 | Test loss: 0.5018488764762878\n",
      "Epoch: 61830 | Loss: 0.4824658930301666 | Test loss: 0.5018350481987\n",
      "Epoch: 61840 | Loss: 0.4824541211128235 | Test loss: 0.5018211603164673\n",
      "Epoch: 61850 | Loss: 0.48244237899780273 | Test loss: 0.5018073320388794\n",
      "Epoch: 61860 | Loss: 0.4824305474758148 | Test loss: 0.5017935037612915\n",
      "Epoch: 61870 | Loss: 0.48241883516311646 | Test loss: 0.5017796754837036\n",
      "Epoch: 61880 | Loss: 0.4824070930480957 | Test loss: 0.501765787601471\n",
      "Epoch: 61890 | Loss: 0.48239532113075256 | Test loss: 0.5017519593238831\n",
      "Epoch: 61900 | Loss: 0.4823835492134094 | Test loss: 0.5017380714416504\n",
      "Epoch: 61910 | Loss: 0.48237180709838867 | Test loss: 0.5017242431640625\n",
      "Epoch: 61920 | Loss: 0.48236003518104553 | Test loss: 0.5017104148864746\n",
      "Epoch: 61930 | Loss: 0.4823482632637024 | Test loss: 0.5016965866088867\n",
      "Epoch: 61940 | Loss: 0.48233652114868164 | Test loss: 0.501682698726654\n",
      "Epoch: 61950 | Loss: 0.4823248088359833 | Test loss: 0.5016688704490662\n",
      "Epoch: 61960 | Loss: 0.48231297731399536 | Test loss: 0.5016550421714783\n",
      "Epoch: 61970 | Loss: 0.4823012351989746 | Test loss: 0.5016412138938904\n",
      "Epoch: 61980 | Loss: 0.48228952288627625 | Test loss: 0.5016273856163025\n",
      "Epoch: 61990 | Loss: 0.48227769136428833 | Test loss: 0.5016134977340698\n",
      "Epoch: 62000 | Loss: 0.4822659492492676 | Test loss: 0.5015996694564819\n",
      "Epoch: 62010 | Loss: 0.4822542369365692 | Test loss: 0.5015857815742493\n",
      "Epoch: 62020 | Loss: 0.4822424352169037 | Test loss: 0.5015719532966614\n",
      "Epoch: 62030 | Loss: 0.48223066329956055 | Test loss: 0.5015581250190735\n",
      "Epoch: 62040 | Loss: 0.4822189509868622 | Test loss: 0.5015442371368408\n",
      "Epoch: 62050 | Loss: 0.48220714926719666 | Test loss: 0.5015304088592529\n",
      "Epoch: 62060 | Loss: 0.4821953773498535 | Test loss: 0.501516580581665\n",
      "Epoch: 62070 | Loss: 0.48218366503715515 | Test loss: 0.5015026926994324\n",
      "Epoch: 62080 | Loss: 0.4821719229221344 | Test loss: 0.5014888644218445\n",
      "Epoch: 62090 | Loss: 0.4821600914001465 | Test loss: 0.5014750361442566\n",
      "Epoch: 62100 | Loss: 0.4821483790874481 | Test loss: 0.5014611482620239\n",
      "Epoch: 62110 | Loss: 0.4821365773677826 | Test loss: 0.501447319984436\n",
      "Epoch: 62120 | Loss: 0.48212486505508423 | Test loss: 0.5014334917068481\n",
      "Epoch: 62130 | Loss: 0.4821130931377411 | Test loss: 0.5014196634292603\n",
      "Epoch: 62140 | Loss: 0.48210129141807556 | Test loss: 0.5014058351516724\n",
      "Epoch: 62150 | Loss: 0.4820895195007324 | Test loss: 0.5013919472694397\n",
      "Epoch: 62160 | Loss: 0.48207780718803406 | Test loss: 0.501378059387207\n",
      "Epoch: 62170 | Loss: 0.4820660650730133 | Test loss: 0.5013642311096191\n",
      "Epoch: 62180 | Loss: 0.48205429315567017 | Test loss: 0.5013504028320312\n",
      "Epoch: 62190 | Loss: 0.482042521238327 | Test loss: 0.5013365745544434\n",
      "Epoch: 62200 | Loss: 0.4820307791233063 | Test loss: 0.5013227462768555\n",
      "Epoch: 62210 | Loss: 0.48201900720596313 | Test loss: 0.5013088583946228\n",
      "Epoch: 62220 | Loss: 0.48200723528862 | Test loss: 0.5012950301170349\n",
      "Epoch: 62230 | Loss: 0.48199549317359924 | Test loss: 0.501281201839447\n",
      "Epoch: 62240 | Loss: 0.4819837212562561 | Test loss: 0.5012673735618591\n",
      "Epoch: 62250 | Loss: 0.48197194933891296 | Test loss: 0.5012535452842712\n",
      "Epoch: 62260 | Loss: 0.4819602072238922 | Test loss: 0.5012397170066833\n",
      "Epoch: 62270 | Loss: 0.4819484353065491 | Test loss: 0.5012257695198059\n",
      "Epoch: 62280 | Loss: 0.48193666338920593 | Test loss: 0.501211941242218\n",
      "Epoch: 62290 | Loss: 0.4819249212741852 | Test loss: 0.5011981129646301\n",
      "Epoch: 62300 | Loss: 0.48191317915916443 | Test loss: 0.5011842250823975\n",
      "Epoch: 62310 | Loss: 0.4819013774394989 | Test loss: 0.5011703968048096\n",
      "Epoch: 62320 | Loss: 0.48188963532447815 | Test loss: 0.5011565685272217\n",
      "Epoch: 62330 | Loss: 0.481877863407135 | Test loss: 0.501142680644989\n",
      "Epoch: 62340 | Loss: 0.48186612129211426 | Test loss: 0.5011288523674011\n",
      "Epoch: 62350 | Loss: 0.4818543493747711 | Test loss: 0.5011150240898132\n",
      "Epoch: 62360 | Loss: 0.48184260725975037 | Test loss: 0.5011011362075806\n",
      "Epoch: 62370 | Loss: 0.4818308353424072 | Test loss: 0.5010873675346375\n",
      "Epoch: 62380 | Loss: 0.4818190634250641 | Test loss: 0.5010734796524048\n",
      "Epoch: 62390 | Loss: 0.48180732131004333 | Test loss: 0.5010596513748169\n",
      "Epoch: 62400 | Loss: 0.4817955493927002 | Test loss: 0.501045823097229\n",
      "Epoch: 62410 | Loss: 0.48178383708000183 | Test loss: 0.5010319948196411\n",
      "Epoch: 62420 | Loss: 0.4817720353603363 | Test loss: 0.5010180473327637\n",
      "Epoch: 62430 | Loss: 0.48176026344299316 | Test loss: 0.5010042190551758\n",
      "Epoch: 62440 | Loss: 0.4817485511302948 | Test loss: 0.5009903907775879\n",
      "Epoch: 62450 | Loss: 0.4817367494106293 | Test loss: 0.5009765625\n",
      "Epoch: 62460 | Loss: 0.4817250370979309 | Test loss: 0.5009627342224121\n",
      "Epoch: 62470 | Loss: 0.481713205575943 | Test loss: 0.5009488463401794\n",
      "Epoch: 62480 | Loss: 0.48170146346092224 | Test loss: 0.5009350180625916\n",
      "Epoch: 62490 | Loss: 0.4816896915435791 | Test loss: 0.5009211897850037\n",
      "Epoch: 62500 | Loss: 0.48167791962623596 | Test loss: 0.5009073615074158\n",
      "Epoch: 62510 | Loss: 0.4816661775112152 | Test loss: 0.5008935332298279\n",
      "Epoch: 62520 | Loss: 0.48165446519851685 | Test loss: 0.50087970495224\n",
      "Epoch: 62530 | Loss: 0.4816426932811737 | Test loss: 0.5008658170700073\n",
      "Epoch: 62540 | Loss: 0.4816308915615082 | Test loss: 0.5008519291877747\n",
      "Epoch: 62550 | Loss: 0.4816191792488098 | Test loss: 0.5008381009101868\n",
      "Epoch: 62560 | Loss: 0.4816074073314667 | Test loss: 0.5008242130279541\n",
      "Epoch: 62570 | Loss: 0.48159560561180115 | Test loss: 0.500810444355011\n",
      "Epoch: 62580 | Loss: 0.4815838932991028 | Test loss: 0.5007965564727783\n",
      "Epoch: 62590 | Loss: 0.48157212138175964 | Test loss: 0.5007826685905457\n",
      "Epoch: 62600 | Loss: 0.4815603196620941 | Test loss: 0.5007688403129578\n",
      "Epoch: 62610 | Loss: 0.48154860734939575 | Test loss: 0.5007550120353699\n",
      "Epoch: 62620 | Loss: 0.4815368354320526 | Test loss: 0.500741183757782\n",
      "Epoch: 62630 | Loss: 0.48152509331703186 | Test loss: 0.5007273554801941\n",
      "Epoch: 62640 | Loss: 0.4815133213996887 | Test loss: 0.5007134675979614\n",
      "Epoch: 62650 | Loss: 0.48150157928466797 | Test loss: 0.5006996393203735\n",
      "Epoch: 62660 | Loss: 0.48148974776268005 | Test loss: 0.5006858110427856\n",
      "Epoch: 62670 | Loss: 0.4814780354499817 | Test loss: 0.5006719827651978\n",
      "Epoch: 62680 | Loss: 0.48146629333496094 | Test loss: 0.5006580948829651\n",
      "Epoch: 62690 | Loss: 0.4814545214176178 | Test loss: 0.5006442666053772\n",
      "Epoch: 62700 | Loss: 0.48144274950027466 | Test loss: 0.5006303787231445\n",
      "Epoch: 62710 | Loss: 0.4814310073852539 | Test loss: 0.5006165504455566\n",
      "Epoch: 62720 | Loss: 0.48141923546791077 | Test loss: 0.5006027221679688\n",
      "Epoch: 62730 | Loss: 0.4814074635505676 | Test loss: 0.5005888938903809\n",
      "Epoch: 62740 | Loss: 0.4813957214355469 | Test loss: 0.5005750060081482\n",
      "Epoch: 62750 | Loss: 0.4813840091228485 | Test loss: 0.5005611777305603\n",
      "Epoch: 62760 | Loss: 0.4813721776008606 | Test loss: 0.5005473494529724\n",
      "Epoch: 62770 | Loss: 0.48136043548583984 | Test loss: 0.5005335211753845\n",
      "Epoch: 62780 | Loss: 0.4813487231731415 | Test loss: 0.5005196928977966\n",
      "Epoch: 62790 | Loss: 0.48133689165115356 | Test loss: 0.500505805015564\n",
      "Epoch: 62800 | Loss: 0.4813251495361328 | Test loss: 0.5004919767379761\n",
      "Epoch: 62810 | Loss: 0.48131343722343445 | Test loss: 0.5004780888557434\n",
      "Epoch: 62820 | Loss: 0.4813016355037689 | Test loss: 0.5004642605781555\n",
      "Epoch: 62830 | Loss: 0.4812898635864258 | Test loss: 0.5004504323005676\n",
      "Epoch: 62840 | Loss: 0.4812781512737274 | Test loss: 0.500436544418335\n",
      "Epoch: 62850 | Loss: 0.4812663495540619 | Test loss: 0.5004227161407471\n",
      "Epoch: 62860 | Loss: 0.48125457763671875 | Test loss: 0.5004088878631592\n",
      "Epoch: 62870 | Loss: 0.4812428653240204 | Test loss: 0.5003949999809265\n",
      "Epoch: 62880 | Loss: 0.48123112320899963 | Test loss: 0.5003811717033386\n",
      "Epoch: 62890 | Loss: 0.4812192916870117 | Test loss: 0.5003673434257507\n",
      "Epoch: 62900 | Loss: 0.48120757937431335 | Test loss: 0.5003534555435181\n",
      "Epoch: 62910 | Loss: 0.4811957776546478 | Test loss: 0.5003396272659302\n",
      "Epoch: 62920 | Loss: 0.48118406534194946 | Test loss: 0.5003257989883423\n",
      "Epoch: 62930 | Loss: 0.4811722934246063 | Test loss: 0.5003119707107544\n",
      "Epoch: 62940 | Loss: 0.4811604917049408 | Test loss: 0.5002981424331665\n",
      "Epoch: 62950 | Loss: 0.48114871978759766 | Test loss: 0.5002842545509338\n",
      "Epoch: 62960 | Loss: 0.4811370074748993 | Test loss: 0.5002703666687012\n",
      "Epoch: 62970 | Loss: 0.48112526535987854 | Test loss: 0.5002565383911133\n",
      "Epoch: 62980 | Loss: 0.4811134934425354 | Test loss: 0.5002427101135254\n",
      "Epoch: 62990 | Loss: 0.48110172152519226 | Test loss: 0.5002288818359375\n",
      "Epoch: 63000 | Loss: 0.4810899794101715 | Test loss: 0.5002150535583496\n",
      "Epoch: 63010 | Loss: 0.48107820749282837 | Test loss: 0.5002011656761169\n",
      "Epoch: 63020 | Loss: 0.48106643557548523 | Test loss: 0.500187337398529\n",
      "Epoch: 63030 | Loss: 0.4810546934604645 | Test loss: 0.5001735091209412\n",
      "Epoch: 63040 | Loss: 0.48104292154312134 | Test loss: 0.5001596808433533\n",
      "Epoch: 63050 | Loss: 0.4810311496257782 | Test loss: 0.5001458525657654\n",
      "Epoch: 63060 | Loss: 0.48101940751075745 | Test loss: 0.5001320242881775\n",
      "Epoch: 63070 | Loss: 0.4810076355934143 | Test loss: 0.5001180768013\n",
      "Epoch: 63080 | Loss: 0.48099586367607117 | Test loss: 0.5001042485237122\n",
      "Epoch: 63090 | Loss: 0.4809841215610504 | Test loss: 0.5000904202461243\n",
      "Epoch: 63100 | Loss: 0.48097237944602966 | Test loss: 0.5000765323638916\n",
      "Epoch: 63110 | Loss: 0.48096057772636414 | Test loss: 0.5000627040863037\n",
      "Epoch: 63120 | Loss: 0.4809488356113434 | Test loss: 0.5000488758087158\n",
      "Epoch: 63130 | Loss: 0.48093706369400024 | Test loss: 0.5000349879264832\n",
      "Epoch: 63140 | Loss: 0.4809253215789795 | Test loss: 0.5000211596488953\n",
      "Epoch: 63150 | Loss: 0.48091354966163635 | Test loss: 0.5000073313713074\n",
      "Epoch: 63160 | Loss: 0.4809018075466156 | Test loss: 0.4999934732913971\n",
      "Epoch: 63170 | Loss: 0.48089003562927246 | Test loss: 0.4999796450138092\n",
      "Epoch: 63180 | Loss: 0.4808782637119293 | Test loss: 0.49996575713157654\n",
      "Epoch: 63190 | Loss: 0.48086652159690857 | Test loss: 0.49995192885398865\n",
      "Epoch: 63200 | Loss: 0.48085474967956543 | Test loss: 0.49993810057640076\n",
      "Epoch: 63210 | Loss: 0.48084303736686707 | Test loss: 0.49992427229881287\n",
      "Epoch: 63220 | Loss: 0.48083123564720154 | Test loss: 0.4999103546142578\n",
      "Epoch: 63230 | Loss: 0.4808194637298584 | Test loss: 0.4998965263366699\n",
      "Epoch: 63240 | Loss: 0.48080775141716003 | Test loss: 0.49988269805908203\n",
      "Epoch: 63250 | Loss: 0.4807959496974945 | Test loss: 0.49986886978149414\n",
      "Epoch: 63260 | Loss: 0.48078423738479614 | Test loss: 0.49985504150390625\n",
      "Epoch: 63270 | Loss: 0.4807724058628082 | Test loss: 0.49984118342399597\n",
      "Epoch: 63280 | Loss: 0.4807606637477875 | Test loss: 0.4998272955417633\n",
      "Epoch: 63290 | Loss: 0.48074889183044434 | Test loss: 0.4998134672641754\n",
      "Epoch: 63300 | Loss: 0.4807371199131012 | Test loss: 0.4997996389865875\n",
      "Epoch: 63310 | Loss: 0.48072537779808044 | Test loss: 0.49978581070899963\n",
      "Epoch: 63320 | Loss: 0.4807136654853821 | Test loss: 0.49977198243141174\n",
      "Epoch: 63330 | Loss: 0.48070189356803894 | Test loss: 0.4997580945491791\n",
      "Epoch: 63340 | Loss: 0.4806900918483734 | Test loss: 0.4997442364692688\n",
      "Epoch: 63350 | Loss: 0.48067837953567505 | Test loss: 0.4997304081916809\n",
      "Epoch: 63360 | Loss: 0.4806666076183319 | Test loss: 0.499716579914093\n",
      "Epoch: 63370 | Loss: 0.4806548058986664 | Test loss: 0.4997027516365051\n",
      "Epoch: 63380 | Loss: 0.480643093585968 | Test loss: 0.49968886375427246\n",
      "Epoch: 63390 | Loss: 0.4806313216686249 | Test loss: 0.4996750056743622\n",
      "Epoch: 63400 | Loss: 0.48061951994895935 | Test loss: 0.4996611773967743\n",
      "Epoch: 63410 | Loss: 0.480607807636261 | Test loss: 0.4996473491191864\n",
      "Epoch: 63420 | Loss: 0.48059603571891785 | Test loss: 0.49963346123695374\n",
      "Epoch: 63430 | Loss: 0.4805842936038971 | Test loss: 0.49961963295936584\n",
      "Epoch: 63440 | Loss: 0.48057252168655396 | Test loss: 0.4996057450771332\n",
      "Epoch: 63450 | Loss: 0.4805607795715332 | Test loss: 0.4995919167995453\n",
      "Epoch: 63460 | Loss: 0.4805489480495453 | Test loss: 0.4995780885219574\n",
      "Epoch: 63470 | Loss: 0.4805372357368469 | Test loss: 0.4995642602443695\n",
      "Epoch: 63480 | Loss: 0.48052549362182617 | Test loss: 0.49955040216445923\n",
      "Epoch: 63490 | Loss: 0.48051372170448303 | Test loss: 0.49953657388687134\n",
      "Epoch: 63500 | Loss: 0.4805019497871399 | Test loss: 0.49952268600463867\n",
      "Epoch: 63510 | Loss: 0.48049020767211914 | Test loss: 0.4995088577270508\n",
      "Epoch: 63520 | Loss: 0.480478435754776 | Test loss: 0.4994950294494629\n",
      "Epoch: 63530 | Loss: 0.48046666383743286 | Test loss: 0.499481201171875\n",
      "Epoch: 63540 | Loss: 0.4804549217224121 | Test loss: 0.4994673430919647\n",
      "Epoch: 63550 | Loss: 0.48044320940971375 | Test loss: 0.49945351481437683\n",
      "Epoch: 63560 | Loss: 0.48043137788772583 | Test loss: 0.49943962693214417\n",
      "Epoch: 63570 | Loss: 0.4804196357727051 | Test loss: 0.4994257986545563\n",
      "Epoch: 63580 | Loss: 0.4804079234600067 | Test loss: 0.4994119703769684\n",
      "Epoch: 63590 | Loss: 0.4803960919380188 | Test loss: 0.4993980824947357\n",
      "Epoch: 63600 | Loss: 0.48038434982299805 | Test loss: 0.4993842542171478\n",
      "Epoch: 63610 | Loss: 0.4803726375102997 | Test loss: 0.49937039613723755\n",
      "Epoch: 63620 | Loss: 0.48036083579063416 | Test loss: 0.49935656785964966\n",
      "Epoch: 63630 | Loss: 0.480349063873291 | Test loss: 0.49934273958206177\n",
      "Epoch: 63640 | Loss: 0.48033735156059265 | Test loss: 0.4993288516998291\n",
      "Epoch: 63650 | Loss: 0.4803255498409271 | Test loss: 0.4993149936199188\n",
      "Epoch: 63660 | Loss: 0.480313777923584 | Test loss: 0.49930116534233093\n",
      "Epoch: 63670 | Loss: 0.4803020656108856 | Test loss: 0.49928733706474304\n",
      "Epoch: 63680 | Loss: 0.48029032349586487 | Test loss: 0.49927350878715515\n",
      "Epoch: 63690 | Loss: 0.48027849197387695 | Test loss: 0.49925968050956726\n",
      "Epoch: 63700 | Loss: 0.4802667796611786 | Test loss: 0.4992457330226898\n",
      "Epoch: 63710 | Loss: 0.48025497794151306 | Test loss: 0.49923190474510193\n",
      "Epoch: 63720 | Loss: 0.4802432656288147 | Test loss: 0.49921807646751404\n",
      "Epoch: 63730 | Loss: 0.48023149371147156 | Test loss: 0.49920424818992615\n",
      "Epoch: 63740 | Loss: 0.48021969199180603 | Test loss: 0.49919041991233826\n",
      "Epoch: 63750 | Loss: 0.4802079200744629 | Test loss: 0.49917659163475037\n",
      "Epoch: 63760 | Loss: 0.4801962077617645 | Test loss: 0.4991626739501953\n",
      "Epoch: 63770 | Loss: 0.4801844656467438 | Test loss: 0.4991488456726074\n",
      "Epoch: 63780 | Loss: 0.48017269372940063 | Test loss: 0.49913501739501953\n",
      "Epoch: 63790 | Loss: 0.4801609218120575 | Test loss: 0.49912118911743164\n",
      "Epoch: 63800 | Loss: 0.48014917969703674 | Test loss: 0.49910736083984375\n",
      "Epoch: 63810 | Loss: 0.4801374077796936 | Test loss: 0.49909350275993347\n",
      "Epoch: 63820 | Loss: 0.48012563586235046 | Test loss: 0.4990796148777008\n",
      "Epoch: 63830 | Loss: 0.4801138937473297 | Test loss: 0.4990657866001129\n",
      "Epoch: 63840 | Loss: 0.4801021218299866 | Test loss: 0.499051958322525\n",
      "Epoch: 63850 | Loss: 0.48009034991264343 | Test loss: 0.49903807044029236\n",
      "Epoch: 63860 | Loss: 0.4800786077976227 | Test loss: 0.49902424216270447\n",
      "Epoch: 63870 | Loss: 0.48006683588027954 | Test loss: 0.4990103840827942\n",
      "Epoch: 63880 | Loss: 0.4800550639629364 | Test loss: 0.4989965558052063\n",
      "Epoch: 63890 | Loss: 0.48004332184791565 | Test loss: 0.4989827275276184\n",
      "Epoch: 63900 | Loss: 0.4800315797328949 | Test loss: 0.49896883964538574\n",
      "Epoch: 63910 | Loss: 0.48001977801322937 | Test loss: 0.49895501136779785\n",
      "Epoch: 63920 | Loss: 0.4800080358982086 | Test loss: 0.49894118309020996\n",
      "Epoch: 63930 | Loss: 0.4799962639808655 | Test loss: 0.4989273250102997\n",
      "Epoch: 63940 | Loss: 0.4799845218658447 | Test loss: 0.4989134967327118\n",
      "Epoch: 63950 | Loss: 0.4799727499485016 | Test loss: 0.4988996684551239\n",
      "Epoch: 63960 | Loss: 0.47996100783348083 | Test loss: 0.49888578057289124\n",
      "Epoch: 63970 | Loss: 0.4799492359161377 | Test loss: 0.49887195229530334\n",
      "Epoch: 63980 | Loss: 0.47993746399879456 | Test loss: 0.4988580644130707\n",
      "Epoch: 63990 | Loss: 0.4799257218837738 | Test loss: 0.4988442361354828\n",
      "Epoch: 64000 | Loss: 0.47991394996643066 | Test loss: 0.4988304078578949\n",
      "Epoch: 64010 | Loss: 0.4799022376537323 | Test loss: 0.498816579580307\n",
      "Epoch: 64020 | Loss: 0.4798904359340668 | Test loss: 0.49880266189575195\n",
      "Epoch: 64030 | Loss: 0.47987866401672363 | Test loss: 0.49878883361816406\n",
      "Epoch: 64040 | Loss: 0.47986695170402527 | Test loss: 0.49877500534057617\n",
      "Epoch: 64050 | Loss: 0.47985514998435974 | Test loss: 0.4987611770629883\n",
      "Epoch: 64060 | Loss: 0.4798434376716614 | Test loss: 0.4987473487854004\n",
      "Epoch: 64070 | Loss: 0.47983160614967346 | Test loss: 0.4987334907054901\n",
      "Epoch: 64080 | Loss: 0.4798198640346527 | Test loss: 0.49871960282325745\n",
      "Epoch: 64090 | Loss: 0.47980809211730957 | Test loss: 0.49870577454566956\n",
      "Epoch: 64100 | Loss: 0.47979632019996643 | Test loss: 0.49869194626808167\n",
      "Epoch: 64110 | Loss: 0.4797845780849457 | Test loss: 0.4986781179904938\n",
      "Epoch: 64120 | Loss: 0.4797728657722473 | Test loss: 0.4986642897129059\n",
      "Epoch: 64130 | Loss: 0.4797610938549042 | Test loss: 0.4986504018306732\n",
      "Epoch: 64140 | Loss: 0.47974929213523865 | Test loss: 0.49863654375076294\n",
      "Epoch: 64150 | Loss: 0.4797375798225403 | Test loss: 0.49862271547317505\n",
      "Epoch: 64160 | Loss: 0.47972580790519714 | Test loss: 0.49860888719558716\n",
      "Epoch: 64170 | Loss: 0.4797140061855316 | Test loss: 0.49859505891799927\n",
      "Epoch: 64180 | Loss: 0.47970229387283325 | Test loss: 0.4985811710357666\n",
      "Epoch: 64190 | Loss: 0.4796905219554901 | Test loss: 0.4985673129558563\n",
      "Epoch: 64200 | Loss: 0.4796787202358246 | Test loss: 0.49855348467826843\n",
      "Epoch: 64210 | Loss: 0.4796670079231262 | Test loss: 0.49853965640068054\n",
      "Epoch: 64220 | Loss: 0.4796552360057831 | Test loss: 0.4985257685184479\n",
      "Epoch: 64230 | Loss: 0.47964349389076233 | Test loss: 0.49851194024086\n",
      "Epoch: 64240 | Loss: 0.4796317219734192 | Test loss: 0.4984980523586273\n",
      "Epoch: 64250 | Loss: 0.47961997985839844 | Test loss: 0.49848422408103943\n",
      "Epoch: 64260 | Loss: 0.4796081483364105 | Test loss: 0.49847039580345154\n",
      "Epoch: 64270 | Loss: 0.47959643602371216 | Test loss: 0.49845656752586365\n",
      "Epoch: 64280 | Loss: 0.4795846939086914 | Test loss: 0.49844270944595337\n",
      "Epoch: 64290 | Loss: 0.47957292199134827 | Test loss: 0.4984288811683655\n",
      "Epoch: 64300 | Loss: 0.4795611500740051 | Test loss: 0.4984149932861328\n",
      "Epoch: 64310 | Loss: 0.4795494079589844 | Test loss: 0.4984011650085449\n",
      "Epoch: 64320 | Loss: 0.47953763604164124 | Test loss: 0.49838733673095703\n",
      "Epoch: 64330 | Loss: 0.4795258641242981 | Test loss: 0.49837350845336914\n",
      "Epoch: 64340 | Loss: 0.47951412200927734 | Test loss: 0.49835965037345886\n",
      "Epoch: 64350 | Loss: 0.479502409696579 | Test loss: 0.49834582209587097\n",
      "Epoch: 64360 | Loss: 0.47949057817459106 | Test loss: 0.4983319342136383\n",
      "Epoch: 64370 | Loss: 0.4794788360595703 | Test loss: 0.4983181059360504\n",
      "Epoch: 64380 | Loss: 0.47946712374687195 | Test loss: 0.4983042776584625\n",
      "Epoch: 64390 | Loss: 0.47945529222488403 | Test loss: 0.49829038977622986\n",
      "Epoch: 64400 | Loss: 0.4794435501098633 | Test loss: 0.49827656149864197\n",
      "Epoch: 64410 | Loss: 0.4794318377971649 | Test loss: 0.4982627034187317\n",
      "Epoch: 64420 | Loss: 0.4794200360774994 | Test loss: 0.4982488751411438\n",
      "Epoch: 64430 | Loss: 0.47940826416015625 | Test loss: 0.4982350468635559\n",
      "Epoch: 64440 | Loss: 0.4793965518474579 | Test loss: 0.49822115898132324\n",
      "Epoch: 64450 | Loss: 0.47938475012779236 | Test loss: 0.49820733070373535\n",
      "Epoch: 64460 | Loss: 0.4793729782104492 | Test loss: 0.4981934726238251\n",
      "Epoch: 64470 | Loss: 0.47936126589775085 | Test loss: 0.4981796443462372\n",
      "Epoch: 64480 | Loss: 0.4793495237827301 | Test loss: 0.4981658160686493\n",
      "Epoch: 64490 | Loss: 0.4793376922607422 | Test loss: 0.4981519281864166\n",
      "Epoch: 64500 | Loss: 0.4793259799480438 | Test loss: 0.49813809990882874\n",
      "Epoch: 64510 | Loss: 0.4793141782283783 | Test loss: 0.49812421202659607\n",
      "Epoch: 64520 | Loss: 0.47930246591567993 | Test loss: 0.4981103837490082\n",
      "Epoch: 64530 | Loss: 0.4792906939983368 | Test loss: 0.4980965554714203\n",
      "Epoch: 64540 | Loss: 0.47927889227867126 | Test loss: 0.4980827271938324\n",
      "Epoch: 64550 | Loss: 0.4792671203613281 | Test loss: 0.4980688691139221\n",
      "Epoch: 64560 | Loss: 0.47925540804862976 | Test loss: 0.49805504083633423\n",
      "Epoch: 64570 | Loss: 0.479243665933609 | Test loss: 0.49804115295410156\n",
      "Epoch: 64580 | Loss: 0.47923189401626587 | Test loss: 0.49802732467651367\n",
      "Epoch: 64590 | Loss: 0.47922012209892273 | Test loss: 0.4980134963989258\n",
      "Epoch: 64600 | Loss: 0.479208379983902 | Test loss: 0.4979996383190155\n",
      "Epoch: 64610 | Loss: 0.47919660806655884 | Test loss: 0.4979858100414276\n",
      "Epoch: 64620 | Loss: 0.4791848361492157 | Test loss: 0.49797192215919495\n",
      "Epoch: 64630 | Loss: 0.47917309403419495 | Test loss: 0.49795809388160706\n",
      "Epoch: 64640 | Loss: 0.4791613221168518 | Test loss: 0.49794426560401917\n",
      "Epoch: 64650 | Loss: 0.47914955019950867 | Test loss: 0.4979303777217865\n",
      "Epoch: 64660 | Loss: 0.4791378080844879 | Test loss: 0.4979165494441986\n",
      "Epoch: 64670 | Loss: 0.4791260361671448 | Test loss: 0.49790269136428833\n",
      "Epoch: 64680 | Loss: 0.47911426424980164 | Test loss: 0.49788886308670044\n",
      "Epoch: 64690 | Loss: 0.4791025221347809 | Test loss: 0.49787503480911255\n",
      "Epoch: 64700 | Loss: 0.47909078001976013 | Test loss: 0.4978611469268799\n",
      "Epoch: 64710 | Loss: 0.4790789783000946 | Test loss: 0.497847318649292\n",
      "Epoch: 64720 | Loss: 0.47906723618507385 | Test loss: 0.4978334903717041\n",
      "Epoch: 64730 | Loss: 0.4790554642677307 | Test loss: 0.4978196322917938\n",
      "Epoch: 64740 | Loss: 0.47904372215270996 | Test loss: 0.49780580401420593\n",
      "Epoch: 64750 | Loss: 0.4790319502353668 | Test loss: 0.49779197573661804\n",
      "Epoch: 64760 | Loss: 0.47902020812034607 | Test loss: 0.4977780878543854\n",
      "Epoch: 64770 | Loss: 0.47900843620300293 | Test loss: 0.4977642595767975\n",
      "Epoch: 64780 | Loss: 0.4789966642856598 | Test loss: 0.4977503716945648\n",
      "Epoch: 64790 | Loss: 0.47898492217063904 | Test loss: 0.49773654341697693\n",
      "Epoch: 64800 | Loss: 0.4789731502532959 | Test loss: 0.49772271513938904\n",
      "Epoch: 64810 | Loss: 0.47896143794059753 | Test loss: 0.49770888686180115\n",
      "Epoch: 64820 | Loss: 0.478949636220932 | Test loss: 0.49769502878189087\n",
      "Epoch: 64830 | Loss: 0.47893786430358887 | Test loss: 0.4976811408996582\n",
      "Epoch: 64840 | Loss: 0.4789261519908905 | Test loss: 0.4976673126220703\n",
      "Epoch: 64850 | Loss: 0.478914350271225 | Test loss: 0.4976534843444824\n",
      "Epoch: 64860 | Loss: 0.4789026379585266 | Test loss: 0.49763962626457214\n",
      "Epoch: 64870 | Loss: 0.4788908064365387 | Test loss: 0.49762579798698425\n",
      "Epoch: 64880 | Loss: 0.47887906432151794 | Test loss: 0.4976119101047516\n",
      "Epoch: 64890 | Loss: 0.4788672924041748 | Test loss: 0.4975980818271637\n",
      "Epoch: 64900 | Loss: 0.47885552048683167 | Test loss: 0.4975842535495758\n",
      "Epoch: 64910 | Loss: 0.4788437783718109 | Test loss: 0.4975704252719879\n",
      "Epoch: 64920 | Loss: 0.47883206605911255 | Test loss: 0.49755653738975525\n",
      "Epoch: 64930 | Loss: 0.4788202941417694 | Test loss: 0.49754270911216736\n",
      "Epoch: 64940 | Loss: 0.4788084924221039 | Test loss: 0.4975288510322571\n",
      "Epoch: 64950 | Loss: 0.4787967801094055 | Test loss: 0.4975150227546692\n",
      "Epoch: 64960 | Loss: 0.4787850081920624 | Test loss: 0.4975011944770813\n",
      "Epoch: 64970 | Loss: 0.47877320647239685 | Test loss: 0.4974873661994934\n",
      "Epoch: 64980 | Loss: 0.4787614941596985 | Test loss: 0.49747347831726074\n",
      "Epoch: 64990 | Loss: 0.47874972224235535 | Test loss: 0.49745965003967285\n",
      "Epoch: 65000 | Loss: 0.4787379205226898 | Test loss: 0.4974457919597626\n",
      "Epoch: 65010 | Loss: 0.47872620820999146 | Test loss: 0.4974319636821747\n",
      "Epoch: 65020 | Loss: 0.4787144362926483 | Test loss: 0.497418075799942\n",
      "Epoch: 65030 | Loss: 0.47870269417762756 | Test loss: 0.4974042475223541\n",
      "Epoch: 65040 | Loss: 0.4786909222602844 | Test loss: 0.49739035964012146\n",
      "Epoch: 65050 | Loss: 0.47867918014526367 | Test loss: 0.49737653136253357\n",
      "Epoch: 65060 | Loss: 0.47866734862327576 | Test loss: 0.4973627030849457\n",
      "Epoch: 65070 | Loss: 0.4786556363105774 | Test loss: 0.4973488748073578\n",
      "Epoch: 65080 | Loss: 0.47864389419555664 | Test loss: 0.4973350167274475\n",
      "Epoch: 65090 | Loss: 0.4786321222782135 | Test loss: 0.4973211884498596\n",
      "Epoch: 65100 | Loss: 0.47862035036087036 | Test loss: 0.49730730056762695\n",
      "Epoch: 65110 | Loss: 0.4786086082458496 | Test loss: 0.49729347229003906\n",
      "Epoch: 65120 | Loss: 0.47859683632850647 | Test loss: 0.49727964401245117\n",
      "Epoch: 65130 | Loss: 0.47858506441116333 | Test loss: 0.4972658157348633\n",
      "Epoch: 65140 | Loss: 0.4785733222961426 | Test loss: 0.497251957654953\n",
      "Epoch: 65150 | Loss: 0.4785616099834442 | Test loss: 0.4972381293773651\n",
      "Epoch: 65160 | Loss: 0.4785497784614563 | Test loss: 0.49722424149513245\n",
      "Epoch: 65170 | Loss: 0.47853803634643555 | Test loss: 0.49721041321754456\n",
      "Epoch: 65180 | Loss: 0.4785263240337372 | Test loss: 0.49719658493995667\n",
      "Epoch: 65190 | Loss: 0.47851449251174927 | Test loss: 0.497182697057724\n",
      "Epoch: 65200 | Loss: 0.4785027503967285 | Test loss: 0.4971688687801361\n",
      "Epoch: 65210 | Loss: 0.47849103808403015 | Test loss: 0.49715501070022583\n",
      "Epoch: 65220 | Loss: 0.4784792363643646 | Test loss: 0.49714118242263794\n",
      "Epoch: 65230 | Loss: 0.4784674644470215 | Test loss: 0.49712735414505005\n",
      "Epoch: 65240 | Loss: 0.4784557521343231 | Test loss: 0.4971134662628174\n",
      "Epoch: 65250 | Loss: 0.4784439504146576 | Test loss: 0.4970996379852295\n",
      "Epoch: 65260 | Loss: 0.47843217849731445 | Test loss: 0.4970857799053192\n",
      "Epoch: 65270 | Loss: 0.4784204661846161 | Test loss: 0.4970719516277313\n",
      "Epoch: 65280 | Loss: 0.47840872406959534 | Test loss: 0.49705812335014343\n",
      "Epoch: 65290 | Loss: 0.4783968925476074 | Test loss: 0.49704423546791077\n",
      "Epoch: 65300 | Loss: 0.47838518023490906 | Test loss: 0.4970304071903229\n",
      "Epoch: 65310 | Loss: 0.47837337851524353 | Test loss: 0.4970165193080902\n",
      "Epoch: 65320 | Loss: 0.47836166620254517 | Test loss: 0.4970026910305023\n",
      "Epoch: 65330 | Loss: 0.478349894285202 | Test loss: 0.49698886275291443\n",
      "Epoch: 65340 | Loss: 0.4783380925655365 | Test loss: 0.49697503447532654\n",
      "Epoch: 65350 | Loss: 0.47832632064819336 | Test loss: 0.49696117639541626\n",
      "Epoch: 65360 | Loss: 0.478314608335495 | Test loss: 0.49694734811782837\n",
      "Epoch: 65370 | Loss: 0.47830286622047424 | Test loss: 0.4969334602355957\n",
      "Epoch: 65380 | Loss: 0.4782910943031311 | Test loss: 0.4969196319580078\n",
      "Epoch: 65390 | Loss: 0.47827932238578796 | Test loss: 0.4969058036804199\n",
      "Epoch: 65400 | Loss: 0.4782675802707672 | Test loss: 0.49689194560050964\n",
      "Epoch: 65410 | Loss: 0.4782558083534241 | Test loss: 0.49687811732292175\n",
      "Epoch: 65420 | Loss: 0.47824403643608093 | Test loss: 0.4968642294406891\n",
      "Epoch: 65430 | Loss: 0.4782322943210602 | Test loss: 0.4968504011631012\n",
      "Epoch: 65440 | Loss: 0.47822052240371704 | Test loss: 0.4968365728855133\n",
      "Epoch: 65450 | Loss: 0.4782087504863739 | Test loss: 0.49682268500328064\n",
      "Epoch: 65460 | Loss: 0.47819700837135315 | Test loss: 0.49680885672569275\n",
      "Epoch: 65470 | Loss: 0.47818523645401 | Test loss: 0.49679499864578247\n",
      "Epoch: 65480 | Loss: 0.47817346453666687 | Test loss: 0.4967811703681946\n",
      "Epoch: 65490 | Loss: 0.4781617224216461 | Test loss: 0.4967673420906067\n",
      "Epoch: 65500 | Loss: 0.47814998030662537 | Test loss: 0.496753454208374\n",
      "Epoch: 65510 | Loss: 0.47813817858695984 | Test loss: 0.49673962593078613\n",
      "Epoch: 65520 | Loss: 0.4781264364719391 | Test loss: 0.49672579765319824\n",
      "Epoch: 65530 | Loss: 0.47811466455459595 | Test loss: 0.49671193957328796\n",
      "Epoch: 65540 | Loss: 0.4781029224395752 | Test loss: 0.4966981112957001\n",
      "Epoch: 65550 | Loss: 0.47809115052223206 | Test loss: 0.4966842830181122\n",
      "Epoch: 65560 | Loss: 0.4780794084072113 | Test loss: 0.4966703951358795\n",
      "Epoch: 65570 | Loss: 0.47806763648986816 | Test loss: 0.4966565668582916\n",
      "Epoch: 65580 | Loss: 0.478055864572525 | Test loss: 0.49664267897605896\n",
      "Epoch: 65590 | Loss: 0.4780441224575043 | Test loss: 0.49662885069847107\n",
      "Epoch: 65600 | Loss: 0.47803235054016113 | Test loss: 0.4966150224208832\n",
      "Epoch: 65610 | Loss: 0.47802063822746277 | Test loss: 0.4966011941432953\n",
      "Epoch: 65620 | Loss: 0.47800883650779724 | Test loss: 0.496587336063385\n",
      "Epoch: 65630 | Loss: 0.4779970645904541 | Test loss: 0.49657344818115234\n",
      "Epoch: 65640 | Loss: 0.47798535227775574 | Test loss: 0.49655961990356445\n",
      "Epoch: 65650 | Loss: 0.4779735505580902 | Test loss: 0.49654579162597656\n",
      "Epoch: 65660 | Loss: 0.47796183824539185 | Test loss: 0.4965319335460663\n",
      "Epoch: 65670 | Loss: 0.47795000672340393 | Test loss: 0.4965181052684784\n",
      "Epoch: 65680 | Loss: 0.4779382646083832 | Test loss: 0.4965042173862457\n",
      "Epoch: 65690 | Loss: 0.47792649269104004 | Test loss: 0.49649038910865784\n",
      "Epoch: 65700 | Loss: 0.4779147207736969 | Test loss: 0.49647656083106995\n",
      "Epoch: 65710 | Loss: 0.47790297865867615 | Test loss: 0.49646273255348206\n",
      "Epoch: 65720 | Loss: 0.4778912663459778 | Test loss: 0.4964488446712494\n",
      "Epoch: 65730 | Loss: 0.47787949442863464 | Test loss: 0.4964350163936615\n",
      "Epoch: 65740 | Loss: 0.4778676927089691 | Test loss: 0.4964211583137512\n",
      "Epoch: 65750 | Loss: 0.47785598039627075 | Test loss: 0.49640733003616333\n",
      "Epoch: 65760 | Loss: 0.4778442084789276 | Test loss: 0.49639350175857544\n",
      "Epoch: 65770 | Loss: 0.4778324067592621 | Test loss: 0.49637967348098755\n",
      "Epoch: 65780 | Loss: 0.4778206944465637 | Test loss: 0.4963657855987549\n",
      "Epoch: 65790 | Loss: 0.4778089225292206 | Test loss: 0.496351957321167\n",
      "Epoch: 65800 | Loss: 0.47779712080955505 | Test loss: 0.4963380992412567\n",
      "Epoch: 65810 | Loss: 0.4777854084968567 | Test loss: 0.4963242709636688\n",
      "Epoch: 65820 | Loss: 0.47777363657951355 | Test loss: 0.49631038308143616\n",
      "Epoch: 65830 | Loss: 0.4777618944644928 | Test loss: 0.49629655480384827\n",
      "Epoch: 65840 | Loss: 0.47775012254714966 | Test loss: 0.4962826669216156\n",
      "Epoch: 65850 | Loss: 0.4777383804321289 | Test loss: 0.4962688386440277\n",
      "Epoch: 65860 | Loss: 0.477726548910141 | Test loss: 0.4962550103664398\n",
      "Epoch: 65870 | Loss: 0.4777148365974426 | Test loss: 0.49624118208885193\n",
      "Epoch: 65880 | Loss: 0.4777030944824219 | Test loss: 0.49622732400894165\n",
      "Epoch: 65890 | Loss: 0.47769132256507874 | Test loss: 0.49621349573135376\n",
      "Epoch: 65900 | Loss: 0.4776795506477356 | Test loss: 0.4961996078491211\n",
      "Epoch: 65910 | Loss: 0.47766780853271484 | Test loss: 0.4961857795715332\n",
      "Epoch: 65920 | Loss: 0.4776560366153717 | Test loss: 0.4961719512939453\n",
      "Epoch: 65930 | Loss: 0.47764426469802856 | Test loss: 0.4961581230163574\n",
      "Epoch: 65940 | Loss: 0.4776325225830078 | Test loss: 0.49614426493644714\n",
      "Epoch: 65950 | Loss: 0.47762081027030945 | Test loss: 0.49613043665885925\n",
      "Epoch: 65960 | Loss: 0.47760897874832153 | Test loss: 0.4961165487766266\n",
      "Epoch: 65970 | Loss: 0.4775972366333008 | Test loss: 0.4961027204990387\n",
      "Epoch: 65980 | Loss: 0.4775855243206024 | Test loss: 0.4960888922214508\n",
      "Epoch: 65990 | Loss: 0.4775736927986145 | Test loss: 0.49607500433921814\n",
      "Epoch: 66000 | Loss: 0.47756195068359375 | Test loss: 0.49606117606163025\n",
      "Epoch: 66010 | Loss: 0.4775502383708954 | Test loss: 0.49604731798171997\n",
      "Epoch: 66020 | Loss: 0.47753843665122986 | Test loss: 0.4960334897041321\n",
      "Epoch: 66030 | Loss: 0.4775266647338867 | Test loss: 0.4960196614265442\n",
      "Epoch: 66040 | Loss: 0.47751495242118835 | Test loss: 0.4960057735443115\n",
      "Epoch: 66050 | Loss: 0.4775031507015228 | Test loss: 0.49599194526672363\n",
      "Epoch: 66060 | Loss: 0.4774913787841797 | Test loss: 0.49597808718681335\n",
      "Epoch: 66070 | Loss: 0.4774796664714813 | Test loss: 0.49596425890922546\n",
      "Epoch: 66080 | Loss: 0.47746792435646057 | Test loss: 0.4959504306316376\n",
      "Epoch: 66090 | Loss: 0.47745609283447266 | Test loss: 0.4959365427494049\n",
      "Epoch: 66100 | Loss: 0.4774443805217743 | Test loss: 0.495922714471817\n",
      "Epoch: 66110 | Loss: 0.47743257880210876 | Test loss: 0.49590882658958435\n",
      "Epoch: 66120 | Loss: 0.4774208664894104 | Test loss: 0.49589499831199646\n",
      "Epoch: 66130 | Loss: 0.47740909457206726 | Test loss: 0.49588117003440857\n",
      "Epoch: 66140 | Loss: 0.47739729285240173 | Test loss: 0.4958673417568207\n",
      "Epoch: 66150 | Loss: 0.4773855209350586 | Test loss: 0.4958534836769104\n",
      "Epoch: 66160 | Loss: 0.47737380862236023 | Test loss: 0.4958396553993225\n",
      "Epoch: 66170 | Loss: 0.4773620665073395 | Test loss: 0.49582576751708984\n",
      "Epoch: 66180 | Loss: 0.47735029458999634 | Test loss: 0.49581193923950195\n",
      "Epoch: 66190 | Loss: 0.4773385226726532 | Test loss: 0.49579811096191406\n",
      "Epoch: 66200 | Loss: 0.47732678055763245 | Test loss: 0.4957842528820038\n",
      "Epoch: 66210 | Loss: 0.4773150086402893 | Test loss: 0.4957704246044159\n",
      "Epoch: 66220 | Loss: 0.47730323672294617 | Test loss: 0.4957565367221832\n",
      "Epoch: 66230 | Loss: 0.4772914946079254 | Test loss: 0.49574270844459534\n",
      "Epoch: 66240 | Loss: 0.4772797226905823 | Test loss: 0.49572888016700745\n",
      "Epoch: 66250 | Loss: 0.47726795077323914 | Test loss: 0.4957149922847748\n",
      "Epoch: 66260 | Loss: 0.4772562086582184 | Test loss: 0.4957011640071869\n",
      "Epoch: 66270 | Loss: 0.47724443674087524 | Test loss: 0.4956873059272766\n",
      "Epoch: 66280 | Loss: 0.4772326648235321 | Test loss: 0.4956734776496887\n",
      "Epoch: 66290 | Loss: 0.47722092270851135 | Test loss: 0.49565964937210083\n",
      "Epoch: 66300 | Loss: 0.4772091805934906 | Test loss: 0.49564576148986816\n",
      "Epoch: 66310 | Loss: 0.4771973788738251 | Test loss: 0.4956319332122803\n",
      "Epoch: 66320 | Loss: 0.4771856367588043 | Test loss: 0.4956181049346924\n",
      "Epoch: 66330 | Loss: 0.4771738648414612 | Test loss: 0.4956042468547821\n",
      "Epoch: 66340 | Loss: 0.47716212272644043 | Test loss: 0.4955904185771942\n",
      "Epoch: 66350 | Loss: 0.4771503508090973 | Test loss: 0.4955765902996063\n",
      "Epoch: 66360 | Loss: 0.47713860869407654 | Test loss: 0.49556270241737366\n",
      "Epoch: 66370 | Loss: 0.4771268367767334 | Test loss: 0.49554887413978577\n",
      "Epoch: 66380 | Loss: 0.47711506485939026 | Test loss: 0.4955349862575531\n",
      "Epoch: 66390 | Loss: 0.4771033227443695 | Test loss: 0.4955211579799652\n",
      "Epoch: 66400 | Loss: 0.47709155082702637 | Test loss: 0.4955073297023773\n",
      "Epoch: 66410 | Loss: 0.477079838514328 | Test loss: 0.49549350142478943\n",
      "Epoch: 66420 | Loss: 0.4770680367946625 | Test loss: 0.49547964334487915\n",
      "Epoch: 66430 | Loss: 0.47705626487731934 | Test loss: 0.4954657554626465\n",
      "Epoch: 66440 | Loss: 0.47704455256462097 | Test loss: 0.4954519271850586\n",
      "Epoch: 66450 | Loss: 0.47703275084495544 | Test loss: 0.4954380989074707\n",
      "Epoch: 66460 | Loss: 0.4770210385322571 | Test loss: 0.4954242408275604\n",
      "Epoch: 66470 | Loss: 0.47700920701026917 | Test loss: 0.49541041254997253\n",
      "Epoch: 66480 | Loss: 0.4769974648952484 | Test loss: 0.49539652466773987\n",
      "Epoch: 66490 | Loss: 0.4769856929779053 | Test loss: 0.495382696390152\n",
      "Epoch: 66500 | Loss: 0.47697392106056213 | Test loss: 0.4953688681125641\n",
      "Epoch: 66510 | Loss: 0.4769621789455414 | Test loss: 0.4953550398349762\n",
      "Epoch: 66520 | Loss: 0.476950466632843 | Test loss: 0.49534115195274353\n",
      "Epoch: 66530 | Loss: 0.4769386947154999 | Test loss: 0.49532732367515564\n",
      "Epoch: 66540 | Loss: 0.47692689299583435 | Test loss: 0.49531346559524536\n",
      "Epoch: 66550 | Loss: 0.476915180683136 | Test loss: 0.49529963731765747\n",
      "Epoch: 66560 | Loss: 0.47690340876579285 | Test loss: 0.4952858090400696\n",
      "Epoch: 66570 | Loss: 0.4768916070461273 | Test loss: 0.4952719807624817\n",
      "Epoch: 66580 | Loss: 0.47687989473342896 | Test loss: 0.495258092880249\n",
      "Epoch: 66590 | Loss: 0.4768681228160858 | Test loss: 0.49524426460266113\n",
      "Epoch: 66600 | Loss: 0.4768563210964203 | Test loss: 0.49523040652275085\n",
      "Epoch: 66610 | Loss: 0.4768446087837219 | Test loss: 0.49521657824516296\n",
      "Epoch: 66620 | Loss: 0.4768328368663788 | Test loss: 0.4952026903629303\n",
      "Epoch: 66630 | Loss: 0.47682109475135803 | Test loss: 0.4951888620853424\n",
      "Epoch: 66640 | Loss: 0.4768093228340149 | Test loss: 0.49517497420310974\n",
      "Epoch: 66650 | Loss: 0.47679758071899414 | Test loss: 0.49516114592552185\n",
      "Epoch: 66660 | Loss: 0.4767857491970062 | Test loss: 0.49514731764793396\n",
      "Epoch: 66670 | Loss: 0.47677403688430786 | Test loss: 0.49513348937034607\n",
      "Epoch: 66680 | Loss: 0.4767622947692871 | Test loss: 0.4951196312904358\n",
      "Epoch: 66690 | Loss: 0.47675052285194397 | Test loss: 0.4951058030128479\n",
      "Epoch: 66700 | Loss: 0.47673875093460083 | Test loss: 0.49509191513061523\n",
      "Epoch: 66710 | Loss: 0.4767270088195801 | Test loss: 0.49507808685302734\n",
      "Epoch: 66720 | Loss: 0.47671523690223694 | Test loss: 0.49506425857543945\n",
      "Epoch: 66730 | Loss: 0.4767034649848938 | Test loss: 0.49505043029785156\n",
      "Epoch: 66740 | Loss: 0.47669172286987305 | Test loss: 0.4950365722179413\n",
      "Epoch: 66750 | Loss: 0.4766800105571747 | Test loss: 0.4950227439403534\n",
      "Epoch: 66760 | Loss: 0.47666817903518677 | Test loss: 0.4950088560581207\n",
      "Epoch: 66770 | Loss: 0.476656436920166 | Test loss: 0.49499502778053284\n",
      "Epoch: 66780 | Loss: 0.47664472460746765 | Test loss: 0.49498119950294495\n",
      "Epoch: 66790 | Loss: 0.47663289308547974 | Test loss: 0.4949673116207123\n",
      "Epoch: 66800 | Loss: 0.476621150970459 | Test loss: 0.4949534833431244\n",
      "Epoch: 66810 | Loss: 0.4766094386577606 | Test loss: 0.4949396252632141\n",
      "Epoch: 66820 | Loss: 0.4765976369380951 | Test loss: 0.4949257969856262\n",
      "Epoch: 66830 | Loss: 0.47658586502075195 | Test loss: 0.49491196870803833\n",
      "Epoch: 66840 | Loss: 0.4765741527080536 | Test loss: 0.49489808082580566\n",
      "Epoch: 66850 | Loss: 0.47656235098838806 | Test loss: 0.4948842525482178\n",
      "Epoch: 66860 | Loss: 0.4765505790710449 | Test loss: 0.4948703944683075\n",
      "Epoch: 66870 | Loss: 0.47653886675834656 | Test loss: 0.4948565661907196\n",
      "Epoch: 66880 | Loss: 0.4765271246433258 | Test loss: 0.4948427379131317\n",
      "Epoch: 66890 | Loss: 0.4765152931213379 | Test loss: 0.49482885003089905\n",
      "Epoch: 66900 | Loss: 0.4765035808086395 | Test loss: 0.49481502175331116\n",
      "Epoch: 66910 | Loss: 0.476491779088974 | Test loss: 0.4948011338710785\n",
      "Epoch: 66920 | Loss: 0.47648006677627563 | Test loss: 0.4947873055934906\n",
      "Epoch: 66930 | Loss: 0.4764682948589325 | Test loss: 0.4947734773159027\n",
      "Epoch: 66940 | Loss: 0.47645649313926697 | Test loss: 0.4947596490383148\n",
      "Epoch: 66950 | Loss: 0.47644472122192383 | Test loss: 0.49474579095840454\n",
      "Epoch: 66960 | Loss: 0.47643300890922546 | Test loss: 0.49473196268081665\n",
      "Epoch: 66970 | Loss: 0.4764212667942047 | Test loss: 0.494718074798584\n",
      "Epoch: 66980 | Loss: 0.4764094948768616 | Test loss: 0.4947042465209961\n",
      "Epoch: 66990 | Loss: 0.47639772295951843 | Test loss: 0.4946904182434082\n",
      "Epoch: 67000 | Loss: 0.4763859808444977 | Test loss: 0.4946765601634979\n",
      "Epoch: 67010 | Loss: 0.47637420892715454 | Test loss: 0.49466273188591003\n",
      "Epoch: 67020 | Loss: 0.4763624370098114 | Test loss: 0.49464884400367737\n",
      "Epoch: 67030 | Loss: 0.47635069489479065 | Test loss: 0.4946350157260895\n",
      "Epoch: 67040 | Loss: 0.4763389229774475 | Test loss: 0.4946211874485016\n",
      "Epoch: 67050 | Loss: 0.47632715106010437 | Test loss: 0.4946072995662689\n",
      "Epoch: 67060 | Loss: 0.4763154089450836 | Test loss: 0.49459347128868103\n",
      "Epoch: 67070 | Loss: 0.4763036370277405 | Test loss: 0.49457961320877075\n",
      "Epoch: 67080 | Loss: 0.47629186511039734 | Test loss: 0.49456578493118286\n",
      "Epoch: 67090 | Loss: 0.4762801229953766 | Test loss: 0.49455195665359497\n",
      "Epoch: 67100 | Loss: 0.47626838088035583 | Test loss: 0.4945380687713623\n",
      "Epoch: 67110 | Loss: 0.4762565791606903 | Test loss: 0.4945242404937744\n",
      "Epoch: 67120 | Loss: 0.47624483704566956 | Test loss: 0.4945104122161865\n",
      "Epoch: 67130 | Loss: 0.4762330651283264 | Test loss: 0.49449655413627625\n",
      "Epoch: 67140 | Loss: 0.47622132301330566 | Test loss: 0.49448272585868835\n",
      "Epoch: 67150 | Loss: 0.4762095510959625 | Test loss: 0.49446889758110046\n",
      "Epoch: 67160 | Loss: 0.4761978089809418 | Test loss: 0.4944550096988678\n",
      "Epoch: 67170 | Loss: 0.47618603706359863 | Test loss: 0.4944411814212799\n",
      "Epoch: 67180 | Loss: 0.4761742651462555 | Test loss: 0.49442729353904724\n",
      "Epoch: 67190 | Loss: 0.47616252303123474 | Test loss: 0.49441346526145935\n",
      "Epoch: 67200 | Loss: 0.4761507511138916 | Test loss: 0.49439963698387146\n",
      "Epoch: 67210 | Loss: 0.47613903880119324 | Test loss: 0.49438580870628357\n",
      "Epoch: 67220 | Loss: 0.4761272370815277 | Test loss: 0.4943719506263733\n",
      "Epoch: 67230 | Loss: 0.47611546516418457 | Test loss: 0.4943580627441406\n",
      "Epoch: 67240 | Loss: 0.4761037528514862 | Test loss: 0.49434423446655273\n",
      "Epoch: 67250 | Loss: 0.4760919511318207 | Test loss: 0.49433040618896484\n",
      "Epoch: 67260 | Loss: 0.4760802388191223 | Test loss: 0.49431654810905457\n",
      "Epoch: 67270 | Loss: 0.4760684072971344 | Test loss: 0.4943027198314667\n",
      "Epoch: 67280 | Loss: 0.47605666518211365 | Test loss: 0.494288831949234\n",
      "Epoch: 67290 | Loss: 0.4760448932647705 | Test loss: 0.4942750036716461\n",
      "Epoch: 67300 | Loss: 0.47603312134742737 | Test loss: 0.4942611753940582\n",
      "Epoch: 67310 | Loss: 0.4760213792324066 | Test loss: 0.49424734711647034\n",
      "Epoch: 67320 | Loss: 0.47600966691970825 | Test loss: 0.49423345923423767\n",
      "Epoch: 67330 | Loss: 0.4759978950023651 | Test loss: 0.4942196309566498\n",
      "Epoch: 67340 | Loss: 0.4759860932826996 | Test loss: 0.4942057728767395\n",
      "Epoch: 67350 | Loss: 0.4759743809700012 | Test loss: 0.4941919445991516\n",
      "Epoch: 67360 | Loss: 0.4759626090526581 | Test loss: 0.4941781163215637\n",
      "Epoch: 67370 | Loss: 0.47595080733299255 | Test loss: 0.49416428804397583\n",
      "Epoch: 67380 | Loss: 0.4759390950202942 | Test loss: 0.49415040016174316\n",
      "Epoch: 67390 | Loss: 0.47592732310295105 | Test loss: 0.4941365718841553\n",
      "Epoch: 67400 | Loss: 0.4759155213832855 | Test loss: 0.494122713804245\n",
      "Epoch: 67410 | Loss: 0.47590380907058716 | Test loss: 0.4941088855266571\n",
      "Epoch: 67420 | Loss: 0.475892037153244 | Test loss: 0.49409499764442444\n",
      "Epoch: 67430 | Loss: 0.47588029503822327 | Test loss: 0.49408116936683655\n",
      "Epoch: 67440 | Loss: 0.4758685231208801 | Test loss: 0.4940672814846039\n",
      "Epoch: 67450 | Loss: 0.4758567810058594 | Test loss: 0.494053453207016\n",
      "Epoch: 67460 | Loss: 0.47584494948387146 | Test loss: 0.4940396249294281\n",
      "Epoch: 67470 | Loss: 0.4758332371711731 | Test loss: 0.4940257966518402\n",
      "Epoch: 67480 | Loss: 0.47582149505615234 | Test loss: 0.49401193857192993\n",
      "Epoch: 67490 | Loss: 0.4758097231388092 | Test loss: 0.49399811029434204\n",
      "Epoch: 67500 | Loss: 0.47579795122146606 | Test loss: 0.4939842224121094\n",
      "Epoch: 67510 | Loss: 0.4757862091064453 | Test loss: 0.4939703941345215\n",
      "Epoch: 67520 | Loss: 0.4757744371891022 | Test loss: 0.4939565658569336\n",
      "Epoch: 67530 | Loss: 0.47576266527175903 | Test loss: 0.4939427375793457\n",
      "Epoch: 67540 | Loss: 0.4757509231567383 | Test loss: 0.4939288794994354\n",
      "Epoch: 67550 | Loss: 0.4757392108440399 | Test loss: 0.49391505122184753\n",
      "Epoch: 67560 | Loss: 0.475727379322052 | Test loss: 0.49390116333961487\n",
      "Epoch: 67570 | Loss: 0.47571563720703125 | Test loss: 0.493887335062027\n",
      "Epoch: 67580 | Loss: 0.4757039248943329 | Test loss: 0.4938735067844391\n",
      "Epoch: 67590 | Loss: 0.47569209337234497 | Test loss: 0.4938596189022064\n",
      "Epoch: 67600 | Loss: 0.4756803512573242 | Test loss: 0.49384579062461853\n",
      "Epoch: 67610 | Loss: 0.47566863894462585 | Test loss: 0.49383193254470825\n",
      "Epoch: 67620 | Loss: 0.4756568372249603 | Test loss: 0.49381810426712036\n",
      "Epoch: 67630 | Loss: 0.4756450653076172 | Test loss: 0.49380427598953247\n",
      "Epoch: 67640 | Loss: 0.4756333529949188 | Test loss: 0.4937903881072998\n",
      "Epoch: 67650 | Loss: 0.4756215512752533 | Test loss: 0.4937765598297119\n",
      "Epoch: 67660 | Loss: 0.47560977935791016 | Test loss: 0.49376270174980164\n",
      "Epoch: 67670 | Loss: 0.4755980670452118 | Test loss: 0.49374887347221375\n",
      "Epoch: 67680 | Loss: 0.47558632493019104 | Test loss: 0.49373504519462585\n",
      "Epoch: 67690 | Loss: 0.4755744934082031 | Test loss: 0.4937211573123932\n",
      "Epoch: 67700 | Loss: 0.47556278109550476 | Test loss: 0.4937073290348053\n",
      "Epoch: 67710 | Loss: 0.47555097937583923 | Test loss: 0.49369344115257263\n",
      "Epoch: 67720 | Loss: 0.47553926706314087 | Test loss: 0.49367961287498474\n",
      "Epoch: 67730 | Loss: 0.47552749514579773 | Test loss: 0.49366578459739685\n",
      "Epoch: 67740 | Loss: 0.4755156934261322 | Test loss: 0.49365195631980896\n",
      "Epoch: 67750 | Loss: 0.47550392150878906 | Test loss: 0.4936380982398987\n",
      "Epoch: 67760 | Loss: 0.4754922091960907 | Test loss: 0.4936242699623108\n",
      "Epoch: 67770 | Loss: 0.47548046708106995 | Test loss: 0.4936103820800781\n",
      "Epoch: 67780 | Loss: 0.4754686951637268 | Test loss: 0.49359655380249023\n",
      "Epoch: 67790 | Loss: 0.47545692324638367 | Test loss: 0.49358272552490234\n",
      "Epoch: 67800 | Loss: 0.4754451811313629 | Test loss: 0.49356886744499207\n",
      "Epoch: 67810 | Loss: 0.4754334092140198 | Test loss: 0.4935550391674042\n",
      "Epoch: 67820 | Loss: 0.47542163729667664 | Test loss: 0.4935411512851715\n",
      "Epoch: 67830 | Loss: 0.4754098951816559 | Test loss: 0.4935273230075836\n",
      "Epoch: 67840 | Loss: 0.47539812326431274 | Test loss: 0.4935134947299957\n",
      "Epoch: 67850 | Loss: 0.4753863513469696 | Test loss: 0.49349960684776306\n",
      "Epoch: 67860 | Loss: 0.47537460923194885 | Test loss: 0.49348577857017517\n",
      "Epoch: 67870 | Loss: 0.4753628373146057 | Test loss: 0.4934719204902649\n",
      "Epoch: 67880 | Loss: 0.4753510653972626 | Test loss: 0.493458092212677\n",
      "Epoch: 67890 | Loss: 0.4753393232822418 | Test loss: 0.4934442639350891\n",
      "Epoch: 67900 | Loss: 0.47532758116722107 | Test loss: 0.49343037605285645\n",
      "Epoch: 67910 | Loss: 0.47531577944755554 | Test loss: 0.49341654777526855\n",
      "Epoch: 67920 | Loss: 0.4753040373325348 | Test loss: 0.49340271949768066\n",
      "Epoch: 67930 | Loss: 0.47529226541519165 | Test loss: 0.4933888614177704\n",
      "Epoch: 67940 | Loss: 0.4752805233001709 | Test loss: 0.4933750331401825\n",
      "Epoch: 67950 | Loss: 0.47526875138282776 | Test loss: 0.4933612048625946\n",
      "Epoch: 67960 | Loss: 0.475257009267807 | Test loss: 0.49334731698036194\n",
      "Epoch: 67970 | Loss: 0.47524523735046387 | Test loss: 0.49333348870277405\n",
      "Epoch: 67980 | Loss: 0.4752334654331207 | Test loss: 0.4933196008205414\n",
      "Epoch: 67990 | Loss: 0.4752217233181 | Test loss: 0.4933057725429535\n",
      "Epoch: 68000 | Loss: 0.47520995140075684 | Test loss: 0.4932919442653656\n",
      "Epoch: 68010 | Loss: 0.47519823908805847 | Test loss: 0.4932781159877777\n",
      "Epoch: 68020 | Loss: 0.47518643736839294 | Test loss: 0.49326425790786743\n",
      "Epoch: 68030 | Loss: 0.4751746654510498 | Test loss: 0.49325037002563477\n",
      "Epoch: 68040 | Loss: 0.47516295313835144 | Test loss: 0.4932365417480469\n",
      "Epoch: 68050 | Loss: 0.4751511514186859 | Test loss: 0.493222713470459\n",
      "Epoch: 68060 | Loss: 0.47513943910598755 | Test loss: 0.4932088553905487\n",
      "Epoch: 68070 | Loss: 0.47512760758399963 | Test loss: 0.4931950271129608\n",
      "Epoch: 68080 | Loss: 0.4751158654689789 | Test loss: 0.49318113923072815\n",
      "Epoch: 68090 | Loss: 0.47510409355163574 | Test loss: 0.49316731095314026\n",
      "Epoch: 68100 | Loss: 0.4750923216342926 | Test loss: 0.49315348267555237\n",
      "Epoch: 68110 | Loss: 0.47508057951927185 | Test loss: 0.4931396543979645\n",
      "Epoch: 68120 | Loss: 0.4750688672065735 | Test loss: 0.4931257665157318\n",
      "Epoch: 68130 | Loss: 0.47505709528923035 | Test loss: 0.4931119382381439\n",
      "Epoch: 68140 | Loss: 0.4750452935695648 | Test loss: 0.49309808015823364\n",
      "Epoch: 68150 | Loss: 0.47503358125686646 | Test loss: 0.49308425188064575\n",
      "Epoch: 68160 | Loss: 0.4750218093395233 | Test loss: 0.49307042360305786\n",
      "Epoch: 68170 | Loss: 0.4750100076198578 | Test loss: 0.49305659532546997\n",
      "Epoch: 68180 | Loss: 0.4749982953071594 | Test loss: 0.4930427074432373\n",
      "Epoch: 68190 | Loss: 0.4749865233898163 | Test loss: 0.4930288791656494\n",
      "Epoch: 68200 | Loss: 0.47497472167015076 | Test loss: 0.49301502108573914\n",
      "Epoch: 68210 | Loss: 0.4749630093574524 | Test loss: 0.49300119280815125\n",
      "Epoch: 68220 | Loss: 0.47495123744010925 | Test loss: 0.4929873049259186\n",
      "Epoch: 68230 | Loss: 0.4749394953250885 | Test loss: 0.4929734766483307\n",
      "Epoch: 68240 | Loss: 0.47492772340774536 | Test loss: 0.492959588766098\n",
      "Epoch: 68250 | Loss: 0.4749159812927246 | Test loss: 0.49294576048851013\n",
      "Epoch: 68260 | Loss: 0.4749041497707367 | Test loss: 0.49293193221092224\n",
      "Epoch: 68270 | Loss: 0.47489243745803833 | Test loss: 0.49291810393333435\n",
      "Epoch: 68280 | Loss: 0.4748806953430176 | Test loss: 0.4929042458534241\n",
      "Epoch: 68290 | Loss: 0.47486892342567444 | Test loss: 0.4928904175758362\n",
      "Epoch: 68300 | Loss: 0.4748571515083313 | Test loss: 0.4928765296936035\n",
      "Epoch: 68310 | Loss: 0.47484540939331055 | Test loss: 0.4928627014160156\n",
      "Epoch: 68320 | Loss: 0.4748336374759674 | Test loss: 0.49284887313842773\n",
      "Epoch: 68330 | Loss: 0.47482186555862427 | Test loss: 0.49283504486083984\n",
      "Epoch: 68340 | Loss: 0.4748101234436035 | Test loss: 0.49282118678092957\n",
      "Epoch: 68350 | Loss: 0.47479841113090515 | Test loss: 0.4928073585033417\n",
      "Epoch: 68360 | Loss: 0.47478657960891724 | Test loss: 0.492793470621109\n",
      "Epoch: 68370 | Loss: 0.4747748374938965 | Test loss: 0.4927796423435211\n",
      "Epoch: 68380 | Loss: 0.4747631251811981 | Test loss: 0.4927658140659332\n",
      "Epoch: 68390 | Loss: 0.4747512936592102 | Test loss: 0.49275192618370056\n",
      "Epoch: 68400 | Loss: 0.47473955154418945 | Test loss: 0.49273809790611267\n",
      "Epoch: 68410 | Loss: 0.4747278392314911 | Test loss: 0.4927242398262024\n",
      "Epoch: 68420 | Loss: 0.47471603751182556 | Test loss: 0.4927104115486145\n",
      "Epoch: 68430 | Loss: 0.4747042655944824 | Test loss: 0.4926965832710266\n",
      "Epoch: 68440 | Loss: 0.47469255328178406 | Test loss: 0.49268269538879395\n",
      "Epoch: 68450 | Loss: 0.47468075156211853 | Test loss: 0.49266886711120605\n",
      "Epoch: 68460 | Loss: 0.4746689796447754 | Test loss: 0.4926550090312958\n",
      "Epoch: 68470 | Loss: 0.474657267332077 | Test loss: 0.4926411807537079\n",
      "Epoch: 68480 | Loss: 0.4746455252170563 | Test loss: 0.49262735247612\n",
      "Epoch: 68490 | Loss: 0.47463369369506836 | Test loss: 0.49261346459388733\n",
      "Epoch: 68500 | Loss: 0.47462198138237 | Test loss: 0.49259963631629944\n",
      "Epoch: 68510 | Loss: 0.47461017966270447 | Test loss: 0.4925857484340668\n",
      "Epoch: 68520 | Loss: 0.4745984673500061 | Test loss: 0.4925719201564789\n",
      "Epoch: 68530 | Loss: 0.47458669543266296 | Test loss: 0.492558091878891\n",
      "Epoch: 68540 | Loss: 0.47457489371299744 | Test loss: 0.4925442636013031\n",
      "Epoch: 68550 | Loss: 0.4745631217956543 | Test loss: 0.4925304055213928\n",
      "Epoch: 68560 | Loss: 0.47455140948295593 | Test loss: 0.49251657724380493\n",
      "Epoch: 68570 | Loss: 0.4745396673679352 | Test loss: 0.49250268936157227\n",
      "Epoch: 68580 | Loss: 0.47452789545059204 | Test loss: 0.4924888610839844\n",
      "Epoch: 68590 | Loss: 0.4745161235332489 | Test loss: 0.4924750328063965\n",
      "Epoch: 68600 | Loss: 0.47450438141822815 | Test loss: 0.4924611747264862\n",
      "Epoch: 68610 | Loss: 0.474492609500885 | Test loss: 0.4924473464488983\n",
      "Epoch: 68620 | Loss: 0.47448083758354187 | Test loss: 0.49243345856666565\n",
      "Epoch: 68630 | Loss: 0.4744690954685211 | Test loss: 0.49241963028907776\n",
      "Epoch: 68640 | Loss: 0.474457323551178 | Test loss: 0.49240580201148987\n",
      "Epoch: 68650 | Loss: 0.47444555163383484 | Test loss: 0.4923919141292572\n",
      "Epoch: 68660 | Loss: 0.4744338095188141 | Test loss: 0.4923780858516693\n",
      "Epoch: 68670 | Loss: 0.47442203760147095 | Test loss: 0.49236422777175903\n",
      "Epoch: 68680 | Loss: 0.4744102656841278 | Test loss: 0.49235039949417114\n",
      "Epoch: 68690 | Loss: 0.47439852356910706 | Test loss: 0.49233657121658325\n",
      "Epoch: 68700 | Loss: 0.4743867814540863 | Test loss: 0.4923226833343506\n",
      "Epoch: 68710 | Loss: 0.4743749797344208 | Test loss: 0.4923088550567627\n",
      "Epoch: 68720 | Loss: 0.4743632376194 | Test loss: 0.4922950267791748\n",
      "Epoch: 68730 | Loss: 0.4743514657020569 | Test loss: 0.4922811686992645\n",
      "Epoch: 68740 | Loss: 0.47433972358703613 | Test loss: 0.49226734042167664\n",
      "Epoch: 68750 | Loss: 0.474327951669693 | Test loss: 0.49225351214408875\n",
      "Epoch: 68760 | Loss: 0.47431620955467224 | Test loss: 0.4922396242618561\n",
      "Epoch: 68770 | Loss: 0.4743044376373291 | Test loss: 0.4922257959842682\n",
      "Epoch: 68780 | Loss: 0.47429266571998596 | Test loss: 0.4922119081020355\n",
      "Epoch: 68790 | Loss: 0.4742809236049652 | Test loss: 0.49219807982444763\n",
      "Epoch: 68800 | Loss: 0.47426915168762207 | Test loss: 0.49218425154685974\n",
      "Epoch: 68810 | Loss: 0.4742574393749237 | Test loss: 0.49217042326927185\n",
      "Epoch: 68820 | Loss: 0.4742456376552582 | Test loss: 0.4921565651893616\n",
      "Epoch: 68830 | Loss: 0.47423386573791504 | Test loss: 0.4921426773071289\n",
      "Epoch: 68840 | Loss: 0.4742221534252167 | Test loss: 0.492128849029541\n",
      "Epoch: 68850 | Loss: 0.47421035170555115 | Test loss: 0.4921150207519531\n",
      "Epoch: 68860 | Loss: 0.4741986393928528 | Test loss: 0.49210116267204285\n",
      "Epoch: 68870 | Loss: 0.47418680787086487 | Test loss: 0.49208733439445496\n",
      "Epoch: 68880 | Loss: 0.4741750657558441 | Test loss: 0.4920734465122223\n",
      "Epoch: 68890 | Loss: 0.474163293838501 | Test loss: 0.4920596182346344\n",
      "Epoch: 68900 | Loss: 0.47415152192115784 | Test loss: 0.4920457899570465\n",
      "Epoch: 68910 | Loss: 0.4741397798061371 | Test loss: 0.4920319616794586\n",
      "Epoch: 68920 | Loss: 0.4741280674934387 | Test loss: 0.49201807379722595\n",
      "Epoch: 68930 | Loss: 0.4741162955760956 | Test loss: 0.49200424551963806\n",
      "Epoch: 68940 | Loss: 0.47410449385643005 | Test loss: 0.4919903874397278\n",
      "Epoch: 68950 | Loss: 0.4740927815437317 | Test loss: 0.4919765591621399\n",
      "Epoch: 68960 | Loss: 0.47408100962638855 | Test loss: 0.491962730884552\n",
      "Epoch: 68970 | Loss: 0.474069207906723 | Test loss: 0.4919489026069641\n",
      "Epoch: 68980 | Loss: 0.47405749559402466 | Test loss: 0.49193501472473145\n",
      "Epoch: 68990 | Loss: 0.4740457236766815 | Test loss: 0.49192118644714355\n",
      "Epoch: 69000 | Loss: 0.474033921957016 | Test loss: 0.4919073283672333\n",
      "Epoch: 69010 | Loss: 0.4740222096443176 | Test loss: 0.4918935000896454\n",
      "Epoch: 69020 | Loss: 0.4740104377269745 | Test loss: 0.4918796122074127\n",
      "Epoch: 69030 | Loss: 0.47399869561195374 | Test loss: 0.49186578392982483\n",
      "Epoch: 69040 | Loss: 0.4739869236946106 | Test loss: 0.49185189604759216\n",
      "Epoch: 69050 | Loss: 0.47397518157958984 | Test loss: 0.4918380677700043\n",
      "Epoch: 69060 | Loss: 0.47396335005760193 | Test loss: 0.4918242394924164\n",
      "Epoch: 69070 | Loss: 0.47395163774490356 | Test loss: 0.4918104112148285\n",
      "Epoch: 69080 | Loss: 0.4739398956298828 | Test loss: 0.4917965531349182\n",
      "Epoch: 69090 | Loss: 0.4739281237125397 | Test loss: 0.4917827248573303\n",
      "Epoch: 69100 | Loss: 0.47391635179519653 | Test loss: 0.49176883697509766\n",
      "Epoch: 69110 | Loss: 0.4739046096801758 | Test loss: 0.49175500869750977\n",
      "Epoch: 69120 | Loss: 0.47389283776283264 | Test loss: 0.4917411804199219\n",
      "Epoch: 69130 | Loss: 0.4738810658454895 | Test loss: 0.491727352142334\n",
      "Epoch: 69140 | Loss: 0.47386932373046875 | Test loss: 0.4917134940624237\n",
      "Epoch: 69150 | Loss: 0.4738576114177704 | Test loss: 0.4916996657848358\n",
      "Epoch: 69160 | Loss: 0.47384577989578247 | Test loss: 0.49168577790260315\n",
      "Epoch: 69170 | Loss: 0.4738340377807617 | Test loss: 0.49167194962501526\n",
      "Epoch: 69180 | Loss: 0.47382232546806335 | Test loss: 0.49165812134742737\n",
      "Epoch: 69190 | Loss: 0.47381049394607544 | Test loss: 0.4916442334651947\n",
      "Epoch: 69200 | Loss: 0.4737987518310547 | Test loss: 0.4916304051876068\n",
      "Epoch: 69210 | Loss: 0.4737870395183563 | Test loss: 0.49161654710769653\n",
      "Epoch: 69220 | Loss: 0.4737752377986908 | Test loss: 0.49160271883010864\n",
      "Epoch: 69230 | Loss: 0.47376346588134766 | Test loss: 0.49158889055252075\n",
      "Epoch: 69240 | Loss: 0.4737517535686493 | Test loss: 0.4915750026702881\n",
      "Epoch: 69250 | Loss: 0.47373995184898376 | Test loss: 0.4915611743927002\n",
      "Epoch: 69260 | Loss: 0.4737281799316406 | Test loss: 0.4915473163127899\n",
      "Epoch: 69270 | Loss: 0.47371646761894226 | Test loss: 0.491533488035202\n",
      "Epoch: 69280 | Loss: 0.4737047255039215 | Test loss: 0.49151965975761414\n",
      "Epoch: 69290 | Loss: 0.4736928939819336 | Test loss: 0.49150577187538147\n",
      "Epoch: 69300 | Loss: 0.47368118166923523 | Test loss: 0.4914919435977936\n",
      "Epoch: 69310 | Loss: 0.4736693799495697 | Test loss: 0.4914780557155609\n",
      "Epoch: 69320 | Loss: 0.47365766763687134 | Test loss: 0.491464227437973\n",
      "Epoch: 69330 | Loss: 0.4736458957195282 | Test loss: 0.49145039916038513\n",
      "Epoch: 69340 | Loss: 0.47363409399986267 | Test loss: 0.49143657088279724\n",
      "Epoch: 69350 | Loss: 0.47362232208251953 | Test loss: 0.49142271280288696\n",
      "Epoch: 69360 | Loss: 0.47361060976982117 | Test loss: 0.4914088845252991\n",
      "Epoch: 69370 | Loss: 0.4735988676548004 | Test loss: 0.4913949966430664\n",
      "Epoch: 69380 | Loss: 0.4735870957374573 | Test loss: 0.4913811683654785\n",
      "Epoch: 69390 | Loss: 0.47357532382011414 | Test loss: 0.4913673400878906\n",
      "Epoch: 69400 | Loss: 0.4735635817050934 | Test loss: 0.49135348200798035\n",
      "Epoch: 69410 | Loss: 0.47355180978775024 | Test loss: 0.49133965373039246\n",
      "Epoch: 69420 | Loss: 0.4735400378704071 | Test loss: 0.4913257658481598\n",
      "Epoch: 69430 | Loss: 0.47352829575538635 | Test loss: 0.4913119375705719\n",
      "Epoch: 69440 | Loss: 0.4735165238380432 | Test loss: 0.491298109292984\n",
      "Epoch: 69450 | Loss: 0.4735047519207001 | Test loss: 0.49128422141075134\n",
      "Epoch: 69460 | Loss: 0.4734930098056793 | Test loss: 0.49127039313316345\n",
      "Epoch: 69470 | Loss: 0.4734812378883362 | Test loss: 0.4912565350532532\n",
      "Epoch: 69480 | Loss: 0.47346946597099304 | Test loss: 0.4912427067756653\n",
      "Epoch: 69490 | Loss: 0.4734577238559723 | Test loss: 0.4912288784980774\n",
      "Epoch: 69500 | Loss: 0.47344598174095154 | Test loss: 0.4912149906158447\n",
      "Epoch: 69510 | Loss: 0.473434180021286 | Test loss: 0.49120116233825684\n",
      "Epoch: 69520 | Loss: 0.47342243790626526 | Test loss: 0.49118733406066895\n",
      "Epoch: 69530 | Loss: 0.4734106659889221 | Test loss: 0.49117347598075867\n",
      "Epoch: 69540 | Loss: 0.47339892387390137 | Test loss: 0.4911596477031708\n",
      "Epoch: 69550 | Loss: 0.4733871519565582 | Test loss: 0.4911458194255829\n",
      "Epoch: 69560 | Loss: 0.4733754098415375 | Test loss: 0.4911319315433502\n",
      "Epoch: 69570 | Loss: 0.47336363792419434 | Test loss: 0.49111810326576233\n",
      "Epoch: 69580 | Loss: 0.4733518660068512 | Test loss: 0.49110421538352966\n",
      "Epoch: 69590 | Loss: 0.47334012389183044 | Test loss: 0.4910903871059418\n",
      "Epoch: 69600 | Loss: 0.4733283519744873 | Test loss: 0.4910765588283539\n",
      "Epoch: 69610 | Loss: 0.47331663966178894 | Test loss: 0.491062730550766\n",
      "Epoch: 69620 | Loss: 0.4733048379421234 | Test loss: 0.4910488724708557\n",
      "Epoch: 69630 | Loss: 0.4732930660247803 | Test loss: 0.49103498458862305\n",
      "Epoch: 69640 | Loss: 0.4732813537120819 | Test loss: 0.49102115631103516\n",
      "Epoch: 69650 | Loss: 0.4732695519924164 | Test loss: 0.49100732803344727\n",
      "Epoch: 69660 | Loss: 0.473257839679718 | Test loss: 0.490993469953537\n",
      "Epoch: 69670 | Loss: 0.4732460081577301 | Test loss: 0.4909796416759491\n",
      "Epoch: 69680 | Loss: 0.47323426604270935 | Test loss: 0.49096575379371643\n",
      "Epoch: 69690 | Loss: 0.4732224941253662 | Test loss: 0.49095192551612854\n",
      "Epoch: 69700 | Loss: 0.47321072220802307 | Test loss: 0.49093809723854065\n",
      "Epoch: 69710 | Loss: 0.4731989800930023 | Test loss: 0.49092426896095276\n",
      "Epoch: 69720 | Loss: 0.47318726778030396 | Test loss: 0.4909103810787201\n",
      "Epoch: 69730 | Loss: 0.4731754958629608 | Test loss: 0.4908965528011322\n",
      "Epoch: 69740 | Loss: 0.4731636941432953 | Test loss: 0.4908826947212219\n",
      "Epoch: 69750 | Loss: 0.4731519818305969 | Test loss: 0.49086886644363403\n",
      "Epoch: 69760 | Loss: 0.4731402099132538 | Test loss: 0.49085503816604614\n",
      "Epoch: 69770 | Loss: 0.47312840819358826 | Test loss: 0.49084120988845825\n",
      "Epoch: 69780 | Loss: 0.4731166958808899 | Test loss: 0.4908273220062256\n",
      "Epoch: 69790 | Loss: 0.47310492396354675 | Test loss: 0.4908134937286377\n",
      "Epoch: 69800 | Loss: 0.4730931222438812 | Test loss: 0.4907996356487274\n",
      "Epoch: 69810 | Loss: 0.47308140993118286 | Test loss: 0.4907858073711395\n",
      "Epoch: 69820 | Loss: 0.4730696380138397 | Test loss: 0.49077191948890686\n",
      "Epoch: 69830 | Loss: 0.47305789589881897 | Test loss: 0.49075809121131897\n",
      "Epoch: 69840 | Loss: 0.47304612398147583 | Test loss: 0.4907442033290863\n",
      "Epoch: 69850 | Loss: 0.4730343818664551 | Test loss: 0.4907303750514984\n",
      "Epoch: 69860 | Loss: 0.47302255034446716 | Test loss: 0.4907165467739105\n",
      "Epoch: 69870 | Loss: 0.4730108380317688 | Test loss: 0.49070271849632263\n",
      "Epoch: 69880 | Loss: 0.47299909591674805 | Test loss: 0.49068886041641235\n",
      "Epoch: 69890 | Loss: 0.4729873239994049 | Test loss: 0.49067503213882446\n",
      "Epoch: 69900 | Loss: 0.47297555208206177 | Test loss: 0.4906611442565918\n",
      "Epoch: 69910 | Loss: 0.472963809967041 | Test loss: 0.4906473159790039\n",
      "Epoch: 69920 | Loss: 0.4729520380496979 | Test loss: 0.490633487701416\n",
      "Epoch: 69930 | Loss: 0.47294026613235474 | Test loss: 0.4906196594238281\n",
      "Epoch: 69940 | Loss: 0.472928524017334 | Test loss: 0.49060580134391785\n",
      "Epoch: 69950 | Loss: 0.4729168117046356 | Test loss: 0.49059197306632996\n",
      "Epoch: 69960 | Loss: 0.4729049801826477 | Test loss: 0.4905780851840973\n",
      "Epoch: 69970 | Loss: 0.47289323806762695 | Test loss: 0.4905642569065094\n",
      "Epoch: 69980 | Loss: 0.4728815257549286 | Test loss: 0.4905504286289215\n",
      "Epoch: 69990 | Loss: 0.4728696942329407 | Test loss: 0.49053654074668884\n",
      "Epoch: 70000 | Loss: 0.4728579521179199 | Test loss: 0.49052271246910095\n",
      "Epoch: 70010 | Loss: 0.47284623980522156 | Test loss: 0.4905088543891907\n",
      "Epoch: 70020 | Loss: 0.47283443808555603 | Test loss: 0.4904950261116028\n",
      "Epoch: 70030 | Loss: 0.4728226661682129 | Test loss: 0.4904811978340149\n",
      "Epoch: 70040 | Loss: 0.4728109538555145 | Test loss: 0.4904673099517822\n",
      "Epoch: 70050 | Loss: 0.472799152135849 | Test loss: 0.49045348167419434\n",
      "Epoch: 70060 | Loss: 0.47278738021850586 | Test loss: 0.49043962359428406\n",
      "Epoch: 70070 | Loss: 0.4727756679058075 | Test loss: 0.49042579531669617\n",
      "Epoch: 70080 | Loss: 0.47276392579078674 | Test loss: 0.4904119670391083\n",
      "Epoch: 70090 | Loss: 0.47275209426879883 | Test loss: 0.4903980791568756\n",
      "Epoch: 70100 | Loss: 0.47274038195610046 | Test loss: 0.4903842508792877\n",
      "Epoch: 70110 | Loss: 0.47272858023643494 | Test loss: 0.49037036299705505\n",
      "Epoch: 70120 | Loss: 0.4727168679237366 | Test loss: 0.49035653471946716\n",
      "Epoch: 70130 | Loss: 0.47270509600639343 | Test loss: 0.4903427064418793\n",
      "Epoch: 70140 | Loss: 0.4726932942867279 | Test loss: 0.4903288781642914\n",
      "Epoch: 70150 | Loss: 0.47268152236938477 | Test loss: 0.4903150200843811\n",
      "Epoch: 70160 | Loss: 0.4726698100566864 | Test loss: 0.4903011918067932\n",
      "Epoch: 70170 | Loss: 0.47265806794166565 | Test loss: 0.49028730392456055\n",
      "Epoch: 70180 | Loss: 0.4726462960243225 | Test loss: 0.49027347564697266\n",
      "Epoch: 70190 | Loss: 0.47263452410697937 | Test loss: 0.49025964736938477\n",
      "Epoch: 70200 | Loss: 0.4726227819919586 | Test loss: 0.4902457892894745\n",
      "Epoch: 70210 | Loss: 0.4726110100746155 | Test loss: 0.4902319610118866\n",
      "Epoch: 70220 | Loss: 0.47259923815727234 | Test loss: 0.49021807312965393\n",
      "Epoch: 70230 | Loss: 0.4725874960422516 | Test loss: 0.49020424485206604\n",
      "Epoch: 70240 | Loss: 0.47257572412490845 | Test loss: 0.49019041657447815\n",
      "Epoch: 70250 | Loss: 0.4725639522075653 | Test loss: 0.4901765286922455\n",
      "Epoch: 70260 | Loss: 0.47255221009254456 | Test loss: 0.4901627004146576\n",
      "Epoch: 70270 | Loss: 0.4725404381752014 | Test loss: 0.4901488423347473\n",
      "Epoch: 70280 | Loss: 0.4725286662578583 | Test loss: 0.4901350140571594\n",
      "Epoch: 70290 | Loss: 0.4725169241428375 | Test loss: 0.49012118577957153\n",
      "Epoch: 70300 | Loss: 0.4725051820278168 | Test loss: 0.49010729789733887\n",
      "Epoch: 70310 | Loss: 0.47249338030815125 | Test loss: 0.490093469619751\n",
      "Epoch: 70320 | Loss: 0.4724816381931305 | Test loss: 0.4900796413421631\n",
      "Epoch: 70330 | Loss: 0.47246986627578735 | Test loss: 0.4900657832622528\n",
      "Epoch: 70340 | Loss: 0.4724581241607666 | Test loss: 0.4900519549846649\n",
      "Epoch: 70350 | Loss: 0.47244635224342346 | Test loss: 0.490038126707077\n",
      "Epoch: 70360 | Loss: 0.4724346101284027 | Test loss: 0.49002423882484436\n",
      "Epoch: 70370 | Loss: 0.47242283821105957 | Test loss: 0.49001041054725647\n",
      "Epoch: 70380 | Loss: 0.47241106629371643 | Test loss: 0.4899965226650238\n",
      "Epoch: 70390 | Loss: 0.4723993241786957 | Test loss: 0.4899826943874359\n",
      "Epoch: 70400 | Loss: 0.47238755226135254 | Test loss: 0.489968866109848\n",
      "Epoch: 70410 | Loss: 0.4723758399486542 | Test loss: 0.48995503783226013\n",
      "Epoch: 70420 | Loss: 0.47236403822898865 | Test loss: 0.48994117975234985\n",
      "Epoch: 70430 | Loss: 0.4723522663116455 | Test loss: 0.4899272918701172\n",
      "Epoch: 70440 | Loss: 0.47234055399894714 | Test loss: 0.4899134635925293\n",
      "Epoch: 70450 | Loss: 0.4723287522792816 | Test loss: 0.4898996353149414\n",
      "Epoch: 70460 | Loss: 0.47231703996658325 | Test loss: 0.48988577723503113\n",
      "Epoch: 70470 | Loss: 0.47230520844459534 | Test loss: 0.48987194895744324\n",
      "Epoch: 70480 | Loss: 0.4722934663295746 | Test loss: 0.48985806107521057\n",
      "Epoch: 70490 | Loss: 0.47228169441223145 | Test loss: 0.4898442327976227\n",
      "Epoch: 70500 | Loss: 0.4722699224948883 | Test loss: 0.4898304045200348\n",
      "Epoch: 70510 | Loss: 0.47225818037986755 | Test loss: 0.4898165762424469\n",
      "Epoch: 70520 | Loss: 0.4722464680671692 | Test loss: 0.48980268836021423\n",
      "Epoch: 70530 | Loss: 0.47223469614982605 | Test loss: 0.48978886008262634\n",
      "Epoch: 70540 | Loss: 0.4722228944301605 | Test loss: 0.48977500200271606\n",
      "Epoch: 70550 | Loss: 0.47221118211746216 | Test loss: 0.4897611737251282\n",
      "Epoch: 70560 | Loss: 0.472199410200119 | Test loss: 0.4897473454475403\n",
      "Epoch: 70570 | Loss: 0.4721876084804535 | Test loss: 0.4897335171699524\n",
      "Epoch: 70580 | Loss: 0.4721758961677551 | Test loss: 0.4897196292877197\n",
      "Epoch: 70590 | Loss: 0.472164124250412 | Test loss: 0.48970580101013184\n",
      "Epoch: 70600 | Loss: 0.47215232253074646 | Test loss: 0.48969194293022156\n",
      "Epoch: 70610 | Loss: 0.4721406102180481 | Test loss: 0.48967811465263367\n",
      "Epoch: 70620 | Loss: 0.47212883830070496 | Test loss: 0.489664226770401\n",
      "Epoch: 70630 | Loss: 0.4721170961856842 | Test loss: 0.4896503984928131\n",
      "Epoch: 70640 | Loss: 0.47210532426834106 | Test loss: 0.48963651061058044\n",
      "Epoch: 70650 | Loss: 0.4720935821533203 | Test loss: 0.48962268233299255\n",
      "Epoch: 70660 | Loss: 0.4720817506313324 | Test loss: 0.48960885405540466\n",
      "Epoch: 70670 | Loss: 0.47207003831863403 | Test loss: 0.4895950257778168\n",
      "Epoch: 70680 | Loss: 0.4720582962036133 | Test loss: 0.4895811676979065\n",
      "Epoch: 70690 | Loss: 0.47204652428627014 | Test loss: 0.4895673394203186\n",
      "Epoch: 70700 | Loss: 0.472034752368927 | Test loss: 0.48955345153808594\n",
      "Epoch: 70710 | Loss: 0.47202301025390625 | Test loss: 0.48953962326049805\n",
      "Epoch: 70720 | Loss: 0.4720112383365631 | Test loss: 0.48952579498291016\n",
      "Epoch: 70730 | Loss: 0.47199946641921997 | Test loss: 0.48951196670532227\n",
      "Epoch: 70740 | Loss: 0.4719877243041992 | Test loss: 0.489498108625412\n",
      "Epoch: 70750 | Loss: 0.47197601199150085 | Test loss: 0.4894842803478241\n",
      "Epoch: 70760 | Loss: 0.47196418046951294 | Test loss: 0.48947039246559143\n",
      "Epoch: 70770 | Loss: 0.4719524383544922 | Test loss: 0.48945656418800354\n",
      "Epoch: 70780 | Loss: 0.4719407260417938 | Test loss: 0.48944273591041565\n",
      "Epoch: 70790 | Loss: 0.4719288945198059 | Test loss: 0.489428848028183\n",
      "Epoch: 70800 | Loss: 0.47191715240478516 | Test loss: 0.4894150197505951\n",
      "Epoch: 70810 | Loss: 0.4719054400920868 | Test loss: 0.4894011616706848\n",
      "Epoch: 70820 | Loss: 0.47189363837242126 | Test loss: 0.4893873333930969\n",
      "Epoch: 70830 | Loss: 0.4718818664550781 | Test loss: 0.48937350511550903\n",
      "Epoch: 70840 | Loss: 0.47187015414237976 | Test loss: 0.48935961723327637\n",
      "Epoch: 70850 | Loss: 0.47185835242271423 | Test loss: 0.4893457889556885\n",
      "Epoch: 70860 | Loss: 0.4718465805053711 | Test loss: 0.4893319308757782\n",
      "Epoch: 70870 | Loss: 0.47183486819267273 | Test loss: 0.4893181025981903\n",
      "Epoch: 70880 | Loss: 0.471823126077652 | Test loss: 0.4893042743206024\n",
      "Epoch: 70890 | Loss: 0.47181129455566406 | Test loss: 0.48929038643836975\n",
      "Epoch: 70900 | Loss: 0.4717995822429657 | Test loss: 0.48927655816078186\n",
      "Epoch: 70910 | Loss: 0.47178778052330017 | Test loss: 0.4892626702785492\n",
      "Epoch: 70920 | Loss: 0.4717760682106018 | Test loss: 0.4892488420009613\n",
      "Epoch: 70930 | Loss: 0.47176429629325867 | Test loss: 0.4892350137233734\n",
      "Epoch: 70940 | Loss: 0.47175249457359314 | Test loss: 0.4892211854457855\n",
      "Epoch: 70950 | Loss: 0.47174072265625 | Test loss: 0.48920732736587524\n",
      "Epoch: 70960 | Loss: 0.47172901034355164 | Test loss: 0.48919349908828735\n",
      "Epoch: 70970 | Loss: 0.4717172682285309 | Test loss: 0.4891796112060547\n",
      "Epoch: 70980 | Loss: 0.47170549631118774 | Test loss: 0.4891657829284668\n",
      "Epoch: 70990 | Loss: 0.4716937243938446 | Test loss: 0.4891519546508789\n",
      "Epoch: 71000 | Loss: 0.47168198227882385 | Test loss: 0.48913809657096863\n",
      "Epoch: 71010 | Loss: 0.4716702103614807 | Test loss: 0.48912426829338074\n",
      "Epoch: 71020 | Loss: 0.4716584384441376 | Test loss: 0.48911038041114807\n",
      "Epoch: 71030 | Loss: 0.4716466963291168 | Test loss: 0.4890965521335602\n",
      "Epoch: 71040 | Loss: 0.4716349244117737 | Test loss: 0.4890827238559723\n",
      "Epoch: 71050 | Loss: 0.47162315249443054 | Test loss: 0.4890688359737396\n",
      "Epoch: 71060 | Loss: 0.4716114103794098 | Test loss: 0.48905500769615173\n",
      "Epoch: 71070 | Loss: 0.47159963846206665 | Test loss: 0.48904114961624146\n",
      "Epoch: 71080 | Loss: 0.4715878665447235 | Test loss: 0.48902732133865356\n",
      "Epoch: 71090 | Loss: 0.47157612442970276 | Test loss: 0.4890134930610657\n",
      "Epoch: 71100 | Loss: 0.471564382314682 | Test loss: 0.488999605178833\n",
      "Epoch: 71110 | Loss: 0.4715525805950165 | Test loss: 0.4889857769012451\n",
      "Epoch: 71120 | Loss: 0.4715408384799957 | Test loss: 0.4889719486236572\n",
      "Epoch: 71130 | Loss: 0.4715290665626526 | Test loss: 0.48895809054374695\n",
      "Epoch: 71140 | Loss: 0.47151732444763184 | Test loss: 0.48894426226615906\n",
      "Epoch: 71150 | Loss: 0.4715055525302887 | Test loss: 0.48893043398857117\n",
      "Epoch: 71160 | Loss: 0.47149381041526794 | Test loss: 0.4889165461063385\n",
      "Epoch: 71170 | Loss: 0.4714820384979248 | Test loss: 0.4889027178287506\n",
      "Epoch: 71180 | Loss: 0.47147026658058167 | Test loss: 0.48888882994651794\n",
      "Epoch: 71190 | Loss: 0.4714585244655609 | Test loss: 0.48887500166893005\n",
      "Epoch: 71200 | Loss: 0.4714467525482178 | Test loss: 0.48886117339134216\n",
      "Epoch: 71210 | Loss: 0.4714350402355194 | Test loss: 0.4888473451137543\n",
      "Epoch: 71220 | Loss: 0.4714232385158539 | Test loss: 0.488833487033844\n",
      "Epoch: 71230 | Loss: 0.47141146659851074 | Test loss: 0.48881959915161133\n",
      "Epoch: 71240 | Loss: 0.4713997542858124 | Test loss: 0.48880577087402344\n",
      "Epoch: 71250 | Loss: 0.47138795256614685 | Test loss: 0.48879194259643555\n",
      "Epoch: 71260 | Loss: 0.4713762402534485 | Test loss: 0.48877808451652527\n",
      "Epoch: 71270 | Loss: 0.47136440873146057 | Test loss: 0.4887642562389374\n",
      "Epoch: 71280 | Loss: 0.4713526666164398 | Test loss: 0.4887503683567047\n",
      "Epoch: 71290 | Loss: 0.4713408946990967 | Test loss: 0.4887365400791168\n",
      "Epoch: 71300 | Loss: 0.47132912278175354 | Test loss: 0.48872271180152893\n",
      "Epoch: 71310 | Loss: 0.4713173806667328 | Test loss: 0.48870888352394104\n",
      "Epoch: 71320 | Loss: 0.4713056683540344 | Test loss: 0.4886949956417084\n",
      "Epoch: 71330 | Loss: 0.4712938964366913 | Test loss: 0.4886811673641205\n",
      "Epoch: 71340 | Loss: 0.47128209471702576 | Test loss: 0.4886673092842102\n",
      "Epoch: 71350 | Loss: 0.4712703824043274 | Test loss: 0.4886534810066223\n",
      "Epoch: 71360 | Loss: 0.47125861048698425 | Test loss: 0.4886396527290344\n",
      "Epoch: 71370 | Loss: 0.4712468087673187 | Test loss: 0.48862582445144653\n",
      "Epoch: 71380 | Loss: 0.47123509645462036 | Test loss: 0.48861193656921387\n",
      "Epoch: 71390 | Loss: 0.4712233245372772 | Test loss: 0.488598108291626\n",
      "Epoch: 71400 | Loss: 0.4712115228176117 | Test loss: 0.4885842502117157\n",
      "Epoch: 71410 | Loss: 0.47119981050491333 | Test loss: 0.4885704219341278\n",
      "Epoch: 71420 | Loss: 0.4711880385875702 | Test loss: 0.48855653405189514\n",
      "Epoch: 71430 | Loss: 0.47117629647254944 | Test loss: 0.48854270577430725\n",
      "Epoch: 71440 | Loss: 0.4711645245552063 | Test loss: 0.4885288178920746\n",
      "Epoch: 71450 | Loss: 0.47115278244018555 | Test loss: 0.4885149896144867\n",
      "Epoch: 71460 | Loss: 0.47114095091819763 | Test loss: 0.4885011613368988\n",
      "Epoch: 71470 | Loss: 0.47112923860549927 | Test loss: 0.4884873330593109\n",
      "Epoch: 71480 | Loss: 0.4711174964904785 | Test loss: 0.48847347497940063\n",
      "Epoch: 71490 | Loss: 0.4711057245731354 | Test loss: 0.48845964670181274\n",
      "Epoch: 71500 | Loss: 0.47109395265579224 | Test loss: 0.4884457588195801\n",
      "Epoch: 71510 | Loss: 0.4710822105407715 | Test loss: 0.4884319305419922\n",
      "Epoch: 71520 | Loss: 0.47107043862342834 | Test loss: 0.4884181022644043\n",
      "Epoch: 71530 | Loss: 0.4710586667060852 | Test loss: 0.4884042739868164\n",
      "Epoch: 71540 | Loss: 0.47104692459106445 | Test loss: 0.48839041590690613\n",
      "Epoch: 71550 | Loss: 0.4710352122783661 | Test loss: 0.48837658762931824\n",
      "Epoch: 71560 | Loss: 0.4710233807563782 | Test loss: 0.48836269974708557\n",
      "Epoch: 71570 | Loss: 0.4710116386413574 | Test loss: 0.4883488714694977\n",
      "Epoch: 71580 | Loss: 0.47099992632865906 | Test loss: 0.4883350431919098\n",
      "Epoch: 71590 | Loss: 0.47098809480667114 | Test loss: 0.4883211553096771\n",
      "Epoch: 71600 | Loss: 0.4709763526916504 | Test loss: 0.48830732703208923\n",
      "Epoch: 71610 | Loss: 0.470964640378952 | Test loss: 0.48829346895217896\n",
      "Epoch: 71620 | Loss: 0.4709528386592865 | Test loss: 0.48827964067459106\n",
      "Epoch: 71630 | Loss: 0.47094106674194336 | Test loss: 0.4882658123970032\n",
      "Epoch: 71640 | Loss: 0.470929354429245 | Test loss: 0.4882519245147705\n",
      "Epoch: 71650 | Loss: 0.47091755270957947 | Test loss: 0.4882380962371826\n",
      "Epoch: 71660 | Loss: 0.47090578079223633 | Test loss: 0.48822423815727234\n",
      "Epoch: 71670 | Loss: 0.47089406847953796 | Test loss: 0.48821040987968445\n",
      "Epoch: 71680 | Loss: 0.4708823263645172 | Test loss: 0.48819658160209656\n",
      "Epoch: 71690 | Loss: 0.4708704948425293 | Test loss: 0.4881826937198639\n",
      "Epoch: 71700 | Loss: 0.47085878252983093 | Test loss: 0.488168865442276\n",
      "Epoch: 71710 | Loss: 0.4708469808101654 | Test loss: 0.48815497756004333\n",
      "Epoch: 71720 | Loss: 0.47083526849746704 | Test loss: 0.48814114928245544\n",
      "Epoch: 71730 | Loss: 0.4708234965801239 | Test loss: 0.48812732100486755\n",
      "Epoch: 71740 | Loss: 0.4708116948604584 | Test loss: 0.48811349272727966\n",
      "Epoch: 71750 | Loss: 0.47079992294311523 | Test loss: 0.4880996346473694\n",
      "Epoch: 71760 | Loss: 0.47078821063041687 | Test loss: 0.4880858063697815\n",
      "Epoch: 71770 | Loss: 0.4707764685153961 | Test loss: 0.48807191848754883\n",
      "Epoch: 71780 | Loss: 0.470764696598053 | Test loss: 0.48805809020996094\n",
      "Epoch: 71790 | Loss: 0.47075292468070984 | Test loss: 0.48804426193237305\n",
      "Epoch: 71800 | Loss: 0.4707411825656891 | Test loss: 0.48803040385246277\n",
      "Epoch: 71810 | Loss: 0.47072941064834595 | Test loss: 0.4880165755748749\n",
      "Epoch: 71820 | Loss: 0.4707176387310028 | Test loss: 0.4880026876926422\n",
      "Epoch: 71830 | Loss: 0.47070589661598206 | Test loss: 0.4879888594150543\n",
      "Epoch: 71840 | Loss: 0.4706941246986389 | Test loss: 0.48797503113746643\n",
      "Epoch: 71850 | Loss: 0.4706823527812958 | Test loss: 0.48796114325523376\n",
      "Epoch: 71860 | Loss: 0.470670610666275 | Test loss: 0.4879473149776459\n",
      "Epoch: 71870 | Loss: 0.4706588387489319 | Test loss: 0.4879334568977356\n",
      "Epoch: 71880 | Loss: 0.47064706683158875 | Test loss: 0.4879196286201477\n",
      "Epoch: 71890 | Loss: 0.470635324716568 | Test loss: 0.4879058003425598\n",
      "Epoch: 71900 | Loss: 0.47062358260154724 | Test loss: 0.48789191246032715\n",
      "Epoch: 71910 | Loss: 0.4706117808818817 | Test loss: 0.48787808418273926\n",
      "Epoch: 71920 | Loss: 0.47060003876686096 | Test loss: 0.48786425590515137\n",
      "Epoch: 71930 | Loss: 0.4705882668495178 | Test loss: 0.4878503978252411\n",
      "Epoch: 71940 | Loss: 0.47057652473449707 | Test loss: 0.4878365695476532\n",
      "Epoch: 71950 | Loss: 0.47056475281715393 | Test loss: 0.4878227412700653\n",
      "Epoch: 71960 | Loss: 0.4705530107021332 | Test loss: 0.48780885338783264\n",
      "Epoch: 71970 | Loss: 0.47054123878479004 | Test loss: 0.48779502511024475\n",
      "Epoch: 71980 | Loss: 0.4705294668674469 | Test loss: 0.4877811372280121\n",
      "Epoch: 71990 | Loss: 0.47051772475242615 | Test loss: 0.4877673089504242\n",
      "Epoch: 72000 | Loss: 0.470505952835083 | Test loss: 0.4877534806728363\n",
      "Epoch: 72010 | Loss: 0.47049424052238464 | Test loss: 0.4877396523952484\n",
      "Epoch: 72020 | Loss: 0.4704824388027191 | Test loss: 0.48772579431533813\n",
      "Epoch: 72030 | Loss: 0.470470666885376 | Test loss: 0.48771190643310547\n",
      "Epoch: 72040 | Loss: 0.4704589545726776 | Test loss: 0.4876980781555176\n",
      "Epoch: 72050 | Loss: 0.4704471528530121 | Test loss: 0.4876842498779297\n",
      "Epoch: 72060 | Loss: 0.4704354405403137 | Test loss: 0.4876703917980194\n",
      "Epoch: 72070 | Loss: 0.4704236090183258 | Test loss: 0.4876565635204315\n",
      "Epoch: 72080 | Loss: 0.47041186690330505 | Test loss: 0.48764267563819885\n",
      "Epoch: 72090 | Loss: 0.4704000949859619 | Test loss: 0.48762884736061096\n",
      "Epoch: 72100 | Loss: 0.4703883230686188 | Test loss: 0.48761501908302307\n",
      "Epoch: 72110 | Loss: 0.470376580953598 | Test loss: 0.4876011908054352\n",
      "Epoch: 72120 | Loss: 0.47036486864089966 | Test loss: 0.4875873029232025\n",
      "Epoch: 72130 | Loss: 0.4703530967235565 | Test loss: 0.4875734746456146\n",
      "Epoch: 72140 | Loss: 0.470341295003891 | Test loss: 0.48755961656570435\n",
      "Epoch: 72150 | Loss: 0.4703295826911926 | Test loss: 0.48754578828811646\n",
      "Epoch: 72160 | Loss: 0.4703178107738495 | Test loss: 0.48753196001052856\n",
      "Epoch: 72170 | Loss: 0.47030600905418396 | Test loss: 0.4875181317329407\n",
      "Epoch: 72180 | Loss: 0.4702942967414856 | Test loss: 0.487504243850708\n",
      "Epoch: 72190 | Loss: 0.47028252482414246 | Test loss: 0.4874904155731201\n",
      "Epoch: 72200 | Loss: 0.47027072310447693 | Test loss: 0.48747655749320984\n",
      "Epoch: 72210 | Loss: 0.47025901079177856 | Test loss: 0.48746272921562195\n",
      "Epoch: 72220 | Loss: 0.4702472388744354 | Test loss: 0.4874488413333893\n",
      "Epoch: 72230 | Loss: 0.4702354967594147 | Test loss: 0.4874350130558014\n",
      "Epoch: 72240 | Loss: 0.47022372484207153 | Test loss: 0.4874211251735687\n",
      "Epoch: 72250 | Loss: 0.4702119827270508 | Test loss: 0.48740729689598083\n",
      "Epoch: 72260 | Loss: 0.47020015120506287 | Test loss: 0.48739346861839294\n",
      "Epoch: 72270 | Loss: 0.4701884388923645 | Test loss: 0.48737964034080505\n",
      "Epoch: 72280 | Loss: 0.47017669677734375 | Test loss: 0.4873657822608948\n",
      "Epoch: 72290 | Loss: 0.4701649248600006 | Test loss: 0.4873519539833069\n",
      "Epoch: 72300 | Loss: 0.47015315294265747 | Test loss: 0.4873380661010742\n",
      "Epoch: 72310 | Loss: 0.4701414108276367 | Test loss: 0.48732423782348633\n",
      "Epoch: 72320 | Loss: 0.4701296389102936 | Test loss: 0.48731040954589844\n",
      "Epoch: 72330 | Loss: 0.47011786699295044 | Test loss: 0.48729658126831055\n",
      "Epoch: 72340 | Loss: 0.4701061248779297 | Test loss: 0.48728272318840027\n",
      "Epoch: 72350 | Loss: 0.4700944125652313 | Test loss: 0.4872688949108124\n",
      "Epoch: 72360 | Loss: 0.4700825810432434 | Test loss: 0.4872550070285797\n",
      "Epoch: 72370 | Loss: 0.47007083892822266 | Test loss: 0.4872411787509918\n",
      "Epoch: 72380 | Loss: 0.4700591266155243 | Test loss: 0.48722735047340393\n",
      "Epoch: 72390 | Loss: 0.4700472950935364 | Test loss: 0.48721346259117126\n",
      "Epoch: 72400 | Loss: 0.4700355529785156 | Test loss: 0.4871996343135834\n",
      "Epoch: 72410 | Loss: 0.47002384066581726 | Test loss: 0.4871857762336731\n",
      "Epoch: 72420 | Loss: 0.47001203894615173 | Test loss: 0.4871719479560852\n",
      "Epoch: 72430 | Loss: 0.4700002670288086 | Test loss: 0.4871581196784973\n",
      "Epoch: 72440 | Loss: 0.46998855471611023 | Test loss: 0.48714423179626465\n",
      "Epoch: 72450 | Loss: 0.4699767529964447 | Test loss: 0.48713040351867676\n",
      "Epoch: 72460 | Loss: 0.46996498107910156 | Test loss: 0.4871165454387665\n",
      "Epoch: 72470 | Loss: 0.4699532687664032 | Test loss: 0.4871027171611786\n",
      "Epoch: 72480 | Loss: 0.46994152665138245 | Test loss: 0.4870888888835907\n",
      "Epoch: 72490 | Loss: 0.46992969512939453 | Test loss: 0.48707500100135803\n",
      "Epoch: 72500 | Loss: 0.46991798281669617 | Test loss: 0.48706117272377014\n",
      "Epoch: 72510 | Loss: 0.46990618109703064 | Test loss: 0.4870472848415375\n",
      "Epoch: 72520 | Loss: 0.4698944687843323 | Test loss: 0.4870334565639496\n",
      "Epoch: 72530 | Loss: 0.46988269686698914 | Test loss: 0.4870196282863617\n",
      "Epoch: 72540 | Loss: 0.4698708951473236 | Test loss: 0.4870058000087738\n",
      "Epoch: 72550 | Loss: 0.46985912322998047 | Test loss: 0.4869919419288635\n",
      "Epoch: 72560 | Loss: 0.4698474109172821 | Test loss: 0.48697811365127563\n",
      "Epoch: 72570 | Loss: 0.46983566880226135 | Test loss: 0.48696422576904297\n",
      "Epoch: 72580 | Loss: 0.4698238968849182 | Test loss: 0.4869503974914551\n",
      "Epoch: 72590 | Loss: 0.4698121249675751 | Test loss: 0.4869365692138672\n",
      "Epoch: 72600 | Loss: 0.4698003828525543 | Test loss: 0.4869227111339569\n",
      "Epoch: 72610 | Loss: 0.4697886109352112 | Test loss: 0.486908882856369\n",
      "Epoch: 72620 | Loss: 0.46977683901786804 | Test loss: 0.48689499497413635\n",
      "Epoch: 72630 | Loss: 0.4697650969028473 | Test loss: 0.48688116669654846\n",
      "Epoch: 72640 | Loss: 0.46975332498550415 | Test loss: 0.48686733841896057\n",
      "Epoch: 72650 | Loss: 0.469741553068161 | Test loss: 0.4868534505367279\n",
      "Epoch: 72660 | Loss: 0.46972981095314026 | Test loss: 0.48683962225914\n",
      "Epoch: 72670 | Loss: 0.4697180390357971 | Test loss: 0.48682576417922974\n",
      "Epoch: 72680 | Loss: 0.469706267118454 | Test loss: 0.48681193590164185\n",
      "Epoch: 72690 | Loss: 0.4696945250034332 | Test loss: 0.48679810762405396\n",
      "Epoch: 72700 | Loss: 0.4696827828884125 | Test loss: 0.4867842197418213\n",
      "Epoch: 72710 | Loss: 0.46967098116874695 | Test loss: 0.4867703914642334\n",
      "Epoch: 72720 | Loss: 0.4696592390537262 | Test loss: 0.4867565631866455\n",
      "Epoch: 72730 | Loss: 0.46964746713638306 | Test loss: 0.48674270510673523\n",
      "Epoch: 72740 | Loss: 0.4696357250213623 | Test loss: 0.48672887682914734\n",
      "Epoch: 72750 | Loss: 0.46962395310401917 | Test loss: 0.48671504855155945\n",
      "Epoch: 72760 | Loss: 0.4696122109889984 | Test loss: 0.4867011606693268\n",
      "Epoch: 72770 | Loss: 0.4696004390716553 | Test loss: 0.4866873323917389\n",
      "Epoch: 72780 | Loss: 0.46958866715431213 | Test loss: 0.4866734445095062\n",
      "Epoch: 72790 | Loss: 0.4695769250392914 | Test loss: 0.48665961623191833\n",
      "Epoch: 72800 | Loss: 0.46956515312194824 | Test loss: 0.48664578795433044\n",
      "Epoch: 72810 | Loss: 0.4695534408092499 | Test loss: 0.48663195967674255\n",
      "Epoch: 72820 | Loss: 0.46954163908958435 | Test loss: 0.4866181015968323\n",
      "Epoch: 72830 | Loss: 0.4695298671722412 | Test loss: 0.4866042137145996\n",
      "Epoch: 72840 | Loss: 0.46951815485954285 | Test loss: 0.4865903854370117\n",
      "Epoch: 72850 | Loss: 0.4695063531398773 | Test loss: 0.48657655715942383\n",
      "Epoch: 72860 | Loss: 0.46949464082717896 | Test loss: 0.48656269907951355\n",
      "Epoch: 72870 | Loss: 0.46948280930519104 | Test loss: 0.48654887080192566\n",
      "Epoch: 72880 | Loss: 0.4694710671901703 | Test loss: 0.486534982919693\n",
      "Epoch: 72890 | Loss: 0.46945929527282715 | Test loss: 0.4865211546421051\n",
      "Epoch: 72900 | Loss: 0.469447523355484 | Test loss: 0.4865073263645172\n",
      "Epoch: 72910 | Loss: 0.46943578124046326 | Test loss: 0.4864934980869293\n",
      "Epoch: 72920 | Loss: 0.4694240689277649 | Test loss: 0.48647961020469666\n",
      "Epoch: 72930 | Loss: 0.46941229701042175 | Test loss: 0.48646578192710876\n",
      "Epoch: 72940 | Loss: 0.4694004952907562 | Test loss: 0.4864519238471985\n",
      "Epoch: 72950 | Loss: 0.46938878297805786 | Test loss: 0.4864380955696106\n",
      "Epoch: 72960 | Loss: 0.4693770110607147 | Test loss: 0.4864242672920227\n",
      "Epoch: 72970 | Loss: 0.4693652093410492 | Test loss: 0.4864104390144348\n",
      "Epoch: 72980 | Loss: 0.46935349702835083 | Test loss: 0.48639655113220215\n",
      "Epoch: 72990 | Loss: 0.4693417251110077 | Test loss: 0.48638272285461426\n",
      "Epoch: 73000 | Loss: 0.46932992339134216 | Test loss: 0.486368864774704\n",
      "Epoch: 73010 | Loss: 0.4693182110786438 | Test loss: 0.4863550364971161\n",
      "Epoch: 73020 | Loss: 0.46930643916130066 | Test loss: 0.4863411486148834\n",
      "Epoch: 73030 | Loss: 0.4692946970462799 | Test loss: 0.48632732033729553\n",
      "Epoch: 73040 | Loss: 0.46928292512893677 | Test loss: 0.48631343245506287\n",
      "Epoch: 73050 | Loss: 0.469271183013916 | Test loss: 0.486299604177475\n",
      "Epoch: 73060 | Loss: 0.4692593514919281 | Test loss: 0.4862857758998871\n",
      "Epoch: 73070 | Loss: 0.46924763917922974 | Test loss: 0.4862719476222992\n",
      "Epoch: 73080 | Loss: 0.469235897064209 | Test loss: 0.4862580895423889\n",
      "Epoch: 73090 | Loss: 0.46922412514686584 | Test loss: 0.486244261264801\n",
      "Epoch: 73100 | Loss: 0.4692123532295227 | Test loss: 0.48623037338256836\n",
      "Epoch: 73110 | Loss: 0.46920061111450195 | Test loss: 0.48621654510498047\n",
      "Epoch: 73120 | Loss: 0.4691888391971588 | Test loss: 0.4862027168273926\n",
      "Epoch: 73130 | Loss: 0.4691770672798157 | Test loss: 0.4861888885498047\n",
      "Epoch: 73140 | Loss: 0.4691653251647949 | Test loss: 0.4861750304698944\n",
      "Epoch: 73150 | Loss: 0.46915361285209656 | Test loss: 0.4861612021923065\n",
      "Epoch: 73160 | Loss: 0.46914178133010864 | Test loss: 0.48614731431007385\n",
      "Epoch: 73170 | Loss: 0.4691300392150879 | Test loss: 0.48613348603248596\n",
      "Epoch: 73180 | Loss: 0.4691183269023895 | Test loss: 0.48611965775489807\n",
      "Epoch: 73190 | Loss: 0.4691064953804016 | Test loss: 0.4861057698726654\n",
      "Epoch: 73200 | Loss: 0.46909475326538086 | Test loss: 0.4860919415950775\n",
      "Epoch: 73210 | Loss: 0.4690830409526825 | Test loss: 0.48607808351516724\n",
      "Epoch: 73220 | Loss: 0.46907123923301697 | Test loss: 0.48606425523757935\n",
      "Epoch: 73230 | Loss: 0.46905946731567383 | Test loss: 0.48605042695999146\n",
      "Epoch: 73240 | Loss: 0.46904775500297546 | Test loss: 0.4860365390777588\n",
      "Epoch: 73250 | Loss: 0.46903595328330994 | Test loss: 0.4860227108001709\n",
      "Epoch: 73260 | Loss: 0.4690241813659668 | Test loss: 0.4860088527202606\n",
      "Epoch: 73270 | Loss: 0.46901246905326843 | Test loss: 0.48599502444267273\n",
      "Epoch: 73280 | Loss: 0.4690007269382477 | Test loss: 0.48598119616508484\n",
      "Epoch: 73290 | Loss: 0.46898889541625977 | Test loss: 0.4859673082828522\n",
      "Epoch: 73300 | Loss: 0.4689771831035614 | Test loss: 0.4859534800052643\n",
      "Epoch: 73310 | Loss: 0.4689653813838959 | Test loss: 0.4859395921230316\n",
      "Epoch: 73320 | Loss: 0.4689536690711975 | Test loss: 0.4859257638454437\n",
      "Epoch: 73330 | Loss: 0.46894189715385437 | Test loss: 0.48591193556785583\n",
      "Epoch: 73340 | Loss: 0.46893009543418884 | Test loss: 0.48589810729026794\n",
      "Epoch: 73350 | Loss: 0.4689183235168457 | Test loss: 0.48588424921035767\n",
      "Epoch: 73360 | Loss: 0.46890661120414734 | Test loss: 0.4858704209327698\n",
      "Epoch: 73370 | Loss: 0.4688948690891266 | Test loss: 0.4858565330505371\n",
      "Epoch: 73380 | Loss: 0.46888309717178345 | Test loss: 0.4858427047729492\n",
      "Epoch: 73390 | Loss: 0.4688713252544403 | Test loss: 0.48582887649536133\n",
      "Epoch: 73400 | Loss: 0.46885958313941956 | Test loss: 0.48581501841545105\n",
      "Epoch: 73410 | Loss: 0.4688478112220764 | Test loss: 0.48580119013786316\n",
      "Epoch: 73420 | Loss: 0.4688360393047333 | Test loss: 0.4857873022556305\n",
      "Epoch: 73430 | Loss: 0.4688242971897125 | Test loss: 0.4857734739780426\n",
      "Epoch: 73440 | Loss: 0.4688125252723694 | Test loss: 0.4857596457004547\n",
      "Epoch: 73450 | Loss: 0.46880075335502625 | Test loss: 0.48574575781822205\n",
      "Epoch: 73460 | Loss: 0.4687890112400055 | Test loss: 0.48573192954063416\n",
      "Epoch: 73470 | Loss: 0.46877723932266235 | Test loss: 0.4857180714607239\n",
      "Epoch: 73480 | Loss: 0.4687654674053192 | Test loss: 0.485704243183136\n",
      "Epoch: 73490 | Loss: 0.46875372529029846 | Test loss: 0.4856904149055481\n",
      "Epoch: 73500 | Loss: 0.4687419831752777 | Test loss: 0.48567652702331543\n",
      "Epoch: 73510 | Loss: 0.4687301814556122 | Test loss: 0.48566269874572754\n",
      "Epoch: 73520 | Loss: 0.46871843934059143 | Test loss: 0.48564887046813965\n",
      "Epoch: 73530 | Loss: 0.4687066674232483 | Test loss: 0.48563501238822937\n",
      "Epoch: 73540 | Loss: 0.46869492530822754 | Test loss: 0.4856211841106415\n",
      "Epoch: 73550 | Loss: 0.4686831533908844 | Test loss: 0.4856073558330536\n",
      "Epoch: 73560 | Loss: 0.46867141127586365 | Test loss: 0.4855934679508209\n",
      "Epoch: 73570 | Loss: 0.4686596393585205 | Test loss: 0.48557963967323303\n",
      "Epoch: 73580 | Loss: 0.46864786744117737 | Test loss: 0.48556575179100037\n",
      "Epoch: 73590 | Loss: 0.4686361253261566 | Test loss: 0.4855519235134125\n",
      "Epoch: 73600 | Loss: 0.4686243534088135 | Test loss: 0.4855380952358246\n",
      "Epoch: 73610 | Loss: 0.4686126410961151 | Test loss: 0.4855242669582367\n",
      "Epoch: 73620 | Loss: 0.4686008393764496 | Test loss: 0.4855104088783264\n",
      "Epoch: 73630 | Loss: 0.46858906745910645 | Test loss: 0.48549652099609375\n",
      "Epoch: 73640 | Loss: 0.4685773551464081 | Test loss: 0.48548269271850586\n",
      "Epoch: 73650 | Loss: 0.46856555342674255 | Test loss: 0.48546886444091797\n",
      "Epoch: 73660 | Loss: 0.4685538411140442 | Test loss: 0.4854550063610077\n",
      "Epoch: 73670 | Loss: 0.4685420095920563 | Test loss: 0.4854411780834198\n",
      "Epoch: 73680 | Loss: 0.4685302674770355 | Test loss: 0.48542729020118713\n",
      "Epoch: 73690 | Loss: 0.4685184955596924 | Test loss: 0.48541346192359924\n",
      "Epoch: 73700 | Loss: 0.46850672364234924 | Test loss: 0.48539963364601135\n",
      "Epoch: 73710 | Loss: 0.4684949815273285 | Test loss: 0.48538580536842346\n",
      "Epoch: 73720 | Loss: 0.4684832692146301 | Test loss: 0.4853719174861908\n",
      "Epoch: 73730 | Loss: 0.468471497297287 | Test loss: 0.4853580892086029\n",
      "Epoch: 73740 | Loss: 0.46845969557762146 | Test loss: 0.4853442311286926\n",
      "Epoch: 73750 | Loss: 0.4684479832649231 | Test loss: 0.48533040285110474\n",
      "Epoch: 73760 | Loss: 0.46843621134757996 | Test loss: 0.48531657457351685\n",
      "Epoch: 73770 | Loss: 0.46842440962791443 | Test loss: 0.48530274629592896\n",
      "Epoch: 73780 | Loss: 0.46841269731521606 | Test loss: 0.4852888584136963\n",
      "Epoch: 73790 | Loss: 0.4684009253978729 | Test loss: 0.4852750301361084\n",
      "Epoch: 73800 | Loss: 0.4683891236782074 | Test loss: 0.4852611720561981\n",
      "Epoch: 73810 | Loss: 0.46837741136550903 | Test loss: 0.48524734377861023\n",
      "Epoch: 73820 | Loss: 0.4683656394481659 | Test loss: 0.48523345589637756\n",
      "Epoch: 73830 | Loss: 0.46835389733314514 | Test loss: 0.4852196276187897\n",
      "Epoch: 73840 | Loss: 0.468342125415802 | Test loss: 0.485205739736557\n",
      "Epoch: 73850 | Loss: 0.46833038330078125 | Test loss: 0.4851919114589691\n",
      "Epoch: 73860 | Loss: 0.46831855177879333 | Test loss: 0.4851780831813812\n",
      "Epoch: 73870 | Loss: 0.46830683946609497 | Test loss: 0.48516425490379333\n",
      "Epoch: 73880 | Loss: 0.4682950973510742 | Test loss: 0.48515039682388306\n",
      "Epoch: 73890 | Loss: 0.4682833254337311 | Test loss: 0.48513656854629517\n",
      "Epoch: 73900 | Loss: 0.46827155351638794 | Test loss: 0.4851226806640625\n",
      "Epoch: 73910 | Loss: 0.4682598114013672 | Test loss: 0.4851088523864746\n",
      "Epoch: 73920 | Loss: 0.46824803948402405 | Test loss: 0.4850950241088867\n",
      "Epoch: 73930 | Loss: 0.4682362675666809 | Test loss: 0.48508119583129883\n",
      "Epoch: 73940 | Loss: 0.46822452545166016 | Test loss: 0.48506733775138855\n",
      "Epoch: 73950 | Loss: 0.4682128131389618 | Test loss: 0.48505350947380066\n",
      "Epoch: 73960 | Loss: 0.4682009816169739 | Test loss: 0.485039621591568\n",
      "Epoch: 73970 | Loss: 0.4681892395019531 | Test loss: 0.4850257933139801\n",
      "Epoch: 73980 | Loss: 0.46817752718925476 | Test loss: 0.4850119650363922\n",
      "Epoch: 73990 | Loss: 0.46816569566726685 | Test loss: 0.48499807715415955\n",
      "Epoch: 74000 | Loss: 0.4681539535522461 | Test loss: 0.48498424887657166\n",
      "Epoch: 74010 | Loss: 0.46814224123954773 | Test loss: 0.4849703907966614\n",
      "Epoch: 74020 | Loss: 0.4681304395198822 | Test loss: 0.4849565625190735\n",
      "Epoch: 74030 | Loss: 0.46811866760253906 | Test loss: 0.4849427342414856\n",
      "Epoch: 74040 | Loss: 0.4681069552898407 | Test loss: 0.48492884635925293\n",
      "Epoch: 74050 | Loss: 0.46809515357017517 | Test loss: 0.48491501808166504\n",
      "Epoch: 74060 | Loss: 0.46808338165283203 | Test loss: 0.48490116000175476\n",
      "Epoch: 74070 | Loss: 0.46807166934013367 | Test loss: 0.48488733172416687\n",
      "Epoch: 74080 | Loss: 0.4680599272251129 | Test loss: 0.484873503446579\n",
      "Epoch: 74090 | Loss: 0.468048095703125 | Test loss: 0.4848596155643463\n",
      "Epoch: 74100 | Loss: 0.46803638339042664 | Test loss: 0.4848457872867584\n",
      "Epoch: 74110 | Loss: 0.4680245816707611 | Test loss: 0.48483189940452576\n",
      "Epoch: 74120 | Loss: 0.46801286935806274 | Test loss: 0.48481807112693787\n",
      "Epoch: 74130 | Loss: 0.4680010974407196 | Test loss: 0.48480424284935\n",
      "Epoch: 74140 | Loss: 0.4679892957210541 | Test loss: 0.4847904145717621\n",
      "Epoch: 74150 | Loss: 0.46797752380371094 | Test loss: 0.4847765564918518\n",
      "Epoch: 74160 | Loss: 0.4679658114910126 | Test loss: 0.4847627282142639\n",
      "Epoch: 74170 | Loss: 0.4679540693759918 | Test loss: 0.48474884033203125\n",
      "Epoch: 74180 | Loss: 0.4679422974586487 | Test loss: 0.48473501205444336\n",
      "Epoch: 74190 | Loss: 0.46793052554130554 | Test loss: 0.48472118377685547\n",
      "Epoch: 74200 | Loss: 0.4679187834262848 | Test loss: 0.4847073256969452\n",
      "Epoch: 74210 | Loss: 0.46790701150894165 | Test loss: 0.4846934974193573\n",
      "Epoch: 74220 | Loss: 0.4678952395915985 | Test loss: 0.48467960953712463\n",
      "Epoch: 74230 | Loss: 0.46788349747657776 | Test loss: 0.48466578125953674\n",
      "Epoch: 74240 | Loss: 0.4678717255592346 | Test loss: 0.48465195298194885\n",
      "Epoch: 74250 | Loss: 0.4678599536418915 | Test loss: 0.4846380650997162\n",
      "Epoch: 74260 | Loss: 0.4678482115268707 | Test loss: 0.4846242368221283\n",
      "Epoch: 74270 | Loss: 0.4678364396095276 | Test loss: 0.484610378742218\n",
      "Epoch: 74280 | Loss: 0.46782466769218445 | Test loss: 0.4845965504646301\n",
      "Epoch: 74290 | Loss: 0.4678129255771637 | Test loss: 0.48458272218704224\n",
      "Epoch: 74300 | Loss: 0.46780118346214294 | Test loss: 0.48456883430480957\n",
      "Epoch: 74310 | Loss: 0.4677893817424774 | Test loss: 0.4845550060272217\n",
      "Epoch: 74320 | Loss: 0.46777763962745667 | Test loss: 0.4845411777496338\n",
      "Epoch: 74330 | Loss: 0.4677658677101135 | Test loss: 0.4845273196697235\n",
      "Epoch: 74340 | Loss: 0.4677541255950928 | Test loss: 0.4845134913921356\n",
      "Epoch: 74350 | Loss: 0.46774235367774963 | Test loss: 0.48449966311454773\n",
      "Epoch: 74360 | Loss: 0.4677306115627289 | Test loss: 0.48448577523231506\n",
      "Epoch: 74370 | Loss: 0.46771883964538574 | Test loss: 0.4844719469547272\n",
      "Epoch: 74380 | Loss: 0.4677070677280426 | Test loss: 0.4844580590724945\n",
      "Epoch: 74390 | Loss: 0.46769532561302185 | Test loss: 0.4844442307949066\n",
      "Epoch: 74400 | Loss: 0.4676835536956787 | Test loss: 0.4844304025173187\n",
      "Epoch: 74410 | Loss: 0.46767184138298035 | Test loss: 0.48441657423973083\n",
      "Epoch: 74420 | Loss: 0.4676600396633148 | Test loss: 0.48440271615982056\n",
      "Epoch: 74430 | Loss: 0.4676482677459717 | Test loss: 0.4843888282775879\n",
      "Epoch: 74440 | Loss: 0.4676365554332733 | Test loss: 0.484375\n",
      "Epoch: 74450 | Loss: 0.4676247537136078 | Test loss: 0.4843611717224121\n",
      "Epoch: 74460 | Loss: 0.4676130414009094 | Test loss: 0.48434731364250183\n",
      "Epoch: 74470 | Loss: 0.4676012098789215 | Test loss: 0.48433348536491394\n",
      "Epoch: 74480 | Loss: 0.46758946776390076 | Test loss: 0.4843195974826813\n",
      "Epoch: 74490 | Loss: 0.4675776958465576 | Test loss: 0.4843057692050934\n",
      "Epoch: 74500 | Loss: 0.4675659239292145 | Test loss: 0.4842919409275055\n",
      "Epoch: 74510 | Loss: 0.4675541818141937 | Test loss: 0.4842781126499176\n",
      "Epoch: 74520 | Loss: 0.46754246950149536 | Test loss: 0.48426422476768494\n",
      "Epoch: 74530 | Loss: 0.4675306975841522 | Test loss: 0.48425039649009705\n",
      "Epoch: 74540 | Loss: 0.4675188958644867 | Test loss: 0.48423653841018677\n",
      "Epoch: 74550 | Loss: 0.46750718355178833 | Test loss: 0.4842227101325989\n",
      "Epoch: 74560 | Loss: 0.4674954116344452 | Test loss: 0.484208881855011\n",
      "Epoch: 74570 | Loss: 0.46748360991477966 | Test loss: 0.4841950535774231\n",
      "Epoch: 74580 | Loss: 0.4674718976020813 | Test loss: 0.48418116569519043\n",
      "Epoch: 74590 | Loss: 0.46746012568473816 | Test loss: 0.48416733741760254\n",
      "Epoch: 74600 | Loss: 0.46744832396507263 | Test loss: 0.48415347933769226\n",
      "Epoch: 74610 | Loss: 0.46743661165237427 | Test loss: 0.48413965106010437\n",
      "Epoch: 74620 | Loss: 0.46742483973503113 | Test loss: 0.4841257631778717\n",
      "Epoch: 74630 | Loss: 0.4674130976200104 | Test loss: 0.4841119349002838\n",
      "Epoch: 74640 | Loss: 0.46740132570266724 | Test loss: 0.48409804701805115\n",
      "Epoch: 74650 | Loss: 0.4673895835876465 | Test loss: 0.48408421874046326\n",
      "Epoch: 74660 | Loss: 0.46737775206565857 | Test loss: 0.48407039046287537\n",
      "Epoch: 74670 | Loss: 0.4673660397529602 | Test loss: 0.4840565621852875\n",
      "Epoch: 74680 | Loss: 0.46735429763793945 | Test loss: 0.4840427041053772\n",
      "Epoch: 74690 | Loss: 0.4673425257205963 | Test loss: 0.4840288758277893\n",
      "Epoch: 74700 | Loss: 0.4673307538032532 | Test loss: 0.48401498794555664\n",
      "Epoch: 74710 | Loss: 0.4673190116882324 | Test loss: 0.48400115966796875\n",
      "Epoch: 74720 | Loss: 0.4673072397708893 | Test loss: 0.48398733139038086\n",
      "Epoch: 74730 | Loss: 0.46729546785354614 | Test loss: 0.48397350311279297\n",
      "Epoch: 74740 | Loss: 0.4672837257385254 | Test loss: 0.4839596450328827\n",
      "Epoch: 74750 | Loss: 0.467272013425827 | Test loss: 0.4839458167552948\n",
      "Epoch: 74760 | Loss: 0.4672601819038391 | Test loss: 0.48393192887306213\n",
      "Epoch: 74770 | Loss: 0.46724843978881836 | Test loss: 0.48391810059547424\n",
      "Epoch: 74780 | Loss: 0.46723672747612 | Test loss: 0.48390427231788635\n",
      "Epoch: 74790 | Loss: 0.4672248959541321 | Test loss: 0.4838903844356537\n",
      "Epoch: 74800 | Loss: 0.46721315383911133 | Test loss: 0.4838765561580658\n",
      "Epoch: 74810 | Loss: 0.46720144152641296 | Test loss: 0.4838626980781555\n",
      "Epoch: 74820 | Loss: 0.46718963980674744 | Test loss: 0.4838488698005676\n",
      "Epoch: 74830 | Loss: 0.4671778678894043 | Test loss: 0.48383504152297974\n",
      "Epoch: 74840 | Loss: 0.46716615557670593 | Test loss: 0.48382115364074707\n",
      "Epoch: 74850 | Loss: 0.4671543538570404 | Test loss: 0.4838073253631592\n",
      "Epoch: 74860 | Loss: 0.46714258193969727 | Test loss: 0.4837934672832489\n",
      "Epoch: 74870 | Loss: 0.4671308696269989 | Test loss: 0.483779639005661\n",
      "Epoch: 74880 | Loss: 0.46711912751197815 | Test loss: 0.4837658107280731\n",
      "Epoch: 74890 | Loss: 0.46710729598999023 | Test loss: 0.48375192284584045\n",
      "Epoch: 74900 | Loss: 0.46709558367729187 | Test loss: 0.48373809456825256\n",
      "Epoch: 74910 | Loss: 0.46708378195762634 | Test loss: 0.4837242066860199\n",
      "Epoch: 74920 | Loss: 0.467072069644928 | Test loss: 0.483710378408432\n",
      "Epoch: 74930 | Loss: 0.46706029772758484 | Test loss: 0.4836965501308441\n",
      "Epoch: 74940 | Loss: 0.4670484960079193 | Test loss: 0.4836827218532562\n",
      "Epoch: 74950 | Loss: 0.46703672409057617 | Test loss: 0.48366886377334595\n",
      "Epoch: 74960 | Loss: 0.4670250117778778 | Test loss: 0.48365503549575806\n",
      "Epoch: 74970 | Loss: 0.46701326966285706 | Test loss: 0.4836411476135254\n",
      "Epoch: 74980 | Loss: 0.4670014977455139 | Test loss: 0.4836273193359375\n",
      "Epoch: 74990 | Loss: 0.4669897258281708 | Test loss: 0.4836134910583496\n",
      "Epoch: 75000 | Loss: 0.46697798371315 | Test loss: 0.48359963297843933\n",
      "Epoch: 75010 | Loss: 0.4669662117958069 | Test loss: 0.48358580470085144\n",
      "Epoch: 75020 | Loss: 0.46695443987846375 | Test loss: 0.4835719168186188\n",
      "Epoch: 75030 | Loss: 0.466942697763443 | Test loss: 0.4835580885410309\n",
      "Epoch: 75040 | Loss: 0.46693092584609985 | Test loss: 0.483544260263443\n",
      "Epoch: 75050 | Loss: 0.4669191539287567 | Test loss: 0.4835303723812103\n",
      "Epoch: 75060 | Loss: 0.46690741181373596 | Test loss: 0.48351654410362244\n",
      "Epoch: 75070 | Loss: 0.4668956398963928 | Test loss: 0.48350268602371216\n",
      "Epoch: 75080 | Loss: 0.4668838679790497 | Test loss: 0.48348885774612427\n",
      "Epoch: 75090 | Loss: 0.46687212586402893 | Test loss: 0.4834750294685364\n",
      "Epoch: 75100 | Loss: 0.4668603837490082 | Test loss: 0.4834611415863037\n",
      "Epoch: 75110 | Loss: 0.46684858202934265 | Test loss: 0.4834473133087158\n",
      "Epoch: 75120 | Loss: 0.4668368399143219 | Test loss: 0.48343348503112793\n",
      "Epoch: 75130 | Loss: 0.46682506799697876 | Test loss: 0.48341962695121765\n",
      "Epoch: 75140 | Loss: 0.466813325881958 | Test loss: 0.48340579867362976\n",
      "Epoch: 75150 | Loss: 0.46680155396461487 | Test loss: 0.48339197039604187\n",
      "Epoch: 75160 | Loss: 0.4667898118495941 | Test loss: 0.4833780825138092\n",
      "Epoch: 75170 | Loss: 0.466778039932251 | Test loss: 0.4833642542362213\n",
      "Epoch: 75180 | Loss: 0.46676626801490784 | Test loss: 0.48335036635398865\n",
      "Epoch: 75190 | Loss: 0.4667545258998871 | Test loss: 0.48333653807640076\n",
      "Epoch: 75200 | Loss: 0.46674275398254395 | Test loss: 0.48332270979881287\n",
      "Epoch: 75210 | Loss: 0.4667310416698456 | Test loss: 0.483308881521225\n",
      "Epoch: 75220 | Loss: 0.46671923995018005 | Test loss: 0.4832950234413147\n",
      "Epoch: 75230 | Loss: 0.4667074680328369 | Test loss: 0.48328113555908203\n",
      "Epoch: 75240 | Loss: 0.46669575572013855 | Test loss: 0.48326730728149414\n",
      "Epoch: 75250 | Loss: 0.466683954000473 | Test loss: 0.48325347900390625\n",
      "Epoch: 75260 | Loss: 0.46667224168777466 | Test loss: 0.48323962092399597\n",
      "Epoch: 75270 | Loss: 0.46666041016578674 | Test loss: 0.4832257926464081\n",
      "Epoch: 75280 | Loss: 0.466648668050766 | Test loss: 0.4832119047641754\n",
      "Epoch: 75290 | Loss: 0.46663689613342285 | Test loss: 0.4831980764865875\n",
      "Epoch: 75300 | Loss: 0.4666251242160797 | Test loss: 0.48318424820899963\n",
      "Epoch: 75310 | Loss: 0.46661338210105896 | Test loss: 0.48317041993141174\n",
      "Epoch: 75320 | Loss: 0.4666016697883606 | Test loss: 0.4831565320491791\n",
      "Epoch: 75330 | Loss: 0.46658989787101746 | Test loss: 0.4831427037715912\n",
      "Epoch: 75340 | Loss: 0.46657809615135193 | Test loss: 0.4831288456916809\n",
      "Epoch: 75350 | Loss: 0.46656638383865356 | Test loss: 0.483115017414093\n",
      "Epoch: 75360 | Loss: 0.4665546119213104 | Test loss: 0.4831011891365051\n",
      "Epoch: 75370 | Loss: 0.4665428102016449 | Test loss: 0.48308736085891724\n",
      "Epoch: 75380 | Loss: 0.46653109788894653 | Test loss: 0.48307347297668457\n",
      "Epoch: 75390 | Loss: 0.4665193259716034 | Test loss: 0.4830596446990967\n",
      "Epoch: 75400 | Loss: 0.46650752425193787 | Test loss: 0.4830457866191864\n",
      "Epoch: 75410 | Loss: 0.4664958119392395 | Test loss: 0.4830319583415985\n",
      "Epoch: 75420 | Loss: 0.46648404002189636 | Test loss: 0.48301807045936584\n",
      "Epoch: 75430 | Loss: 0.4664722979068756 | Test loss: 0.48300424218177795\n",
      "Epoch: 75440 | Loss: 0.46646052598953247 | Test loss: 0.4829903542995453\n",
      "Epoch: 75450 | Loss: 0.4664487838745117 | Test loss: 0.4829765260219574\n",
      "Epoch: 75460 | Loss: 0.4664369523525238 | Test loss: 0.4829626977443695\n",
      "Epoch: 75470 | Loss: 0.46642524003982544 | Test loss: 0.4829488694667816\n",
      "Epoch: 75480 | Loss: 0.4664134979248047 | Test loss: 0.48293501138687134\n",
      "Epoch: 75490 | Loss: 0.46640172600746155 | Test loss: 0.48292118310928345\n",
      "Epoch: 75500 | Loss: 0.4663899540901184 | Test loss: 0.4829072952270508\n",
      "Epoch: 75510 | Loss: 0.46637821197509766 | Test loss: 0.4828934669494629\n",
      "Epoch: 75520 | Loss: 0.4663664400577545 | Test loss: 0.482879638671875\n",
      "Epoch: 75530 | Loss: 0.4663546681404114 | Test loss: 0.4828658103942871\n",
      "Epoch: 75540 | Loss: 0.4663429260253906 | Test loss: 0.48285195231437683\n",
      "Epoch: 75550 | Loss: 0.46633121371269226 | Test loss: 0.48283812403678894\n",
      "Epoch: 75560 | Loss: 0.46631938219070435 | Test loss: 0.4828242361545563\n",
      "Epoch: 75570 | Loss: 0.4663076400756836 | Test loss: 0.4828104078769684\n",
      "Epoch: 75580 | Loss: 0.46629592776298523 | Test loss: 0.4827965795993805\n",
      "Epoch: 75590 | Loss: 0.4662840962409973 | Test loss: 0.4827826917171478\n",
      "Epoch: 75600 | Loss: 0.46627235412597656 | Test loss: 0.48276886343955994\n",
      "Epoch: 75610 | Loss: 0.4662606418132782 | Test loss: 0.48275500535964966\n",
      "Epoch: 75620 | Loss: 0.46624884009361267 | Test loss: 0.48274117708206177\n",
      "Epoch: 75630 | Loss: 0.46623706817626953 | Test loss: 0.4827273488044739\n",
      "Epoch: 75640 | Loss: 0.46622535586357117 | Test loss: 0.4827134609222412\n",
      "Epoch: 75650 | Loss: 0.46621355414390564 | Test loss: 0.4826996326446533\n",
      "Epoch: 75660 | Loss: 0.4662017822265625 | Test loss: 0.48268577456474304\n",
      "Epoch: 75670 | Loss: 0.46619006991386414 | Test loss: 0.48267194628715515\n",
      "Epoch: 75680 | Loss: 0.4661783277988434 | Test loss: 0.48265811800956726\n",
      "Epoch: 75690 | Loss: 0.46616649627685547 | Test loss: 0.4826442301273346\n",
      "Epoch: 75700 | Loss: 0.4661547839641571 | Test loss: 0.4826304018497467\n",
      "Epoch: 75710 | Loss: 0.4661429822444916 | Test loss: 0.48261651396751404\n",
      "Epoch: 75720 | Loss: 0.4661312699317932 | Test loss: 0.48260268568992615\n",
      "Epoch: 75730 | Loss: 0.4661194980144501 | Test loss: 0.48258885741233826\n",
      "Epoch: 75740 | Loss: 0.46610769629478455 | Test loss: 0.48257502913475037\n",
      "Epoch: 75750 | Loss: 0.4660959243774414 | Test loss: 0.4825611710548401\n",
      "Epoch: 75760 | Loss: 0.46608421206474304 | Test loss: 0.4825473427772522\n",
      "Epoch: 75770 | Loss: 0.4660724699497223 | Test loss: 0.48253345489501953\n",
      "Epoch: 75780 | Loss: 0.46606069803237915 | Test loss: 0.48251962661743164\n",
      "Epoch: 75790 | Loss: 0.466048926115036 | Test loss: 0.48250579833984375\n",
      "Epoch: 75800 | Loss: 0.46603718400001526 | Test loss: 0.48249194025993347\n",
      "Epoch: 75810 | Loss: 0.4660254120826721 | Test loss: 0.4824781119823456\n",
      "Epoch: 75820 | Loss: 0.466013640165329 | Test loss: 0.4824642241001129\n",
      "Epoch: 75830 | Loss: 0.4660018980503082 | Test loss: 0.482450395822525\n",
      "Epoch: 75840 | Loss: 0.4659901261329651 | Test loss: 0.48243656754493713\n",
      "Epoch: 75850 | Loss: 0.46597835421562195 | Test loss: 0.48242267966270447\n",
      "Epoch: 75860 | Loss: 0.4659666121006012 | Test loss: 0.4824088513851166\n",
      "Epoch: 75870 | Loss: 0.46595484018325806 | Test loss: 0.4823949933052063\n",
      "Epoch: 75880 | Loss: 0.4659430682659149 | Test loss: 0.4823811650276184\n",
      "Epoch: 75890 | Loss: 0.46593132615089417 | Test loss: 0.4823673367500305\n",
      "Epoch: 75900 | Loss: 0.4659195840358734 | Test loss: 0.48235344886779785\n",
      "Epoch: 75910 | Loss: 0.4659077823162079 | Test loss: 0.48233962059020996\n",
      "Epoch: 75920 | Loss: 0.46589604020118713 | Test loss: 0.48232579231262207\n",
      "Epoch: 75930 | Loss: 0.465884268283844 | Test loss: 0.4823119342327118\n",
      "Epoch: 75940 | Loss: 0.46587252616882324 | Test loss: 0.4822981059551239\n",
      "Epoch: 75950 | Loss: 0.4658607542514801 | Test loss: 0.482284277677536\n",
      "Epoch: 75960 | Loss: 0.46584901213645935 | Test loss: 0.48227038979530334\n",
      "Epoch: 75970 | Loss: 0.4658372402191162 | Test loss: 0.48225656151771545\n",
      "Epoch: 75980 | Loss: 0.46582546830177307 | Test loss: 0.4822426736354828\n",
      "Epoch: 75990 | Loss: 0.4658137261867523 | Test loss: 0.4822288453578949\n",
      "Epoch: 76000 | Loss: 0.4658019542694092 | Test loss: 0.482215017080307\n",
      "Epoch: 76010 | Loss: 0.4657902419567108 | Test loss: 0.4822011888027191\n",
      "Epoch: 76020 | Loss: 0.4657784402370453 | Test loss: 0.48218733072280884\n",
      "Epoch: 76030 | Loss: 0.46576666831970215 | Test loss: 0.48217344284057617\n",
      "Epoch: 76040 | Loss: 0.4657549560070038 | Test loss: 0.4821596145629883\n",
      "Epoch: 76050 | Loss: 0.46574315428733826 | Test loss: 0.4821457862854004\n",
      "Epoch: 76060 | Loss: 0.4657314419746399 | Test loss: 0.4821319282054901\n",
      "Epoch: 76070 | Loss: 0.465719610452652 | Test loss: 0.4821180999279022\n",
      "Epoch: 76080 | Loss: 0.4657078683376312 | Test loss: 0.48210421204566956\n",
      "Epoch: 76090 | Loss: 0.4656960964202881 | Test loss: 0.48209038376808167\n",
      "Epoch: 76100 | Loss: 0.46568432450294495 | Test loss: 0.4820765554904938\n",
      "Epoch: 76110 | Loss: 0.4656725823879242 | Test loss: 0.4820627272129059\n",
      "Epoch: 76120 | Loss: 0.46566087007522583 | Test loss: 0.4820488393306732\n",
      "Epoch: 76130 | Loss: 0.4656490981578827 | Test loss: 0.4820350110530853\n",
      "Epoch: 76140 | Loss: 0.46563729643821716 | Test loss: 0.48202115297317505\n",
      "Epoch: 76150 | Loss: 0.4656255841255188 | Test loss: 0.48200732469558716\n",
      "Epoch: 76160 | Loss: 0.46561381220817566 | Test loss: 0.48199349641799927\n",
      "Epoch: 76170 | Loss: 0.46560201048851013 | Test loss: 0.4819796681404114\n",
      "Epoch: 76180 | Loss: 0.46559029817581177 | Test loss: 0.4819657802581787\n",
      "Epoch: 76190 | Loss: 0.46557852625846863 | Test loss: 0.4819519519805908\n",
      "Epoch: 76200 | Loss: 0.4655667245388031 | Test loss: 0.48193809390068054\n",
      "Epoch: 76210 | Loss: 0.46555501222610474 | Test loss: 0.48192426562309265\n",
      "Epoch: 76220 | Loss: 0.4655432403087616 | Test loss: 0.48191037774086\n",
      "Epoch: 76230 | Loss: 0.46553149819374084 | Test loss: 0.4818965494632721\n",
      "Epoch: 76240 | Loss: 0.4655197262763977 | Test loss: 0.48188266158103943\n",
      "Epoch: 76250 | Loss: 0.46550798416137695 | Test loss: 0.48186883330345154\n",
      "Epoch: 76260 | Loss: 0.46549615263938904 | Test loss: 0.48185500502586365\n",
      "Epoch: 76270 | Loss: 0.4654844403266907 | Test loss: 0.48184117674827576\n",
      "Epoch: 76280 | Loss: 0.4654726982116699 | Test loss: 0.4818273186683655\n",
      "Epoch: 76290 | Loss: 0.4654609262943268 | Test loss: 0.4818134903907776\n",
      "Epoch: 76300 | Loss: 0.46544915437698364 | Test loss: 0.4817996025085449\n",
      "Epoch: 76310 | Loss: 0.4654374122619629 | Test loss: 0.48178577423095703\n",
      "Epoch: 76320 | Loss: 0.46542564034461975 | Test loss: 0.48177194595336914\n",
      "Epoch: 76330 | Loss: 0.4654138684272766 | Test loss: 0.48175811767578125\n",
      "Epoch: 76340 | Loss: 0.46540212631225586 | Test loss: 0.48174425959587097\n",
      "Epoch: 76350 | Loss: 0.4653904139995575 | Test loss: 0.4817304313182831\n",
      "Epoch: 76360 | Loss: 0.4653785824775696 | Test loss: 0.4817165434360504\n",
      "Epoch: 76370 | Loss: 0.46536684036254883 | Test loss: 0.4817027151584625\n",
      "Epoch: 76380 | Loss: 0.46535512804985046 | Test loss: 0.48168888688087463\n",
      "Epoch: 76390 | Loss: 0.46534329652786255 | Test loss: 0.48167499899864197\n",
      "Epoch: 76400 | Loss: 0.4653315544128418 | Test loss: 0.4816611707210541\n",
      "Epoch: 76410 | Loss: 0.46531984210014343 | Test loss: 0.4816473126411438\n",
      "Epoch: 76420 | Loss: 0.4653080403804779 | Test loss: 0.4816334843635559\n",
      "Epoch: 76430 | Loss: 0.46529626846313477 | Test loss: 0.481619656085968\n",
      "Epoch: 76440 | Loss: 0.4652845561504364 | Test loss: 0.48160576820373535\n",
      "Epoch: 76450 | Loss: 0.4652727544307709 | Test loss: 0.48159193992614746\n",
      "Epoch: 76460 | Loss: 0.46526098251342773 | Test loss: 0.4815780818462372\n",
      "Epoch: 76470 | Loss: 0.46524927020072937 | Test loss: 0.4815642535686493\n",
      "Epoch: 76480 | Loss: 0.4652375280857086 | Test loss: 0.4815504252910614\n",
      "Epoch: 76490 | Loss: 0.4652256965637207 | Test loss: 0.48153653740882874\n",
      "Epoch: 76500 | Loss: 0.46521398425102234 | Test loss: 0.48152270913124084\n",
      "Epoch: 76510 | Loss: 0.4652021825313568 | Test loss: 0.4815088212490082\n",
      "Epoch: 76520 | Loss: 0.46519047021865845 | Test loss: 0.4814949929714203\n",
      "Epoch: 76530 | Loss: 0.4651786983013153 | Test loss: 0.4814811646938324\n",
      "Epoch: 76540 | Loss: 0.4651668965816498 | Test loss: 0.4814673364162445\n",
      "Epoch: 76550 | Loss: 0.46515512466430664 | Test loss: 0.48145347833633423\n",
      "Epoch: 76560 | Loss: 0.4651434123516083 | Test loss: 0.48143965005874634\n",
      "Epoch: 76570 | Loss: 0.4651316702365875 | Test loss: 0.48142576217651367\n",
      "Epoch: 76580 | Loss: 0.4651198983192444 | Test loss: 0.4814119338989258\n",
      "Epoch: 76590 | Loss: 0.46510812640190125 | Test loss: 0.4813981056213379\n",
      "Epoch: 76600 | Loss: 0.4650963842868805 | Test loss: 0.4813842475414276\n",
      "Epoch: 76610 | Loss: 0.46508461236953735 | Test loss: 0.4813704192638397\n",
      "Epoch: 76620 | Loss: 0.4650728404521942 | Test loss: 0.48135653138160706\n",
      "Epoch: 76630 | Loss: 0.46506109833717346 | Test loss: 0.48134270310401917\n",
      "Epoch: 76640 | Loss: 0.4650493264198303 | Test loss: 0.4813288748264313\n",
      "Epoch: 76650 | Loss: 0.4650375545024872 | Test loss: 0.4813149869441986\n",
      "Epoch: 76660 | Loss: 0.46502581238746643 | Test loss: 0.4813011586666107\n",
      "Epoch: 76670 | Loss: 0.4650140404701233 | Test loss: 0.48128730058670044\n",
      "Epoch: 76680 | Loss: 0.46500226855278015 | Test loss: 0.48127347230911255\n",
      "Epoch: 76690 | Loss: 0.4649905264377594 | Test loss: 0.48125964403152466\n",
      "Epoch: 76700 | Loss: 0.46497878432273865 | Test loss: 0.481245756149292\n",
      "Epoch: 76710 | Loss: 0.4649669826030731 | Test loss: 0.4812319278717041\n",
      "Epoch: 76720 | Loss: 0.46495524048805237 | Test loss: 0.4812180995941162\n",
      "Epoch: 76730 | Loss: 0.46494346857070923 | Test loss: 0.48120424151420593\n",
      "Epoch: 76740 | Loss: 0.4649317264556885 | Test loss: 0.48119041323661804\n",
      "Epoch: 76750 | Loss: 0.46491995453834534 | Test loss: 0.48117658495903015\n",
      "Epoch: 76760 | Loss: 0.4649082124233246 | Test loss: 0.4811626970767975\n",
      "Epoch: 76770 | Loss: 0.46489644050598145 | Test loss: 0.4811488687992096\n",
      "Epoch: 76780 | Loss: 0.4648846685886383 | Test loss: 0.48113498091697693\n",
      "Epoch: 76790 | Loss: 0.46487292647361755 | Test loss: 0.48112115263938904\n",
      "Epoch: 76800 | Loss: 0.4648611545562744 | Test loss: 0.48110732436180115\n",
      "Epoch: 76810 | Loss: 0.46484944224357605 | Test loss: 0.48109349608421326\n",
      "Epoch: 76820 | Loss: 0.4648376405239105 | Test loss: 0.481079638004303\n",
      "Epoch: 76830 | Loss: 0.4648258686065674 | Test loss: 0.4810657501220703\n",
      "Epoch: 76840 | Loss: 0.464814156293869 | Test loss: 0.4810519218444824\n",
      "Epoch: 76850 | Loss: 0.4648023545742035 | Test loss: 0.48103809356689453\n",
      "Epoch: 76860 | Loss: 0.4647906422615051 | Test loss: 0.48102423548698425\n",
      "Epoch: 76870 | Loss: 0.4647788107395172 | Test loss: 0.48101040720939636\n",
      "Epoch: 76880 | Loss: 0.46476706862449646 | Test loss: 0.4809965193271637\n",
      "Epoch: 76890 | Loss: 0.4647552967071533 | Test loss: 0.4809826910495758\n",
      "Epoch: 76900 | Loss: 0.4647435247898102 | Test loss: 0.4809688627719879\n",
      "Epoch: 76910 | Loss: 0.46473178267478943 | Test loss: 0.4809550344944\n",
      "Epoch: 76920 | Loss: 0.46472007036209106 | Test loss: 0.48094114661216736\n",
      "Epoch: 76930 | Loss: 0.4647082984447479 | Test loss: 0.48092731833457947\n",
      "Epoch: 76940 | Loss: 0.4646964967250824 | Test loss: 0.4809134602546692\n",
      "Epoch: 76950 | Loss: 0.46468478441238403 | Test loss: 0.4808996319770813\n",
      "Epoch: 76960 | Loss: 0.4646730124950409 | Test loss: 0.4808858036994934\n",
      "Epoch: 76970 | Loss: 0.46466121077537537 | Test loss: 0.4808719754219055\n",
      "Epoch: 76980 | Loss: 0.464649498462677 | Test loss: 0.48085808753967285\n",
      "Epoch: 76990 | Loss: 0.46463772654533386 | Test loss: 0.48084425926208496\n",
      "Epoch: 77000 | Loss: 0.46462592482566833 | Test loss: 0.4808304011821747\n",
      "Epoch: 77010 | Loss: 0.46461421251296997 | Test loss: 0.4808165729045868\n",
      "Epoch: 77020 | Loss: 0.46460244059562683 | Test loss: 0.4808026850223541\n",
      "Epoch: 77030 | Loss: 0.4645906984806061 | Test loss: 0.48078885674476624\n",
      "Epoch: 77040 | Loss: 0.46457892656326294 | Test loss: 0.48077496886253357\n",
      "Epoch: 77050 | Loss: 0.4645671844482422 | Test loss: 0.4807611405849457\n",
      "Epoch: 77060 | Loss: 0.4645553529262543 | Test loss: 0.4807473123073578\n",
      "Epoch: 77070 | Loss: 0.4645436406135559 | Test loss: 0.4807334840297699\n",
      "Epoch: 77080 | Loss: 0.46453189849853516 | Test loss: 0.4807196259498596\n",
      "Epoch: 77090 | Loss: 0.464520126581192 | Test loss: 0.48070579767227173\n",
      "Epoch: 77100 | Loss: 0.4645083546638489 | Test loss: 0.48069190979003906\n",
      "Epoch: 77110 | Loss: 0.4644966125488281 | Test loss: 0.48067808151245117\n",
      "Epoch: 77120 | Loss: 0.464484840631485 | Test loss: 0.4806642532348633\n",
      "Epoch: 77130 | Loss: 0.46447306871414185 | Test loss: 0.4806504249572754\n",
      "Epoch: 77140 | Loss: 0.4644613265991211 | Test loss: 0.4806365668773651\n",
      "Epoch: 77150 | Loss: 0.46444961428642273 | Test loss: 0.4806227385997772\n",
      "Epoch: 77160 | Loss: 0.4644377827644348 | Test loss: 0.48060885071754456\n",
      "Epoch: 77170 | Loss: 0.46442604064941406 | Test loss: 0.48059502243995667\n",
      "Epoch: 77180 | Loss: 0.4644143283367157 | Test loss: 0.4805811941623688\n",
      "Epoch: 77190 | Loss: 0.4644024968147278 | Test loss: 0.4805673062801361\n",
      "Epoch: 77200 | Loss: 0.46439075469970703 | Test loss: 0.4805534780025482\n",
      "Epoch: 77210 | Loss: 0.46437904238700867 | Test loss: 0.48053961992263794\n",
      "Epoch: 77220 | Loss: 0.46436724066734314 | Test loss: 0.48052579164505005\n",
      "Epoch: 77230 | Loss: 0.46435546875 | Test loss: 0.48051196336746216\n",
      "Epoch: 77240 | Loss: 0.46434375643730164 | Test loss: 0.4804980754852295\n",
      "Epoch: 77250 | Loss: 0.4643319547176361 | Test loss: 0.4804842472076416\n",
      "Epoch: 77260 | Loss: 0.46432018280029297 | Test loss: 0.4804703891277313\n",
      "Epoch: 77270 | Loss: 0.4643084704875946 | Test loss: 0.48045656085014343\n",
      "Epoch: 77280 | Loss: 0.46429672837257385 | Test loss: 0.48044273257255554\n",
      "Epoch: 77290 | Loss: 0.46428489685058594 | Test loss: 0.4804288446903229\n",
      "Epoch: 77300 | Loss: 0.4642731845378876 | Test loss: 0.480415016412735\n",
      "Epoch: 77310 | Loss: 0.46426138281822205 | Test loss: 0.4804011285305023\n",
      "Epoch: 77320 | Loss: 0.4642496705055237 | Test loss: 0.48038730025291443\n",
      "Epoch: 77330 | Loss: 0.46423789858818054 | Test loss: 0.48037347197532654\n",
      "Epoch: 77340 | Loss: 0.464226096868515 | Test loss: 0.48035964369773865\n",
      "Epoch: 77350 | Loss: 0.4642143249511719 | Test loss: 0.48034578561782837\n",
      "Epoch: 77360 | Loss: 0.4642026126384735 | Test loss: 0.4803319573402405\n",
      "Epoch: 77370 | Loss: 0.46419087052345276 | Test loss: 0.4803180694580078\n",
      "Epoch: 77380 | Loss: 0.4641790986061096 | Test loss: 0.4803042411804199\n",
      "Epoch: 77390 | Loss: 0.4641673266887665 | Test loss: 0.48029041290283203\n",
      "Epoch: 77400 | Loss: 0.4641555845737457 | Test loss: 0.48027655482292175\n",
      "Epoch: 77410 | Loss: 0.4641438126564026 | Test loss: 0.48026272654533386\n",
      "Epoch: 77420 | Loss: 0.46413204073905945 | Test loss: 0.4802488386631012\n",
      "Epoch: 77430 | Loss: 0.4641202986240387 | Test loss: 0.4802350103855133\n",
      "Epoch: 77440 | Loss: 0.46410852670669556 | Test loss: 0.4802211821079254\n",
      "Epoch: 77450 | Loss: 0.4640967547893524 | Test loss: 0.48020729422569275\n",
      "Epoch: 77460 | Loss: 0.46408501267433167 | Test loss: 0.48019346594810486\n",
      "Epoch: 77470 | Loss: 0.4640732407569885 | Test loss: 0.4801796078681946\n",
      "Epoch: 77480 | Loss: 0.4640614688396454 | Test loss: 0.4801657795906067\n",
      "Epoch: 77490 | Loss: 0.46404972672462463 | Test loss: 0.4801519513130188\n",
      "Epoch: 77500 | Loss: 0.4640379846096039 | Test loss: 0.48013806343078613\n",
      "Epoch: 77510 | Loss: 0.46402618288993835 | Test loss: 0.48012423515319824\n",
      "Epoch: 77520 | Loss: 0.4640144407749176 | Test loss: 0.48011040687561035\n",
      "Epoch: 77530 | Loss: 0.46400266885757446 | Test loss: 0.4800965487957001\n",
      "Epoch: 77540 | Loss: 0.4639909267425537 | Test loss: 0.4800827205181122\n",
      "Epoch: 77550 | Loss: 0.46397915482521057 | Test loss: 0.4800688922405243\n",
      "Epoch: 77560 | Loss: 0.4639674127101898 | Test loss: 0.4800550043582916\n",
      "Epoch: 77570 | Loss: 0.4639556407928467 | Test loss: 0.48004117608070374\n",
      "Epoch: 77580 | Loss: 0.46394386887550354 | Test loss: 0.48002728819847107\n",
      "Epoch: 77590 | Loss: 0.4639321267604828 | Test loss: 0.4800134599208832\n",
      "Epoch: 77600 | Loss: 0.46392035484313965 | Test loss: 0.4799996316432953\n",
      "Epoch: 77610 | Loss: 0.4639086425304413 | Test loss: 0.4799858033657074\n",
      "Epoch: 77620 | Loss: 0.46389684081077576 | Test loss: 0.4799719452857971\n",
      "Epoch: 77630 | Loss: 0.4638850688934326 | Test loss: 0.47995805740356445\n",
      "Epoch: 77640 | Loss: 0.46387335658073425 | Test loss: 0.47994422912597656\n",
      "Epoch: 77650 | Loss: 0.4638615548610687 | Test loss: 0.47993040084838867\n",
      "Epoch: 77660 | Loss: 0.46384984254837036 | Test loss: 0.4799165427684784\n",
      "Epoch: 77670 | Loss: 0.46383801102638245 | Test loss: 0.4799027144908905\n",
      "Epoch: 77680 | Loss: 0.4638262689113617 | Test loss: 0.47988882660865784\n",
      "Epoch: 77690 | Loss: 0.46381449699401855 | Test loss: 0.47987499833106995\n",
      "Epoch: 77700 | Loss: 0.4638027250766754 | Test loss: 0.47986117005348206\n",
      "Epoch: 77710 | Loss: 0.46379098296165466 | Test loss: 0.47984734177589417\n",
      "Epoch: 77720 | Loss: 0.4637792706489563 | Test loss: 0.4798334538936615\n",
      "Epoch: 77730 | Loss: 0.46376749873161316 | Test loss: 0.4798196256160736\n",
      "Epoch: 77740 | Loss: 0.46375569701194763 | Test loss: 0.47980576753616333\n",
      "Epoch: 77750 | Loss: 0.46374398469924927 | Test loss: 0.47979193925857544\n",
      "Epoch: 77760 | Loss: 0.46373221278190613 | Test loss: 0.47977811098098755\n",
      "Epoch: 77770 | Loss: 0.4637204110622406 | Test loss: 0.47976428270339966\n",
      "Epoch: 77780 | Loss: 0.46370869874954224 | Test loss: 0.479750394821167\n",
      "Epoch: 77790 | Loss: 0.4636969268321991 | Test loss: 0.4797365665435791\n",
      "Epoch: 77800 | Loss: 0.46368512511253357 | Test loss: 0.4797227084636688\n",
      "Epoch: 77810 | Loss: 0.4636734127998352 | Test loss: 0.47970888018608093\n",
      "Epoch: 77820 | Loss: 0.46366164088249207 | Test loss: 0.47969499230384827\n",
      "Epoch: 77830 | Loss: 0.4636498987674713 | Test loss: 0.4796811640262604\n",
      "Epoch: 77840 | Loss: 0.4636381268501282 | Test loss: 0.4796672761440277\n",
      "Epoch: 77850 | Loss: 0.4636263847351074 | Test loss: 0.4796534478664398\n",
      "Epoch: 77860 | Loss: 0.4636145532131195 | Test loss: 0.47963961958885193\n",
      "Epoch: 77870 | Loss: 0.46360284090042114 | Test loss: 0.47962579131126404\n",
      "Epoch: 77880 | Loss: 0.4635910987854004 | Test loss: 0.47961193323135376\n",
      "Epoch: 77890 | Loss: 0.46357932686805725 | Test loss: 0.47959810495376587\n",
      "Epoch: 77900 | Loss: 0.4635675549507141 | Test loss: 0.4795842170715332\n",
      "Epoch: 77910 | Loss: 0.46355581283569336 | Test loss: 0.4795703887939453\n",
      "Epoch: 77920 | Loss: 0.4635440409183502 | Test loss: 0.4795565605163574\n",
      "Epoch: 77930 | Loss: 0.4635322690010071 | Test loss: 0.47954273223876953\n",
      "Epoch: 77940 | Loss: 0.46352052688598633 | Test loss: 0.47952887415885925\n",
      "Epoch: 77950 | Loss: 0.46350881457328796 | Test loss: 0.47951504588127136\n",
      "Epoch: 77960 | Loss: 0.46349698305130005 | Test loss: 0.4795011579990387\n",
      "Epoch: 77970 | Loss: 0.4634852409362793 | Test loss: 0.4794873297214508\n",
      "Epoch: 77980 | Loss: 0.46347352862358093 | Test loss: 0.4794735014438629\n",
      "Epoch: 77990 | Loss: 0.463461697101593 | Test loss: 0.47945961356163025\n",
      "Epoch: 78000 | Loss: 0.46344995498657227 | Test loss: 0.47944578528404236\n",
      "Epoch: 78010 | Loss: 0.4634382426738739 | Test loss: 0.4794319272041321\n",
      "Epoch: 78020 | Loss: 0.4634264409542084 | Test loss: 0.4794180989265442\n",
      "Epoch: 78030 | Loss: 0.46341466903686523 | Test loss: 0.4794042706489563\n",
      "Epoch: 78040 | Loss: 0.46340295672416687 | Test loss: 0.47939038276672363\n",
      "Epoch: 78050 | Loss: 0.46339115500450134 | Test loss: 0.47937655448913574\n",
      "Epoch: 78060 | Loss: 0.4633793830871582 | Test loss: 0.47936269640922546\n",
      "Epoch: 78070 | Loss: 0.46336767077445984 | Test loss: 0.4793488681316376\n",
      "Epoch: 78080 | Loss: 0.4633559286594391 | Test loss: 0.4793350398540497\n",
      "Epoch: 78090 | Loss: 0.46334409713745117 | Test loss: 0.479321151971817\n",
      "Epoch: 78100 | Loss: 0.4633323848247528 | Test loss: 0.4793073236942291\n",
      "Epoch: 78110 | Loss: 0.4633205831050873 | Test loss: 0.47929343581199646\n",
      "Epoch: 78120 | Loss: 0.4633088707923889 | Test loss: 0.47927960753440857\n",
      "Epoch: 78130 | Loss: 0.4632970988750458 | Test loss: 0.4792657792568207\n",
      "Epoch: 78140 | Loss: 0.46328529715538025 | Test loss: 0.4792519509792328\n",
      "Epoch: 78150 | Loss: 0.4632735252380371 | Test loss: 0.4792380928993225\n",
      "Epoch: 78160 | Loss: 0.46326181292533875 | Test loss: 0.4792242646217346\n",
      "Epoch: 78170 | Loss: 0.463250070810318 | Test loss: 0.47921037673950195\n",
      "Epoch: 78180 | Loss: 0.46323829889297485 | Test loss: 0.47919654846191406\n",
      "Epoch: 78190 | Loss: 0.4632265269756317 | Test loss: 0.47918272018432617\n",
      "Epoch: 78200 | Loss: 0.46321478486061096 | Test loss: 0.4791688621044159\n",
      "Epoch: 78210 | Loss: 0.4632030129432678 | Test loss: 0.479155033826828\n",
      "Epoch: 78220 | Loss: 0.4631912410259247 | Test loss: 0.47914114594459534\n",
      "Epoch: 78230 | Loss: 0.46317949891090393 | Test loss: 0.47912731766700745\n",
      "Epoch: 78240 | Loss: 0.4631677269935608 | Test loss: 0.47911348938941956\n",
      "Epoch: 78250 | Loss: 0.46315595507621765 | Test loss: 0.4790996015071869\n",
      "Epoch: 78260 | Loss: 0.4631442129611969 | Test loss: 0.479085773229599\n",
      "Epoch: 78270 | Loss: 0.46313244104385376 | Test loss: 0.4790719151496887\n",
      "Epoch: 78280 | Loss: 0.4631206691265106 | Test loss: 0.47905808687210083\n",
      "Epoch: 78290 | Loss: 0.46310892701148987 | Test loss: 0.47904425859451294\n",
      "Epoch: 78300 | Loss: 0.4630971848964691 | Test loss: 0.4790303707122803\n",
      "Epoch: 78310 | Loss: 0.4630853831768036 | Test loss: 0.4790165424346924\n",
      "Epoch: 78320 | Loss: 0.46307364106178284 | Test loss: 0.4790027141571045\n",
      "Epoch: 78330 | Loss: 0.4630618691444397 | Test loss: 0.4789888560771942\n",
      "Epoch: 78340 | Loss: 0.46305012702941895 | Test loss: 0.4789750277996063\n",
      "Epoch: 78350 | Loss: 0.4630383551120758 | Test loss: 0.47896119952201843\n",
      "Epoch: 78360 | Loss: 0.46302661299705505 | Test loss: 0.47894731163978577\n",
      "Epoch: 78370 | Loss: 0.4630148410797119 | Test loss: 0.4789334833621979\n",
      "Epoch: 78380 | Loss: 0.4630030691623688 | Test loss: 0.4789195954799652\n",
      "Epoch: 78390 | Loss: 0.462991327047348 | Test loss: 0.4789057672023773\n",
      "Epoch: 78400 | Loss: 0.4629795551300049 | Test loss: 0.47889193892478943\n",
      "Epoch: 78410 | Loss: 0.4629678428173065 | Test loss: 0.47887811064720154\n",
      "Epoch: 78420 | Loss: 0.462956041097641 | Test loss: 0.47886425256729126\n",
      "Epoch: 78430 | Loss: 0.46294426918029785 | Test loss: 0.4788503646850586\n",
      "Epoch: 78440 | Loss: 0.4629325568675995 | Test loss: 0.4788365364074707\n",
      "Epoch: 78450 | Loss: 0.46292075514793396 | Test loss: 0.4788227081298828\n",
      "Epoch: 78460 | Loss: 0.4629090428352356 | Test loss: 0.47880885004997253\n",
      "Epoch: 78470 | Loss: 0.4628972113132477 | Test loss: 0.47879502177238464\n",
      "Epoch: 78480 | Loss: 0.46288546919822693 | Test loss: 0.478781133890152\n",
      "Epoch: 78490 | Loss: 0.4628736972808838 | Test loss: 0.4787673056125641\n",
      "Epoch: 78500 | Loss: 0.46286192536354065 | Test loss: 0.4787534773349762\n",
      "Epoch: 78510 | Loss: 0.4628501832485199 | Test loss: 0.4787396490573883\n",
      "Epoch: 78520 | Loss: 0.46283847093582153 | Test loss: 0.47872576117515564\n",
      "Epoch: 78530 | Loss: 0.4628266990184784 | Test loss: 0.47871193289756775\n",
      "Epoch: 78540 | Loss: 0.46281489729881287 | Test loss: 0.47869807481765747\n",
      "Epoch: 78550 | Loss: 0.4628031849861145 | Test loss: 0.4786842465400696\n",
      "Epoch: 78560 | Loss: 0.46279141306877136 | Test loss: 0.4786704182624817\n",
      "Epoch: 78570 | Loss: 0.46277961134910583 | Test loss: 0.4786565899848938\n",
      "Epoch: 78580 | Loss: 0.46276789903640747 | Test loss: 0.47864270210266113\n",
      "Epoch: 78590 | Loss: 0.46275612711906433 | Test loss: 0.47862887382507324\n",
      "Epoch: 78600 | Loss: 0.4627443253993988 | Test loss: 0.47861501574516296\n",
      "Epoch: 78610 | Loss: 0.46273261308670044 | Test loss: 0.4786011874675751\n",
      "Epoch: 78620 | Loss: 0.4627208411693573 | Test loss: 0.4785872995853424\n",
      "Epoch: 78630 | Loss: 0.46270909905433655 | Test loss: 0.4785734713077545\n",
      "Epoch: 78640 | Loss: 0.4626973271369934 | Test loss: 0.47855958342552185\n",
      "Epoch: 78650 | Loss: 0.46268558502197266 | Test loss: 0.47854575514793396\n",
      "Epoch: 78660 | Loss: 0.46267375349998474 | Test loss: 0.47853192687034607\n",
      "Epoch: 78670 | Loss: 0.4626620411872864 | Test loss: 0.4785180985927582\n",
      "Epoch: 78680 | Loss: 0.4626502990722656 | Test loss: 0.4785042405128479\n",
      "Epoch: 78690 | Loss: 0.4626385271549225 | Test loss: 0.47849041223526\n",
      "Epoch: 78700 | Loss: 0.46262675523757935 | Test loss: 0.47847652435302734\n",
      "Epoch: 78710 | Loss: 0.4626150131225586 | Test loss: 0.47846269607543945\n",
      "Epoch: 78720 | Loss: 0.46260324120521545 | Test loss: 0.47844886779785156\n",
      "Epoch: 78730 | Loss: 0.4625914692878723 | Test loss: 0.47843503952026367\n",
      "Epoch: 78740 | Loss: 0.46257972717285156 | Test loss: 0.4784211814403534\n",
      "Epoch: 78750 | Loss: 0.4625680148601532 | Test loss: 0.4784073531627655\n",
      "Epoch: 78760 | Loss: 0.4625561833381653 | Test loss: 0.47839346528053284\n",
      "Epoch: 78770 | Loss: 0.46254444122314453 | Test loss: 0.47837963700294495\n",
      "Epoch: 78780 | Loss: 0.46253272891044617 | Test loss: 0.47836580872535706\n",
      "Epoch: 78790 | Loss: 0.46252089738845825 | Test loss: 0.4783519208431244\n",
      "Epoch: 78800 | Loss: 0.4625091552734375 | Test loss: 0.4783380925655365\n",
      "Epoch: 78810 | Loss: 0.46249744296073914 | Test loss: 0.4783242344856262\n",
      "Epoch: 78820 | Loss: 0.4624856412410736 | Test loss: 0.47831040620803833\n",
      "Epoch: 78830 | Loss: 0.46247386932373047 | Test loss: 0.47829657793045044\n",
      "Epoch: 78840 | Loss: 0.4624621570110321 | Test loss: 0.4782826900482178\n",
      "Epoch: 78850 | Loss: 0.4624503552913666 | Test loss: 0.4782688617706299\n",
      "Epoch: 78860 | Loss: 0.46243858337402344 | Test loss: 0.4782550036907196\n",
      "Epoch: 78870 | Loss: 0.4624268710613251 | Test loss: 0.4782411754131317\n",
      "Epoch: 78880 | Loss: 0.4624151289463043 | Test loss: 0.4782273471355438\n",
      "Epoch: 78890 | Loss: 0.4624032974243164 | Test loss: 0.47821345925331116\n",
      "Epoch: 78900 | Loss: 0.46239158511161804 | Test loss: 0.47819963097572327\n",
      "Epoch: 78910 | Loss: 0.4623797833919525 | Test loss: 0.4781857430934906\n",
      "Epoch: 78920 | Loss: 0.46236807107925415 | Test loss: 0.4781719148159027\n",
      "Epoch: 78930 | Loss: 0.462356299161911 | Test loss: 0.4781580865383148\n",
      "Epoch: 78940 | Loss: 0.4623444974422455 | Test loss: 0.47814425826072693\n",
      "Epoch: 78950 | Loss: 0.46233272552490234 | Test loss: 0.47813040018081665\n",
      "Epoch: 78960 | Loss: 0.462321013212204 | Test loss: 0.47811657190322876\n",
      "Epoch: 78970 | Loss: 0.4623092710971832 | Test loss: 0.4781026840209961\n",
      "Epoch: 78980 | Loss: 0.4622974991798401 | Test loss: 0.4780888557434082\n",
      "Epoch: 78990 | Loss: 0.46228572726249695 | Test loss: 0.4780750274658203\n",
      "Epoch: 79000 | Loss: 0.4622739851474762 | Test loss: 0.47806116938591003\n",
      "Epoch: 79010 | Loss: 0.46226221323013306 | Test loss: 0.47804734110832214\n",
      "Epoch: 79020 | Loss: 0.4622504413127899 | Test loss: 0.4780334532260895\n",
      "Epoch: 79030 | Loss: 0.46223869919776917 | Test loss: 0.4780196249485016\n",
      "Epoch: 79040 | Loss: 0.462226927280426 | Test loss: 0.4780057966709137\n",
      "Epoch: 79050 | Loss: 0.4622151553630829 | Test loss: 0.47799190878868103\n",
      "Epoch: 79060 | Loss: 0.46220341324806213 | Test loss: 0.47797808051109314\n",
      "Epoch: 79070 | Loss: 0.462191641330719 | Test loss: 0.47796422243118286\n",
      "Epoch: 79080 | Loss: 0.46217986941337585 | Test loss: 0.47795039415359497\n",
      "Epoch: 79090 | Loss: 0.4621681272983551 | Test loss: 0.4779365658760071\n",
      "Epoch: 79100 | Loss: 0.46215638518333435 | Test loss: 0.4779226779937744\n",
      "Epoch: 79110 | Loss: 0.4621445834636688 | Test loss: 0.4779088497161865\n",
      "Epoch: 79120 | Loss: 0.46213284134864807 | Test loss: 0.47789502143859863\n",
      "Epoch: 79130 | Loss: 0.46212106943130493 | Test loss: 0.47788116335868835\n",
      "Epoch: 79140 | Loss: 0.4621093273162842 | Test loss: 0.47786733508110046\n",
      "Epoch: 79150 | Loss: 0.46209755539894104 | Test loss: 0.4778535068035126\n",
      "Epoch: 79160 | Loss: 0.4620858132839203 | Test loss: 0.4778396189212799\n",
      "Epoch: 79170 | Loss: 0.46207404136657715 | Test loss: 0.477825790643692\n",
      "Epoch: 79180 | Loss: 0.462062269449234 | Test loss: 0.47781190276145935\n",
      "Epoch: 79190 | Loss: 0.46205052733421326 | Test loss: 0.47779807448387146\n",
      "Epoch: 79200 | Loss: 0.4620387554168701 | Test loss: 0.47778424620628357\n",
      "Epoch: 79210 | Loss: 0.46202704310417175 | Test loss: 0.4777704179286957\n",
      "Epoch: 79220 | Loss: 0.4620152413845062 | Test loss: 0.4777565598487854\n",
      "Epoch: 79230 | Loss: 0.4620034694671631 | Test loss: 0.47774267196655273\n",
      "Epoch: 79240 | Loss: 0.4619917571544647 | Test loss: 0.47772884368896484\n",
      "Epoch: 79250 | Loss: 0.4619799554347992 | Test loss: 0.47771501541137695\n",
      "Epoch: 79260 | Loss: 0.46196824312210083 | Test loss: 0.4777011573314667\n",
      "Epoch: 79270 | Loss: 0.4619564116001129 | Test loss: 0.4776873290538788\n",
      "Epoch: 79280 | Loss: 0.46194466948509216 | Test loss: 0.4776734411716461\n",
      "Epoch: 79290 | Loss: 0.461932897567749 | Test loss: 0.4776596128940582\n",
      "Epoch: 79300 | Loss: 0.4619211256504059 | Test loss: 0.47764578461647034\n",
      "Epoch: 79310 | Loss: 0.46190938353538513 | Test loss: 0.47763195633888245\n",
      "Epoch: 79320 | Loss: 0.46189767122268677 | Test loss: 0.4776180684566498\n",
      "Epoch: 79330 | Loss: 0.46188589930534363 | Test loss: 0.4776042401790619\n",
      "Epoch: 79340 | Loss: 0.4618740975856781 | Test loss: 0.4775903820991516\n",
      "Epoch: 79350 | Loss: 0.46186238527297974 | Test loss: 0.4775765538215637\n",
      "Epoch: 79360 | Loss: 0.4618506133556366 | Test loss: 0.47756272554397583\n",
      "Epoch: 79370 | Loss: 0.46183881163597107 | Test loss: 0.47754889726638794\n",
      "Epoch: 79380 | Loss: 0.4618270993232727 | Test loss: 0.4775350093841553\n",
      "Epoch: 79390 | Loss: 0.46181532740592957 | Test loss: 0.4775211811065674\n",
      "Epoch: 79400 | Loss: 0.46180352568626404 | Test loss: 0.4775073230266571\n",
      "Epoch: 79410 | Loss: 0.4617918133735657 | Test loss: 0.4774934947490692\n",
      "Epoch: 79420 | Loss: 0.46178004145622253 | Test loss: 0.47747960686683655\n",
      "Epoch: 79430 | Loss: 0.4617682993412018 | Test loss: 0.47746577858924866\n",
      "Epoch: 79440 | Loss: 0.46175652742385864 | Test loss: 0.477451890707016\n",
      "Epoch: 79450 | Loss: 0.4617447853088379 | Test loss: 0.4774380624294281\n",
      "Epoch: 79460 | Loss: 0.46173295378685 | Test loss: 0.4774242341518402\n",
      "Epoch: 79470 | Loss: 0.4617212414741516 | Test loss: 0.4774104058742523\n",
      "Epoch: 79480 | Loss: 0.46170949935913086 | Test loss: 0.47739654779434204\n",
      "Epoch: 79490 | Loss: 0.4616977274417877 | Test loss: 0.47738271951675415\n",
      "Epoch: 79500 | Loss: 0.4616859555244446 | Test loss: 0.4773688316345215\n",
      "Epoch: 79510 | Loss: 0.46167421340942383 | Test loss: 0.4773550033569336\n",
      "Epoch: 79520 | Loss: 0.4616624414920807 | Test loss: 0.4773411750793457\n",
      "Epoch: 79530 | Loss: 0.46165066957473755 | Test loss: 0.4773273468017578\n",
      "Epoch: 79540 | Loss: 0.4616389274597168 | Test loss: 0.47731348872184753\n",
      "Epoch: 79550 | Loss: 0.46162721514701843 | Test loss: 0.47729966044425964\n",
      "Epoch: 79560 | Loss: 0.4616153836250305 | Test loss: 0.477285772562027\n",
      "Epoch: 79570 | Loss: 0.46160364151000977 | Test loss: 0.4772719442844391\n",
      "Epoch: 79580 | Loss: 0.4615919291973114 | Test loss: 0.4772581160068512\n",
      "Epoch: 79590 | Loss: 0.4615800976753235 | Test loss: 0.47724422812461853\n",
      "Epoch: 79600 | Loss: 0.46156835556030273 | Test loss: 0.47723039984703064\n",
      "Epoch: 79610 | Loss: 0.46155664324760437 | Test loss: 0.47721654176712036\n",
      "Epoch: 79620 | Loss: 0.46154484152793884 | Test loss: 0.47720271348953247\n",
      "Epoch: 79630 | Loss: 0.4615330696105957 | Test loss: 0.4771888852119446\n",
      "Epoch: 79640 | Loss: 0.46152135729789734 | Test loss: 0.4771749973297119\n",
      "Epoch: 79650 | Loss: 0.4615095555782318 | Test loss: 0.477161169052124\n",
      "Epoch: 79660 | Loss: 0.46149778366088867 | Test loss: 0.47714731097221375\n",
      "Epoch: 79670 | Loss: 0.4614860713481903 | Test loss: 0.47713348269462585\n",
      "Epoch: 79680 | Loss: 0.46147432923316956 | Test loss: 0.47711965441703796\n",
      "Epoch: 79690 | Loss: 0.46146249771118164 | Test loss: 0.4771057665348053\n",
      "Epoch: 79700 | Loss: 0.4614507853984833 | Test loss: 0.4770919382572174\n",
      "Epoch: 79710 | Loss: 0.46143898367881775 | Test loss: 0.47707805037498474\n",
      "Epoch: 79720 | Loss: 0.4614272713661194 | Test loss: 0.47706422209739685\n",
      "Epoch: 79730 | Loss: 0.46141549944877625 | Test loss: 0.47705039381980896\n",
      "Epoch: 79740 | Loss: 0.4614036977291107 | Test loss: 0.47703656554222107\n",
      "Epoch: 79750 | Loss: 0.4613919258117676 | Test loss: 0.4770227074623108\n",
      "Epoch: 79760 | Loss: 0.4613802134990692 | Test loss: 0.4770088791847229\n",
      "Epoch: 79770 | Loss: 0.46136847138404846 | Test loss: 0.47699499130249023\n",
      "Epoch: 79780 | Loss: 0.4613566994667053 | Test loss: 0.47698116302490234\n",
      "Epoch: 79790 | Loss: 0.4613449275493622 | Test loss: 0.47696733474731445\n",
      "Epoch: 79800 | Loss: 0.46133318543434143 | Test loss: 0.4769534766674042\n",
      "Epoch: 79810 | Loss: 0.4613214135169983 | Test loss: 0.4769396483898163\n",
      "Epoch: 79820 | Loss: 0.46130964159965515 | Test loss: 0.4769257605075836\n",
      "Epoch: 79830 | Loss: 0.4612978994846344 | Test loss: 0.4769119322299957\n",
      "Epoch: 79840 | Loss: 0.46128612756729126 | Test loss: 0.47689810395240784\n",
      "Epoch: 79850 | Loss: 0.4612743556499481 | Test loss: 0.47688421607017517\n",
      "Epoch: 79860 | Loss: 0.46126261353492737 | Test loss: 0.4768703877925873\n",
      "Epoch: 79870 | Loss: 0.46125084161758423 | Test loss: 0.476856529712677\n",
      "Epoch: 79880 | Loss: 0.4612390697002411 | Test loss: 0.4768427014350891\n",
      "Epoch: 79890 | Loss: 0.46122732758522034 | Test loss: 0.4768288731575012\n",
      "Epoch: 79900 | Loss: 0.4612155854701996 | Test loss: 0.47681498527526855\n",
      "Epoch: 79910 | Loss: 0.46120378375053406 | Test loss: 0.47680115699768066\n",
      "Epoch: 79920 | Loss: 0.4611920416355133 | Test loss: 0.4767873287200928\n",
      "Epoch: 79930 | Loss: 0.46118026971817017 | Test loss: 0.4767734706401825\n",
      "Epoch: 79940 | Loss: 0.4611685276031494 | Test loss: 0.4767596423625946\n",
      "Epoch: 79950 | Loss: 0.4611567556858063 | Test loss: 0.4767458140850067\n",
      "Epoch: 79960 | Loss: 0.4611450135707855 | Test loss: 0.47673192620277405\n",
      "Epoch: 79970 | Loss: 0.4611332416534424 | Test loss: 0.47671809792518616\n",
      "Epoch: 79980 | Loss: 0.46112146973609924 | Test loss: 0.4767042100429535\n",
      "Epoch: 79990 | Loss: 0.4611097276210785 | Test loss: 0.4766903817653656\n",
      "Epoch: 80000 | Loss: 0.46109795570373535 | Test loss: 0.4766765534877777\n",
      "Epoch: 80010 | Loss: 0.461086243391037 | Test loss: 0.4766627252101898\n",
      "Epoch: 80020 | Loss: 0.46107444167137146 | Test loss: 0.47664886713027954\n",
      "Epoch: 80030 | Loss: 0.4610626697540283 | Test loss: 0.4766349792480469\n",
      "Epoch: 80040 | Loss: 0.46105095744132996 | Test loss: 0.476621150970459\n",
      "Epoch: 80050 | Loss: 0.46103915572166443 | Test loss: 0.4766073226928711\n",
      "Epoch: 80060 | Loss: 0.46102744340896606 | Test loss: 0.4765934646129608\n",
      "Epoch: 80070 | Loss: 0.46101561188697815 | Test loss: 0.4765796363353729\n",
      "Epoch: 80080 | Loss: 0.4610038697719574 | Test loss: 0.47656574845314026\n",
      "Epoch: 80090 | Loss: 0.46099209785461426 | Test loss: 0.47655192017555237\n",
      "Epoch: 80100 | Loss: 0.4609803259372711 | Test loss: 0.4765380918979645\n",
      "Epoch: 80110 | Loss: 0.46096858382225037 | Test loss: 0.4765242636203766\n",
      "Epoch: 80120 | Loss: 0.460956871509552 | Test loss: 0.4765103757381439\n",
      "Epoch: 80130 | Loss: 0.46094509959220886 | Test loss: 0.47649654746055603\n",
      "Epoch: 80140 | Loss: 0.46093329787254333 | Test loss: 0.47648268938064575\n",
      "Epoch: 80150 | Loss: 0.46092158555984497 | Test loss: 0.47646886110305786\n",
      "Epoch: 80160 | Loss: 0.46090981364250183 | Test loss: 0.47645503282546997\n",
      "Epoch: 80170 | Loss: 0.4608980119228363 | Test loss: 0.4764412045478821\n",
      "Epoch: 80180 | Loss: 0.46088629961013794 | Test loss: 0.4764273166656494\n",
      "Epoch: 80190 | Loss: 0.4608745276927948 | Test loss: 0.4764134883880615\n",
      "Epoch: 80200 | Loss: 0.4608627259731293 | Test loss: 0.47639963030815125\n",
      "Epoch: 80210 | Loss: 0.4608510136604309 | Test loss: 0.47638580203056335\n",
      "Epoch: 80220 | Loss: 0.46083924174308777 | Test loss: 0.4763719141483307\n",
      "Epoch: 80230 | Loss: 0.460827499628067 | Test loss: 0.4763580858707428\n",
      "Epoch: 80240 | Loss: 0.4608157277107239 | Test loss: 0.47634419798851013\n",
      "Epoch: 80250 | Loss: 0.4608039855957031 | Test loss: 0.47633036971092224\n",
      "Epoch: 80260 | Loss: 0.4607921540737152 | Test loss: 0.47631654143333435\n",
      "Epoch: 80270 | Loss: 0.46078044176101685 | Test loss: 0.47630271315574646\n",
      "Epoch: 80280 | Loss: 0.4607686996459961 | Test loss: 0.4762888550758362\n",
      "Epoch: 80290 | Loss: 0.46075692772865295 | Test loss: 0.4762750267982483\n",
      "Epoch: 80300 | Loss: 0.4607451558113098 | Test loss: 0.4762611389160156\n",
      "Epoch: 80310 | Loss: 0.46073341369628906 | Test loss: 0.47624731063842773\n",
      "Epoch: 80320 | Loss: 0.4607216417789459 | Test loss: 0.47623348236083984\n",
      "Epoch: 80330 | Loss: 0.4607098698616028 | Test loss: 0.47621965408325195\n",
      "Epoch: 80340 | Loss: 0.46069812774658203 | Test loss: 0.4762057960033417\n",
      "Epoch: 80350 | Loss: 0.46068641543388367 | Test loss: 0.4761919677257538\n",
      "Epoch: 80360 | Loss: 0.46067458391189575 | Test loss: 0.4761780798435211\n",
      "Epoch: 80370 | Loss: 0.460662841796875 | Test loss: 0.4761642515659332\n",
      "Epoch: 80380 | Loss: 0.46065112948417664 | Test loss: 0.47615042328834534\n",
      "Epoch: 80390 | Loss: 0.4606392979621887 | Test loss: 0.47613653540611267\n",
      "Epoch: 80400 | Loss: 0.46062755584716797 | Test loss: 0.4761227071285248\n",
      "Epoch: 80410 | Loss: 0.4606158435344696 | Test loss: 0.4761088490486145\n",
      "Epoch: 80420 | Loss: 0.4606040418148041 | Test loss: 0.4760950207710266\n",
      "Epoch: 80430 | Loss: 0.46059226989746094 | Test loss: 0.4760811924934387\n",
      "Epoch: 80440 | Loss: 0.4605805575847626 | Test loss: 0.47606730461120605\n",
      "Epoch: 80450 | Loss: 0.46056875586509705 | Test loss: 0.47605347633361816\n",
      "Epoch: 80460 | Loss: 0.4605569839477539 | Test loss: 0.4760396182537079\n",
      "Epoch: 80470 | Loss: 0.46054527163505554 | Test loss: 0.47602578997612\n",
      "Epoch: 80480 | Loss: 0.4605335295200348 | Test loss: 0.4760119616985321\n",
      "Epoch: 80490 | Loss: 0.4605216979980469 | Test loss: 0.47599807381629944\n",
      "Epoch: 80500 | Loss: 0.4605099856853485 | Test loss: 0.47598424553871155\n",
      "Epoch: 80510 | Loss: 0.460498183965683 | Test loss: 0.4759703576564789\n",
      "Epoch: 80520 | Loss: 0.4604864716529846 | Test loss: 0.475956529378891\n",
      "Epoch: 80530 | Loss: 0.4604746997356415 | Test loss: 0.4759427011013031\n",
      "Epoch: 80540 | Loss: 0.46046289801597595 | Test loss: 0.4759288728237152\n",
      "Epoch: 80550 | Loss: 0.4604511260986328 | Test loss: 0.47591501474380493\n",
      "Epoch: 80560 | Loss: 0.46043941378593445 | Test loss: 0.47590118646621704\n",
      "Epoch: 80570 | Loss: 0.4604276716709137 | Test loss: 0.4758872985839844\n",
      "Epoch: 80580 | Loss: 0.46041589975357056 | Test loss: 0.4758734703063965\n",
      "Epoch: 80590 | Loss: 0.4604041278362274 | Test loss: 0.4758596420288086\n",
      "Epoch: 80600 | Loss: 0.46039238572120667 | Test loss: 0.4758457839488983\n",
      "Epoch: 80610 | Loss: 0.4603806138038635 | Test loss: 0.4758319556713104\n",
      "Epoch: 80620 | Loss: 0.4603688418865204 | Test loss: 0.47581806778907776\n",
      "Epoch: 80630 | Loss: 0.46035709977149963 | Test loss: 0.47580423951148987\n",
      "Epoch: 80640 | Loss: 0.4603453278541565 | Test loss: 0.475790411233902\n",
      "Epoch: 80650 | Loss: 0.46033355593681335 | Test loss: 0.4757765233516693\n",
      "Epoch: 80660 | Loss: 0.4603218138217926 | Test loss: 0.4757626950740814\n",
      "Epoch: 80670 | Loss: 0.46031004190444946 | Test loss: 0.47574883699417114\n",
      "Epoch: 80680 | Loss: 0.4602982699871063 | Test loss: 0.47573500871658325\n",
      "Epoch: 80690 | Loss: 0.46028652787208557 | Test loss: 0.47572118043899536\n",
      "Epoch: 80700 | Loss: 0.4602747857570648 | Test loss: 0.4757072925567627\n",
      "Epoch: 80710 | Loss: 0.4602629840373993 | Test loss: 0.4756934642791748\n",
      "Epoch: 80720 | Loss: 0.46025124192237854 | Test loss: 0.4756796360015869\n",
      "Epoch: 80730 | Loss: 0.4602394700050354 | Test loss: 0.47566577792167664\n",
      "Epoch: 80740 | Loss: 0.46022772789001465 | Test loss: 0.47565194964408875\n",
      "Epoch: 80750 | Loss: 0.4602159559726715 | Test loss: 0.47563812136650085\n",
      "Epoch: 80760 | Loss: 0.46020421385765076 | Test loss: 0.4756242334842682\n",
      "Epoch: 80770 | Loss: 0.4601924419403076 | Test loss: 0.4756104052066803\n",
      "Epoch: 80780 | Loss: 0.4601806700229645 | Test loss: 0.47559651732444763\n",
      "Epoch: 80790 | Loss: 0.4601689279079437 | Test loss: 0.47558268904685974\n",
      "Epoch: 80800 | Loss: 0.4601571559906006 | Test loss: 0.47556886076927185\n",
      "Epoch: 80810 | Loss: 0.4601454436779022 | Test loss: 0.47555503249168396\n",
      "Epoch: 80820 | Loss: 0.4601336419582367 | Test loss: 0.4755411744117737\n",
      "Epoch: 80830 | Loss: 0.46012187004089355 | Test loss: 0.475527286529541\n",
      "Epoch: 80840 | Loss: 0.4601101577281952 | Test loss: 0.4755134582519531\n",
      "Epoch: 80850 | Loss: 0.46009835600852966 | Test loss: 0.47549962997436523\n",
      "Epoch: 80860 | Loss: 0.4600866436958313 | Test loss: 0.47548577189445496\n",
      "Epoch: 80870 | Loss: 0.4600748121738434 | Test loss: 0.47547194361686707\n",
      "Epoch: 80880 | Loss: 0.46006307005882263 | Test loss: 0.4754580557346344\n",
      "Epoch: 80890 | Loss: 0.4600512981414795 | Test loss: 0.4754442274570465\n",
      "Epoch: 80900 | Loss: 0.46003952622413635 | Test loss: 0.4754303991794586\n",
      "Epoch: 80910 | Loss: 0.4600277841091156 | Test loss: 0.4754165709018707\n",
      "Epoch: 80920 | Loss: 0.46001607179641724 | Test loss: 0.47540268301963806\n",
      "Epoch: 80930 | Loss: 0.4600042998790741 | Test loss: 0.47538885474205017\n",
      "Epoch: 80940 | Loss: 0.45999249815940857 | Test loss: 0.4753749966621399\n",
      "Epoch: 80950 | Loss: 0.4599807858467102 | Test loss: 0.475361168384552\n",
      "Epoch: 80960 | Loss: 0.45996901392936707 | Test loss: 0.4753473401069641\n",
      "Epoch: 80970 | Loss: 0.45995721220970154 | Test loss: 0.4753335118293762\n",
      "Epoch: 80980 | Loss: 0.4599454998970032 | Test loss: 0.47531962394714355\n",
      "Epoch: 80990 | Loss: 0.45993372797966003 | Test loss: 0.47530579566955566\n",
      "Epoch: 81000 | Loss: 0.4599219262599945 | Test loss: 0.4752919375896454\n",
      "Epoch: 81010 | Loss: 0.45991021394729614 | Test loss: 0.4752781093120575\n",
      "Epoch: 81020 | Loss: 0.459898442029953 | Test loss: 0.47526422142982483\n",
      "Epoch: 81030 | Loss: 0.45988669991493225 | Test loss: 0.47525039315223694\n",
      "Epoch: 81040 | Loss: 0.4598749279975891 | Test loss: 0.4752365052700043\n",
      "Epoch: 81050 | Loss: 0.45986318588256836 | Test loss: 0.4752226769924164\n",
      "Epoch: 81060 | Loss: 0.45985135436058044 | Test loss: 0.4752088487148285\n",
      "Epoch: 81070 | Loss: 0.4598396420478821 | Test loss: 0.4751950204372406\n",
      "Epoch: 81080 | Loss: 0.45982789993286133 | Test loss: 0.4751811623573303\n",
      "Epoch: 81090 | Loss: 0.4598161280155182 | Test loss: 0.47516733407974243\n",
      "Epoch: 81100 | Loss: 0.45980435609817505 | Test loss: 0.47515344619750977\n",
      "Epoch: 81110 | Loss: 0.4597926139831543 | Test loss: 0.4751396179199219\n",
      "Epoch: 81120 | Loss: 0.45978084206581116 | Test loss: 0.475125789642334\n",
      "Epoch: 81130 | Loss: 0.459769070148468 | Test loss: 0.4751119613647461\n",
      "Epoch: 81140 | Loss: 0.45975732803344727 | Test loss: 0.4750981032848358\n",
      "Epoch: 81150 | Loss: 0.4597456157207489 | Test loss: 0.4750842750072479\n",
      "Epoch: 81160 | Loss: 0.459733784198761 | Test loss: 0.47507038712501526\n",
      "Epoch: 81170 | Loss: 0.45972204208374023 | Test loss: 0.47505655884742737\n",
      "Epoch: 81180 | Loss: 0.45971032977104187 | Test loss: 0.4750427305698395\n",
      "Epoch: 81190 | Loss: 0.45969849824905396 | Test loss: 0.4750288426876068\n",
      "Epoch: 81200 | Loss: 0.4596867561340332 | Test loss: 0.4750150144100189\n",
      "Epoch: 81210 | Loss: 0.45967504382133484 | Test loss: 0.47500115633010864\n",
      "Epoch: 81220 | Loss: 0.4596632421016693 | Test loss: 0.47498732805252075\n",
      "Epoch: 81230 | Loss: 0.45965147018432617 | Test loss: 0.47497349977493286\n",
      "Epoch: 81240 | Loss: 0.4596397578716278 | Test loss: 0.4749596118927002\n",
      "Epoch: 81250 | Loss: 0.4596279561519623 | Test loss: 0.4749457836151123\n",
      "Epoch: 81260 | Loss: 0.45961618423461914 | Test loss: 0.474931925535202\n",
      "Epoch: 81270 | Loss: 0.4596044719219208 | Test loss: 0.47491809725761414\n",
      "Epoch: 81280 | Loss: 0.4595927298069 | Test loss: 0.47490426898002625\n",
      "Epoch: 81290 | Loss: 0.4595808982849121 | Test loss: 0.4748903810977936\n",
      "Epoch: 81300 | Loss: 0.45956918597221375 | Test loss: 0.4748765528202057\n",
      "Epoch: 81310 | Loss: 0.4595573842525482 | Test loss: 0.474862664937973\n",
      "Epoch: 81320 | Loss: 0.45954567193984985 | Test loss: 0.47484883666038513\n",
      "Epoch: 81330 | Loss: 0.4595339000225067 | Test loss: 0.47483500838279724\n",
      "Epoch: 81340 | Loss: 0.4595220983028412 | Test loss: 0.47482118010520935\n",
      "Epoch: 81350 | Loss: 0.45951032638549805 | Test loss: 0.4748073220252991\n",
      "Epoch: 81360 | Loss: 0.4594986140727997 | Test loss: 0.4747934937477112\n",
      "Epoch: 81370 | Loss: 0.45948687195777893 | Test loss: 0.4747796058654785\n",
      "Epoch: 81380 | Loss: 0.4594751000404358 | Test loss: 0.4747657775878906\n",
      "Epoch: 81390 | Loss: 0.45946332812309265 | Test loss: 0.47475194931030273\n",
      "Epoch: 81400 | Loss: 0.4594515860080719 | Test loss: 0.47473809123039246\n",
      "Epoch: 81410 | Loss: 0.45943981409072876 | Test loss: 0.47472426295280457\n",
      "Epoch: 81420 | Loss: 0.4594280421733856 | Test loss: 0.4747103750705719\n",
      "Epoch: 81430 | Loss: 0.45941630005836487 | Test loss: 0.474696546792984\n",
      "Epoch: 81440 | Loss: 0.45940452814102173 | Test loss: 0.4746827185153961\n",
      "Epoch: 81450 | Loss: 0.4593927562236786 | Test loss: 0.47466883063316345\n",
      "Epoch: 81460 | Loss: 0.45938101410865784 | Test loss: 0.47465500235557556\n",
      "Epoch: 81470 | Loss: 0.4593692421913147 | Test loss: 0.4746411442756653\n",
      "Epoch: 81480 | Loss: 0.45935747027397156 | Test loss: 0.4746273159980774\n",
      "Epoch: 81490 | Loss: 0.4593457281589508 | Test loss: 0.4746134877204895\n",
      "Epoch: 81500 | Loss: 0.45933398604393005 | Test loss: 0.47459959983825684\n",
      "Epoch: 81510 | Loss: 0.4593221843242645 | Test loss: 0.47458577156066895\n",
      "Epoch: 81520 | Loss: 0.4593104422092438 | Test loss: 0.47457194328308105\n",
      "Epoch: 81530 | Loss: 0.45929867029190063 | Test loss: 0.4745580852031708\n",
      "Epoch: 81540 | Loss: 0.4592869281768799 | Test loss: 0.4745442569255829\n",
      "Epoch: 81550 | Loss: 0.45927515625953674 | Test loss: 0.474530428647995\n",
      "Epoch: 81560 | Loss: 0.459263414144516 | Test loss: 0.47451654076576233\n",
      "Epoch: 81570 | Loss: 0.45925164222717285 | Test loss: 0.47450271248817444\n",
      "Epoch: 81580 | Loss: 0.4592398703098297 | Test loss: 0.4744888246059418\n",
      "Epoch: 81590 | Loss: 0.45922812819480896 | Test loss: 0.4744749963283539\n",
      "Epoch: 81600 | Loss: 0.4592163562774658 | Test loss: 0.474461168050766\n",
      "Epoch: 81610 | Loss: 0.45920464396476746 | Test loss: 0.4744473397731781\n",
      "Epoch: 81620 | Loss: 0.45919284224510193 | Test loss: 0.4744334816932678\n",
      "Epoch: 81630 | Loss: 0.4591810703277588 | Test loss: 0.47441959381103516\n",
      "Epoch: 81640 | Loss: 0.4591693580150604 | Test loss: 0.47440576553344727\n",
      "Epoch: 81650 | Loss: 0.4591575562953949 | Test loss: 0.4743919372558594\n",
      "Epoch: 81660 | Loss: 0.45914584398269653 | Test loss: 0.4743780791759491\n",
      "Epoch: 81670 | Loss: 0.4591340124607086 | Test loss: 0.4743642508983612\n",
      "Epoch: 81680 | Loss: 0.45912227034568787 | Test loss: 0.47435036301612854\n",
      "Epoch: 81690 | Loss: 0.4591104984283447 | Test loss: 0.47433653473854065\n",
      "Epoch: 81700 | Loss: 0.4590987265110016 | Test loss: 0.47432270646095276\n",
      "Epoch: 81710 | Loss: 0.45908698439598083 | Test loss: 0.47430887818336487\n",
      "Epoch: 81720 | Loss: 0.45907527208328247 | Test loss: 0.4742949903011322\n",
      "Epoch: 81730 | Loss: 0.45906350016593933 | Test loss: 0.4742811620235443\n",
      "Epoch: 81740 | Loss: 0.4590516984462738 | Test loss: 0.47426730394363403\n",
      "Epoch: 81750 | Loss: 0.45903998613357544 | Test loss: 0.47425347566604614\n",
      "Epoch: 81760 | Loss: 0.4590282142162323 | Test loss: 0.47423964738845825\n",
      "Epoch: 81770 | Loss: 0.4590164124965668 | Test loss: 0.47422581911087036\n",
      "Epoch: 81780 | Loss: 0.4590047001838684 | Test loss: 0.4742119312286377\n",
      "Epoch: 81790 | Loss: 0.45899292826652527 | Test loss: 0.4741981029510498\n",
      "Epoch: 81800 | Loss: 0.45898112654685974 | Test loss: 0.4741842448711395\n",
      "Epoch: 81810 | Loss: 0.4589694142341614 | Test loss: 0.47417041659355164\n",
      "Epoch: 81820 | Loss: 0.45895764231681824 | Test loss: 0.47415652871131897\n",
      "Epoch: 81830 | Loss: 0.4589459002017975 | Test loss: 0.4741427004337311\n",
      "Epoch: 81840 | Loss: 0.45893412828445435 | Test loss: 0.4741288125514984\n",
      "Epoch: 81850 | Loss: 0.4589223861694336 | Test loss: 0.4741149842739105\n",
      "Epoch: 81860 | Loss: 0.4589105546474457 | Test loss: 0.47410115599632263\n",
      "Epoch: 81870 | Loss: 0.4588988423347473 | Test loss: 0.47408732771873474\n",
      "Epoch: 81880 | Loss: 0.45888710021972656 | Test loss: 0.47407346963882446\n",
      "Epoch: 81890 | Loss: 0.4588753283023834 | Test loss: 0.4740596413612366\n",
      "Epoch: 81900 | Loss: 0.4588635563850403 | Test loss: 0.4740457534790039\n",
      "Epoch: 81910 | Loss: 0.45885181427001953 | Test loss: 0.474031925201416\n",
      "Epoch: 81920 | Loss: 0.4588400423526764 | Test loss: 0.4740180969238281\n",
      "Epoch: 81930 | Loss: 0.45882827043533325 | Test loss: 0.47400426864624023\n",
      "Epoch: 81940 | Loss: 0.4588165283203125 | Test loss: 0.47399041056632996\n",
      "Epoch: 81950 | Loss: 0.45880481600761414 | Test loss: 0.47397658228874207\n",
      "Epoch: 81960 | Loss: 0.4587929844856262 | Test loss: 0.4739626944065094\n",
      "Epoch: 81970 | Loss: 0.45878124237060547 | Test loss: 0.4739488661289215\n",
      "Epoch: 81980 | Loss: 0.4587695300579071 | Test loss: 0.4739350378513336\n",
      "Epoch: 81990 | Loss: 0.4587576985359192 | Test loss: 0.47392114996910095\n",
      "Epoch: 82000 | Loss: 0.45874595642089844 | Test loss: 0.47390732169151306\n",
      "Epoch: 82010 | Loss: 0.4587342441082001 | Test loss: 0.4738934636116028\n",
      "Epoch: 82020 | Loss: 0.45872244238853455 | Test loss: 0.4738796353340149\n",
      "Epoch: 82030 | Loss: 0.4587106704711914 | Test loss: 0.473865807056427\n",
      "Epoch: 82040 | Loss: 0.45869895815849304 | Test loss: 0.47385191917419434\n",
      "Epoch: 82050 | Loss: 0.4586871564388275 | Test loss: 0.47383809089660645\n",
      "Epoch: 82060 | Loss: 0.4586753845214844 | Test loss: 0.47382423281669617\n",
      "Epoch: 82070 | Loss: 0.458663672208786 | Test loss: 0.4738104045391083\n",
      "Epoch: 82080 | Loss: 0.45865193009376526 | Test loss: 0.4737965762615204\n",
      "Epoch: 82090 | Loss: 0.45864009857177734 | Test loss: 0.4737826883792877\n",
      "Epoch: 82100 | Loss: 0.458628386259079 | Test loss: 0.47376886010169983\n",
      "Epoch: 82110 | Loss: 0.45861658453941345 | Test loss: 0.47375497221946716\n",
      "Epoch: 82120 | Loss: 0.4586048722267151 | Test loss: 0.4737411439418793\n",
      "Epoch: 82130 | Loss: 0.45859310030937195 | Test loss: 0.4737273156642914\n",
      "Epoch: 82140 | Loss: 0.4585812985897064 | Test loss: 0.4737134873867035\n",
      "Epoch: 82150 | Loss: 0.4585695266723633 | Test loss: 0.4736996293067932\n",
      "Epoch: 82160 | Loss: 0.4585578143596649 | Test loss: 0.4736858010292053\n",
      "Epoch: 82170 | Loss: 0.45854607224464417 | Test loss: 0.47367191314697266\n",
      "Epoch: 82180 | Loss: 0.458534300327301 | Test loss: 0.47365808486938477\n",
      "Epoch: 82190 | Loss: 0.4585225284099579 | Test loss: 0.4736442565917969\n",
      "Epoch: 82200 | Loss: 0.45851078629493713 | Test loss: 0.4736303985118866\n",
      "Epoch: 82210 | Loss: 0.458499014377594 | Test loss: 0.4736165702342987\n",
      "Epoch: 82220 | Loss: 0.45848724246025085 | Test loss: 0.47360268235206604\n",
      "Epoch: 82230 | Loss: 0.4584755003452301 | Test loss: 0.47358885407447815\n",
      "Epoch: 82240 | Loss: 0.45846372842788696 | Test loss: 0.47357502579689026\n",
      "Epoch: 82250 | Loss: 0.4584519565105438 | Test loss: 0.4735611379146576\n",
      "Epoch: 82260 | Loss: 0.45844021439552307 | Test loss: 0.4735473096370697\n",
      "Epoch: 82270 | Loss: 0.45842844247817993 | Test loss: 0.4735334515571594\n",
      "Epoch: 82280 | Loss: 0.4584166705608368 | Test loss: 0.47351962327957153\n",
      "Epoch: 82290 | Loss: 0.45840492844581604 | Test loss: 0.47350579500198364\n",
      "Epoch: 82300 | Loss: 0.4583931863307953 | Test loss: 0.473491907119751\n",
      "Epoch: 82310 | Loss: 0.45838138461112976 | Test loss: 0.4734780788421631\n",
      "Epoch: 82320 | Loss: 0.458369642496109 | Test loss: 0.4734642505645752\n",
      "Epoch: 82330 | Loss: 0.45835787057876587 | Test loss: 0.4734503924846649\n",
      "Epoch: 82340 | Loss: 0.4583461284637451 | Test loss: 0.473436564207077\n",
      "Epoch: 82350 | Loss: 0.458334356546402 | Test loss: 0.47342273592948914\n",
      "Epoch: 82360 | Loss: 0.4583226144313812 | Test loss: 0.47340884804725647\n",
      "Epoch: 82370 | Loss: 0.4583108425140381 | Test loss: 0.4733950197696686\n",
      "Epoch: 82380 | Loss: 0.45829907059669495 | Test loss: 0.4733811318874359\n",
      "Epoch: 82390 | Loss: 0.4582873284816742 | Test loss: 0.473367303609848\n",
      "Epoch: 82400 | Loss: 0.45827555656433105 | Test loss: 0.47335347533226013\n",
      "Epoch: 82410 | Loss: 0.4582638442516327 | Test loss: 0.47333964705467224\n",
      "Epoch: 82420 | Loss: 0.45825204253196716 | Test loss: 0.47332578897476196\n",
      "Epoch: 82430 | Loss: 0.458240270614624 | Test loss: 0.4733119010925293\n",
      "Epoch: 82440 | Loss: 0.45822855830192566 | Test loss: 0.4732980728149414\n",
      "Epoch: 82450 | Loss: 0.45821675658226013 | Test loss: 0.4732842445373535\n",
      "Epoch: 82460 | Loss: 0.45820504426956177 | Test loss: 0.47327038645744324\n",
      "Epoch: 82470 | Loss: 0.45819321274757385 | Test loss: 0.47325655817985535\n",
      "Epoch: 82480 | Loss: 0.4581814706325531 | Test loss: 0.4732426702976227\n",
      "Epoch: 82490 | Loss: 0.45816969871520996 | Test loss: 0.4732288420200348\n",
      "Epoch: 82500 | Loss: 0.4581579267978668 | Test loss: 0.4732150137424469\n",
      "Epoch: 82510 | Loss: 0.45814618468284607 | Test loss: 0.473201185464859\n",
      "Epoch: 82520 | Loss: 0.4581344723701477 | Test loss: 0.47318729758262634\n",
      "Epoch: 82530 | Loss: 0.45812270045280457 | Test loss: 0.47317346930503845\n",
      "Epoch: 82540 | Loss: 0.45811089873313904 | Test loss: 0.4731596112251282\n",
      "Epoch: 82550 | Loss: 0.4580991864204407 | Test loss: 0.4731457829475403\n",
      "Epoch: 82560 | Loss: 0.45808741450309753 | Test loss: 0.4731319546699524\n",
      "Epoch: 82570 | Loss: 0.458075612783432 | Test loss: 0.4731181263923645\n",
      "Epoch: 82580 | Loss: 0.45806390047073364 | Test loss: 0.47310423851013184\n",
      "Epoch: 82590 | Loss: 0.4580521285533905 | Test loss: 0.47309041023254395\n",
      "Epoch: 82600 | Loss: 0.458040326833725 | Test loss: 0.47307655215263367\n",
      "Epoch: 82610 | Loss: 0.4580286145210266 | Test loss: 0.4730627238750458\n",
      "Epoch: 82620 | Loss: 0.45801684260368347 | Test loss: 0.4730488359928131\n",
      "Epoch: 82630 | Loss: 0.4580051004886627 | Test loss: 0.4730350077152252\n",
      "Epoch: 82640 | Loss: 0.4579933285713196 | Test loss: 0.47302111983299255\n",
      "Epoch: 82650 | Loss: 0.45798158645629883 | Test loss: 0.47300729155540466\n",
      "Epoch: 82660 | Loss: 0.4579697549343109 | Test loss: 0.4729934632778168\n",
      "Epoch: 82670 | Loss: 0.45795804262161255 | Test loss: 0.4729796350002289\n",
      "Epoch: 82680 | Loss: 0.4579463005065918 | Test loss: 0.4729657769203186\n",
      "Epoch: 82690 | Loss: 0.45793452858924866 | Test loss: 0.4729519486427307\n",
      "Epoch: 82700 | Loss: 0.4579227566719055 | Test loss: 0.47293806076049805\n",
      "Epoch: 82710 | Loss: 0.45791101455688477 | Test loss: 0.47292423248291016\n",
      "Epoch: 82720 | Loss: 0.4578992426395416 | Test loss: 0.47291040420532227\n",
      "Epoch: 82730 | Loss: 0.4578874707221985 | Test loss: 0.4728965759277344\n",
      "Epoch: 82740 | Loss: 0.45787572860717773 | Test loss: 0.4728827178478241\n",
      "Epoch: 82750 | Loss: 0.45786401629447937 | Test loss: 0.4728688895702362\n",
      "Epoch: 82760 | Loss: 0.45785218477249146 | Test loss: 0.47285500168800354\n",
      "Epoch: 82770 | Loss: 0.4578404426574707 | Test loss: 0.47284117341041565\n",
      "Epoch: 82780 | Loss: 0.45782873034477234 | Test loss: 0.47282734513282776\n",
      "Epoch: 82790 | Loss: 0.4578168988227844 | Test loss: 0.4728134572505951\n",
      "Epoch: 82800 | Loss: 0.45780515670776367 | Test loss: 0.4727996289730072\n",
      "Epoch: 82810 | Loss: 0.4577934443950653 | Test loss: 0.4727857708930969\n",
      "Epoch: 82820 | Loss: 0.4577816426753998 | Test loss: 0.47277194261550903\n",
      "Epoch: 82830 | Loss: 0.45776987075805664 | Test loss: 0.47275811433792114\n",
      "Epoch: 82840 | Loss: 0.4577581584453583 | Test loss: 0.4727442264556885\n",
      "Epoch: 82850 | Loss: 0.45774635672569275 | Test loss: 0.4727303981781006\n",
      "Epoch: 82860 | Loss: 0.4577345848083496 | Test loss: 0.4727165400981903\n",
      "Epoch: 82870 | Loss: 0.45772287249565125 | Test loss: 0.4727027118206024\n",
      "Epoch: 82880 | Loss: 0.4577111303806305 | Test loss: 0.4726888835430145\n",
      "Epoch: 82890 | Loss: 0.4576992988586426 | Test loss: 0.47267499566078186\n",
      "Epoch: 82900 | Loss: 0.4576875865459442 | Test loss: 0.47266116738319397\n",
      "Epoch: 82910 | Loss: 0.4576757848262787 | Test loss: 0.4726472795009613\n",
      "Epoch: 82920 | Loss: 0.4576640725135803 | Test loss: 0.4726334512233734\n",
      "Epoch: 82930 | Loss: 0.4576523005962372 | Test loss: 0.4726196229457855\n",
      "Epoch: 82940 | Loss: 0.45764049887657166 | Test loss: 0.47260579466819763\n",
      "Epoch: 82950 | Loss: 0.4576287269592285 | Test loss: 0.47259193658828735\n",
      "Epoch: 82960 | Loss: 0.45761701464653015 | Test loss: 0.47257810831069946\n",
      "Epoch: 82970 | Loss: 0.4576052725315094 | Test loss: 0.4725642204284668\n",
      "Epoch: 82980 | Loss: 0.45759350061416626 | Test loss: 0.4725503921508789\n",
      "Epoch: 82990 | Loss: 0.4575817286968231 | Test loss: 0.472536563873291\n",
      "Epoch: 83000 | Loss: 0.45756998658180237 | Test loss: 0.47252270579338074\n",
      "Epoch: 83010 | Loss: 0.45755821466445923 | Test loss: 0.47250887751579285\n",
      "Epoch: 83020 | Loss: 0.4575464427471161 | Test loss: 0.4724949896335602\n",
      "Epoch: 83030 | Loss: 0.45753470063209534 | Test loss: 0.4724811613559723\n",
      "Epoch: 83040 | Loss: 0.4575229287147522 | Test loss: 0.4724673330783844\n",
      "Epoch: 83050 | Loss: 0.45751115679740906 | Test loss: 0.47245344519615173\n",
      "Epoch: 83060 | Loss: 0.4574994146823883 | Test loss: 0.47243961691856384\n",
      "Epoch: 83070 | Loss: 0.45748764276504517 | Test loss: 0.47242575883865356\n",
      "Epoch: 83080 | Loss: 0.457475870847702 | Test loss: 0.4724119305610657\n",
      "Epoch: 83090 | Loss: 0.4574641287326813 | Test loss: 0.4723981022834778\n",
      "Epoch: 83100 | Loss: 0.4574523866176605 | Test loss: 0.4723842144012451\n",
      "Epoch: 83110 | Loss: 0.457440584897995 | Test loss: 0.4723703861236572\n",
      "Epoch: 83120 | Loss: 0.45742884278297424 | Test loss: 0.47235655784606934\n",
      "Epoch: 83130 | Loss: 0.4574170708656311 | Test loss: 0.47234269976615906\n",
      "Epoch: 83140 | Loss: 0.45740532875061035 | Test loss: 0.47232887148857117\n",
      "Epoch: 83150 | Loss: 0.4573935568332672 | Test loss: 0.4723150432109833\n",
      "Epoch: 83160 | Loss: 0.45738181471824646 | Test loss: 0.4723011553287506\n",
      "Epoch: 83170 | Loss: 0.4573700428009033 | Test loss: 0.4722873270511627\n",
      "Epoch: 83180 | Loss: 0.4573582708835602 | Test loss: 0.47227343916893005\n",
      "Epoch: 83190 | Loss: 0.45734652876853943 | Test loss: 0.47225961089134216\n",
      "Epoch: 83200 | Loss: 0.4573347568511963 | Test loss: 0.4722457826137543\n",
      "Epoch: 83210 | Loss: 0.4573230445384979 | Test loss: 0.4722319543361664\n",
      "Epoch: 83220 | Loss: 0.4573112428188324 | Test loss: 0.4722180962562561\n",
      "Epoch: 83230 | Loss: 0.45729947090148926 | Test loss: 0.47220420837402344\n",
      "Epoch: 83240 | Loss: 0.4572877585887909 | Test loss: 0.47219038009643555\n",
      "Epoch: 83250 | Loss: 0.45727595686912537 | Test loss: 0.47217655181884766\n",
      "Epoch: 83260 | Loss: 0.457264244556427 | Test loss: 0.4721626937389374\n",
      "Epoch: 83270 | Loss: 0.4572524130344391 | Test loss: 0.4721488654613495\n",
      "Epoch: 83280 | Loss: 0.45724067091941833 | Test loss: 0.4721349775791168\n",
      "Epoch: 83290 | Loss: 0.4572288990020752 | Test loss: 0.47212114930152893\n",
      "Epoch: 83300 | Loss: 0.45721712708473206 | Test loss: 0.47210732102394104\n",
      "Epoch: 83310 | Loss: 0.4572053849697113 | Test loss: 0.47209349274635315\n",
      "Epoch: 83320 | Loss: 0.45719367265701294 | Test loss: 0.4720796048641205\n",
      "Epoch: 83330 | Loss: 0.4571819007396698 | Test loss: 0.4720657765865326\n",
      "Epoch: 83340 | Loss: 0.4571700990200043 | Test loss: 0.4720519185066223\n",
      "Epoch: 83350 | Loss: 0.4571583867073059 | Test loss: 0.4720380902290344\n",
      "Epoch: 83360 | Loss: 0.45714661478996277 | Test loss: 0.47202426195144653\n",
      "Epoch: 83370 | Loss: 0.45713481307029724 | Test loss: 0.47201043367385864\n",
      "Epoch: 83380 | Loss: 0.4571231007575989 | Test loss: 0.471996545791626\n",
      "Epoch: 83390 | Loss: 0.45711132884025574 | Test loss: 0.4719827175140381\n",
      "Epoch: 83400 | Loss: 0.4570995271205902 | Test loss: 0.4719688594341278\n",
      "Epoch: 83410 | Loss: 0.45708781480789185 | Test loss: 0.4719550311565399\n",
      "Epoch: 83420 | Loss: 0.4570760428905487 | Test loss: 0.47194114327430725\n",
      "Epoch: 83430 | Loss: 0.45706430077552795 | Test loss: 0.47192731499671936\n",
      "Epoch: 83440 | Loss: 0.4570525288581848 | Test loss: 0.4719134271144867\n",
      "Epoch: 83450 | Loss: 0.45704078674316406 | Test loss: 0.4718995988368988\n",
      "Epoch: 83460 | Loss: 0.45702895522117615 | Test loss: 0.4718857705593109\n",
      "Epoch: 83470 | Loss: 0.4570172429084778 | Test loss: 0.471871942281723\n",
      "Epoch: 83480 | Loss: 0.45700550079345703 | Test loss: 0.47185808420181274\n",
      "Epoch: 83490 | Loss: 0.4569937288761139 | Test loss: 0.47184425592422485\n",
      "Epoch: 83500 | Loss: 0.45698195695877075 | Test loss: 0.4718303680419922\n",
      "Epoch: 83510 | Loss: 0.45697021484375 | Test loss: 0.4718165397644043\n",
      "Epoch: 83520 | Loss: 0.45695844292640686 | Test loss: 0.4718027114868164\n",
      "Epoch: 83530 | Loss: 0.4569466710090637 | Test loss: 0.4717888832092285\n",
      "Epoch: 83540 | Loss: 0.45693492889404297 | Test loss: 0.47177502512931824\n",
      "Epoch: 83550 | Loss: 0.4569232165813446 | Test loss: 0.47176119685173035\n",
      "Epoch: 83560 | Loss: 0.4569113850593567 | Test loss: 0.4717473089694977\n",
      "Epoch: 83570 | Loss: 0.45689964294433594 | Test loss: 0.4717334806919098\n",
      "Epoch: 83580 | Loss: 0.4568879306316376 | Test loss: 0.4717196524143219\n",
      "Epoch: 83590 | Loss: 0.45687609910964966 | Test loss: 0.47170576453208923\n",
      "Epoch: 83600 | Loss: 0.4568643569946289 | Test loss: 0.47169193625450134\n",
      "Epoch: 83610 | Loss: 0.45685264468193054 | Test loss: 0.47167807817459106\n",
      "Epoch: 83620 | Loss: 0.456840842962265 | Test loss: 0.4716642498970032\n",
      "Epoch: 83630 | Loss: 0.4568290710449219 | Test loss: 0.4716504216194153\n",
      "Epoch: 83640 | Loss: 0.4568173587322235 | Test loss: 0.4716365337371826\n",
      "Epoch: 83650 | Loss: 0.456805557012558 | Test loss: 0.4716227054595947\n",
      "Epoch: 83660 | Loss: 0.45679378509521484 | Test loss: 0.47160884737968445\n",
      "Epoch: 83670 | Loss: 0.4567820727825165 | Test loss: 0.47159501910209656\n",
      "Epoch: 83680 | Loss: 0.4567703306674957 | Test loss: 0.47158119082450867\n",
      "Epoch: 83690 | Loss: 0.4567584991455078 | Test loss: 0.471567302942276\n",
      "Epoch: 83700 | Loss: 0.45674678683280945 | Test loss: 0.4715534746646881\n",
      "Epoch: 83710 | Loss: 0.4567349851131439 | Test loss: 0.47153958678245544\n",
      "Epoch: 83720 | Loss: 0.45672327280044556 | Test loss: 0.47152575850486755\n",
      "Epoch: 83730 | Loss: 0.4567115008831024 | Test loss: 0.47151193022727966\n",
      "Epoch: 83740 | Loss: 0.4566996991634369 | Test loss: 0.4714981019496918\n",
      "Epoch: 83750 | Loss: 0.45668792724609375 | Test loss: 0.4714842438697815\n",
      "Epoch: 83760 | Loss: 0.4566762149333954 | Test loss: 0.4714704155921936\n",
      "Epoch: 83770 | Loss: 0.45666447281837463 | Test loss: 0.47145652770996094\n",
      "Epoch: 83780 | Loss: 0.4566527009010315 | Test loss: 0.47144269943237305\n",
      "Epoch: 83790 | Loss: 0.45664092898368835 | Test loss: 0.47142887115478516\n",
      "Epoch: 83800 | Loss: 0.4566291868686676 | Test loss: 0.4714150130748749\n",
      "Epoch: 83810 | Loss: 0.45661741495132446 | Test loss: 0.471401184797287\n",
      "Epoch: 83820 | Loss: 0.4566056430339813 | Test loss: 0.4713872969150543\n",
      "Epoch: 83830 | Loss: 0.45659390091896057 | Test loss: 0.47137346863746643\n",
      "Epoch: 83840 | Loss: 0.45658212900161743 | Test loss: 0.47135964035987854\n",
      "Epoch: 83850 | Loss: 0.4565703570842743 | Test loss: 0.4713457524776459\n",
      "Epoch: 83860 | Loss: 0.45655861496925354 | Test loss: 0.471331924200058\n",
      "Epoch: 83870 | Loss: 0.4565468430519104 | Test loss: 0.4713180661201477\n",
      "Epoch: 83880 | Loss: 0.45653507113456726 | Test loss: 0.4713042378425598\n",
      "Epoch: 83890 | Loss: 0.4565233290195465 | Test loss: 0.4712904095649719\n",
      "Epoch: 83900 | Loss: 0.45651158690452576 | Test loss: 0.47127652168273926\n",
      "Epoch: 83910 | Loss: 0.45649978518486023 | Test loss: 0.47126269340515137\n",
      "Epoch: 83920 | Loss: 0.4564880430698395 | Test loss: 0.4712488651275635\n",
      "Epoch: 83930 | Loss: 0.45647627115249634 | Test loss: 0.4712350070476532\n",
      "Epoch: 83940 | Loss: 0.4564645290374756 | Test loss: 0.4712211787700653\n",
      "Epoch: 83950 | Loss: 0.45645275712013245 | Test loss: 0.4712073504924774\n",
      "Epoch: 83960 | Loss: 0.4564410150051117 | Test loss: 0.47119346261024475\n",
      "Epoch: 83970 | Loss: 0.45642924308776855 | Test loss: 0.47117963433265686\n",
      "Epoch: 83980 | Loss: 0.4564174711704254 | Test loss: 0.4711657464504242\n",
      "Epoch: 83990 | Loss: 0.45640572905540466 | Test loss: 0.4711519181728363\n",
      "Epoch: 84000 | Loss: 0.4563939571380615 | Test loss: 0.4711380898952484\n",
      "Epoch: 84010 | Loss: 0.45638224482536316 | Test loss: 0.4711242616176605\n",
      "Epoch: 84020 | Loss: 0.45637044310569763 | Test loss: 0.47111040353775024\n",
      "Epoch: 84030 | Loss: 0.4563586711883545 | Test loss: 0.4710965156555176\n",
      "Epoch: 84040 | Loss: 0.45634695887565613 | Test loss: 0.4710826873779297\n",
      "Epoch: 84050 | Loss: 0.4563351571559906 | Test loss: 0.4710688591003418\n",
      "Epoch: 84060 | Loss: 0.45632344484329224 | Test loss: 0.4710550010204315\n",
      "Epoch: 84070 | Loss: 0.4563116133213043 | Test loss: 0.47104117274284363\n",
      "Epoch: 84080 | Loss: 0.45629987120628357 | Test loss: 0.47102728486061096\n",
      "Epoch: 84090 | Loss: 0.45628809928894043 | Test loss: 0.47101345658302307\n",
      "Epoch: 84100 | Loss: 0.4562763273715973 | Test loss: 0.4709996283054352\n",
      "Epoch: 84110 | Loss: 0.45626458525657654 | Test loss: 0.4709858000278473\n",
      "Epoch: 84120 | Loss: 0.4562528729438782 | Test loss: 0.4709719121456146\n",
      "Epoch: 84130 | Loss: 0.45624110102653503 | Test loss: 0.47095808386802673\n",
      "Epoch: 84140 | Loss: 0.4562292993068695 | Test loss: 0.47094422578811646\n",
      "Epoch: 84150 | Loss: 0.45621758699417114 | Test loss: 0.47093039751052856\n",
      "Epoch: 84160 | Loss: 0.456205815076828 | Test loss: 0.4709165692329407\n",
      "Epoch: 84170 | Loss: 0.4561940133571625 | Test loss: 0.4709027409553528\n",
      "Epoch: 84180 | Loss: 0.4561823010444641 | Test loss: 0.4708888530731201\n",
      "Epoch: 84190 | Loss: 0.45617052912712097 | Test loss: 0.4708750247955322\n",
      "Epoch: 84200 | Loss: 0.45615872740745544 | Test loss: 0.47086116671562195\n",
      "Epoch: 84210 | Loss: 0.4561470150947571 | Test loss: 0.47084733843803406\n",
      "Epoch: 84220 | Loss: 0.45613524317741394 | Test loss: 0.4708334505558014\n",
      "Epoch: 84230 | Loss: 0.4561235010623932 | Test loss: 0.4708196222782135\n",
      "Epoch: 84240 | Loss: 0.45611172914505005 | Test loss: 0.47080573439598083\n",
      "Epoch: 84250 | Loss: 0.4560999870300293 | Test loss: 0.47079190611839294\n",
      "Epoch: 84260 | Loss: 0.4560881555080414 | Test loss: 0.47077807784080505\n",
      "Epoch: 84270 | Loss: 0.456076443195343 | Test loss: 0.47076424956321716\n",
      "Epoch: 84280 | Loss: 0.45606470108032227 | Test loss: 0.4707503914833069\n",
      "Epoch: 84290 | Loss: 0.4560529291629791 | Test loss: 0.470736563205719\n",
      "Epoch: 84300 | Loss: 0.456041157245636 | Test loss: 0.47072267532348633\n",
      "Epoch: 84310 | Loss: 0.45602941513061523 | Test loss: 0.47070884704589844\n",
      "Epoch: 84320 | Loss: 0.4560176432132721 | Test loss: 0.47069501876831055\n",
      "Epoch: 84330 | Loss: 0.45600587129592896 | Test loss: 0.47068119049072266\n",
      "Epoch: 84340 | Loss: 0.4559941291809082 | Test loss: 0.4706673324108124\n",
      "Epoch: 84350 | Loss: 0.45598241686820984 | Test loss: 0.4706535041332245\n",
      "Epoch: 84360 | Loss: 0.4559705853462219 | Test loss: 0.4706396162509918\n",
      "Epoch: 84370 | Loss: 0.45595884323120117 | Test loss: 0.47062578797340393\n",
      "Epoch: 84380 | Loss: 0.4559471309185028 | Test loss: 0.47061195969581604\n",
      "Epoch: 84390 | Loss: 0.4559352993965149 | Test loss: 0.4705980718135834\n",
      "Epoch: 84400 | Loss: 0.45592355728149414 | Test loss: 0.4705842435359955\n",
      "Epoch: 84410 | Loss: 0.4559118449687958 | Test loss: 0.4705703854560852\n",
      "Epoch: 84420 | Loss: 0.45590004324913025 | Test loss: 0.4705565571784973\n",
      "Epoch: 84430 | Loss: 0.4558882713317871 | Test loss: 0.4705427289009094\n",
      "Epoch: 84440 | Loss: 0.45587655901908875 | Test loss: 0.47052884101867676\n",
      "Epoch: 84450 | Loss: 0.4558647572994232 | Test loss: 0.47051501274108887\n",
      "Epoch: 84460 | Loss: 0.4558529853820801 | Test loss: 0.4705011546611786\n",
      "Epoch: 84470 | Loss: 0.4558412730693817 | Test loss: 0.4704873263835907\n",
      "Epoch: 84480 | Loss: 0.45582953095436096 | Test loss: 0.4704734981060028\n",
      "Epoch: 84490 | Loss: 0.45581769943237305 | Test loss: 0.47045961022377014\n",
      "Epoch: 84500 | Loss: 0.4558059871196747 | Test loss: 0.47044578194618225\n",
      "Epoch: 84510 | Loss: 0.45579418540000916 | Test loss: 0.4704318940639496\n",
      "Epoch: 84520 | Loss: 0.4557824730873108 | Test loss: 0.4704180657863617\n",
      "Epoch: 84530 | Loss: 0.45577070116996765 | Test loss: 0.4704042375087738\n",
      "Epoch: 84540 | Loss: 0.4557588994503021 | Test loss: 0.4703904092311859\n",
      "Epoch: 84550 | Loss: 0.455747127532959 | Test loss: 0.47037655115127563\n",
      "Epoch: 84560 | Loss: 0.4557354152202606 | Test loss: 0.47036272287368774\n",
      "Epoch: 84570 | Loss: 0.45572367310523987 | Test loss: 0.4703488349914551\n",
      "Epoch: 84580 | Loss: 0.45571190118789673 | Test loss: 0.4703350067138672\n",
      "Epoch: 84590 | Loss: 0.4557001292705536 | Test loss: 0.4703211784362793\n",
      "Epoch: 84600 | Loss: 0.45568838715553284 | Test loss: 0.470307320356369\n",
      "Epoch: 84610 | Loss: 0.4556766152381897 | Test loss: 0.47029349207878113\n",
      "Epoch: 84620 | Loss: 0.45566484332084656 | Test loss: 0.47027960419654846\n",
      "Epoch: 84630 | Loss: 0.4556531012058258 | Test loss: 0.47026577591896057\n",
      "Epoch: 84640 | Loss: 0.45564132928848267 | Test loss: 0.4702519476413727\n",
      "Epoch: 84650 | Loss: 0.4556295573711395 | Test loss: 0.47023805975914\n",
      "Epoch: 84660 | Loss: 0.4556178152561188 | Test loss: 0.4702242314815521\n",
      "Epoch: 84670 | Loss: 0.45560604333877563 | Test loss: 0.47021037340164185\n",
      "Epoch: 84680 | Loss: 0.4555942714214325 | Test loss: 0.47019654512405396\n",
      "Epoch: 84690 | Loss: 0.45558252930641174 | Test loss: 0.47018271684646606\n",
      "Epoch: 84700 | Loss: 0.455570787191391 | Test loss: 0.4701688289642334\n",
      "Epoch: 84710 | Loss: 0.45555898547172546 | Test loss: 0.4701550006866455\n",
      "Epoch: 84720 | Loss: 0.4555472433567047 | Test loss: 0.4701411724090576\n",
      "Epoch: 84730 | Loss: 0.4555354714393616 | Test loss: 0.47012731432914734\n",
      "Epoch: 84740 | Loss: 0.4555237293243408 | Test loss: 0.47011348605155945\n",
      "Epoch: 84750 | Loss: 0.4555119574069977 | Test loss: 0.47009965777397156\n",
      "Epoch: 84760 | Loss: 0.45550021529197693 | Test loss: 0.4700857698917389\n",
      "Epoch: 84770 | Loss: 0.4554884433746338 | Test loss: 0.470071941614151\n",
      "Epoch: 84780 | Loss: 0.45547667145729065 | Test loss: 0.47005805373191833\n",
      "Epoch: 84790 | Loss: 0.4554649293422699 | Test loss: 0.47004422545433044\n",
      "Epoch: 84800 | Loss: 0.45545315742492676 | Test loss: 0.47003039717674255\n",
      "Epoch: 84810 | Loss: 0.4554414451122284 | Test loss: 0.47001656889915466\n",
      "Epoch: 84820 | Loss: 0.45542964339256287 | Test loss: 0.4700027108192444\n",
      "Epoch: 84830 | Loss: 0.4554178714752197 | Test loss: 0.4699888229370117\n",
      "Epoch: 84840 | Loss: 0.45540615916252136 | Test loss: 0.46997499465942383\n",
      "Epoch: 84850 | Loss: 0.45539435744285583 | Test loss: 0.46996116638183594\n",
      "Epoch: 84860 | Loss: 0.45538264513015747 | Test loss: 0.46994730830192566\n",
      "Epoch: 84870 | Loss: 0.45537081360816956 | Test loss: 0.46993348002433777\n",
      "Epoch: 84880 | Loss: 0.4553590714931488 | Test loss: 0.4699195921421051\n",
      "Epoch: 84890 | Loss: 0.45534729957580566 | Test loss: 0.4699057638645172\n",
      "Epoch: 84900 | Loss: 0.4553355276584625 | Test loss: 0.4698919355869293\n",
      "Epoch: 84910 | Loss: 0.4553237855434418 | Test loss: 0.46987810730934143\n",
      "Epoch: 84920 | Loss: 0.4553120732307434 | Test loss: 0.46986421942710876\n",
      "Epoch: 84930 | Loss: 0.45530030131340027 | Test loss: 0.4698503911495209\n",
      "Epoch: 84940 | Loss: 0.45528849959373474 | Test loss: 0.4698365330696106\n",
      "Epoch: 84950 | Loss: 0.4552767872810364 | Test loss: 0.4698227047920227\n",
      "Epoch: 84960 | Loss: 0.45526501536369324 | Test loss: 0.4698088765144348\n",
      "Epoch: 84970 | Loss: 0.4552532136440277 | Test loss: 0.4697950482368469\n",
      "Epoch: 84980 | Loss: 0.45524150133132935 | Test loss: 0.46978116035461426\n",
      "Epoch: 84990 | Loss: 0.4552297294139862 | Test loss: 0.46976733207702637\n",
      "Epoch: 85000 | Loss: 0.4552179276943207 | Test loss: 0.4697534739971161\n",
      "Epoch: 85010 | Loss: 0.4552062153816223 | Test loss: 0.4697396457195282\n",
      "Epoch: 85020 | Loss: 0.4551944434642792 | Test loss: 0.46972575783729553\n",
      "Epoch: 85030 | Loss: 0.4551827013492584 | Test loss: 0.46971192955970764\n",
      "Epoch: 85040 | Loss: 0.4551709294319153 | Test loss: 0.469698041677475\n",
      "Epoch: 85050 | Loss: 0.45515918731689453 | Test loss: 0.4696842133998871\n",
      "Epoch: 85060 | Loss: 0.4551473557949066 | Test loss: 0.4696703851222992\n",
      "Epoch: 85070 | Loss: 0.45513564348220825 | Test loss: 0.4696565568447113\n",
      "Epoch: 85080 | Loss: 0.4551239013671875 | Test loss: 0.469642698764801\n",
      "Epoch: 85090 | Loss: 0.45511212944984436 | Test loss: 0.46962887048721313\n",
      "Epoch: 85100 | Loss: 0.4551003575325012 | Test loss: 0.46961498260498047\n",
      "Epoch: 85110 | Loss: 0.45508861541748047 | Test loss: 0.4696011543273926\n",
      "Epoch: 85120 | Loss: 0.45507684350013733 | Test loss: 0.4695873260498047\n",
      "Epoch: 85130 | Loss: 0.4550650715827942 | Test loss: 0.4695734977722168\n",
      "Epoch: 85140 | Loss: 0.45505332946777344 | Test loss: 0.4695596396923065\n",
      "Epoch: 85150 | Loss: 0.4550416171550751 | Test loss: 0.46954581141471863\n",
      "Epoch: 85160 | Loss: 0.45502978563308716 | Test loss: 0.46953192353248596\n",
      "Epoch: 85170 | Loss: 0.4550180435180664 | Test loss: 0.46951809525489807\n",
      "Epoch: 85180 | Loss: 0.45500633120536804 | Test loss: 0.4695042669773102\n",
      "Epoch: 85190 | Loss: 0.4549944996833801 | Test loss: 0.4694903790950775\n",
      "Epoch: 85200 | Loss: 0.4549827575683594 | Test loss: 0.4694765508174896\n",
      "Epoch: 85210 | Loss: 0.454971045255661 | Test loss: 0.46946269273757935\n",
      "Epoch: 85220 | Loss: 0.4549592435359955 | Test loss: 0.46944886445999146\n",
      "Epoch: 85230 | Loss: 0.45494747161865234 | Test loss: 0.46943503618240356\n",
      "Epoch: 85240 | Loss: 0.454935759305954 | Test loss: 0.4694211483001709\n",
      "Epoch: 85250 | Loss: 0.45492395758628845 | Test loss: 0.469407320022583\n",
      "Epoch: 85260 | Loss: 0.4549121856689453 | Test loss: 0.46939346194267273\n",
      "Epoch: 85270 | Loss: 0.45490047335624695 | Test loss: 0.46937963366508484\n",
      "Epoch: 85280 | Loss: 0.4548887312412262 | Test loss: 0.46936580538749695\n",
      "Epoch: 85290 | Loss: 0.4548768997192383 | Test loss: 0.4693519175052643\n",
      "Epoch: 85300 | Loss: 0.4548651874065399 | Test loss: 0.4693380892276764\n",
      "Epoch: 85310 | Loss: 0.4548533856868744 | Test loss: 0.4693242013454437\n",
      "Epoch: 85320 | Loss: 0.454841673374176 | Test loss: 0.46931037306785583\n",
      "Epoch: 85330 | Loss: 0.4548299014568329 | Test loss: 0.46929654479026794\n",
      "Epoch: 85340 | Loss: 0.45481809973716736 | Test loss: 0.46928271651268005\n",
      "Epoch: 85350 | Loss: 0.4548063278198242 | Test loss: 0.4692688584327698\n",
      "Epoch: 85360 | Loss: 0.45479461550712585 | Test loss: 0.4692550301551819\n",
      "Epoch: 85370 | Loss: 0.4547828733921051 | Test loss: 0.4692411422729492\n",
      "Epoch: 85380 | Loss: 0.45477110147476196 | Test loss: 0.46922731399536133\n",
      "Epoch: 85390 | Loss: 0.4547593295574188 | Test loss: 0.46921348571777344\n",
      "Epoch: 85400 | Loss: 0.45474758744239807 | Test loss: 0.46919962763786316\n",
      "Epoch: 85410 | Loss: 0.45473581552505493 | Test loss: 0.46918579936027527\n",
      "Epoch: 85420 | Loss: 0.4547240436077118 | Test loss: 0.4691719114780426\n",
      "Epoch: 85430 | Loss: 0.45471230149269104 | Test loss: 0.4691580832004547\n",
      "Epoch: 85440 | Loss: 0.4547005295753479 | Test loss: 0.4691442549228668\n",
      "Epoch: 85450 | Loss: 0.45468875765800476 | Test loss: 0.46913036704063416\n",
      "Epoch: 85460 | Loss: 0.454677015542984 | Test loss: 0.46911653876304626\n",
      "Epoch: 85470 | Loss: 0.45466524362564087 | Test loss: 0.469102680683136\n",
      "Epoch: 85480 | Loss: 0.45465347170829773 | Test loss: 0.4690888524055481\n",
      "Epoch: 85490 | Loss: 0.454641729593277 | Test loss: 0.4690750241279602\n",
      "Epoch: 85500 | Loss: 0.4546299874782562 | Test loss: 0.46906113624572754\n",
      "Epoch: 85510 | Loss: 0.4546181857585907 | Test loss: 0.46904730796813965\n",
      "Epoch: 85520 | Loss: 0.45460644364356995 | Test loss: 0.46903347969055176\n",
      "Epoch: 85530 | Loss: 0.4545946717262268 | Test loss: 0.4690196216106415\n",
      "Epoch: 85540 | Loss: 0.45458292961120605 | Test loss: 0.4690057933330536\n",
      "Epoch: 85550 | Loss: 0.4545711576938629 | Test loss: 0.4689919650554657\n",
      "Epoch: 85560 | Loss: 0.45455941557884216 | Test loss: 0.46897807717323303\n",
      "Epoch: 85570 | Loss: 0.454547643661499 | Test loss: 0.46896424889564514\n",
      "Epoch: 85580 | Loss: 0.4545358717441559 | Test loss: 0.4689503610134125\n",
      "Epoch: 85590 | Loss: 0.45452412962913513 | Test loss: 0.4689365327358246\n",
      "Epoch: 85600 | Loss: 0.454512357711792 | Test loss: 0.4689227044582367\n",
      "Epoch: 85610 | Loss: 0.45450064539909363 | Test loss: 0.4689088761806488\n",
      "Epoch: 85620 | Loss: 0.4544888436794281 | Test loss: 0.4688950181007385\n",
      "Epoch: 85630 | Loss: 0.45447707176208496 | Test loss: 0.46888113021850586\n",
      "Epoch: 85640 | Loss: 0.4544653594493866 | Test loss: 0.46886730194091797\n",
      "Epoch: 85650 | Loss: 0.45445355772972107 | Test loss: 0.4688534736633301\n",
      "Epoch: 85660 | Loss: 0.4544418454170227 | Test loss: 0.4688396155834198\n",
      "Epoch: 85670 | Loss: 0.4544300138950348 | Test loss: 0.4688257873058319\n",
      "Epoch: 85680 | Loss: 0.45441827178001404 | Test loss: 0.46881189942359924\n",
      "Epoch: 85690 | Loss: 0.4544064998626709 | Test loss: 0.46879807114601135\n",
      "Epoch: 85700 | Loss: 0.45439472794532776 | Test loss: 0.46878424286842346\n",
      "Epoch: 85710 | Loss: 0.454382985830307 | Test loss: 0.46877041459083557\n",
      "Epoch: 85720 | Loss: 0.45437127351760864 | Test loss: 0.4687565267086029\n",
      "Epoch: 85730 | Loss: 0.4543595016002655 | Test loss: 0.468742698431015\n",
      "Epoch: 85740 | Loss: 0.4543476998806 | Test loss: 0.46872884035110474\n",
      "Epoch: 85750 | Loss: 0.4543359875679016 | Test loss: 0.46871501207351685\n",
      "Epoch: 85760 | Loss: 0.45432421565055847 | Test loss: 0.46870118379592896\n",
      "Epoch: 85770 | Loss: 0.45431241393089294 | Test loss: 0.46868735551834106\n",
      "Epoch: 85780 | Loss: 0.4543007016181946 | Test loss: 0.4686734676361084\n",
      "Epoch: 85790 | Loss: 0.45428892970085144 | Test loss: 0.4686596393585205\n",
      "Epoch: 85800 | Loss: 0.4542771279811859 | Test loss: 0.46864578127861023\n",
      "Epoch: 85810 | Loss: 0.45426541566848755 | Test loss: 0.46863195300102234\n",
      "Epoch: 85820 | Loss: 0.4542536437511444 | Test loss: 0.4686180651187897\n",
      "Epoch: 85830 | Loss: 0.45424190163612366 | Test loss: 0.4686042368412018\n",
      "Epoch: 85840 | Loss: 0.4542301297187805 | Test loss: 0.4685903489589691\n",
      "Epoch: 85850 | Loss: 0.45421838760375977 | Test loss: 0.4685765206813812\n",
      "Epoch: 85860 | Loss: 0.45420655608177185 | Test loss: 0.46856269240379333\n",
      "Epoch: 85870 | Loss: 0.4541948437690735 | Test loss: 0.46854886412620544\n",
      "Epoch: 85880 | Loss: 0.45418310165405273 | Test loss: 0.46853500604629517\n",
      "Epoch: 85890 | Loss: 0.4541713297367096 | Test loss: 0.4685211777687073\n",
      "Epoch: 85900 | Loss: 0.45415955781936646 | Test loss: 0.4685072898864746\n",
      "Epoch: 85910 | Loss: 0.4541478157043457 | Test loss: 0.4684934616088867\n",
      "Epoch: 85920 | Loss: 0.45413604378700256 | Test loss: 0.46847963333129883\n",
      "Epoch: 85930 | Loss: 0.4541242718696594 | Test loss: 0.46846580505371094\n",
      "Epoch: 85940 | Loss: 0.45411252975463867 | Test loss: 0.46845194697380066\n",
      "Epoch: 85950 | Loss: 0.4541008174419403 | Test loss: 0.46843811869621277\n",
      "Epoch: 85960 | Loss: 0.4540889859199524 | Test loss: 0.4684242308139801\n",
      "Epoch: 85970 | Loss: 0.45407724380493164 | Test loss: 0.4684104025363922\n",
      "Epoch: 85980 | Loss: 0.4540655314922333 | Test loss: 0.4683965742588043\n",
      "Epoch: 85990 | Loss: 0.45405369997024536 | Test loss: 0.46838268637657166\n",
      "Epoch: 86000 | Loss: 0.4540419578552246 | Test loss: 0.46836885809898376\n",
      "Epoch: 86010 | Loss: 0.45403024554252625 | Test loss: 0.4683550000190735\n",
      "Epoch: 86020 | Loss: 0.4540184438228607 | Test loss: 0.4683411717414856\n",
      "Epoch: 86030 | Loss: 0.4540066719055176 | Test loss: 0.4683273434638977\n",
      "Epoch: 86040 | Loss: 0.4539949595928192 | Test loss: 0.46831345558166504\n",
      "Epoch: 86050 | Loss: 0.4539831578731537 | Test loss: 0.46829962730407715\n",
      "Epoch: 86060 | Loss: 0.45397138595581055 | Test loss: 0.46828576922416687\n",
      "Epoch: 86070 | Loss: 0.4539596736431122 | Test loss: 0.468271940946579\n",
      "Epoch: 86080 | Loss: 0.45394793152809143 | Test loss: 0.4682581126689911\n",
      "Epoch: 86090 | Loss: 0.4539361000061035 | Test loss: 0.4682442247867584\n",
      "Epoch: 86100 | Loss: 0.45392438769340515 | Test loss: 0.46823039650917053\n",
      "Epoch: 86110 | Loss: 0.4539125859737396 | Test loss: 0.46821650862693787\n",
      "Epoch: 86120 | Loss: 0.45390087366104126 | Test loss: 0.46820268034935\n",
      "Epoch: 86130 | Loss: 0.4538891017436981 | Test loss: 0.4681888520717621\n",
      "Epoch: 86140 | Loss: 0.4538773000240326 | Test loss: 0.4681750237941742\n",
      "Epoch: 86150 | Loss: 0.45386552810668945 | Test loss: 0.4681611657142639\n",
      "Epoch: 86160 | Loss: 0.4538538157939911 | Test loss: 0.468147337436676\n",
      "Epoch: 86170 | Loss: 0.45384207367897034 | Test loss: 0.46813344955444336\n",
      "Epoch: 86180 | Loss: 0.4538303017616272 | Test loss: 0.46811962127685547\n",
      "Epoch: 86190 | Loss: 0.45381852984428406 | Test loss: 0.4681057929992676\n",
      "Epoch: 86200 | Loss: 0.4538067877292633 | Test loss: 0.4680919349193573\n",
      "Epoch: 86210 | Loss: 0.45379501581192017 | Test loss: 0.4680781066417694\n",
      "Epoch: 86220 | Loss: 0.453783243894577 | Test loss: 0.46806421875953674\n",
      "Epoch: 86230 | Loss: 0.4537715017795563 | Test loss: 0.46805039048194885\n",
      "Epoch: 86240 | Loss: 0.45375972986221313 | Test loss: 0.46803656220436096\n",
      "Epoch: 86250 | Loss: 0.45374795794487 | Test loss: 0.4680226743221283\n",
      "Epoch: 86260 | Loss: 0.45373621582984924 | Test loss: 0.4680088460445404\n",
      "Epoch: 86270 | Loss: 0.4537244439125061 | Test loss: 0.4679949879646301\n",
      "Epoch: 86280 | Loss: 0.45371267199516296 | Test loss: 0.46798115968704224\n",
      "Epoch: 86290 | Loss: 0.4537009298801422 | Test loss: 0.46796733140945435\n",
      "Epoch: 86300 | Loss: 0.45368918776512146 | Test loss: 0.4679534435272217\n",
      "Epoch: 86310 | Loss: 0.45367738604545593 | Test loss: 0.4679396152496338\n",
      "Epoch: 86320 | Loss: 0.4536656439304352 | Test loss: 0.4679257869720459\n",
      "Epoch: 86330 | Loss: 0.45365387201309204 | Test loss: 0.4679119288921356\n",
      "Epoch: 86340 | Loss: 0.4536421298980713 | Test loss: 0.46789810061454773\n",
      "Epoch: 86350 | Loss: 0.45363035798072815 | Test loss: 0.46788427233695984\n",
      "Epoch: 86360 | Loss: 0.4536186158657074 | Test loss: 0.4678703844547272\n",
      "Epoch: 86370 | Loss: 0.45360684394836426 | Test loss: 0.4678565561771393\n",
      "Epoch: 86380 | Loss: 0.4535950720310211 | Test loss: 0.4678426682949066\n",
      "Epoch: 86390 | Loss: 0.45358332991600037 | Test loss: 0.4678288400173187\n",
      "Epoch: 86400 | Loss: 0.4535715579986572 | Test loss: 0.46781501173973083\n",
      "Epoch: 86410 | Loss: 0.45355984568595886 | Test loss: 0.46780118346214294\n",
      "Epoch: 86420 | Loss: 0.45354804396629333 | Test loss: 0.46778732538223267\n",
      "Epoch: 86430 | Loss: 0.4535362720489502 | Test loss: 0.4677734375\n",
      "Epoch: 86440 | Loss: 0.45352455973625183 | Test loss: 0.4677596092224121\n",
      "Epoch: 86450 | Loss: 0.4535127580165863 | Test loss: 0.4677457809448242\n",
      "Epoch: 86460 | Loss: 0.45350104570388794 | Test loss: 0.46773192286491394\n",
      "Epoch: 86470 | Loss: 0.4534892141819 | Test loss: 0.46771809458732605\n",
      "Epoch: 86480 | Loss: 0.4534774720668793 | Test loss: 0.4677042067050934\n",
      "Epoch: 86490 | Loss: 0.45346570014953613 | Test loss: 0.4676903784275055\n",
      "Epoch: 86500 | Loss: 0.453453928232193 | Test loss: 0.4676765501499176\n",
      "Epoch: 86510 | Loss: 0.45344218611717224 | Test loss: 0.4676627218723297\n",
      "Epoch: 86520 | Loss: 0.4534304738044739 | Test loss: 0.46764883399009705\n",
      "Epoch: 86530 | Loss: 0.45341870188713074 | Test loss: 0.46763500571250916\n",
      "Epoch: 86540 | Loss: 0.4534069001674652 | Test loss: 0.4676211476325989\n",
      "Epoch: 86550 | Loss: 0.45339518785476685 | Test loss: 0.467607319355011\n",
      "Epoch: 86560 | Loss: 0.4533834159374237 | Test loss: 0.4675934910774231\n",
      "Epoch: 86570 | Loss: 0.4533716142177582 | Test loss: 0.4675796627998352\n",
      "Epoch: 86580 | Loss: 0.4533599019050598 | Test loss: 0.46756577491760254\n",
      "Epoch: 86590 | Loss: 0.4533481299877167 | Test loss: 0.46755194664001465\n",
      "Epoch: 86600 | Loss: 0.45333632826805115 | Test loss: 0.46753808856010437\n",
      "Epoch: 86610 | Loss: 0.4533246159553528 | Test loss: 0.4675242602825165\n",
      "Epoch: 86620 | Loss: 0.45331284403800964 | Test loss: 0.4675103724002838\n",
      "Epoch: 86630 | Loss: 0.4533011019229889 | Test loss: 0.4674965441226959\n",
      "Epoch: 86640 | Loss: 0.45328933000564575 | Test loss: 0.46748265624046326\n",
      "Epoch: 86650 | Loss: 0.453277587890625 | Test loss: 0.46746882796287537\n",
      "Epoch: 86660 | Loss: 0.4532657563686371 | Test loss: 0.4674549996852875\n",
      "Epoch: 86670 | Loss: 0.4532540440559387 | Test loss: 0.4674411714076996\n",
      "Epoch: 86680 | Loss: 0.45324230194091797 | Test loss: 0.4674273133277893\n",
      "Epoch: 86690 | Loss: 0.45323053002357483 | Test loss: 0.4674134850502014\n",
      "Epoch: 86700 | Loss: 0.4532187581062317 | Test loss: 0.46739959716796875\n",
      "Epoch: 86710 | Loss: 0.45320701599121094 | Test loss: 0.46738576889038086\n",
      "Epoch: 86720 | Loss: 0.4531952440738678 | Test loss: 0.46737194061279297\n",
      "Epoch: 86730 | Loss: 0.45318347215652466 | Test loss: 0.4673581123352051\n",
      "Epoch: 86740 | Loss: 0.4531717300415039 | Test loss: 0.4673442542552948\n",
      "Epoch: 86750 | Loss: 0.45316001772880554 | Test loss: 0.4673304259777069\n",
      "Epoch: 86760 | Loss: 0.4531481862068176 | Test loss: 0.46731653809547424\n",
      "Epoch: 86770 | Loss: 0.4531364440917969 | Test loss: 0.46730270981788635\n",
      "Epoch: 86780 | Loss: 0.4531247317790985 | Test loss: 0.46728888154029846\n",
      "Epoch: 86790 | Loss: 0.4531129002571106 | Test loss: 0.4672749936580658\n",
      "Epoch: 86800 | Loss: 0.45310115814208984 | Test loss: 0.4672611653804779\n",
      "Epoch: 86810 | Loss: 0.4530894458293915 | Test loss: 0.4672473073005676\n",
      "Epoch: 86820 | Loss: 0.45307764410972595 | Test loss: 0.46723347902297974\n",
      "Epoch: 86830 | Loss: 0.4530658721923828 | Test loss: 0.46721965074539185\n",
      "Epoch: 86840 | Loss: 0.45305415987968445 | Test loss: 0.4672057628631592\n",
      "Epoch: 86850 | Loss: 0.4530423581600189 | Test loss: 0.4671919345855713\n",
      "Epoch: 86860 | Loss: 0.4530305862426758 | Test loss: 0.467178076505661\n",
      "Epoch: 86870 | Loss: 0.4530188739299774 | Test loss: 0.4671642482280731\n",
      "Epoch: 86880 | Loss: 0.45300713181495667 | Test loss: 0.46715041995048523\n",
      "Epoch: 86890 | Loss: 0.45299530029296875 | Test loss: 0.46713653206825256\n",
      "Epoch: 86900 | Loss: 0.4529835879802704 | Test loss: 0.4671227037906647\n",
      "Epoch: 86910 | Loss: 0.45297178626060486 | Test loss: 0.467108815908432\n",
      "Epoch: 86920 | Loss: 0.4529600739479065 | Test loss: 0.4670949876308441\n",
      "Epoch: 86930 | Loss: 0.45294830203056335 | Test loss: 0.4670811593532562\n",
      "Epoch: 86940 | Loss: 0.4529365003108978 | Test loss: 0.46706733107566833\n",
      "Epoch: 86950 | Loss: 0.4529247283935547 | Test loss: 0.46705347299575806\n",
      "Epoch: 86960 | Loss: 0.4529130160808563 | Test loss: 0.46703964471817017\n",
      "Epoch: 86970 | Loss: 0.45290127396583557 | Test loss: 0.4670257568359375\n",
      "Epoch: 86980 | Loss: 0.45288950204849243 | Test loss: 0.4670119285583496\n",
      "Epoch: 86990 | Loss: 0.4528777301311493 | Test loss: 0.4669981002807617\n",
      "Epoch: 87000 | Loss: 0.45286598801612854 | Test loss: 0.46698424220085144\n",
      "Epoch: 87010 | Loss: 0.4528542160987854 | Test loss: 0.46697041392326355\n",
      "Epoch: 87020 | Loss: 0.45284244418144226 | Test loss: 0.4669565260410309\n",
      "Epoch: 87030 | Loss: 0.4528307020664215 | Test loss: 0.466942697763443\n",
      "Epoch: 87040 | Loss: 0.45281893014907837 | Test loss: 0.4669288694858551\n",
      "Epoch: 87050 | Loss: 0.45280715823173523 | Test loss: 0.46691498160362244\n",
      "Epoch: 87060 | Loss: 0.4527954161167145 | Test loss: 0.46690115332603455\n",
      "Epoch: 87070 | Loss: 0.45278364419937134 | Test loss: 0.46688729524612427\n",
      "Epoch: 87080 | Loss: 0.4527718722820282 | Test loss: 0.4668734669685364\n",
      "Epoch: 87090 | Loss: 0.45276013016700745 | Test loss: 0.4668596386909485\n",
      "Epoch: 87100 | Loss: 0.4527483880519867 | Test loss: 0.4668457508087158\n",
      "Epoch: 87110 | Loss: 0.45273658633232117 | Test loss: 0.46683192253112793\n",
      "Epoch: 87120 | Loss: 0.4527248442173004 | Test loss: 0.46681809425354004\n",
      "Epoch: 87130 | Loss: 0.4527130722999573 | Test loss: 0.46680423617362976\n",
      "Epoch: 87140 | Loss: 0.4527013301849365 | Test loss: 0.46679040789604187\n",
      "Epoch: 87150 | Loss: 0.4526895582675934 | Test loss: 0.466776579618454\n",
      "Epoch: 87160 | Loss: 0.45267781615257263 | Test loss: 0.4667626917362213\n",
      "Epoch: 87170 | Loss: 0.4526660442352295 | Test loss: 0.4667488634586334\n",
      "Epoch: 87180 | Loss: 0.45265427231788635 | Test loss: 0.46673497557640076\n",
      "Epoch: 87190 | Loss: 0.4526425302028656 | Test loss: 0.46672114729881287\n",
      "Epoch: 87200 | Loss: 0.45263075828552246 | Test loss: 0.466707319021225\n",
      "Epoch: 87210 | Loss: 0.4526190459728241 | Test loss: 0.4666934907436371\n",
      "Epoch: 87220 | Loss: 0.45260724425315857 | Test loss: 0.4666796326637268\n",
      "Epoch: 87230 | Loss: 0.45259547233581543 | Test loss: 0.46666574478149414\n",
      "Epoch: 87240 | Loss: 0.45258376002311707 | Test loss: 0.46665191650390625\n",
      "Epoch: 87250 | Loss: 0.45257195830345154 | Test loss: 0.46663808822631836\n",
      "Epoch: 87260 | Loss: 0.4525602459907532 | Test loss: 0.4666242301464081\n",
      "Epoch: 87270 | Loss: 0.45254841446876526 | Test loss: 0.4666104018688202\n",
      "Epoch: 87280 | Loss: 0.4525366723537445 | Test loss: 0.4665965139865875\n",
      "Epoch: 87290 | Loss: 0.45252490043640137 | Test loss: 0.46658268570899963\n",
      "Epoch: 87300 | Loss: 0.4525131285190582 | Test loss: 0.46656885743141174\n",
      "Epoch: 87310 | Loss: 0.4525013864040375 | Test loss: 0.46655502915382385\n",
      "Epoch: 87320 | Loss: 0.4524896740913391 | Test loss: 0.4665411412715912\n",
      "Epoch: 87330 | Loss: 0.45247790217399597 | Test loss: 0.4665273129940033\n",
      "Epoch: 87340 | Loss: 0.45246610045433044 | Test loss: 0.466513454914093\n",
      "Epoch: 87350 | Loss: 0.4524543881416321 | Test loss: 0.4664996266365051\n",
      "Epoch: 87360 | Loss: 0.45244261622428894 | Test loss: 0.46648579835891724\n",
      "Epoch: 87370 | Loss: 0.4524308145046234 | Test loss: 0.46647197008132935\n",
      "Epoch: 87380 | Loss: 0.45241910219192505 | Test loss: 0.4664580821990967\n",
      "Epoch: 87390 | Loss: 0.4524073302745819 | Test loss: 0.4664442539215088\n",
      "Epoch: 87400 | Loss: 0.4523955285549164 | Test loss: 0.4664303958415985\n",
      "Epoch: 87410 | Loss: 0.452383816242218 | Test loss: 0.4664165675640106\n",
      "Epoch: 87420 | Loss: 0.4523720443248749 | Test loss: 0.46640267968177795\n",
      "Epoch: 87430 | Loss: 0.4523603022098541 | Test loss: 0.46638885140419006\n",
      "Epoch: 87440 | Loss: 0.452348530292511 | Test loss: 0.4663749635219574\n",
      "Epoch: 87450 | Loss: 0.45233678817749023 | Test loss: 0.4663611352443695\n",
      "Epoch: 87460 | Loss: 0.4523249566555023 | Test loss: 0.4663473069667816\n",
      "Epoch: 87470 | Loss: 0.45231324434280396 | Test loss: 0.4663334786891937\n",
      "Epoch: 87480 | Loss: 0.4523015022277832 | Test loss: 0.46631962060928345\n",
      "Epoch: 87490 | Loss: 0.45228973031044006 | Test loss: 0.46630579233169556\n",
      "Epoch: 87500 | Loss: 0.4522779583930969 | Test loss: 0.4662919044494629\n",
      "Epoch: 87510 | Loss: 0.45226621627807617 | Test loss: 0.466278076171875\n",
      "Epoch: 87520 | Loss: 0.45225444436073303 | Test loss: 0.4662642478942871\n",
      "Epoch: 87530 | Loss: 0.4522426724433899 | Test loss: 0.4662504196166992\n",
      "Epoch: 87540 | Loss: 0.45223093032836914 | Test loss: 0.46623656153678894\n",
      "Epoch: 87550 | Loss: 0.4522192180156708 | Test loss: 0.46622273325920105\n",
      "Epoch: 87560 | Loss: 0.45220738649368286 | Test loss: 0.4662088453769684\n",
      "Epoch: 87570 | Loss: 0.4521956443786621 | Test loss: 0.4661950170993805\n",
      "Epoch: 87580 | Loss: 0.45218393206596375 | Test loss: 0.4661811888217926\n",
      "Epoch: 87590 | Loss: 0.45217210054397583 | Test loss: 0.46616730093955994\n",
      "Epoch: 87600 | Loss: 0.4521603584289551 | Test loss: 0.46615347266197205\n",
      "Epoch: 87610 | Loss: 0.4521486461162567 | Test loss: 0.46613961458206177\n",
      "Epoch: 87620 | Loss: 0.4521368443965912 | Test loss: 0.4661257863044739\n",
      "Epoch: 87630 | Loss: 0.45212507247924805 | Test loss: 0.466111958026886\n",
      "Epoch: 87640 | Loss: 0.4521133601665497 | Test loss: 0.4660980701446533\n",
      "Epoch: 87650 | Loss: 0.45210155844688416 | Test loss: 0.46608424186706543\n",
      "Epoch: 87660 | Loss: 0.452089786529541 | Test loss: 0.46607038378715515\n",
      "Epoch: 87670 | Loss: 0.45207807421684265 | Test loss: 0.46605655550956726\n",
      "Epoch: 87680 | Loss: 0.4520663321018219 | Test loss: 0.46604272723197937\n",
      "Epoch: 87690 | Loss: 0.452054500579834 | Test loss: 0.4660288393497467\n",
      "Epoch: 87700 | Loss: 0.4520427882671356 | Test loss: 0.4660150110721588\n",
      "Epoch: 87710 | Loss: 0.4520309865474701 | Test loss: 0.46600112318992615\n",
      "Epoch: 87720 | Loss: 0.45201927423477173 | Test loss: 0.46598729491233826\n",
      "Epoch: 87730 | Loss: 0.4520075023174286 | Test loss: 0.46597346663475037\n",
      "Epoch: 87740 | Loss: 0.45199570059776306 | Test loss: 0.4659596383571625\n",
      "Epoch: 87750 | Loss: 0.4519839286804199 | Test loss: 0.4659457802772522\n",
      "Epoch: 87760 | Loss: 0.45197221636772156 | Test loss: 0.4659319519996643\n",
      "Epoch: 87770 | Loss: 0.4519604742527008 | Test loss: 0.46591806411743164\n",
      "Epoch: 87780 | Loss: 0.45194870233535767 | Test loss: 0.46590423583984375\n",
      "Epoch: 87790 | Loss: 0.4519369304180145 | Test loss: 0.46589040756225586\n",
      "Epoch: 87800 | Loss: 0.4519251883029938 | Test loss: 0.4658765494823456\n",
      "Epoch: 87810 | Loss: 0.45191341638565063 | Test loss: 0.4658627212047577\n",
      "Epoch: 87820 | Loss: 0.4519016444683075 | Test loss: 0.465848833322525\n",
      "Epoch: 87830 | Loss: 0.45188990235328674 | Test loss: 0.46583500504493713\n",
      "Epoch: 87840 | Loss: 0.4518781304359436 | Test loss: 0.46582117676734924\n",
      "Epoch: 87850 | Loss: 0.45186635851860046 | Test loss: 0.4658072888851166\n",
      "Epoch: 87860 | Loss: 0.4518546164035797 | Test loss: 0.4657934606075287\n",
      "Epoch: 87870 | Loss: 0.4518428444862366 | Test loss: 0.4657796025276184\n",
      "Epoch: 87880 | Loss: 0.45183107256889343 | Test loss: 0.4657657742500305\n",
      "Epoch: 87890 | Loss: 0.4518193304538727 | Test loss: 0.4657519459724426\n",
      "Epoch: 87900 | Loss: 0.45180758833885193 | Test loss: 0.46573805809020996\n",
      "Epoch: 87910 | Loss: 0.4517957866191864 | Test loss: 0.46572422981262207\n",
      "Epoch: 87920 | Loss: 0.45178404450416565 | Test loss: 0.4657104015350342\n",
      "Epoch: 87930 | Loss: 0.4517722725868225 | Test loss: 0.4656965434551239\n",
      "Epoch: 87940 | Loss: 0.45176053047180176 | Test loss: 0.465682715177536\n",
      "Epoch: 87950 | Loss: 0.4517487585544586 | Test loss: 0.4656688868999481\n",
      "Epoch: 87960 | Loss: 0.45173701643943787 | Test loss: 0.46565499901771545\n",
      "Epoch: 87970 | Loss: 0.4517252445220947 | Test loss: 0.46564117074012756\n",
      "Epoch: 87980 | Loss: 0.4517134726047516 | Test loss: 0.4656272828578949\n",
      "Epoch: 87990 | Loss: 0.45170173048973083 | Test loss: 0.465613454580307\n",
      "Epoch: 88000 | Loss: 0.4516899585723877 | Test loss: 0.4655996263027191\n",
      "Epoch: 88010 | Loss: 0.45167824625968933 | Test loss: 0.4655857980251312\n",
      "Epoch: 88020 | Loss: 0.4516664445400238 | Test loss: 0.46557193994522095\n",
      "Epoch: 88030 | Loss: 0.45165467262268066 | Test loss: 0.4655580520629883\n",
      "Epoch: 88040 | Loss: 0.4516429603099823 | Test loss: 0.4655442237854004\n",
      "Epoch: 88050 | Loss: 0.4516311585903168 | Test loss: 0.4655303955078125\n",
      "Epoch: 88060 | Loss: 0.4516194462776184 | Test loss: 0.4655165374279022\n",
      "Epoch: 88070 | Loss: 0.4516076147556305 | Test loss: 0.46550270915031433\n",
      "Epoch: 88080 | Loss: 0.45159587264060974 | Test loss: 0.46548882126808167\n",
      "Epoch: 88090 | Loss: 0.4515841007232666 | Test loss: 0.4654749929904938\n",
      "Epoch: 88100 | Loss: 0.45157232880592346 | Test loss: 0.4654611647129059\n",
      "Epoch: 88110 | Loss: 0.4515605866909027 | Test loss: 0.465447336435318\n",
      "Epoch: 88120 | Loss: 0.45154887437820435 | Test loss: 0.4654334485530853\n",
      "Epoch: 88130 | Loss: 0.4515371024608612 | Test loss: 0.46541962027549744\n",
      "Epoch: 88140 | Loss: 0.4515253007411957 | Test loss: 0.46540576219558716\n",
      "Epoch: 88150 | Loss: 0.4515135884284973 | Test loss: 0.46539193391799927\n",
      "Epoch: 88160 | Loss: 0.4515018165111542 | Test loss: 0.4653781056404114\n",
      "Epoch: 88170 | Loss: 0.45149001479148865 | Test loss: 0.4653642773628235\n",
      "Epoch: 88180 | Loss: 0.4514783024787903 | Test loss: 0.4653503894805908\n",
      "Epoch: 88190 | Loss: 0.45146653056144714 | Test loss: 0.46533656120300293\n",
      "Epoch: 88200 | Loss: 0.4514547288417816 | Test loss: 0.46532270312309265\n",
      "Epoch: 88210 | Loss: 0.45144301652908325 | Test loss: 0.46530887484550476\n",
      "Epoch: 88220 | Loss: 0.4514312446117401 | Test loss: 0.4652949869632721\n",
      "Epoch: 88230 | Loss: 0.45141950249671936 | Test loss: 0.4652811586856842\n",
      "Epoch: 88240 | Loss: 0.4514077305793762 | Test loss: 0.46526727080345154\n",
      "Epoch: 88250 | Loss: 0.45139598846435547 | Test loss: 0.46525344252586365\n",
      "Epoch: 88260 | Loss: 0.45138415694236755 | Test loss: 0.46523961424827576\n",
      "Epoch: 88270 | Loss: 0.4513724446296692 | Test loss: 0.46522578597068787\n",
      "Epoch: 88280 | Loss: 0.45136070251464844 | Test loss: 0.4652119278907776\n",
      "Epoch: 88290 | Loss: 0.4513489305973053 | Test loss: 0.4651980996131897\n",
      "Epoch: 88300 | Loss: 0.45133715867996216 | Test loss: 0.46518421173095703\n",
      "Epoch: 88310 | Loss: 0.4513254165649414 | Test loss: 0.46517038345336914\n",
      "Epoch: 88320 | Loss: 0.45131364464759827 | Test loss: 0.46515655517578125\n",
      "Epoch: 88330 | Loss: 0.4513018727302551 | Test loss: 0.46514272689819336\n",
      "Epoch: 88340 | Loss: 0.4512901306152344 | Test loss: 0.4651288688182831\n",
      "Epoch: 88350 | Loss: 0.451278418302536 | Test loss: 0.4651150405406952\n",
      "Epoch: 88360 | Loss: 0.4512665867805481 | Test loss: 0.4651011526584625\n",
      "Epoch: 88370 | Loss: 0.45125484466552734 | Test loss: 0.46508732438087463\n",
      "Epoch: 88380 | Loss: 0.451243132352829 | Test loss: 0.46507349610328674\n",
      "Epoch: 88390 | Loss: 0.45123130083084106 | Test loss: 0.4650596082210541\n",
      "Epoch: 88400 | Loss: 0.4512195587158203 | Test loss: 0.4650457799434662\n",
      "Epoch: 88410 | Loss: 0.45120784640312195 | Test loss: 0.4650319218635559\n",
      "Epoch: 88420 | Loss: 0.4511960446834564 | Test loss: 0.465018093585968\n",
      "Epoch: 88430 | Loss: 0.4511842727661133 | Test loss: 0.4650042653083801\n",
      "Epoch: 88440 | Loss: 0.4511725604534149 | Test loss: 0.46499037742614746\n",
      "Epoch: 88450 | Loss: 0.4511607587337494 | Test loss: 0.46497654914855957\n",
      "Epoch: 88460 | Loss: 0.45114898681640625 | Test loss: 0.4649626910686493\n",
      "Epoch: 88470 | Loss: 0.4511372745037079 | Test loss: 0.4649488627910614\n",
      "Epoch: 88480 | Loss: 0.45112553238868713 | Test loss: 0.4649350345134735\n",
      "Epoch: 88490 | Loss: 0.4511137008666992 | Test loss: 0.46492114663124084\n",
      "Epoch: 88500 | Loss: 0.45110198855400085 | Test loss: 0.46490731835365295\n",
      "Epoch: 88510 | Loss: 0.4510901868343353 | Test loss: 0.4648934304714203\n",
      "Epoch: 88520 | Loss: 0.45107847452163696 | Test loss: 0.4648796021938324\n",
      "Epoch: 88530 | Loss: 0.4510667026042938 | Test loss: 0.4648657739162445\n",
      "Epoch: 88540 | Loss: 0.4510549008846283 | Test loss: 0.4648519456386566\n",
      "Epoch: 88550 | Loss: 0.45104312896728516 | Test loss: 0.46483808755874634\n",
      "Epoch: 88560 | Loss: 0.4510314166545868 | Test loss: 0.46482425928115845\n",
      "Epoch: 88570 | Loss: 0.45101967453956604 | Test loss: 0.4648103713989258\n",
      "Epoch: 88580 | Loss: 0.4510079026222229 | Test loss: 0.4647965431213379\n",
      "Epoch: 88590 | Loss: 0.45099613070487976 | Test loss: 0.46478271484375\n",
      "Epoch: 88600 | Loss: 0.450984388589859 | Test loss: 0.4647688567638397\n",
      "Epoch: 88610 | Loss: 0.45097261667251587 | Test loss: 0.46475502848625183\n",
      "Epoch: 88620 | Loss: 0.45096084475517273 | Test loss: 0.46474114060401917\n",
      "Epoch: 88630 | Loss: 0.450949102640152 | Test loss: 0.4647273123264313\n",
      "Epoch: 88640 | Loss: 0.45093733072280884 | Test loss: 0.4647134840488434\n",
      "Epoch: 88650 | Loss: 0.4509255588054657 | Test loss: 0.4646995961666107\n",
      "Epoch: 88660 | Loss: 0.45091381669044495 | Test loss: 0.4646857678890228\n",
      "Epoch: 88670 | Loss: 0.4509020447731018 | Test loss: 0.46467190980911255\n",
      "Epoch: 88680 | Loss: 0.45089027285575867 | Test loss: 0.46465808153152466\n",
      "Epoch: 88690 | Loss: 0.4508785307407379 | Test loss: 0.46464425325393677\n",
      "Epoch: 88700 | Loss: 0.45086678862571716 | Test loss: 0.4646303653717041\n",
      "Epoch: 88710 | Loss: 0.45085498690605164 | Test loss: 0.4646165370941162\n",
      "Epoch: 88720 | Loss: 0.4508432447910309 | Test loss: 0.4646027088165283\n",
      "Epoch: 88730 | Loss: 0.45083147287368774 | Test loss: 0.46458885073661804\n",
      "Epoch: 88740 | Loss: 0.450819730758667 | Test loss: 0.46457502245903015\n",
      "Epoch: 88750 | Loss: 0.45080795884132385 | Test loss: 0.46456119418144226\n",
      "Epoch: 88760 | Loss: 0.4507962167263031 | Test loss: 0.4645473062992096\n",
      "Epoch: 88770 | Loss: 0.45078444480895996 | Test loss: 0.4645334780216217\n",
      "Epoch: 88780 | Loss: 0.4507726728916168 | Test loss: 0.46451959013938904\n",
      "Epoch: 88790 | Loss: 0.45076093077659607 | Test loss: 0.46450576186180115\n",
      "Epoch: 88800 | Loss: 0.45074915885925293 | Test loss: 0.46449193358421326\n",
      "Epoch: 88810 | Loss: 0.45073744654655457 | Test loss: 0.46447810530662537\n",
      "Epoch: 88820 | Loss: 0.45072564482688904 | Test loss: 0.4644642472267151\n",
      "Epoch: 88830 | Loss: 0.4507138729095459 | Test loss: 0.4644503593444824\n",
      "Epoch: 88840 | Loss: 0.45070216059684753 | Test loss: 0.46443653106689453\n",
      "Epoch: 88850 | Loss: 0.450690358877182 | Test loss: 0.46442270278930664\n",
      "Epoch: 88860 | Loss: 0.45067864656448364 | Test loss: 0.46440884470939636\n",
      "Epoch: 88870 | Loss: 0.4506668150424957 | Test loss: 0.46439501643180847\n",
      "Epoch: 88880 | Loss: 0.450655072927475 | Test loss: 0.4643811285495758\n",
      "Epoch: 88890 | Loss: 0.45064330101013184 | Test loss: 0.4643673002719879\n",
      "Epoch: 88900 | Loss: 0.4506315290927887 | Test loss: 0.4643534719944\n",
      "Epoch: 88910 | Loss: 0.45061978697776794 | Test loss: 0.46433964371681213\n",
      "Epoch: 88920 | Loss: 0.4506080746650696 | Test loss: 0.46432575583457947\n",
      "Epoch: 88930 | Loss: 0.45059630274772644 | Test loss: 0.4643119275569916\n",
      "Epoch: 88940 | Loss: 0.4505845010280609 | Test loss: 0.4642980694770813\n",
      "Epoch: 88950 | Loss: 0.45057278871536255 | Test loss: 0.4642842411994934\n",
      "Epoch: 88960 | Loss: 0.4505610167980194 | Test loss: 0.4642704129219055\n",
      "Epoch: 88970 | Loss: 0.4505492150783539 | Test loss: 0.4642565846443176\n",
      "Epoch: 88980 | Loss: 0.4505375027656555 | Test loss: 0.46424269676208496\n",
      "Epoch: 88990 | Loss: 0.4505257308483124 | Test loss: 0.46422886848449707\n",
      "Epoch: 89000 | Loss: 0.45051392912864685 | Test loss: 0.4642150104045868\n",
      "Epoch: 89010 | Loss: 0.4505022168159485 | Test loss: 0.4642011821269989\n",
      "Epoch: 89020 | Loss: 0.45049044489860535 | Test loss: 0.46418729424476624\n",
      "Epoch: 89030 | Loss: 0.4504787027835846 | Test loss: 0.46417346596717834\n",
      "Epoch: 89040 | Loss: 0.45046693086624146 | Test loss: 0.4641595780849457\n",
      "Epoch: 89050 | Loss: 0.4504551887512207 | Test loss: 0.4641457498073578\n",
      "Epoch: 89060 | Loss: 0.4504433572292328 | Test loss: 0.4641319215297699\n",
      "Epoch: 89070 | Loss: 0.4504316449165344 | Test loss: 0.464118093252182\n",
      "Epoch: 89080 | Loss: 0.45041990280151367 | Test loss: 0.46410423517227173\n",
      "Epoch: 89090 | Loss: 0.45040813088417053 | Test loss: 0.46409040689468384\n",
      "Epoch: 89100 | Loss: 0.4503963589668274 | Test loss: 0.46407651901245117\n",
      "Epoch: 89110 | Loss: 0.45038461685180664 | Test loss: 0.4640626907348633\n",
      "Epoch: 89120 | Loss: 0.4503728449344635 | Test loss: 0.4640488624572754\n",
      "Epoch: 89130 | Loss: 0.45036107301712036 | Test loss: 0.4640350341796875\n",
      "Epoch: 89140 | Loss: 0.4503493309020996 | Test loss: 0.4640211760997772\n",
      "Epoch: 89150 | Loss: 0.45033761858940125 | Test loss: 0.46400734782218933\n",
      "Epoch: 89160 | Loss: 0.45032578706741333 | Test loss: 0.46399345993995667\n",
      "Epoch: 89170 | Loss: 0.4503140449523926 | Test loss: 0.4639796316623688\n",
      "Epoch: 89180 | Loss: 0.4503023326396942 | Test loss: 0.4639658033847809\n",
      "Epoch: 89190 | Loss: 0.4502905011177063 | Test loss: 0.4639519155025482\n",
      "Epoch: 89200 | Loss: 0.45027875900268555 | Test loss: 0.4639380872249603\n",
      "Epoch: 89210 | Loss: 0.4502670466899872 | Test loss: 0.46392422914505005\n",
      "Epoch: 89220 | Loss: 0.45025524497032166 | Test loss: 0.46391040086746216\n",
      "Epoch: 89230 | Loss: 0.4502434730529785 | Test loss: 0.46389657258987427\n",
      "Epoch: 89240 | Loss: 0.45023176074028015 | Test loss: 0.4638826847076416\n",
      "Epoch: 89250 | Loss: 0.4502199590206146 | Test loss: 0.4638688564300537\n",
      "Epoch: 89260 | Loss: 0.4502081871032715 | Test loss: 0.46385499835014343\n",
      "Epoch: 89270 | Loss: 0.4501964747905731 | Test loss: 0.46384117007255554\n",
      "Epoch: 89280 | Loss: 0.45018473267555237 | Test loss: 0.46382734179496765\n",
      "Epoch: 89290 | Loss: 0.45017290115356445 | Test loss: 0.463813453912735\n",
      "Epoch: 89300 | Loss: 0.4501611888408661 | Test loss: 0.4637996256351471\n",
      "Epoch: 89310 | Loss: 0.45014938712120056 | Test loss: 0.46378573775291443\n",
      "Epoch: 89320 | Loss: 0.4501376748085022 | Test loss: 0.46377190947532654\n",
      "Epoch: 89330 | Loss: 0.45012590289115906 | Test loss: 0.46375808119773865\n",
      "Epoch: 89340 | Loss: 0.45011410117149353 | Test loss: 0.46374425292015076\n",
      "Epoch: 89350 | Loss: 0.4501023292541504 | Test loss: 0.4637303948402405\n",
      "Epoch: 89360 | Loss: 0.450090616941452 | Test loss: 0.4637165665626526\n",
      "Epoch: 89370 | Loss: 0.4500788748264313 | Test loss: 0.4637026786804199\n",
      "Epoch: 89380 | Loss: 0.45006710290908813 | Test loss: 0.46368885040283203\n",
      "Epoch: 89390 | Loss: 0.450055330991745 | Test loss: 0.46367502212524414\n",
      "Epoch: 89400 | Loss: 0.45004358887672424 | Test loss: 0.46366116404533386\n",
      "Epoch: 89410 | Loss: 0.4500318169593811 | Test loss: 0.46364733576774597\n",
      "Epoch: 89420 | Loss: 0.45002004504203796 | Test loss: 0.4636334478855133\n",
      "Epoch: 89430 | Loss: 0.4500083029270172 | Test loss: 0.4636196196079254\n",
      "Epoch: 89440 | Loss: 0.4499965310096741 | Test loss: 0.4636057913303375\n",
      "Epoch: 89450 | Loss: 0.44998475909233093 | Test loss: 0.46359190344810486\n",
      "Epoch: 89460 | Loss: 0.4499730169773102 | Test loss: 0.46357807517051697\n",
      "Epoch: 89470 | Loss: 0.44996124505996704 | Test loss: 0.4635642170906067\n",
      "Epoch: 89480 | Loss: 0.4499494731426239 | Test loss: 0.4635503888130188\n",
      "Epoch: 89490 | Loss: 0.44993773102760315 | Test loss: 0.4635365605354309\n",
      "Epoch: 89500 | Loss: 0.4499259889125824 | Test loss: 0.46352267265319824\n",
      "Epoch: 89510 | Loss: 0.44991418719291687 | Test loss: 0.46350884437561035\n",
      "Epoch: 89520 | Loss: 0.4499024450778961 | Test loss: 0.46349501609802246\n",
      "Epoch: 89530 | Loss: 0.449890673160553 | Test loss: 0.4634811580181122\n",
      "Epoch: 89540 | Loss: 0.4498789310455322 | Test loss: 0.4634673297405243\n",
      "Epoch: 89550 | Loss: 0.4498671591281891 | Test loss: 0.4634535014629364\n",
      "Epoch: 89560 | Loss: 0.44985541701316833 | Test loss: 0.46343961358070374\n",
      "Epoch: 89570 | Loss: 0.4498436450958252 | Test loss: 0.46342578530311584\n",
      "Epoch: 89580 | Loss: 0.44983187317848206 | Test loss: 0.4634118974208832\n",
      "Epoch: 89590 | Loss: 0.4498201310634613 | Test loss: 0.4633980691432953\n",
      "Epoch: 89600 | Loss: 0.44980835914611816 | Test loss: 0.4633842408657074\n",
      "Epoch: 89610 | Loss: 0.4497966468334198 | Test loss: 0.4633704125881195\n",
      "Epoch: 89620 | Loss: 0.4497848451137543 | Test loss: 0.46335655450820923\n",
      "Epoch: 89630 | Loss: 0.44977307319641113 | Test loss: 0.46334266662597656\n",
      "Epoch: 89640 | Loss: 0.44976136088371277 | Test loss: 0.46332883834838867\n",
      "Epoch: 89650 | Loss: 0.44974955916404724 | Test loss: 0.4633150100708008\n",
      "Epoch: 89660 | Loss: 0.4497378468513489 | Test loss: 0.4633011519908905\n",
      "Epoch: 89670 | Loss: 0.44972601532936096 | Test loss: 0.4632873237133026\n",
      "Epoch: 89680 | Loss: 0.4497142732143402 | Test loss: 0.46327343583106995\n",
      "Epoch: 89690 | Loss: 0.44970250129699707 | Test loss: 0.46325960755348206\n",
      "Epoch: 89700 | Loss: 0.44969072937965393 | Test loss: 0.46324577927589417\n",
      "Epoch: 89710 | Loss: 0.4496789872646332 | Test loss: 0.4632319509983063\n",
      "Epoch: 89720 | Loss: 0.4496672749519348 | Test loss: 0.4632180631160736\n",
      "Epoch: 89730 | Loss: 0.4496555030345917 | Test loss: 0.4632042348384857\n",
      "Epoch: 89740 | Loss: 0.44964370131492615 | Test loss: 0.46319037675857544\n",
      "Epoch: 89750 | Loss: 0.4496319890022278 | Test loss: 0.46317654848098755\n",
      "Epoch: 89760 | Loss: 0.44962021708488464 | Test loss: 0.46316272020339966\n",
      "Epoch: 89770 | Loss: 0.4496084153652191 | Test loss: 0.46314889192581177\n",
      "Epoch: 89780 | Loss: 0.44959670305252075 | Test loss: 0.4631350040435791\n",
      "Epoch: 89790 | Loss: 0.4495849311351776 | Test loss: 0.4631211757659912\n",
      "Epoch: 89800 | Loss: 0.4495731294155121 | Test loss: 0.46310731768608093\n",
      "Epoch: 89810 | Loss: 0.4495614171028137 | Test loss: 0.46309348940849304\n",
      "Epoch: 89820 | Loss: 0.4495496451854706 | Test loss: 0.4630796015262604\n",
      "Epoch: 89830 | Loss: 0.44953790307044983 | Test loss: 0.4630657732486725\n",
      "Epoch: 89840 | Loss: 0.4495261311531067 | Test loss: 0.4630518853664398\n",
      "Epoch: 89850 | Loss: 0.44951438903808594 | Test loss: 0.46303805708885193\n",
      "Epoch: 89860 | Loss: 0.449502557516098 | Test loss: 0.46302422881126404\n",
      "Epoch: 89870 | Loss: 0.44949084520339966 | Test loss: 0.46301040053367615\n",
      "Epoch: 89880 | Loss: 0.4494791030883789 | Test loss: 0.46299654245376587\n",
      "Epoch: 89890 | Loss: 0.44946733117103577 | Test loss: 0.462982714176178\n",
      "Epoch: 89900 | Loss: 0.4494555592536926 | Test loss: 0.4629688262939453\n",
      "Epoch: 89910 | Loss: 0.4494438171386719 | Test loss: 0.4629549980163574\n",
      "Epoch: 89920 | Loss: 0.44943204522132874 | Test loss: 0.46294116973876953\n",
      "Epoch: 89930 | Loss: 0.4494202733039856 | Test loss: 0.46292734146118164\n",
      "Epoch: 89940 | Loss: 0.44940853118896484 | Test loss: 0.46291348338127136\n",
      "Epoch: 89950 | Loss: 0.4493968188762665 | Test loss: 0.46289965510368347\n",
      "Epoch: 89960 | Loss: 0.44938498735427856 | Test loss: 0.4628857672214508\n",
      "Epoch: 89970 | Loss: 0.4493732452392578 | Test loss: 0.4628719389438629\n",
      "Epoch: 89980 | Loss: 0.44936153292655945 | Test loss: 0.462858110666275\n",
      "Epoch: 89990 | Loss: 0.44934970140457153 | Test loss: 0.46284422278404236\n",
      "Epoch: 90000 | Loss: 0.4493379592895508 | Test loss: 0.46283039450645447\n",
      "Epoch: 90010 | Loss: 0.4493262469768524 | Test loss: 0.4628165364265442\n",
      "Epoch: 90020 | Loss: 0.4493144452571869 | Test loss: 0.4628027081489563\n",
      "Epoch: 90030 | Loss: 0.44930267333984375 | Test loss: 0.4627888798713684\n",
      "Epoch: 90040 | Loss: 0.4492909610271454 | Test loss: 0.46277499198913574\n",
      "Epoch: 90050 | Loss: 0.44927915930747986 | Test loss: 0.46276116371154785\n",
      "Epoch: 90060 | Loss: 0.4492673873901367 | Test loss: 0.4627473056316376\n",
      "Epoch: 90070 | Loss: 0.44925567507743835 | Test loss: 0.4627334773540497\n",
      "Epoch: 90080 | Loss: 0.4492439329624176 | Test loss: 0.4627196490764618\n",
      "Epoch: 90090 | Loss: 0.4492321014404297 | Test loss: 0.4627057611942291\n",
      "Epoch: 90100 | Loss: 0.4492203891277313 | Test loss: 0.46269193291664124\n",
      "Epoch: 90110 | Loss: 0.4492085874080658 | Test loss: 0.46267804503440857\n",
      "Epoch: 90120 | Loss: 0.44919687509536743 | Test loss: 0.4626642167568207\n",
      "Epoch: 90130 | Loss: 0.4491851031780243 | Test loss: 0.4626503884792328\n",
      "Epoch: 90140 | Loss: 0.44917330145835876 | Test loss: 0.4626365602016449\n",
      "Epoch: 90150 | Loss: 0.4491615295410156 | Test loss: 0.4626227021217346\n",
      "Epoch: 90160 | Loss: 0.44914981722831726 | Test loss: 0.46260887384414673\n",
      "Epoch: 90170 | Loss: 0.4491380751132965 | Test loss: 0.46259498596191406\n",
      "Epoch: 90180 | Loss: 0.44912630319595337 | Test loss: 0.46258115768432617\n",
      "Epoch: 90190 | Loss: 0.44911453127861023 | Test loss: 0.4625673294067383\n",
      "Epoch: 90200 | Loss: 0.4491027891635895 | Test loss: 0.462553471326828\n",
      "Epoch: 90210 | Loss: 0.44909101724624634 | Test loss: 0.4625396430492401\n",
      "Epoch: 90220 | Loss: 0.4490792453289032 | Test loss: 0.46252575516700745\n",
      "Epoch: 90230 | Loss: 0.44906750321388245 | Test loss: 0.46251192688941956\n",
      "Epoch: 90240 | Loss: 0.4490557312965393 | Test loss: 0.46249809861183167\n",
      "Epoch: 90250 | Loss: 0.44904395937919617 | Test loss: 0.462484210729599\n",
      "Epoch: 90260 | Loss: 0.4490322172641754 | Test loss: 0.4624703824520111\n",
      "Epoch: 90270 | Loss: 0.4490204453468323 | Test loss: 0.46245652437210083\n",
      "Epoch: 90280 | Loss: 0.44900867342948914 | Test loss: 0.46244269609451294\n",
      "Epoch: 90290 | Loss: 0.4489969313144684 | Test loss: 0.46242886781692505\n",
      "Epoch: 90300 | Loss: 0.44898518919944763 | Test loss: 0.4624149799346924\n",
      "Epoch: 90310 | Loss: 0.4489733874797821 | Test loss: 0.4624011516571045\n",
      "Epoch: 90320 | Loss: 0.44896164536476135 | Test loss: 0.4623873233795166\n",
      "Epoch: 90330 | Loss: 0.4489498734474182 | Test loss: 0.4623734652996063\n",
      "Epoch: 90340 | Loss: 0.44893813133239746 | Test loss: 0.46235963702201843\n",
      "Epoch: 90350 | Loss: 0.4489263594150543 | Test loss: 0.46234580874443054\n",
      "Epoch: 90360 | Loss: 0.44891461730003357 | Test loss: 0.4623319208621979\n",
      "Epoch: 90370 | Loss: 0.44890284538269043 | Test loss: 0.46231809258461\n",
      "Epoch: 90380 | Loss: 0.4488910734653473 | Test loss: 0.4623042047023773\n",
      "Epoch: 90390 | Loss: 0.44887933135032654 | Test loss: 0.46229037642478943\n",
      "Epoch: 90400 | Loss: 0.4488675594329834 | Test loss: 0.46227654814720154\n",
      "Epoch: 90410 | Loss: 0.44885584712028503 | Test loss: 0.46226271986961365\n",
      "Epoch: 90420 | Loss: 0.4488440454006195 | Test loss: 0.46224886178970337\n",
      "Epoch: 90430 | Loss: 0.44883227348327637 | Test loss: 0.4622349739074707\n",
      "Epoch: 90440 | Loss: 0.448820561170578 | Test loss: 0.4622211456298828\n",
      "Epoch: 90450 | Loss: 0.4488087594509125 | Test loss: 0.4622073173522949\n",
      "Epoch: 90460 | Loss: 0.4487970471382141 | Test loss: 0.46219345927238464\n",
      "Epoch: 90470 | Loss: 0.4487852156162262 | Test loss: 0.46217963099479675\n",
      "Epoch: 90480 | Loss: 0.44877347350120544 | Test loss: 0.4621657431125641\n",
      "Epoch: 90490 | Loss: 0.4487617015838623 | Test loss: 0.4621519148349762\n",
      "Epoch: 90500 | Loss: 0.44874992966651917 | Test loss: 0.4621380865573883\n",
      "Epoch: 90510 | Loss: 0.4487381875514984 | Test loss: 0.4621242582798004\n",
      "Epoch: 90520 | Loss: 0.44872647523880005 | Test loss: 0.46211037039756775\n",
      "Epoch: 90530 | Loss: 0.4487147033214569 | Test loss: 0.46209654211997986\n",
      "Epoch: 90540 | Loss: 0.4487029016017914 | Test loss: 0.4620826840400696\n",
      "Epoch: 90550 | Loss: 0.448691189289093 | Test loss: 0.4620688557624817\n",
      "Epoch: 90560 | Loss: 0.4486794173717499 | Test loss: 0.4620550274848938\n",
      "Epoch: 90570 | Loss: 0.44866761565208435 | Test loss: 0.4620411992073059\n",
      "Epoch: 90580 | Loss: 0.448655903339386 | Test loss: 0.46202731132507324\n",
      "Epoch: 90590 | Loss: 0.44864413142204285 | Test loss: 0.46201348304748535\n",
      "Epoch: 90600 | Loss: 0.4486323297023773 | Test loss: 0.4619996249675751\n",
      "Epoch: 90610 | Loss: 0.44862061738967896 | Test loss: 0.4619857966899872\n",
      "Epoch: 90620 | Loss: 0.4486088454723358 | Test loss: 0.4619719088077545\n",
      "Epoch: 90630 | Loss: 0.44859710335731506 | Test loss: 0.4619580805301666\n",
      "Epoch: 90640 | Loss: 0.4485853314399719 | Test loss: 0.46194419264793396\n",
      "Epoch: 90650 | Loss: 0.44857358932495117 | Test loss: 0.46193036437034607\n",
      "Epoch: 90660 | Loss: 0.44856175780296326 | Test loss: 0.4619165360927582\n",
      "Epoch: 90670 | Loss: 0.4485500454902649 | Test loss: 0.4619027078151703\n",
      "Epoch: 90680 | Loss: 0.44853830337524414 | Test loss: 0.46188884973526\n",
      "Epoch: 90690 | Loss: 0.448526531457901 | Test loss: 0.4618750214576721\n",
      "Epoch: 90700 | Loss: 0.44851475954055786 | Test loss: 0.46186113357543945\n",
      "Epoch: 90710 | Loss: 0.4485030174255371 | Test loss: 0.46184730529785156\n",
      "Epoch: 90720 | Loss: 0.44849124550819397 | Test loss: 0.46183347702026367\n",
      "Epoch: 90730 | Loss: 0.44847947359085083 | Test loss: 0.4618196487426758\n",
      "Epoch: 90740 | Loss: 0.4484677314758301 | Test loss: 0.4618057906627655\n",
      "Epoch: 90750 | Loss: 0.4484560191631317 | Test loss: 0.4617919623851776\n",
      "Epoch: 90760 | Loss: 0.4484441876411438 | Test loss: 0.46177807450294495\n",
      "Epoch: 90770 | Loss: 0.44843244552612305 | Test loss: 0.46176424622535706\n",
      "Epoch: 90780 | Loss: 0.4484207332134247 | Test loss: 0.46175041794776917\n",
      "Epoch: 90790 | Loss: 0.44840890169143677 | Test loss: 0.4617365300655365\n",
      "Epoch: 90800 | Loss: 0.448397159576416 | Test loss: 0.4617227017879486\n",
      "Epoch: 90810 | Loss: 0.44838544726371765 | Test loss: 0.46170884370803833\n",
      "Epoch: 90820 | Loss: 0.44837361574172974 | Test loss: 0.46169501543045044\n",
      "Epoch: 90830 | Loss: 0.448361873626709 | Test loss: 0.46168118715286255\n",
      "Epoch: 90840 | Loss: 0.4483501613140106 | Test loss: 0.4616672992706299\n",
      "Epoch: 90850 | Loss: 0.4483383595943451 | Test loss: 0.461653470993042\n",
      "Epoch: 90860 | Loss: 0.44832658767700195 | Test loss: 0.4616396129131317\n",
      "Epoch: 90870 | Loss: 0.4483148753643036 | Test loss: 0.4616257846355438\n",
      "Epoch: 90880 | Loss: 0.44830313324928284 | Test loss: 0.46161195635795593\n",
      "Epoch: 90890 | Loss: 0.4482913017272949 | Test loss: 0.46159806847572327\n",
      "Epoch: 90900 | Loss: 0.44827958941459656 | Test loss: 0.4615842401981354\n",
      "Epoch: 90910 | Loss: 0.44826778769493103 | Test loss: 0.4615703523159027\n",
      "Epoch: 90920 | Loss: 0.4482560157775879 | Test loss: 0.4615565240383148\n",
      "Epoch: 90930 | Loss: 0.4482443034648895 | Test loss: 0.46154269576072693\n",
      "Epoch: 90940 | Loss: 0.448232501745224 | Test loss: 0.46152886748313904\n",
      "Epoch: 90950 | Loss: 0.44822072982788086 | Test loss: 0.46151500940322876\n",
      "Epoch: 90960 | Loss: 0.4482090175151825 | Test loss: 0.46150118112564087\n",
      "Epoch: 90970 | Loss: 0.44819727540016174 | Test loss: 0.4614872932434082\n",
      "Epoch: 90980 | Loss: 0.4481855034828186 | Test loss: 0.4614734649658203\n",
      "Epoch: 90990 | Loss: 0.44817373156547546 | Test loss: 0.4614596366882324\n",
      "Epoch: 91000 | Loss: 0.4481619894504547 | Test loss: 0.46144577860832214\n",
      "Epoch: 91010 | Loss: 0.4481502175331116 | Test loss: 0.46143195033073425\n",
      "Epoch: 91020 | Loss: 0.44813844561576843 | Test loss: 0.4614180624485016\n",
      "Epoch: 91030 | Loss: 0.4481267035007477 | Test loss: 0.4614042341709137\n",
      "Epoch: 91040 | Loss: 0.44811493158340454 | Test loss: 0.4613904058933258\n",
      "Epoch: 91050 | Loss: 0.4481031596660614 | Test loss: 0.46137651801109314\n",
      "Epoch: 91060 | Loss: 0.44809141755104065 | Test loss: 0.46136268973350525\n",
      "Epoch: 91070 | Loss: 0.4480796456336975 | Test loss: 0.46134883165359497\n",
      "Epoch: 91080 | Loss: 0.44806787371635437 | Test loss: 0.4613350033760071\n",
      "Epoch: 91090 | Loss: 0.4480561316013336 | Test loss: 0.4613211750984192\n",
      "Epoch: 91100 | Loss: 0.44804438948631287 | Test loss: 0.4613072872161865\n",
      "Epoch: 91110 | Loss: 0.44803258776664734 | Test loss: 0.46129345893859863\n",
      "Epoch: 91120 | Loss: 0.4480208456516266 | Test loss: 0.46127963066101074\n",
      "Epoch: 91130 | Loss: 0.44800907373428345 | Test loss: 0.46126577258110046\n",
      "Epoch: 91140 | Loss: 0.4479973316192627 | Test loss: 0.4612519443035126\n",
      "Epoch: 91150 | Loss: 0.44798555970191956 | Test loss: 0.4612381160259247\n",
      "Epoch: 91160 | Loss: 0.4479738175868988 | Test loss: 0.461224228143692\n",
      "Epoch: 91170 | Loss: 0.44796204566955566 | Test loss: 0.4612103998661041\n",
      "Epoch: 91180 | Loss: 0.4479502737522125 | Test loss: 0.46119651198387146\n",
      "Epoch: 91190 | Loss: 0.4479385316371918 | Test loss: 0.46118268370628357\n",
      "Epoch: 91200 | Loss: 0.44792675971984863 | Test loss: 0.4611688554286957\n",
      "Epoch: 91210 | Loss: 0.44791504740715027 | Test loss: 0.4611550271511078\n",
      "Epoch: 91220 | Loss: 0.44790324568748474 | Test loss: 0.4611411690711975\n",
      "Epoch: 91230 | Loss: 0.4478914737701416 | Test loss: 0.46112728118896484\n",
      "Epoch: 91240 | Loss: 0.44787976145744324 | Test loss: 0.46111345291137695\n",
      "Epoch: 91250 | Loss: 0.4478679597377777 | Test loss: 0.46109962463378906\n",
      "Epoch: 91260 | Loss: 0.44785624742507935 | Test loss: 0.4610857665538788\n",
      "Epoch: 91270 | Loss: 0.44784441590309143 | Test loss: 0.4610719382762909\n",
      "Epoch: 91280 | Loss: 0.4478326737880707 | Test loss: 0.4610580503940582\n",
      "Epoch: 91290 | Loss: 0.44782090187072754 | Test loss: 0.46104422211647034\n",
      "Epoch: 91300 | Loss: 0.4478091299533844 | Test loss: 0.46103039383888245\n",
      "Epoch: 91310 | Loss: 0.44779738783836365 | Test loss: 0.46101656556129456\n",
      "Epoch: 91320 | Loss: 0.4477856755256653 | Test loss: 0.4610026776790619\n",
      "Epoch: 91330 | Loss: 0.44777390360832214 | Test loss: 0.460988849401474\n",
      "Epoch: 91340 | Loss: 0.4477621018886566 | Test loss: 0.4609749913215637\n",
      "Epoch: 91350 | Loss: 0.44775038957595825 | Test loss: 0.46096116304397583\n",
      "Epoch: 91360 | Loss: 0.4477386176586151 | Test loss: 0.46094733476638794\n",
      "Epoch: 91370 | Loss: 0.4477268159389496 | Test loss: 0.46093350648880005\n",
      "Epoch: 91380 | Loss: 0.4477151036262512 | Test loss: 0.4609196186065674\n",
      "Epoch: 91390 | Loss: 0.4477033317089081 | Test loss: 0.4609057903289795\n",
      "Epoch: 91400 | Loss: 0.44769152998924255 | Test loss: 0.4608919322490692\n",
      "Epoch: 91410 | Loss: 0.4476798176765442 | Test loss: 0.4608781039714813\n",
      "Epoch: 91420 | Loss: 0.44766804575920105 | Test loss: 0.46086421608924866\n",
      "Epoch: 91430 | Loss: 0.4476563036441803 | Test loss: 0.46085038781166077\n",
      "Epoch: 91440 | Loss: 0.44764453172683716 | Test loss: 0.4608364999294281\n",
      "Epoch: 91450 | Loss: 0.4476327896118164 | Test loss: 0.4608226716518402\n",
      "Epoch: 91460 | Loss: 0.4476209580898285 | Test loss: 0.4608088433742523\n",
      "Epoch: 91470 | Loss: 0.4476092457771301 | Test loss: 0.46079501509666443\n",
      "Epoch: 91480 | Loss: 0.4475975036621094 | Test loss: 0.46078115701675415\n",
      "Epoch: 91490 | Loss: 0.44758573174476624 | Test loss: 0.46076732873916626\n",
      "Epoch: 91500 | Loss: 0.4475739598274231 | Test loss: 0.4607534408569336\n",
      "Epoch: 91510 | Loss: 0.44756221771240234 | Test loss: 0.4607396125793457\n",
      "Epoch: 91520 | Loss: 0.4475504457950592 | Test loss: 0.4607257843017578\n",
      "Epoch: 91530 | Loss: 0.44753867387771606 | Test loss: 0.4607119560241699\n",
      "Epoch: 91540 | Loss: 0.4475269317626953 | Test loss: 0.46069809794425964\n",
      "Epoch: 91550 | Loss: 0.44751521944999695 | Test loss: 0.46068426966667175\n",
      "Epoch: 91560 | Loss: 0.44750338792800903 | Test loss: 0.4606703817844391\n",
      "Epoch: 91570 | Loss: 0.4474916458129883 | Test loss: 0.4606565535068512\n",
      "Epoch: 91580 | Loss: 0.4474799335002899 | Test loss: 0.4606427252292633\n",
      "Epoch: 91590 | Loss: 0.447468101978302 | Test loss: 0.46062883734703064\n",
      "Epoch: 91600 | Loss: 0.44745635986328125 | Test loss: 0.46061500906944275\n",
      "Epoch: 91610 | Loss: 0.4474446475505829 | Test loss: 0.46060115098953247\n",
      "Epoch: 91620 | Loss: 0.44743281602859497 | Test loss: 0.4605873227119446\n",
      "Epoch: 91630 | Loss: 0.4474210739135742 | Test loss: 0.4605734944343567\n",
      "Epoch: 91640 | Loss: 0.44740936160087585 | Test loss: 0.460559606552124\n",
      "Epoch: 91650 | Loss: 0.4473975598812103 | Test loss: 0.46054577827453613\n",
      "Epoch: 91660 | Loss: 0.4473857879638672 | Test loss: 0.46053192019462585\n",
      "Epoch: 91670 | Loss: 0.4473740756511688 | Test loss: 0.46051809191703796\n",
      "Epoch: 91680 | Loss: 0.44736233353614807 | Test loss: 0.4605042636394501\n",
      "Epoch: 91690 | Loss: 0.44735050201416016 | Test loss: 0.4604903757572174\n",
      "Epoch: 91700 | Loss: 0.4473387897014618 | Test loss: 0.4604765474796295\n",
      "Epoch: 91710 | Loss: 0.44732698798179626 | Test loss: 0.46046265959739685\n",
      "Epoch: 91720 | Loss: 0.4473152160644531 | Test loss: 0.46044883131980896\n",
      "Epoch: 91730 | Loss: 0.44730350375175476 | Test loss: 0.46043500304222107\n",
      "Epoch: 91740 | Loss: 0.44729170203208923 | Test loss: 0.4604211747646332\n",
      "Epoch: 91750 | Loss: 0.4472799301147461 | Test loss: 0.4604073166847229\n",
      "Epoch: 91760 | Loss: 0.44726821780204773 | Test loss: 0.460393488407135\n",
      "Epoch: 91770 | Loss: 0.447256475687027 | Test loss: 0.46037960052490234\n",
      "Epoch: 91780 | Loss: 0.44724470376968384 | Test loss: 0.46036577224731445\n",
      "Epoch: 91790 | Loss: 0.4472329318523407 | Test loss: 0.46035194396972656\n",
      "Epoch: 91800 | Loss: 0.44722118973731995 | Test loss: 0.4603380858898163\n",
      "Epoch: 91810 | Loss: 0.4472094178199768 | Test loss: 0.4603242576122284\n",
      "Epoch: 91820 | Loss: 0.44719764590263367 | Test loss: 0.4603103697299957\n",
      "Epoch: 91830 | Loss: 0.4471859037876129 | Test loss: 0.46029654145240784\n",
      "Epoch: 91840 | Loss: 0.4471741318702698 | Test loss: 0.46028271317481995\n",
      "Epoch: 91850 | Loss: 0.44716235995292664 | Test loss: 0.4602688252925873\n",
      "Epoch: 91860 | Loss: 0.4471506178379059 | Test loss: 0.4602549970149994\n",
      "Epoch: 91870 | Loss: 0.44713884592056274 | Test loss: 0.4602411389350891\n",
      "Epoch: 91880 | Loss: 0.4471270740032196 | Test loss: 0.4602273106575012\n",
      "Epoch: 91890 | Loss: 0.44711533188819885 | Test loss: 0.46021348237991333\n",
      "Epoch: 91900 | Loss: 0.4471035897731781 | Test loss: 0.46019959449768066\n",
      "Epoch: 91910 | Loss: 0.4470917880535126 | Test loss: 0.4601857662200928\n",
      "Epoch: 91920 | Loss: 0.4470800459384918 | Test loss: 0.4601719379425049\n",
      "Epoch: 91930 | Loss: 0.4470682740211487 | Test loss: 0.4601580798625946\n",
      "Epoch: 91940 | Loss: 0.44705653190612793 | Test loss: 0.4601442515850067\n",
      "Epoch: 91950 | Loss: 0.4470447599887848 | Test loss: 0.4601304233074188\n",
      "Epoch: 91960 | Loss: 0.44703301787376404 | Test loss: 0.46011653542518616\n",
      "Epoch: 91970 | Loss: 0.4470212459564209 | Test loss: 0.46010270714759827\n",
      "Epoch: 91980 | Loss: 0.44700947403907776 | Test loss: 0.4600888192653656\n",
      "Epoch: 91990 | Loss: 0.446997731924057 | Test loss: 0.4600749909877777\n",
      "Epoch: 92000 | Loss: 0.44698596000671387 | Test loss: 0.4600611627101898\n",
      "Epoch: 92010 | Loss: 0.4469742476940155 | Test loss: 0.46004733443260193\n",
      "Epoch: 92020 | Loss: 0.44696244597435 | Test loss: 0.46003347635269165\n",
      "Epoch: 92030 | Loss: 0.44695067405700684 | Test loss: 0.460019588470459\n",
      "Epoch: 92040 | Loss: 0.44693896174430847 | Test loss: 0.4600057601928711\n",
      "Epoch: 92050 | Loss: 0.44692716002464294 | Test loss: 0.4599919319152832\n",
      "Epoch: 92060 | Loss: 0.4469154477119446 | Test loss: 0.4599780738353729\n",
      "Epoch: 92070 | Loss: 0.44690361618995667 | Test loss: 0.45996424555778503\n",
      "Epoch: 92080 | Loss: 0.4468918740749359 | Test loss: 0.45995035767555237\n",
      "Epoch: 92090 | Loss: 0.4468801021575928 | Test loss: 0.4599365293979645\n",
      "Epoch: 92100 | Loss: 0.44686833024024963 | Test loss: 0.4599227011203766\n",
      "Epoch: 92110 | Loss: 0.4468565881252289 | Test loss: 0.4599088728427887\n",
      "Epoch: 92120 | Loss: 0.4468448758125305 | Test loss: 0.45989498496055603\n",
      "Epoch: 92130 | Loss: 0.4468331038951874 | Test loss: 0.45988115668296814\n",
      "Epoch: 92140 | Loss: 0.44682130217552185 | Test loss: 0.45986729860305786\n",
      "Epoch: 92150 | Loss: 0.4468095898628235 | Test loss: 0.45985347032546997\n",
      "Epoch: 92160 | Loss: 0.44679781794548035 | Test loss: 0.4598396420478821\n",
      "Epoch: 92170 | Loss: 0.4467860162258148 | Test loss: 0.4598258137702942\n",
      "Epoch: 92180 | Loss: 0.44677430391311646 | Test loss: 0.4598119258880615\n",
      "Epoch: 92190 | Loss: 0.4467625319957733 | Test loss: 0.45979809761047363\n",
      "Epoch: 92200 | Loss: 0.4467507302761078 | Test loss: 0.45978423953056335\n",
      "Epoch: 92210 | Loss: 0.4467390179634094 | Test loss: 0.45977041125297546\n",
      "Epoch: 92220 | Loss: 0.4467272460460663 | Test loss: 0.4597565233707428\n",
      "Epoch: 92230 | Loss: 0.44671550393104553 | Test loss: 0.4597426950931549\n",
      "Epoch: 92240 | Loss: 0.4467037320137024 | Test loss: 0.45972880721092224\n",
      "Epoch: 92250 | Loss: 0.44669198989868164 | Test loss: 0.45971497893333435\n",
      "Epoch: 92260 | Loss: 0.4466801583766937 | Test loss: 0.45970115065574646\n",
      "Epoch: 92270 | Loss: 0.44666844606399536 | Test loss: 0.45968732237815857\n",
      "Epoch: 92280 | Loss: 0.4466567039489746 | Test loss: 0.4596734642982483\n",
      "Epoch: 92290 | Loss: 0.44664493203163147 | Test loss: 0.4596596360206604\n",
      "Epoch: 92300 | Loss: 0.44663316011428833 | Test loss: 0.45964574813842773\n",
      "Epoch: 92310 | Loss: 0.4466214179992676 | Test loss: 0.45963191986083984\n",
      "Epoch: 92320 | Loss: 0.44660964608192444 | Test loss: 0.45961809158325195\n",
      "Epoch: 92330 | Loss: 0.4465978741645813 | Test loss: 0.45960426330566406\n",
      "Epoch: 92340 | Loss: 0.44658613204956055 | Test loss: 0.4595904052257538\n",
      "Epoch: 92350 | Loss: 0.4465744197368622 | Test loss: 0.4595765769481659\n",
      "Epoch: 92360 | Loss: 0.44656258821487427 | Test loss: 0.4595626890659332\n",
      "Epoch: 92370 | Loss: 0.4465508460998535 | Test loss: 0.45954886078834534\n",
      "Epoch: 92380 | Loss: 0.44653913378715515 | Test loss: 0.45953503251075745\n",
      "Epoch: 92390 | Loss: 0.44652730226516724 | Test loss: 0.4595211446285248\n",
      "Epoch: 92400 | Loss: 0.4465155601501465 | Test loss: 0.4595073163509369\n",
      "Epoch: 92410 | Loss: 0.4465038478374481 | Test loss: 0.4594934582710266\n",
      "Epoch: 92420 | Loss: 0.4464920163154602 | Test loss: 0.4594796299934387\n",
      "Epoch: 92430 | Loss: 0.44648027420043945 | Test loss: 0.45946580171585083\n",
      "Epoch: 92440 | Loss: 0.4464685618877411 | Test loss: 0.45945191383361816\n",
      "Epoch: 92450 | Loss: 0.44645676016807556 | Test loss: 0.4594380855560303\n",
      "Epoch: 92460 | Loss: 0.4464449882507324 | Test loss: 0.45942422747612\n",
      "Epoch: 92470 | Loss: 0.44643327593803406 | Test loss: 0.4594103991985321\n",
      "Epoch: 92480 | Loss: 0.4464215338230133 | Test loss: 0.4593965709209442\n",
      "Epoch: 92490 | Loss: 0.4464097023010254 | Test loss: 0.45938268303871155\n",
      "Epoch: 92500 | Loss: 0.446397989988327 | Test loss: 0.45936885476112366\n",
      "Epoch: 92510 | Loss: 0.4463861882686615 | Test loss: 0.459354966878891\n",
      "Epoch: 92520 | Loss: 0.44637441635131836 | Test loss: 0.4593411386013031\n",
      "Epoch: 92530 | Loss: 0.44636270403862 | Test loss: 0.4593273103237152\n",
      "Epoch: 92540 | Loss: 0.44635090231895447 | Test loss: 0.4593134820461273\n",
      "Epoch: 92550 | Loss: 0.44633913040161133 | Test loss: 0.45929962396621704\n",
      "Epoch: 92560 | Loss: 0.44632741808891296 | Test loss: 0.45928579568862915\n",
      "Epoch: 92570 | Loss: 0.4463156759738922 | Test loss: 0.4592719078063965\n",
      "Epoch: 92580 | Loss: 0.4463039040565491 | Test loss: 0.4592580795288086\n",
      "Epoch: 92590 | Loss: 0.44629213213920593 | Test loss: 0.4592442512512207\n",
      "Epoch: 92600 | Loss: 0.4462803900241852 | Test loss: 0.4592303931713104\n",
      "Epoch: 92610 | Loss: 0.44626861810684204 | Test loss: 0.45921656489372253\n",
      "Epoch: 92620 | Loss: 0.4462568461894989 | Test loss: 0.45920267701148987\n",
      "Epoch: 92630 | Loss: 0.44624510407447815 | Test loss: 0.459188848733902\n",
      "Epoch: 92640 | Loss: 0.446233332157135 | Test loss: 0.4591750204563141\n",
      "Epoch: 92650 | Loss: 0.44622156023979187 | Test loss: 0.4591611325740814\n",
      "Epoch: 92660 | Loss: 0.4462098181247711 | Test loss: 0.45914730429649353\n",
      "Epoch: 92670 | Loss: 0.446198046207428 | Test loss: 0.45913344621658325\n",
      "Epoch: 92680 | Loss: 0.44618627429008484 | Test loss: 0.45911961793899536\n",
      "Epoch: 92690 | Loss: 0.4461745321750641 | Test loss: 0.45910578966140747\n",
      "Epoch: 92700 | Loss: 0.44616279006004333 | Test loss: 0.4590919017791748\n",
      "Epoch: 92710 | Loss: 0.4461509883403778 | Test loss: 0.4590780735015869\n",
      "Epoch: 92720 | Loss: 0.44613924622535706 | Test loss: 0.459064245223999\n",
      "Epoch: 92730 | Loss: 0.4461274743080139 | Test loss: 0.45905038714408875\n",
      "Epoch: 92740 | Loss: 0.44611573219299316 | Test loss: 0.45903655886650085\n",
      "Epoch: 92750 | Loss: 0.44610396027565 | Test loss: 0.45902273058891296\n",
      "Epoch: 92760 | Loss: 0.4460922181606293 | Test loss: 0.4590088427066803\n",
      "Epoch: 92770 | Loss: 0.44608044624328613 | Test loss: 0.4589950144290924\n",
      "Epoch: 92780 | Loss: 0.446068674325943 | Test loss: 0.45898112654685974\n",
      "Epoch: 92790 | Loss: 0.44605693221092224 | Test loss: 0.45896729826927185\n",
      "Epoch: 92800 | Loss: 0.4460451602935791 | Test loss: 0.45895346999168396\n",
      "Epoch: 92810 | Loss: 0.44603344798088074 | Test loss: 0.45893964171409607\n",
      "Epoch: 92820 | Loss: 0.4460216462612152 | Test loss: 0.4589257836341858\n",
      "Epoch: 92830 | Loss: 0.44600987434387207 | Test loss: 0.4589118957519531\n",
      "Epoch: 92840 | Loss: 0.4459981620311737 | Test loss: 0.45889806747436523\n",
      "Epoch: 92850 | Loss: 0.4459863603115082 | Test loss: 0.45888423919677734\n",
      "Epoch: 92860 | Loss: 0.4459746479988098 | Test loss: 0.45887038111686707\n",
      "Epoch: 92870 | Loss: 0.4459628164768219 | Test loss: 0.4588565528392792\n",
      "Epoch: 92880 | Loss: 0.44595107436180115 | Test loss: 0.4588426649570465\n",
      "Epoch: 92890 | Loss: 0.445939302444458 | Test loss: 0.4588288366794586\n",
      "Epoch: 92900 | Loss: 0.44592753052711487 | Test loss: 0.4588150084018707\n",
      "Epoch: 92910 | Loss: 0.4459157884120941 | Test loss: 0.45880118012428284\n",
      "Epoch: 92920 | Loss: 0.44590407609939575 | Test loss: 0.45878729224205017\n",
      "Epoch: 92930 | Loss: 0.4458923041820526 | Test loss: 0.4587734639644623\n",
      "Epoch: 92940 | Loss: 0.4458805024623871 | Test loss: 0.458759605884552\n",
      "Epoch: 92950 | Loss: 0.4458687901496887 | Test loss: 0.4587457776069641\n",
      "Epoch: 92960 | Loss: 0.4458570182323456 | Test loss: 0.4587319493293762\n",
      "Epoch: 92970 | Loss: 0.44584521651268005 | Test loss: 0.45871812105178833\n",
      "Epoch: 92980 | Loss: 0.4458335041999817 | Test loss: 0.45870423316955566\n",
      "Epoch: 92990 | Loss: 0.44582173228263855 | Test loss: 0.4586904048919678\n",
      "Epoch: 93000 | Loss: 0.445809930562973 | Test loss: 0.4586765468120575\n",
      "Epoch: 93010 | Loss: 0.44579821825027466 | Test loss: 0.4586627185344696\n",
      "Epoch: 93020 | Loss: 0.4457864463329315 | Test loss: 0.45864883065223694\n",
      "Epoch: 93030 | Loss: 0.44577470421791077 | Test loss: 0.45863500237464905\n",
      "Epoch: 93040 | Loss: 0.4457629323005676 | Test loss: 0.4586211144924164\n",
      "Epoch: 93050 | Loss: 0.4457511901855469 | Test loss: 0.4586072862148285\n",
      "Epoch: 93060 | Loss: 0.44573935866355896 | Test loss: 0.4585934579372406\n",
      "Epoch: 93070 | Loss: 0.4457276463508606 | Test loss: 0.4585796296596527\n",
      "Epoch: 93080 | Loss: 0.44571590423583984 | Test loss: 0.45856577157974243\n",
      "Epoch: 93090 | Loss: 0.4457041323184967 | Test loss: 0.45855194330215454\n",
      "Epoch: 93100 | Loss: 0.44569236040115356 | Test loss: 0.4585380554199219\n",
      "Epoch: 93110 | Loss: 0.4456806182861328 | Test loss: 0.458524227142334\n",
      "Epoch: 93120 | Loss: 0.4456688463687897 | Test loss: 0.4585103988647461\n",
      "Epoch: 93130 | Loss: 0.44565707445144653 | Test loss: 0.4584965705871582\n",
      "Epoch: 93140 | Loss: 0.4456453323364258 | Test loss: 0.4584827125072479\n",
      "Epoch: 93150 | Loss: 0.4456336200237274 | Test loss: 0.45846888422966003\n",
      "Epoch: 93160 | Loss: 0.4456217885017395 | Test loss: 0.45845499634742737\n",
      "Epoch: 93170 | Loss: 0.44561004638671875 | Test loss: 0.4584411680698395\n",
      "Epoch: 93180 | Loss: 0.4455983340740204 | Test loss: 0.4584273397922516\n",
      "Epoch: 93190 | Loss: 0.44558650255203247 | Test loss: 0.4584134519100189\n",
      "Epoch: 93200 | Loss: 0.4455747604370117 | Test loss: 0.45839962363243103\n",
      "Epoch: 93210 | Loss: 0.44556304812431335 | Test loss: 0.45838576555252075\n",
      "Epoch: 93220 | Loss: 0.44555121660232544 | Test loss: 0.45837193727493286\n",
      "Epoch: 93230 | Loss: 0.4455394744873047 | Test loss: 0.45835810899734497\n",
      "Epoch: 93240 | Loss: 0.4455277621746063 | Test loss: 0.4583442211151123\n",
      "Epoch: 93250 | Loss: 0.4455159604549408 | Test loss: 0.4583303928375244\n",
      "Epoch: 93260 | Loss: 0.44550418853759766 | Test loss: 0.45831653475761414\n",
      "Epoch: 93270 | Loss: 0.4454924762248993 | Test loss: 0.45830270648002625\n",
      "Epoch: 93280 | Loss: 0.44548073410987854 | Test loss: 0.45828887820243835\n",
      "Epoch: 93290 | Loss: 0.4454689025878906 | Test loss: 0.4582749903202057\n",
      "Epoch: 93300 | Loss: 0.44545719027519226 | Test loss: 0.4582611620426178\n",
      "Epoch: 93310 | Loss: 0.44544538855552673 | Test loss: 0.45824727416038513\n",
      "Epoch: 93320 | Loss: 0.4454336166381836 | Test loss: 0.45823344588279724\n",
      "Epoch: 93330 | Loss: 0.44542190432548523 | Test loss: 0.45821961760520935\n",
      "Epoch: 93340 | Loss: 0.4454101026058197 | Test loss: 0.45820578932762146\n",
      "Epoch: 93350 | Loss: 0.44539833068847656 | Test loss: 0.4581919312477112\n",
      "Epoch: 93360 | Loss: 0.4453866183757782 | Test loss: 0.4581781029701233\n",
      "Epoch: 93370 | Loss: 0.44537487626075745 | Test loss: 0.4581642150878906\n",
      "Epoch: 93380 | Loss: 0.4453631043434143 | Test loss: 0.45815038681030273\n",
      "Epoch: 93390 | Loss: 0.44535133242607117 | Test loss: 0.45813655853271484\n",
      "Epoch: 93400 | Loss: 0.4453395903110504 | Test loss: 0.45812270045280457\n",
      "Epoch: 93410 | Loss: 0.4453278183937073 | Test loss: 0.4581088721752167\n",
      "Epoch: 93420 | Loss: 0.44531604647636414 | Test loss: 0.458094984292984\n",
      "Epoch: 93430 | Loss: 0.4453043043613434 | Test loss: 0.4580811560153961\n",
      "Epoch: 93440 | Loss: 0.44529253244400024 | Test loss: 0.4580673277378082\n",
      "Epoch: 93450 | Loss: 0.4452807605266571 | Test loss: 0.45805343985557556\n",
      "Epoch: 93460 | Loss: 0.44526901841163635 | Test loss: 0.45803961157798767\n",
      "Epoch: 93470 | Loss: 0.4452572464942932 | Test loss: 0.4580257534980774\n",
      "Epoch: 93480 | Loss: 0.4452454745769501 | Test loss: 0.4580119252204895\n",
      "Epoch: 93490 | Loss: 0.4452337324619293 | Test loss: 0.4579980969429016\n",
      "Epoch: 93500 | Loss: 0.44522199034690857 | Test loss: 0.45798420906066895\n",
      "Epoch: 93510 | Loss: 0.44521018862724304 | Test loss: 0.45797038078308105\n",
      "Epoch: 93520 | Loss: 0.4451984465122223 | Test loss: 0.45795655250549316\n",
      "Epoch: 93530 | Loss: 0.44518667459487915 | Test loss: 0.4579426944255829\n",
      "Epoch: 93540 | Loss: 0.4451749324798584 | Test loss: 0.457928866147995\n",
      "Epoch: 93550 | Loss: 0.44516316056251526 | Test loss: 0.4579150378704071\n",
      "Epoch: 93560 | Loss: 0.4451514184474945 | Test loss: 0.45790114998817444\n",
      "Epoch: 93570 | Loss: 0.44513964653015137 | Test loss: 0.45788732171058655\n",
      "Epoch: 93580 | Loss: 0.4451278746128082 | Test loss: 0.4578734338283539\n",
      "Epoch: 93590 | Loss: 0.4451161324977875 | Test loss: 0.457859605550766\n",
      "Epoch: 93600 | Loss: 0.44510436058044434 | Test loss: 0.4578457772731781\n",
      "Epoch: 93610 | Loss: 0.44509264826774597 | Test loss: 0.4578319489955902\n",
      "Epoch: 93620 | Loss: 0.44508084654808044 | Test loss: 0.45781809091567993\n",
      "Epoch: 93630 | Loss: 0.4450690746307373 | Test loss: 0.45780420303344727\n",
      "Epoch: 93640 | Loss: 0.44505736231803894 | Test loss: 0.4577903747558594\n",
      "Epoch: 93650 | Loss: 0.4450455605983734 | Test loss: 0.4577765464782715\n",
      "Epoch: 93660 | Loss: 0.44503384828567505 | Test loss: 0.4577626883983612\n",
      "Epoch: 93670 | Loss: 0.44502201676368713 | Test loss: 0.4577488601207733\n",
      "Epoch: 93680 | Loss: 0.4450102746486664 | Test loss: 0.45773497223854065\n",
      "Epoch: 93690 | Loss: 0.44499850273132324 | Test loss: 0.45772114396095276\n",
      "Epoch: 93700 | Loss: 0.4449867308139801 | Test loss: 0.45770731568336487\n",
      "Epoch: 93710 | Loss: 0.44497498869895935 | Test loss: 0.457693487405777\n",
      "Epoch: 93720 | Loss: 0.444963276386261 | Test loss: 0.4576795995235443\n",
      "Epoch: 93730 | Loss: 0.44495150446891785 | Test loss: 0.4576657712459564\n",
      "Epoch: 93740 | Loss: 0.4449397027492523 | Test loss: 0.45765191316604614\n",
      "Epoch: 93750 | Loss: 0.44492799043655396 | Test loss: 0.45763808488845825\n",
      "Epoch: 93760 | Loss: 0.4449162185192108 | Test loss: 0.45762425661087036\n",
      "Epoch: 93770 | Loss: 0.4449044167995453 | Test loss: 0.45761042833328247\n",
      "Epoch: 93780 | Loss: 0.4448927044868469 | Test loss: 0.4575965404510498\n",
      "Epoch: 93790 | Loss: 0.4448809325695038 | Test loss: 0.4575827121734619\n",
      "Epoch: 93800 | Loss: 0.44486913084983826 | Test loss: 0.45756885409355164\n",
      "Epoch: 93810 | Loss: 0.4448574185371399 | Test loss: 0.45755502581596375\n",
      "Epoch: 93820 | Loss: 0.44484564661979675 | Test loss: 0.4575411379337311\n",
      "Epoch: 93830 | Loss: 0.444833904504776 | Test loss: 0.4575273096561432\n",
      "Epoch: 93840 | Loss: 0.44482213258743286 | Test loss: 0.4575134217739105\n",
      "Epoch: 93850 | Loss: 0.4448103904724121 | Test loss: 0.45749959349632263\n",
      "Epoch: 93860 | Loss: 0.4447985589504242 | Test loss: 0.45748576521873474\n",
      "Epoch: 93870 | Loss: 0.44478684663772583 | Test loss: 0.45747193694114685\n",
      "Epoch: 93880 | Loss: 0.4447751045227051 | Test loss: 0.4574580788612366\n",
      "Epoch: 93890 | Loss: 0.44476333260536194 | Test loss: 0.4574442505836487\n",
      "Epoch: 93900 | Loss: 0.4447515606880188 | Test loss: 0.457430362701416\n",
      "Epoch: 93910 | Loss: 0.44473981857299805 | Test loss: 0.4574165344238281\n",
      "Epoch: 93920 | Loss: 0.4447280466556549 | Test loss: 0.45740270614624023\n",
      "Epoch: 93930 | Loss: 0.44471627473831177 | Test loss: 0.45738887786865234\n",
      "Epoch: 93940 | Loss: 0.444704532623291 | Test loss: 0.45737501978874207\n",
      "Epoch: 93950 | Loss: 0.44469282031059265 | Test loss: 0.4573611915111542\n",
      "Epoch: 93960 | Loss: 0.44468098878860474 | Test loss: 0.4573473036289215\n",
      "Epoch: 93970 | Loss: 0.444669246673584 | Test loss: 0.4573334753513336\n",
      "Epoch: 93980 | Loss: 0.4446575343608856 | Test loss: 0.4573196470737457\n",
      "Epoch: 93990 | Loss: 0.4446457028388977 | Test loss: 0.45730575919151306\n",
      "Epoch: 94000 | Loss: 0.44463396072387695 | Test loss: 0.45729193091392517\n",
      "Epoch: 94010 | Loss: 0.4446222484111786 | Test loss: 0.4572780728340149\n",
      "Epoch: 94020 | Loss: 0.4446104168891907 | Test loss: 0.457264244556427\n",
      "Epoch: 94030 | Loss: 0.4445986747741699 | Test loss: 0.4572504162788391\n",
      "Epoch: 94040 | Loss: 0.44458696246147156 | Test loss: 0.45723652839660645\n",
      "Epoch: 94050 | Loss: 0.44457516074180603 | Test loss: 0.45722270011901855\n",
      "Epoch: 94060 | Loss: 0.4445633888244629 | Test loss: 0.4572088420391083\n",
      "Epoch: 94070 | Loss: 0.4445516765117645 | Test loss: 0.4571950137615204\n",
      "Epoch: 94080 | Loss: 0.4445399343967438 | Test loss: 0.4571811854839325\n",
      "Epoch: 94090 | Loss: 0.44452810287475586 | Test loss: 0.45716729760169983\n",
      "Epoch: 94100 | Loss: 0.4445163905620575 | Test loss: 0.45715346932411194\n",
      "Epoch: 94110 | Loss: 0.44450458884239197 | Test loss: 0.4571395814418793\n",
      "Epoch: 94120 | Loss: 0.44449281692504883 | Test loss: 0.4571257531642914\n",
      "Epoch: 94130 | Loss: 0.44448110461235046 | Test loss: 0.4571119248867035\n",
      "Epoch: 94140 | Loss: 0.44446930289268494 | Test loss: 0.4570980966091156\n",
      "Epoch: 94150 | Loss: 0.4444575309753418 | Test loss: 0.4570842385292053\n",
      "Epoch: 94160 | Loss: 0.44444581866264343 | Test loss: 0.45707041025161743\n",
      "Epoch: 94170 | Loss: 0.4444340765476227 | Test loss: 0.45705652236938477\n",
      "Epoch: 94180 | Loss: 0.44442230463027954 | Test loss: 0.4570426940917969\n",
      "Epoch: 94190 | Loss: 0.4444105327129364 | Test loss: 0.457028865814209\n",
      "Epoch: 94200 | Loss: 0.44439879059791565 | Test loss: 0.4570150077342987\n",
      "Epoch: 94210 | Loss: 0.4443870186805725 | Test loss: 0.4570011794567108\n",
      "Epoch: 94220 | Loss: 0.44437524676322937 | Test loss: 0.45698729157447815\n",
      "Epoch: 94230 | Loss: 0.4443635046482086 | Test loss: 0.45697346329689026\n",
      "Epoch: 94240 | Loss: 0.4443517327308655 | Test loss: 0.45695963501930237\n",
      "Epoch: 94250 | Loss: 0.44433996081352234 | Test loss: 0.4569457471370697\n",
      "Epoch: 94260 | Loss: 0.4443282186985016 | Test loss: 0.4569319188594818\n",
      "Epoch: 94270 | Loss: 0.44431644678115845 | Test loss: 0.45691806077957153\n",
      "Epoch: 94280 | Loss: 0.4443046748638153 | Test loss: 0.45690423250198364\n",
      "Epoch: 94290 | Loss: 0.44429293274879456 | Test loss: 0.45689040422439575\n",
      "Epoch: 94300 | Loss: 0.4442811906337738 | Test loss: 0.4568765163421631\n",
      "Epoch: 94310 | Loss: 0.4442693889141083 | Test loss: 0.4568626880645752\n",
      "Epoch: 94320 | Loss: 0.4442576467990875 | Test loss: 0.4568488597869873\n",
      "Epoch: 94330 | Loss: 0.4442458748817444 | Test loss: 0.456835001707077\n",
      "Epoch: 94340 | Loss: 0.44423413276672363 | Test loss: 0.45682117342948914\n",
      "Epoch: 94350 | Loss: 0.4442223608493805 | Test loss: 0.45680734515190125\n",
      "Epoch: 94360 | Loss: 0.44421061873435974 | Test loss: 0.4567934572696686\n",
      "Epoch: 94370 | Loss: 0.4441988468170166 | Test loss: 0.4567796289920807\n",
      "Epoch: 94380 | Loss: 0.44418707489967346 | Test loss: 0.456765741109848\n",
      "Epoch: 94390 | Loss: 0.4441753327846527 | Test loss: 0.45675191283226013\n",
      "Epoch: 94400 | Loss: 0.44416356086730957 | Test loss: 0.45673808455467224\n",
      "Epoch: 94410 | Loss: 0.4441518485546112 | Test loss: 0.45672425627708435\n",
      "Epoch: 94420 | Loss: 0.4441400468349457 | Test loss: 0.4567103981971741\n",
      "Epoch: 94430 | Loss: 0.44412827491760254 | Test loss: 0.4566965103149414\n",
      "Epoch: 94440 | Loss: 0.4441165626049042 | Test loss: 0.4566826820373535\n",
      "Epoch: 94450 | Loss: 0.44410476088523865 | Test loss: 0.4566688537597656\n",
      "Epoch: 94460 | Loss: 0.4440930485725403 | Test loss: 0.45665499567985535\n",
      "Epoch: 94470 | Loss: 0.44408121705055237 | Test loss: 0.45664116740226746\n",
      "Epoch: 94480 | Loss: 0.4440694749355316 | Test loss: 0.4566272795200348\n",
      "Epoch: 94490 | Loss: 0.4440577030181885 | Test loss: 0.4566134512424469\n",
      "Epoch: 94500 | Loss: 0.44404593110084534 | Test loss: 0.456599622964859\n",
      "Epoch: 94510 | Loss: 0.4440341889858246 | Test loss: 0.4565857946872711\n",
      "Epoch: 94520 | Loss: 0.4440224766731262 | Test loss: 0.45657190680503845\n",
      "Epoch: 94530 | Loss: 0.4440107047557831 | Test loss: 0.45655807852745056\n",
      "Epoch: 94540 | Loss: 0.44399890303611755 | Test loss: 0.4565442204475403\n",
      "Epoch: 94550 | Loss: 0.4439871907234192 | Test loss: 0.4565303921699524\n",
      "Epoch: 94560 | Loss: 0.44397541880607605 | Test loss: 0.4565165638923645\n",
      "Epoch: 94570 | Loss: 0.4439636170864105 | Test loss: 0.4565027356147766\n",
      "Epoch: 94580 | Loss: 0.44395190477371216 | Test loss: 0.45648884773254395\n",
      "Epoch: 94590 | Loss: 0.443940132856369 | Test loss: 0.45647501945495605\n",
      "Epoch: 94600 | Loss: 0.4439283311367035 | Test loss: 0.4564611613750458\n",
      "Epoch: 94610 | Loss: 0.4439166188240051 | Test loss: 0.4564473330974579\n",
      "Epoch: 94620 | Loss: 0.443904846906662 | Test loss: 0.4564334452152252\n",
      "Epoch: 94630 | Loss: 0.44389310479164124 | Test loss: 0.45641961693763733\n",
      "Epoch: 94640 | Loss: 0.4438813328742981 | Test loss: 0.45640572905540466\n",
      "Epoch: 94650 | Loss: 0.44386959075927734 | Test loss: 0.4563919007778168\n",
      "Epoch: 94660 | Loss: 0.44385775923728943 | Test loss: 0.4563780725002289\n",
      "Epoch: 94670 | Loss: 0.44384604692459106 | Test loss: 0.456364244222641\n",
      "Epoch: 94680 | Loss: 0.4438343048095703 | Test loss: 0.4563503861427307\n",
      "Epoch: 94690 | Loss: 0.4438225328922272 | Test loss: 0.4563365578651428\n",
      "Epoch: 94700 | Loss: 0.44381076097488403 | Test loss: 0.45632266998291016\n",
      "Epoch: 94710 | Loss: 0.4437990188598633 | Test loss: 0.45630884170532227\n",
      "Epoch: 94720 | Loss: 0.44378724694252014 | Test loss: 0.4562950134277344\n",
      "Epoch: 94730 | Loss: 0.443775475025177 | Test loss: 0.4562811851501465\n",
      "Epoch: 94740 | Loss: 0.44376373291015625 | Test loss: 0.4562673270702362\n",
      "Epoch: 94750 | Loss: 0.4437520205974579 | Test loss: 0.4562534987926483\n",
      "Epoch: 94760 | Loss: 0.44374018907546997 | Test loss: 0.45623961091041565\n",
      "Epoch: 94770 | Loss: 0.4437284469604492 | Test loss: 0.45622578263282776\n",
      "Epoch: 94780 | Loss: 0.44371673464775085 | Test loss: 0.45621195435523987\n",
      "Epoch: 94790 | Loss: 0.44370490312576294 | Test loss: 0.4561980664730072\n",
      "Epoch: 94800 | Loss: 0.4436931610107422 | Test loss: 0.4561842381954193\n",
      "Epoch: 94810 | Loss: 0.4436814486980438 | Test loss: 0.45617038011550903\n",
      "Epoch: 94820 | Loss: 0.4436696171760559 | Test loss: 0.45615655183792114\n",
      "Epoch: 94830 | Loss: 0.44365787506103516 | Test loss: 0.45614272356033325\n",
      "Epoch: 94840 | Loss: 0.4436461627483368 | Test loss: 0.4561288356781006\n",
      "Epoch: 94850 | Loss: 0.44363436102867126 | Test loss: 0.4561150074005127\n",
      "Epoch: 94860 | Loss: 0.4436225891113281 | Test loss: 0.4561011493206024\n",
      "Epoch: 94870 | Loss: 0.44361087679862976 | Test loss: 0.4560873210430145\n",
      "Epoch: 94880 | Loss: 0.443599134683609 | Test loss: 0.45607349276542664\n",
      "Epoch: 94890 | Loss: 0.4435873031616211 | Test loss: 0.45605960488319397\n",
      "Epoch: 94900 | Loss: 0.44357559084892273 | Test loss: 0.4560457766056061\n",
      "Epoch: 94910 | Loss: 0.4435637891292572 | Test loss: 0.4560318887233734\n",
      "Epoch: 94920 | Loss: 0.44355201721191406 | Test loss: 0.4560180604457855\n",
      "Epoch: 94930 | Loss: 0.4435403048992157 | Test loss: 0.45600423216819763\n",
      "Epoch: 94940 | Loss: 0.44352850317955017 | Test loss: 0.45599040389060974\n",
      "Epoch: 94950 | Loss: 0.44351673126220703 | Test loss: 0.45597654581069946\n",
      "Epoch: 94960 | Loss: 0.44350501894950867 | Test loss: 0.4559627175331116\n",
      "Epoch: 94970 | Loss: 0.4434932768344879 | Test loss: 0.4559488296508789\n",
      "Epoch: 94980 | Loss: 0.4434815049171448 | Test loss: 0.455935001373291\n",
      "Epoch: 94990 | Loss: 0.44346973299980164 | Test loss: 0.4559211730957031\n",
      "Epoch: 95000 | Loss: 0.4434579908847809 | Test loss: 0.45590731501579285\n",
      "Epoch: 95010 | Loss: 0.44344621896743774 | Test loss: 0.45589348673820496\n",
      "Epoch: 95020 | Loss: 0.4434344470500946 | Test loss: 0.4558795988559723\n",
      "Epoch: 95030 | Loss: 0.44342270493507385 | Test loss: 0.4558657705783844\n",
      "Epoch: 95040 | Loss: 0.4434109330177307 | Test loss: 0.4558519423007965\n",
      "Epoch: 95050 | Loss: 0.4433991611003876 | Test loss: 0.45583805441856384\n",
      "Epoch: 95060 | Loss: 0.4433874189853668 | Test loss: 0.45582422614097595\n",
      "Epoch: 95070 | Loss: 0.4433756470680237 | Test loss: 0.4558103680610657\n",
      "Epoch: 95080 | Loss: 0.44336387515068054 | Test loss: 0.4557965397834778\n",
      "Epoch: 95090 | Loss: 0.4433521330356598 | Test loss: 0.4557827115058899\n",
      "Epoch: 95100 | Loss: 0.44334039092063904 | Test loss: 0.4557688236236572\n",
      "Epoch: 95110 | Loss: 0.4433285892009735 | Test loss: 0.45575499534606934\n",
      "Epoch: 95120 | Loss: 0.44331684708595276 | Test loss: 0.45574116706848145\n",
      "Epoch: 95130 | Loss: 0.4433050751686096 | Test loss: 0.45572730898857117\n",
      "Epoch: 95140 | Loss: 0.44329333305358887 | Test loss: 0.4557134807109833\n",
      "Epoch: 95150 | Loss: 0.4432815611362457 | Test loss: 0.4556996524333954\n",
      "Epoch: 95160 | Loss: 0.443269819021225 | Test loss: 0.4556857645511627\n",
      "Epoch: 95170 | Loss: 0.44325804710388184 | Test loss: 0.45567193627357483\n",
      "Epoch: 95180 | Loss: 0.4432462751865387 | Test loss: 0.45565804839134216\n",
      "Epoch: 95190 | Loss: 0.44323453307151794 | Test loss: 0.4556442201137543\n",
      "Epoch: 95200 | Loss: 0.4432227611541748 | Test loss: 0.4556303918361664\n",
      "Epoch: 95210 | Loss: 0.44321104884147644 | Test loss: 0.4556165635585785\n",
      "Epoch: 95220 | Loss: 0.4431992471218109 | Test loss: 0.4556027054786682\n",
      "Epoch: 95230 | Loss: 0.4431874752044678 | Test loss: 0.45558881759643555\n",
      "Epoch: 95240 | Loss: 0.4431757628917694 | Test loss: 0.45557498931884766\n",
      "Epoch: 95250 | Loss: 0.4431639611721039 | Test loss: 0.45556116104125977\n",
      "Epoch: 95260 | Loss: 0.4431522488594055 | Test loss: 0.4555473029613495\n",
      "Epoch: 95270 | Loss: 0.4431404173374176 | Test loss: 0.4555334746837616\n",
      "Epoch: 95280 | Loss: 0.44312867522239685 | Test loss: 0.45551958680152893\n",
      "Epoch: 95290 | Loss: 0.4431169033050537 | Test loss: 0.45550575852394104\n",
      "Epoch: 95300 | Loss: 0.44310513138771057 | Test loss: 0.45549193024635315\n",
      "Epoch: 95310 | Loss: 0.4430933892726898 | Test loss: 0.45547810196876526\n",
      "Epoch: 95320 | Loss: 0.44308167695999146 | Test loss: 0.4554642140865326\n",
      "Epoch: 95330 | Loss: 0.4430699050426483 | Test loss: 0.4554503858089447\n",
      "Epoch: 95340 | Loss: 0.4430581033229828 | Test loss: 0.4554365277290344\n",
      "Epoch: 95350 | Loss: 0.4430463910102844 | Test loss: 0.45542269945144653\n",
      "Epoch: 95360 | Loss: 0.4430346190929413 | Test loss: 0.45540887117385864\n",
      "Epoch: 95370 | Loss: 0.44302281737327576 | Test loss: 0.45539504289627075\n",
      "Epoch: 95380 | Loss: 0.4430111050605774 | Test loss: 0.4553811550140381\n",
      "Epoch: 95390 | Loss: 0.44299933314323425 | Test loss: 0.4553673267364502\n",
      "Epoch: 95400 | Loss: 0.4429875314235687 | Test loss: 0.4553534686565399\n",
      "Epoch: 95410 | Loss: 0.44297581911087036 | Test loss: 0.455339640378952\n",
      "Epoch: 95420 | Loss: 0.4429640471935272 | Test loss: 0.45532575249671936\n",
      "Epoch: 95430 | Loss: 0.44295230507850647 | Test loss: 0.45531192421913147\n",
      "Epoch: 95440 | Loss: 0.44294053316116333 | Test loss: 0.4552980363368988\n",
      "Epoch: 95450 | Loss: 0.4429287910461426 | Test loss: 0.4552842080593109\n",
      "Epoch: 95460 | Loss: 0.44291695952415466 | Test loss: 0.455270379781723\n",
      "Epoch: 95470 | Loss: 0.4429052472114563 | Test loss: 0.45525655150413513\n",
      "Epoch: 95480 | Loss: 0.44289350509643555 | Test loss: 0.45524269342422485\n",
      "Epoch: 95490 | Loss: 0.4428817331790924 | Test loss: 0.45522886514663696\n",
      "Epoch: 95500 | Loss: 0.44286996126174927 | Test loss: 0.4552149772644043\n",
      "Epoch: 95510 | Loss: 0.4428582191467285 | Test loss: 0.4552011489868164\n",
      "Epoch: 95520 | Loss: 0.4428464472293854 | Test loss: 0.4551873207092285\n",
      "Epoch: 95530 | Loss: 0.44283467531204224 | Test loss: 0.4551734924316406\n",
      "Epoch: 95540 | Loss: 0.4428229331970215 | Test loss: 0.45515963435173035\n",
      "Epoch: 95550 | Loss: 0.4428112208843231 | Test loss: 0.45514580607414246\n",
      "Epoch: 95560 | Loss: 0.4427993893623352 | Test loss: 0.4551319181919098\n",
      "Epoch: 95570 | Loss: 0.44278764724731445 | Test loss: 0.4551180899143219\n",
      "Epoch: 95580 | Loss: 0.4427759349346161 | Test loss: 0.455104261636734\n",
      "Epoch: 95590 | Loss: 0.4427641034126282 | Test loss: 0.45509037375450134\n",
      "Epoch: 95600 | Loss: 0.4427523612976074 | Test loss: 0.45507654547691345\n",
      "Epoch: 95610 | Loss: 0.44274064898490906 | Test loss: 0.4550626873970032\n",
      "Epoch: 95620 | Loss: 0.44272881746292114 | Test loss: 0.4550488591194153\n",
      "Epoch: 95630 | Loss: 0.4427170753479004 | Test loss: 0.4550350308418274\n",
      "Epoch: 95640 | Loss: 0.442705363035202 | Test loss: 0.4550211429595947\n",
      "Epoch: 95650 | Loss: 0.4426935613155365 | Test loss: 0.45500731468200684\n",
      "Epoch: 95660 | Loss: 0.44268178939819336 | Test loss: 0.45499345660209656\n",
      "Epoch: 95670 | Loss: 0.442670077085495 | Test loss: 0.45497962832450867\n",
      "Epoch: 95680 | Loss: 0.44265833497047424 | Test loss: 0.4549658000469208\n",
      "Epoch: 95690 | Loss: 0.44264650344848633 | Test loss: 0.4549519121646881\n",
      "Epoch: 95700 | Loss: 0.44263479113578796 | Test loss: 0.4549380838871002\n",
      "Epoch: 95710 | Loss: 0.44262298941612244 | Test loss: 0.45492419600486755\n",
      "Epoch: 95720 | Loss: 0.4426112174987793 | Test loss: 0.45491036772727966\n",
      "Epoch: 95730 | Loss: 0.44259950518608093 | Test loss: 0.4548965394496918\n",
      "Epoch: 95740 | Loss: 0.4425877034664154 | Test loss: 0.4548827111721039\n",
      "Epoch: 95750 | Loss: 0.44257593154907227 | Test loss: 0.4548688530921936\n",
      "Epoch: 95760 | Loss: 0.4425642192363739 | Test loss: 0.4548550248146057\n",
      "Epoch: 95770 | Loss: 0.44255247712135315 | Test loss: 0.45484113693237305\n",
      "Epoch: 95780 | Loss: 0.44254070520401 | Test loss: 0.45482730865478516\n",
      "Epoch: 95790 | Loss: 0.44252893328666687 | Test loss: 0.45481348037719727\n",
      "Epoch: 95800 | Loss: 0.4425171911716461 | Test loss: 0.454799622297287\n",
      "Epoch: 95810 | Loss: 0.442505419254303 | Test loss: 0.4547857940196991\n",
      "Epoch: 95820 | Loss: 0.44249364733695984 | Test loss: 0.45477190613746643\n",
      "Epoch: 95830 | Loss: 0.4424819052219391 | Test loss: 0.45475807785987854\n",
      "Epoch: 95840 | Loss: 0.44247013330459595 | Test loss: 0.45474424958229065\n",
      "Epoch: 95850 | Loss: 0.4424583613872528 | Test loss: 0.454730361700058\n",
      "Epoch: 95860 | Loss: 0.44244661927223206 | Test loss: 0.4547165334224701\n",
      "Epoch: 95870 | Loss: 0.4424348473548889 | Test loss: 0.4547026753425598\n",
      "Epoch: 95880 | Loss: 0.4424230754375458 | Test loss: 0.4546888470649719\n",
      "Epoch: 95890 | Loss: 0.442411333322525 | Test loss: 0.45467501878738403\n",
      "Epoch: 95900 | Loss: 0.4423995912075043 | Test loss: 0.45466113090515137\n",
      "Epoch: 95910 | Loss: 0.44238778948783875 | Test loss: 0.4546473026275635\n",
      "Epoch: 95920 | Loss: 0.442376047372818 | Test loss: 0.4546334743499756\n",
      "Epoch: 95930 | Loss: 0.44236427545547485 | Test loss: 0.4546196162700653\n",
      "Epoch: 95940 | Loss: 0.4423525333404541 | Test loss: 0.4546057879924774\n",
      "Epoch: 95950 | Loss: 0.44234076142311096 | Test loss: 0.4545919597148895\n",
      "Epoch: 95960 | Loss: 0.4423290193080902 | Test loss: 0.45457807183265686\n",
      "Epoch: 95970 | Loss: 0.44231724739074707 | Test loss: 0.45456424355506897\n",
      "Epoch: 95980 | Loss: 0.44230547547340393 | Test loss: 0.4545503556728363\n",
      "Epoch: 95990 | Loss: 0.4422937333583832 | Test loss: 0.4545365273952484\n",
      "Epoch: 96000 | Loss: 0.44228196144104004 | Test loss: 0.4545226991176605\n",
      "Epoch: 96010 | Loss: 0.4422702491283417 | Test loss: 0.45450887084007263\n",
      "Epoch: 96020 | Loss: 0.44225844740867615 | Test loss: 0.45449501276016235\n",
      "Epoch: 96030 | Loss: 0.442246675491333 | Test loss: 0.4544811248779297\n",
      "Epoch: 96040 | Loss: 0.44223496317863464 | Test loss: 0.4544672966003418\n",
      "Epoch: 96050 | Loss: 0.4422231614589691 | Test loss: 0.4544534683227539\n",
      "Epoch: 96060 | Loss: 0.44221144914627075 | Test loss: 0.45443961024284363\n",
      "Epoch: 96070 | Loss: 0.44219961762428284 | Test loss: 0.45442578196525574\n",
      "Epoch: 96080 | Loss: 0.4421878755092621 | Test loss: 0.45441189408302307\n",
      "Epoch: 96090 | Loss: 0.44217610359191895 | Test loss: 0.4543980658054352\n",
      "Epoch: 96100 | Loss: 0.4421643316745758 | Test loss: 0.4543842375278473\n",
      "Epoch: 96110 | Loss: 0.44215258955955505 | Test loss: 0.4543704092502594\n",
      "Epoch: 96120 | Loss: 0.4421408772468567 | Test loss: 0.45435652136802673\n",
      "Epoch: 96130 | Loss: 0.44212910532951355 | Test loss: 0.45434269309043884\n",
      "Epoch: 96140 | Loss: 0.442117303609848 | Test loss: 0.45432883501052856\n",
      "Epoch: 96150 | Loss: 0.44210559129714966 | Test loss: 0.4543150067329407\n",
      "Epoch: 96160 | Loss: 0.4420938193798065 | Test loss: 0.4543011784553528\n",
      "Epoch: 96170 | Loss: 0.442082017660141 | Test loss: 0.4542873501777649\n",
      "Epoch: 96180 | Loss: 0.4420703053474426 | Test loss: 0.4542734622955322\n",
      "Epoch: 96190 | Loss: 0.4420585334300995 | Test loss: 0.45425963401794434\n",
      "Epoch: 96200 | Loss: 0.44204673171043396 | Test loss: 0.45424577593803406\n",
      "Epoch: 96210 | Loss: 0.4420350193977356 | Test loss: 0.45423194766044617\n",
      "Epoch: 96220 | Loss: 0.44202324748039246 | Test loss: 0.4542180597782135\n",
      "Epoch: 96230 | Loss: 0.4420115053653717 | Test loss: 0.4542042315006256\n",
      "Epoch: 96240 | Loss: 0.44199973344802856 | Test loss: 0.45419034361839294\n",
      "Epoch: 96250 | Loss: 0.4419879913330078 | Test loss: 0.45417651534080505\n",
      "Epoch: 96260 | Loss: 0.4419761598110199 | Test loss: 0.45416268706321716\n",
      "Epoch: 96270 | Loss: 0.44196444749832153 | Test loss: 0.4541488587856293\n",
      "Epoch: 96280 | Loss: 0.4419527053833008 | Test loss: 0.454135000705719\n",
      "Epoch: 96290 | Loss: 0.44194093346595764 | Test loss: 0.4541211724281311\n",
      "Epoch: 96300 | Loss: 0.4419291615486145 | Test loss: 0.45410728454589844\n",
      "Epoch: 96310 | Loss: 0.44191741943359375 | Test loss: 0.45409345626831055\n",
      "Epoch: 96320 | Loss: 0.4419056475162506 | Test loss: 0.45407962799072266\n",
      "Epoch: 96330 | Loss: 0.44189387559890747 | Test loss: 0.45406579971313477\n",
      "Epoch: 96340 | Loss: 0.4418821334838867 | Test loss: 0.4540519416332245\n",
      "Epoch: 96350 | Loss: 0.44187042117118835 | Test loss: 0.4540381133556366\n",
      "Epoch: 96360 | Loss: 0.44185858964920044 | Test loss: 0.45402422547340393\n",
      "Epoch: 96370 | Loss: 0.4418468475341797 | Test loss: 0.45401039719581604\n",
      "Epoch: 96380 | Loss: 0.4418351352214813 | Test loss: 0.45399656891822815\n",
      "Epoch: 96390 | Loss: 0.4418233036994934 | Test loss: 0.4539826810359955\n",
      "Epoch: 96400 | Loss: 0.44181156158447266 | Test loss: 0.4539688527584076\n",
      "Epoch: 96410 | Loss: 0.4417998492717743 | Test loss: 0.4539549946784973\n",
      "Epoch: 96420 | Loss: 0.4417880177497864 | Test loss: 0.4539411664009094\n",
      "Epoch: 96430 | Loss: 0.4417762756347656 | Test loss: 0.45392733812332153\n",
      "Epoch: 96440 | Loss: 0.44176456332206726 | Test loss: 0.45391345024108887\n",
      "Epoch: 96450 | Loss: 0.44175276160240173 | Test loss: 0.453899621963501\n",
      "Epoch: 96460 | Loss: 0.4417409896850586 | Test loss: 0.4538857638835907\n",
      "Epoch: 96470 | Loss: 0.44172927737236023 | Test loss: 0.4538719356060028\n",
      "Epoch: 96480 | Loss: 0.4417175352573395 | Test loss: 0.4538581073284149\n",
      "Epoch: 96490 | Loss: 0.44170570373535156 | Test loss: 0.45384421944618225\n",
      "Epoch: 96500 | Loss: 0.4416939914226532 | Test loss: 0.45383039116859436\n",
      "Epoch: 96510 | Loss: 0.44168218970298767 | Test loss: 0.4538165032863617\n",
      "Epoch: 96520 | Loss: 0.44167041778564453 | Test loss: 0.4538026750087738\n",
      "Epoch: 96530 | Loss: 0.44165870547294617 | Test loss: 0.4537888467311859\n",
      "Epoch: 96540 | Loss: 0.44164690375328064 | Test loss: 0.453775018453598\n",
      "Epoch: 96550 | Loss: 0.4416351318359375 | Test loss: 0.45376116037368774\n",
      "Epoch: 96560 | Loss: 0.44162341952323914 | Test loss: 0.45374733209609985\n",
      "Epoch: 96570 | Loss: 0.4416116774082184 | Test loss: 0.4537334442138672\n",
      "Epoch: 96580 | Loss: 0.44159990549087524 | Test loss: 0.4537196159362793\n",
      "Epoch: 96590 | Loss: 0.4415881335735321 | Test loss: 0.4537057876586914\n",
      "Epoch: 96600 | Loss: 0.44157639145851135 | Test loss: 0.45369192957878113\n",
      "Epoch: 96610 | Loss: 0.4415646195411682 | Test loss: 0.45367810130119324\n",
      "Epoch: 96620 | Loss: 0.4415528476238251 | Test loss: 0.45366421341896057\n",
      "Epoch: 96630 | Loss: 0.4415411055088043 | Test loss: 0.4536503851413727\n",
      "Epoch: 96640 | Loss: 0.4415293335914612 | Test loss: 0.4536365568637848\n",
      "Epoch: 96650 | Loss: 0.44151756167411804 | Test loss: 0.4536226689815521\n",
      "Epoch: 96660 | Loss: 0.4415058195590973 | Test loss: 0.45360884070396423\n",
      "Epoch: 96670 | Loss: 0.44149404764175415 | Test loss: 0.45359498262405396\n",
      "Epoch: 96680 | Loss: 0.441482275724411 | Test loss: 0.45358115434646606\n",
      "Epoch: 96690 | Loss: 0.44147053360939026 | Test loss: 0.4535673260688782\n",
      "Epoch: 96700 | Loss: 0.4414587914943695 | Test loss: 0.4535534381866455\n",
      "Epoch: 96710 | Loss: 0.441446989774704 | Test loss: 0.4535396099090576\n",
      "Epoch: 96720 | Loss: 0.4414352476596832 | Test loss: 0.4535257816314697\n",
      "Epoch: 96730 | Loss: 0.4414234757423401 | Test loss: 0.45351192355155945\n",
      "Epoch: 96740 | Loss: 0.44141173362731934 | Test loss: 0.45349809527397156\n",
      "Epoch: 96750 | Loss: 0.4413999617099762 | Test loss: 0.45348426699638367\n",
      "Epoch: 96760 | Loss: 0.44138821959495544 | Test loss: 0.453470379114151\n",
      "Epoch: 96770 | Loss: 0.4413764476776123 | Test loss: 0.4534565508365631\n",
      "Epoch: 96780 | Loss: 0.44136467576026917 | Test loss: 0.45344266295433044\n",
      "Epoch: 96790 | Loss: 0.4413529336452484 | Test loss: 0.45342883467674255\n",
      "Epoch: 96800 | Loss: 0.4413411617279053 | Test loss: 0.45341500639915466\n",
      "Epoch: 96810 | Loss: 0.4413294494152069 | Test loss: 0.4534011781215668\n",
      "Epoch: 96820 | Loss: 0.4413176476955414 | Test loss: 0.4533873200416565\n",
      "Epoch: 96830 | Loss: 0.44130587577819824 | Test loss: 0.45337343215942383\n",
      "Epoch: 96840 | Loss: 0.4412941634654999 | Test loss: 0.45335960388183594\n",
      "Epoch: 96850 | Loss: 0.44128236174583435 | Test loss: 0.45334577560424805\n",
      "Epoch: 96860 | Loss: 0.441270649433136 | Test loss: 0.45333191752433777\n",
      "Epoch: 96870 | Loss: 0.44125881791114807 | Test loss: 0.4533180892467499\n",
      "Epoch: 96880 | Loss: 0.4412470757961273 | Test loss: 0.4533042013645172\n",
      "Epoch: 96890 | Loss: 0.4412353038787842 | Test loss: 0.4532903730869293\n",
      "Epoch: 96900 | Loss: 0.44122353196144104 | Test loss: 0.45327654480934143\n",
      "Epoch: 96910 | Loss: 0.4412117898464203 | Test loss: 0.45326271653175354\n",
      "Epoch: 96920 | Loss: 0.4412000775337219 | Test loss: 0.4532488286495209\n",
      "Epoch: 96930 | Loss: 0.4411883056163788 | Test loss: 0.453235000371933\n",
      "Epoch: 96940 | Loss: 0.44117650389671326 | Test loss: 0.4532211422920227\n",
      "Epoch: 96950 | Loss: 0.4411647915840149 | Test loss: 0.4532073140144348\n",
      "Epoch: 96960 | Loss: 0.44115301966667175 | Test loss: 0.4531934857368469\n",
      "Epoch: 96970 | Loss: 0.4411412179470062 | Test loss: 0.45317965745925903\n",
      "Epoch: 96980 | Loss: 0.44112950563430786 | Test loss: 0.45316576957702637\n",
      "Epoch: 96990 | Loss: 0.4411177337169647 | Test loss: 0.4531519412994385\n",
      "Epoch: 97000 | Loss: 0.4411059319972992 | Test loss: 0.4531380832195282\n",
      "Epoch: 97010 | Loss: 0.44109421968460083 | Test loss: 0.4531242549419403\n",
      "Epoch: 97020 | Loss: 0.4410824477672577 | Test loss: 0.45311036705970764\n",
      "Epoch: 97030 | Loss: 0.44107070565223694 | Test loss: 0.45309653878211975\n",
      "Epoch: 97040 | Loss: 0.4410589337348938 | Test loss: 0.4530826508998871\n",
      "Epoch: 97050 | Loss: 0.44104719161987305 | Test loss: 0.4530688226222992\n",
      "Epoch: 97060 | Loss: 0.44103536009788513 | Test loss: 0.4530549943447113\n",
      "Epoch: 97070 | Loss: 0.44102364778518677 | Test loss: 0.4530411660671234\n",
      "Epoch: 97080 | Loss: 0.441011905670166 | Test loss: 0.45302730798721313\n",
      "Epoch: 97090 | Loss: 0.4410001337528229 | Test loss: 0.45301347970962524\n",
      "Epoch: 97100 | Loss: 0.44098836183547974 | Test loss: 0.4529995918273926\n",
      "Epoch: 97110 | Loss: 0.440976619720459 | Test loss: 0.4529857635498047\n",
      "Epoch: 97120 | Loss: 0.44096484780311584 | Test loss: 0.4529719352722168\n",
      "Epoch: 97130 | Loss: 0.4409530758857727 | Test loss: 0.4529581069946289\n",
      "Epoch: 97140 | Loss: 0.44094133377075195 | Test loss: 0.45294424891471863\n",
      "Epoch: 97150 | Loss: 0.4409296214580536 | Test loss: 0.45293042063713074\n",
      "Epoch: 97160 | Loss: 0.4409177899360657 | Test loss: 0.45291653275489807\n",
      "Epoch: 97170 | Loss: 0.4409060478210449 | Test loss: 0.4529027044773102\n",
      "Epoch: 97180 | Loss: 0.44089433550834656 | Test loss: 0.4528888761997223\n",
      "Epoch: 97190 | Loss: 0.44088250398635864 | Test loss: 0.4528749883174896\n",
      "Epoch: 97200 | Loss: 0.4408707618713379 | Test loss: 0.45286116003990173\n",
      "Epoch: 97210 | Loss: 0.4408590495586395 | Test loss: 0.45284730195999146\n",
      "Epoch: 97220 | Loss: 0.4408472180366516 | Test loss: 0.45283347368240356\n",
      "Epoch: 97230 | Loss: 0.44083547592163086 | Test loss: 0.4528196454048157\n",
      "Epoch: 97240 | Loss: 0.4408237636089325 | Test loss: 0.452805757522583\n",
      "Epoch: 97250 | Loss: 0.44081196188926697 | Test loss: 0.4527919292449951\n",
      "Epoch: 97260 | Loss: 0.44080018997192383 | Test loss: 0.45277807116508484\n",
      "Epoch: 97270 | Loss: 0.44078847765922546 | Test loss: 0.45276424288749695\n",
      "Epoch: 97280 | Loss: 0.4407767355442047 | Test loss: 0.45275041460990906\n",
      "Epoch: 97290 | Loss: 0.4407649040222168 | Test loss: 0.4527365267276764\n",
      "Epoch: 97300 | Loss: 0.44075319170951843 | Test loss: 0.4527226984500885\n",
      "Epoch: 97310 | Loss: 0.4407413899898529 | Test loss: 0.45270881056785583\n",
      "Epoch: 97320 | Loss: 0.44072961807250977 | Test loss: 0.45269498229026794\n",
      "Epoch: 97330 | Loss: 0.4407179057598114 | Test loss: 0.45268115401268005\n",
      "Epoch: 97340 | Loss: 0.4407061040401459 | Test loss: 0.45266732573509216\n",
      "Epoch: 97350 | Loss: 0.44069433212280273 | Test loss: 0.4526534676551819\n",
      "Epoch: 97360 | Loss: 0.44068261981010437 | Test loss: 0.452639639377594\n",
      "Epoch: 97370 | Loss: 0.4406708776950836 | Test loss: 0.45262575149536133\n",
      "Epoch: 97380 | Loss: 0.4406591057777405 | Test loss: 0.45261192321777344\n",
      "Epoch: 97390 | Loss: 0.44064733386039734 | Test loss: 0.45259809494018555\n",
      "Epoch: 97400 | Loss: 0.4406355917453766 | Test loss: 0.45258423686027527\n",
      "Epoch: 97410 | Loss: 0.44062381982803345 | Test loss: 0.4525704085826874\n",
      "Epoch: 97420 | Loss: 0.4406120479106903 | Test loss: 0.4525565207004547\n",
      "Epoch: 97430 | Loss: 0.44060030579566956 | Test loss: 0.4525426924228668\n",
      "Epoch: 97440 | Loss: 0.4405885338783264 | Test loss: 0.45252886414527893\n",
      "Epoch: 97450 | Loss: 0.4405767619609833 | Test loss: 0.45251497626304626\n",
      "Epoch: 97460 | Loss: 0.4405650198459625 | Test loss: 0.4525011479854584\n",
      "Epoch: 97470 | Loss: 0.4405532479286194 | Test loss: 0.4524872899055481\n",
      "Epoch: 97480 | Loss: 0.44054147601127625 | Test loss: 0.4524734616279602\n",
      "Epoch: 97490 | Loss: 0.4405297338962555 | Test loss: 0.4524596333503723\n",
      "Epoch: 97500 | Loss: 0.44051799178123474 | Test loss: 0.45244574546813965\n",
      "Epoch: 97510 | Loss: 0.4405061900615692 | Test loss: 0.45243191719055176\n",
      "Epoch: 97520 | Loss: 0.44049444794654846 | Test loss: 0.45241808891296387\n",
      "Epoch: 97530 | Loss: 0.4404826760292053 | Test loss: 0.4524042308330536\n",
      "Epoch: 97540 | Loss: 0.44047093391418457 | Test loss: 0.4523904025554657\n",
      "Epoch: 97550 | Loss: 0.44045916199684143 | Test loss: 0.4523765742778778\n",
      "Epoch: 97560 | Loss: 0.4404474198818207 | Test loss: 0.45236268639564514\n",
      "Epoch: 97570 | Loss: 0.44043564796447754 | Test loss: 0.45234885811805725\n",
      "Epoch: 97580 | Loss: 0.4404238760471344 | Test loss: 0.4523349702358246\n",
      "Epoch: 97590 | Loss: 0.44041213393211365 | Test loss: 0.4523211419582367\n",
      "Epoch: 97600 | Loss: 0.4404003620147705 | Test loss: 0.4523073136806488\n",
      "Epoch: 97610 | Loss: 0.44038864970207214 | Test loss: 0.4522934854030609\n",
      "Epoch: 97620 | Loss: 0.4403768479824066 | Test loss: 0.45227962732315063\n",
      "Epoch: 97630 | Loss: 0.4403650760650635 | Test loss: 0.45226573944091797\n",
      "Epoch: 97640 | Loss: 0.4403533637523651 | Test loss: 0.4522519111633301\n",
      "Epoch: 97650 | Loss: 0.4403415620326996 | Test loss: 0.4522380828857422\n",
      "Epoch: 97660 | Loss: 0.4403298497200012 | Test loss: 0.4522242248058319\n",
      "Epoch: 97670 | Loss: 0.4403180181980133 | Test loss: 0.452210396528244\n",
      "Epoch: 97680 | Loss: 0.44030627608299255 | Test loss: 0.45219650864601135\n",
      "Epoch: 97690 | Loss: 0.4402945041656494 | Test loss: 0.45218268036842346\n",
      "Epoch: 97700 | Loss: 0.4402827322483063 | Test loss: 0.45216885209083557\n",
      "Epoch: 97710 | Loss: 0.4402709901332855 | Test loss: 0.4521550238132477\n",
      "Epoch: 97720 | Loss: 0.44025927782058716 | Test loss: 0.452141135931015\n",
      "Epoch: 97730 | Loss: 0.440247505903244 | Test loss: 0.4521273076534271\n",
      "Epoch: 97740 | Loss: 0.4402357041835785 | Test loss: 0.45211344957351685\n",
      "Epoch: 97750 | Loss: 0.4402239918708801 | Test loss: 0.45209962129592896\n",
      "Epoch: 97760 | Loss: 0.440212219953537 | Test loss: 0.45208579301834106\n",
      "Epoch: 97770 | Loss: 0.44020041823387146 | Test loss: 0.4520719647407532\n",
      "Epoch: 97780 | Loss: 0.4401887059211731 | Test loss: 0.4520580768585205\n",
      "Epoch: 97790 | Loss: 0.44017693400382996 | Test loss: 0.4520442485809326\n",
      "Epoch: 97800 | Loss: 0.44016513228416443 | Test loss: 0.45203039050102234\n",
      "Epoch: 97810 | Loss: 0.44015341997146606 | Test loss: 0.45201656222343445\n",
      "Epoch: 97820 | Loss: 0.4401416480541229 | Test loss: 0.4520026743412018\n",
      "Epoch: 97830 | Loss: 0.4401299059391022 | Test loss: 0.4519888460636139\n",
      "Epoch: 97840 | Loss: 0.44011813402175903 | Test loss: 0.4519749581813812\n",
      "Epoch: 97850 | Loss: 0.4401063919067383 | Test loss: 0.45196112990379333\n",
      "Epoch: 97860 | Loss: 0.44009456038475037 | Test loss: 0.45194730162620544\n",
      "Epoch: 97870 | Loss: 0.440082848072052 | Test loss: 0.45193347334861755\n",
      "Epoch: 97880 | Loss: 0.44007110595703125 | Test loss: 0.4519196152687073\n",
      "Epoch: 97890 | Loss: 0.4400593340396881 | Test loss: 0.4519057869911194\n",
      "Epoch: 97900 | Loss: 0.44004756212234497 | Test loss: 0.4518918991088867\n",
      "Epoch: 97910 | Loss: 0.4400358200073242 | Test loss: 0.45187807083129883\n",
      "Epoch: 97920 | Loss: 0.4400240480899811 | Test loss: 0.45186424255371094\n",
      "Epoch: 97930 | Loss: 0.44001227617263794 | Test loss: 0.45185041427612305\n",
      "Epoch: 97940 | Loss: 0.4400005340576172 | Test loss: 0.45183655619621277\n",
      "Epoch: 97950 | Loss: 0.4399888217449188 | Test loss: 0.4518227279186249\n",
      "Epoch: 97960 | Loss: 0.4399769902229309 | Test loss: 0.4518088400363922\n",
      "Epoch: 97970 | Loss: 0.43996524810791016 | Test loss: 0.4517950117588043\n",
      "Epoch: 97980 | Loss: 0.4399535357952118 | Test loss: 0.45178118348121643\n",
      "Epoch: 97990 | Loss: 0.4399417042732239 | Test loss: 0.45176729559898376\n",
      "Epoch: 98000 | Loss: 0.4399299621582031 | Test loss: 0.4517534673213959\n",
      "Epoch: 98010 | Loss: 0.43991824984550476 | Test loss: 0.4517396092414856\n",
      "Epoch: 98020 | Loss: 0.43990641832351685 | Test loss: 0.4517257809638977\n",
      "Epoch: 98030 | Loss: 0.4398946762084961 | Test loss: 0.4517119526863098\n",
      "Epoch: 98040 | Loss: 0.43988296389579773 | Test loss: 0.45169806480407715\n",
      "Epoch: 98050 | Loss: 0.4398711621761322 | Test loss: 0.45168423652648926\n",
      "Epoch: 98060 | Loss: 0.43985939025878906 | Test loss: 0.451670378446579\n",
      "Epoch: 98070 | Loss: 0.4398476779460907 | Test loss: 0.4516565501689911\n",
      "Epoch: 98080 | Loss: 0.43983593583106995 | Test loss: 0.4516427218914032\n",
      "Epoch: 98090 | Loss: 0.43982410430908203 | Test loss: 0.45162883400917053\n",
      "Epoch: 98100 | Loss: 0.43981239199638367 | Test loss: 0.45161500573158264\n",
      "Epoch: 98110 | Loss: 0.43980059027671814 | Test loss: 0.45160111784935\n",
      "Epoch: 98120 | Loss: 0.439788818359375 | Test loss: 0.4515872895717621\n",
      "Epoch: 98130 | Loss: 0.43977710604667664 | Test loss: 0.4515734612941742\n",
      "Epoch: 98140 | Loss: 0.4397653043270111 | Test loss: 0.4515596330165863\n",
      "Epoch: 98150 | Loss: 0.43975353240966797 | Test loss: 0.451545774936676\n",
      "Epoch: 98160 | Loss: 0.4397418200969696 | Test loss: 0.45153194665908813\n",
      "Epoch: 98170 | Loss: 0.43973007798194885 | Test loss: 0.45151805877685547\n",
      "Epoch: 98180 | Loss: 0.4397183060646057 | Test loss: 0.4515042304992676\n",
      "Epoch: 98190 | Loss: 0.4397065341472626 | Test loss: 0.4514904022216797\n",
      "Epoch: 98200 | Loss: 0.4396947920322418 | Test loss: 0.4514765441417694\n",
      "Epoch: 98210 | Loss: 0.4396830201148987 | Test loss: 0.4514627158641815\n",
      "Epoch: 98220 | Loss: 0.43967124819755554 | Test loss: 0.45144882798194885\n",
      "Epoch: 98230 | Loss: 0.4396595060825348 | Test loss: 0.45143499970436096\n",
      "Epoch: 98240 | Loss: 0.43964773416519165 | Test loss: 0.45142117142677307\n",
      "Epoch: 98250 | Loss: 0.4396359622478485 | Test loss: 0.4514072835445404\n",
      "Epoch: 98260 | Loss: 0.43962422013282776 | Test loss: 0.4513934552669525\n",
      "Epoch: 98270 | Loss: 0.4396124482154846 | Test loss: 0.45137959718704224\n",
      "Epoch: 98280 | Loss: 0.4396006762981415 | Test loss: 0.45136576890945435\n",
      "Epoch: 98290 | Loss: 0.4395889341831207 | Test loss: 0.45135194063186646\n",
      "Epoch: 98300 | Loss: 0.4395771920681 | Test loss: 0.4513380527496338\n",
      "Epoch: 98310 | Loss: 0.43956539034843445 | Test loss: 0.4513242244720459\n",
      "Epoch: 98320 | Loss: 0.4395536482334137 | Test loss: 0.451310396194458\n",
      "Epoch: 98330 | Loss: 0.43954187631607056 | Test loss: 0.45129653811454773\n",
      "Epoch: 98340 | Loss: 0.4395301342010498 | Test loss: 0.45128270983695984\n",
      "Epoch: 98350 | Loss: 0.43951836228370667 | Test loss: 0.45126888155937195\n",
      "Epoch: 98360 | Loss: 0.4395066201686859 | Test loss: 0.4512549936771393\n",
      "Epoch: 98370 | Loss: 0.4394948482513428 | Test loss: 0.4512411653995514\n",
      "Epoch: 98380 | Loss: 0.43948307633399963 | Test loss: 0.4512272775173187\n",
      "Epoch: 98390 | Loss: 0.4394713342189789 | Test loss: 0.45121344923973083\n",
      "Epoch: 98400 | Loss: 0.43945956230163574 | Test loss: 0.45119962096214294\n",
      "Epoch: 98410 | Loss: 0.4394478499889374 | Test loss: 0.45118579268455505\n",
      "Epoch: 98420 | Loss: 0.43943604826927185 | Test loss: 0.4511719346046448\n",
      "Epoch: 98430 | Loss: 0.4394242763519287 | Test loss: 0.4511580467224121\n",
      "Epoch: 98440 | Loss: 0.43941256403923035 | Test loss: 0.4511442184448242\n",
      "Epoch: 98450 | Loss: 0.4394007623195648 | Test loss: 0.45113039016723633\n",
      "Epoch: 98460 | Loss: 0.43938905000686646 | Test loss: 0.45111653208732605\n",
      "Epoch: 98470 | Loss: 0.43937721848487854 | Test loss: 0.45110270380973816\n",
      "Epoch: 98480 | Loss: 0.4393654763698578 | Test loss: 0.4510888159275055\n",
      "Epoch: 98490 | Loss: 0.43935370445251465 | Test loss: 0.4510749876499176\n",
      "Epoch: 98500 | Loss: 0.4393419325351715 | Test loss: 0.4510611593723297\n",
      "Epoch: 98510 | Loss: 0.43933019042015076 | Test loss: 0.4510473310947418\n",
      "Epoch: 98520 | Loss: 0.4393184781074524 | Test loss: 0.45103344321250916\n",
      "Epoch: 98530 | Loss: 0.43930670619010925 | Test loss: 0.45101961493492126\n",
      "Epoch: 98540 | Loss: 0.4392949044704437 | Test loss: 0.451005756855011\n",
      "Epoch: 98550 | Loss: 0.43928319215774536 | Test loss: 0.4509919285774231\n",
      "Epoch: 98560 | Loss: 0.4392714202404022 | Test loss: 0.4509781002998352\n",
      "Epoch: 98570 | Loss: 0.4392596185207367 | Test loss: 0.4509642720222473\n",
      "Epoch: 98580 | Loss: 0.43924790620803833 | Test loss: 0.45095038414001465\n",
      "Epoch: 98590 | Loss: 0.4392361342906952 | Test loss: 0.45093655586242676\n",
      "Epoch: 98600 | Loss: 0.43922433257102966 | Test loss: 0.4509226977825165\n",
      "Epoch: 98610 | Loss: 0.4392126202583313 | Test loss: 0.4509088695049286\n",
      "Epoch: 98620 | Loss: 0.43920084834098816 | Test loss: 0.4508949816226959\n",
      "Epoch: 98630 | Loss: 0.4391891062259674 | Test loss: 0.45088115334510803\n",
      "Epoch: 98640 | Loss: 0.43917733430862427 | Test loss: 0.45086726546287537\n",
      "Epoch: 98650 | Loss: 0.4391655921936035 | Test loss: 0.4508534371852875\n",
      "Epoch: 98660 | Loss: 0.4391537606716156 | Test loss: 0.4508396089076996\n",
      "Epoch: 98670 | Loss: 0.43914204835891724 | Test loss: 0.4508257806301117\n",
      "Epoch: 98680 | Loss: 0.4391303062438965 | Test loss: 0.4508119225502014\n",
      "Epoch: 98690 | Loss: 0.43911853432655334 | Test loss: 0.4507980942726135\n",
      "Epoch: 98700 | Loss: 0.4391067624092102 | Test loss: 0.45078420639038086\n",
      "Epoch: 98710 | Loss: 0.43909502029418945 | Test loss: 0.45077037811279297\n",
      "Epoch: 98720 | Loss: 0.4390832483768463 | Test loss: 0.4507565498352051\n",
      "Epoch: 98730 | Loss: 0.4390714764595032 | Test loss: 0.4507427215576172\n",
      "Epoch: 98740 | Loss: 0.4390597343444824 | Test loss: 0.4507288634777069\n",
      "Epoch: 98750 | Loss: 0.43904802203178406 | Test loss: 0.450715035200119\n",
      "Epoch: 98760 | Loss: 0.43903619050979614 | Test loss: 0.45070114731788635\n",
      "Epoch: 98770 | Loss: 0.4390244483947754 | Test loss: 0.45068731904029846\n",
      "Epoch: 98780 | Loss: 0.439012736082077 | Test loss: 0.45067349076271057\n",
      "Epoch: 98790 | Loss: 0.4390009045600891 | Test loss: 0.4506596028804779\n",
      "Epoch: 98800 | Loss: 0.43898916244506836 | Test loss: 0.45064577460289\n",
      "Epoch: 98810 | Loss: 0.43897745013237 | Test loss: 0.45063191652297974\n",
      "Epoch: 98820 | Loss: 0.4389656186103821 | Test loss: 0.45061808824539185\n",
      "Epoch: 98830 | Loss: 0.43895387649536133 | Test loss: 0.45060425996780396\n",
      "Epoch: 98840 | Loss: 0.43894216418266296 | Test loss: 0.4505903720855713\n",
      "Epoch: 98850 | Loss: 0.43893036246299744 | Test loss: 0.4505765438079834\n",
      "Epoch: 98860 | Loss: 0.4389185905456543 | Test loss: 0.4505626857280731\n",
      "Epoch: 98870 | Loss: 0.43890687823295593 | Test loss: 0.45054885745048523\n",
      "Epoch: 98880 | Loss: 0.4388951361179352 | Test loss: 0.45053502917289734\n",
      "Epoch: 98890 | Loss: 0.43888330459594727 | Test loss: 0.4505211412906647\n",
      "Epoch: 98900 | Loss: 0.4388715922832489 | Test loss: 0.4505073130130768\n",
      "Epoch: 98910 | Loss: 0.4388597905635834 | Test loss: 0.4504934251308441\n",
      "Epoch: 98920 | Loss: 0.43884801864624023 | Test loss: 0.4504795968532562\n",
      "Epoch: 98930 | Loss: 0.43883630633354187 | Test loss: 0.45046576857566833\n",
      "Epoch: 98940 | Loss: 0.43882450461387634 | Test loss: 0.45045194029808044\n",
      "Epoch: 98950 | Loss: 0.4388127326965332 | Test loss: 0.45043808221817017\n",
      "Epoch: 98960 | Loss: 0.43880102038383484 | Test loss: 0.4504242539405823\n",
      "Epoch: 98970 | Loss: 0.4387892782688141 | Test loss: 0.4504103660583496\n",
      "Epoch: 98980 | Loss: 0.43877750635147095 | Test loss: 0.4503965377807617\n",
      "Epoch: 98990 | Loss: 0.4387657344341278 | Test loss: 0.45038270950317383\n",
      "Epoch: 99000 | Loss: 0.43875399231910706 | Test loss: 0.45036885142326355\n",
      "Epoch: 99010 | Loss: 0.4387422204017639 | Test loss: 0.45035502314567566\n",
      "Epoch: 99020 | Loss: 0.4387304484844208 | Test loss: 0.450341135263443\n",
      "Epoch: 99030 | Loss: 0.4387187063694 | Test loss: 0.4503273069858551\n",
      "Epoch: 99040 | Loss: 0.4387069344520569 | Test loss: 0.4503134787082672\n",
      "Epoch: 99050 | Loss: 0.43869516253471375 | Test loss: 0.45029959082603455\n",
      "Epoch: 99060 | Loss: 0.438683420419693 | Test loss: 0.45028576254844666\n",
      "Epoch: 99070 | Loss: 0.43867164850234985 | Test loss: 0.4502719044685364\n",
      "Epoch: 99080 | Loss: 0.4386598765850067 | Test loss: 0.4502580761909485\n",
      "Epoch: 99090 | Loss: 0.43864813446998596 | Test loss: 0.4502442479133606\n",
      "Epoch: 99100 | Loss: 0.4386363923549652 | Test loss: 0.45023036003112793\n",
      "Epoch: 99110 | Loss: 0.4386245906352997 | Test loss: 0.45021653175354004\n",
      "Epoch: 99120 | Loss: 0.43861284852027893 | Test loss: 0.45020270347595215\n",
      "Epoch: 99130 | Loss: 0.4386010766029358 | Test loss: 0.45018884539604187\n",
      "Epoch: 99140 | Loss: 0.43858933448791504 | Test loss: 0.450175017118454\n",
      "Epoch: 99150 | Loss: 0.4385775625705719 | Test loss: 0.4501611888408661\n",
      "Epoch: 99160 | Loss: 0.43856582045555115 | Test loss: 0.4501473009586334\n",
      "Epoch: 99170 | Loss: 0.438554048538208 | Test loss: 0.45013347268104553\n",
      "Epoch: 99180 | Loss: 0.43854227662086487 | Test loss: 0.45011958479881287\n",
      "Epoch: 99190 | Loss: 0.4385305345058441 | Test loss: 0.450105756521225\n",
      "Epoch: 99200 | Loss: 0.438518762588501 | Test loss: 0.4500919282436371\n",
      "Epoch: 99210 | Loss: 0.4385070502758026 | Test loss: 0.4500780999660492\n",
      "Epoch: 99220 | Loss: 0.4384952485561371 | Test loss: 0.4500642418861389\n",
      "Epoch: 99230 | Loss: 0.43848347663879395 | Test loss: 0.45005035400390625\n",
      "Epoch: 99240 | Loss: 0.4384717643260956 | Test loss: 0.45003652572631836\n",
      "Epoch: 99250 | Loss: 0.43845996260643005 | Test loss: 0.45002269744873047\n",
      "Epoch: 99260 | Loss: 0.4384482502937317 | Test loss: 0.4500088393688202\n",
      "Epoch: 99270 | Loss: 0.4384364187717438 | Test loss: 0.4499950110912323\n",
      "Epoch: 99280 | Loss: 0.438424676656723 | Test loss: 0.44998112320899963\n",
      "Epoch: 99290 | Loss: 0.4384129047393799 | Test loss: 0.44996729493141174\n",
      "Epoch: 99300 | Loss: 0.43840113282203674 | Test loss: 0.44995346665382385\n",
      "Epoch: 99310 | Loss: 0.438389390707016 | Test loss: 0.44993963837623596\n",
      "Epoch: 99320 | Loss: 0.4383776783943176 | Test loss: 0.4499257504940033\n",
      "Epoch: 99330 | Loss: 0.4383659064769745 | Test loss: 0.4499119222164154\n",
      "Epoch: 99340 | Loss: 0.43835410475730896 | Test loss: 0.4498980641365051\n",
      "Epoch: 99350 | Loss: 0.4383423924446106 | Test loss: 0.44988423585891724\n",
      "Epoch: 99360 | Loss: 0.43833062052726746 | Test loss: 0.44987040758132935\n",
      "Epoch: 99370 | Loss: 0.43831881880760193 | Test loss: 0.44985657930374146\n",
      "Epoch: 99380 | Loss: 0.43830710649490356 | Test loss: 0.4498426914215088\n",
      "Epoch: 99390 | Loss: 0.4382953345775604 | Test loss: 0.4498288631439209\n",
      "Epoch: 99400 | Loss: 0.4382835328578949 | Test loss: 0.4498150050640106\n",
      "Epoch: 99410 | Loss: 0.43827182054519653 | Test loss: 0.44980117678642273\n",
      "Epoch: 99420 | Loss: 0.4382600486278534 | Test loss: 0.44978728890419006\n",
      "Epoch: 99430 | Loss: 0.43824830651283264 | Test loss: 0.4497734606266022\n",
      "Epoch: 99440 | Loss: 0.4382365345954895 | Test loss: 0.4497595727443695\n",
      "Epoch: 99450 | Loss: 0.43822479248046875 | Test loss: 0.4497457444667816\n",
      "Epoch: 99460 | Loss: 0.43821296095848083 | Test loss: 0.4497319161891937\n",
      "Epoch: 99470 | Loss: 0.43820124864578247 | Test loss: 0.44971808791160583\n",
      "Epoch: 99480 | Loss: 0.4381895065307617 | Test loss: 0.44970422983169556\n",
      "Epoch: 99490 | Loss: 0.4381777346134186 | Test loss: 0.44969040155410767\n",
      "Epoch: 99500 | Loss: 0.43816596269607544 | Test loss: 0.449676513671875\n",
      "Epoch: 99510 | Loss: 0.4381542205810547 | Test loss: 0.4496626853942871\n",
      "Epoch: 99520 | Loss: 0.43814244866371155 | Test loss: 0.4496488571166992\n",
      "Epoch: 99530 | Loss: 0.4381306767463684 | Test loss: 0.44963502883911133\n",
      "Epoch: 99540 | Loss: 0.43811893463134766 | Test loss: 0.44962117075920105\n",
      "Epoch: 99550 | Loss: 0.4381072223186493 | Test loss: 0.44960734248161316\n",
      "Epoch: 99560 | Loss: 0.4380953907966614 | Test loss: 0.4495934545993805\n",
      "Epoch: 99570 | Loss: 0.4380836486816406 | Test loss: 0.4495796263217926\n",
      "Epoch: 99580 | Loss: 0.43807193636894226 | Test loss: 0.4495657980442047\n",
      "Epoch: 99590 | Loss: 0.43806010484695435 | Test loss: 0.44955191016197205\n",
      "Epoch: 99600 | Loss: 0.4380483627319336 | Test loss: 0.44953808188438416\n",
      "Epoch: 99610 | Loss: 0.43803665041923523 | Test loss: 0.4495242238044739\n",
      "Epoch: 99620 | Loss: 0.4380248188972473 | Test loss: 0.449510395526886\n",
      "Epoch: 99630 | Loss: 0.43801307678222656 | Test loss: 0.4494965672492981\n",
      "Epoch: 99640 | Loss: 0.4380013644695282 | Test loss: 0.44948267936706543\n",
      "Epoch: 99650 | Loss: 0.43798956274986267 | Test loss: 0.44946885108947754\n",
      "Epoch: 99660 | Loss: 0.43797779083251953 | Test loss: 0.44945499300956726\n",
      "Epoch: 99670 | Loss: 0.43796607851982117 | Test loss: 0.44944116473197937\n",
      "Epoch: 99680 | Loss: 0.4379543364048004 | Test loss: 0.4494273364543915\n",
      "Epoch: 99690 | Loss: 0.4379425048828125 | Test loss: 0.4494134485721588\n",
      "Epoch: 99700 | Loss: 0.43793079257011414 | Test loss: 0.4493996202945709\n",
      "Epoch: 99710 | Loss: 0.4379189908504486 | Test loss: 0.44938573241233826\n",
      "Epoch: 99720 | Loss: 0.43790721893310547 | Test loss: 0.44937190413475037\n",
      "Epoch: 99730 | Loss: 0.4378955066204071 | Test loss: 0.4493580758571625\n",
      "Epoch: 99740 | Loss: 0.4378837049007416 | Test loss: 0.4493442475795746\n",
      "Epoch: 99750 | Loss: 0.43787193298339844 | Test loss: 0.4493303894996643\n",
      "Epoch: 99760 | Loss: 0.4378602206707001 | Test loss: 0.4493165612220764\n",
      "Epoch: 99770 | Loss: 0.4378484785556793 | Test loss: 0.44930267333984375\n",
      "Epoch: 99780 | Loss: 0.4378367066383362 | Test loss: 0.44928884506225586\n",
      "Epoch: 99790 | Loss: 0.43782493472099304 | Test loss: 0.44927501678466797\n",
      "Epoch: 99800 | Loss: 0.4378131926059723 | Test loss: 0.4492611587047577\n",
      "Epoch: 99810 | Loss: 0.43780142068862915 | Test loss: 0.4492473304271698\n",
      "Epoch: 99820 | Loss: 0.437789648771286 | Test loss: 0.44923344254493713\n",
      "Epoch: 99830 | Loss: 0.43777790665626526 | Test loss: 0.44921961426734924\n",
      "Epoch: 99840 | Loss: 0.4377661347389221 | Test loss: 0.44920578598976135\n",
      "Epoch: 99850 | Loss: 0.437754362821579 | Test loss: 0.4491918981075287\n",
      "Epoch: 99860 | Loss: 0.4377426207065582 | Test loss: 0.4491780698299408\n",
      "Epoch: 99870 | Loss: 0.4377308487892151 | Test loss: 0.4491642117500305\n",
      "Epoch: 99880 | Loss: 0.43771907687187195 | Test loss: 0.4491503834724426\n",
      "Epoch: 99890 | Loss: 0.4377073347568512 | Test loss: 0.44913655519485474\n",
      "Epoch: 99900 | Loss: 0.43769559264183044 | Test loss: 0.44912266731262207\n",
      "Epoch: 99910 | Loss: 0.4376837909221649 | Test loss: 0.4491088390350342\n",
      "Epoch: 99920 | Loss: 0.43767204880714417 | Test loss: 0.4490950107574463\n",
      "Epoch: 99930 | Loss: 0.437660276889801 | Test loss: 0.449081152677536\n",
      "Epoch: 99940 | Loss: 0.4376485347747803 | Test loss: 0.4490673243999481\n",
      "Epoch: 99950 | Loss: 0.43763676285743713 | Test loss: 0.44905349612236023\n",
      "Epoch: 99960 | Loss: 0.4376250207424164 | Test loss: 0.44903960824012756\n",
      "Epoch: 99970 | Loss: 0.43761324882507324 | Test loss: 0.4490257799625397\n",
      "Epoch: 99980 | Loss: 0.4376014769077301 | Test loss: 0.449011892080307\n",
      "Epoch: 99990 | Loss: 0.43758973479270935 | Test loss: 0.4489980638027191\n",
      "Epoch: 100000 | Loss: 0.4375779628753662 | Test loss: 0.4489842355251312\n",
      "Epoch: 100010 | Loss: 0.43756625056266785 | Test loss: 0.44897040724754333\n",
      "Epoch: 100020 | Loss: 0.4375544488430023 | Test loss: 0.44895654916763306\n",
      "Epoch: 100030 | Loss: 0.4375426769256592 | Test loss: 0.4489426612854004\n",
      "Epoch: 100040 | Loss: 0.4375309646129608 | Test loss: 0.4489288330078125\n",
      "Epoch: 100050 | Loss: 0.4375191628932953 | Test loss: 0.4489150047302246\n",
      "Epoch: 100060 | Loss: 0.4375074505805969 | Test loss: 0.44890114665031433\n",
      "Epoch: 100070 | Loss: 0.437495619058609 | Test loss: 0.44888731837272644\n",
      "Epoch: 100080 | Loss: 0.43748387694358826 | Test loss: 0.4488734304904938\n",
      "Epoch: 100090 | Loss: 0.4374721050262451 | Test loss: 0.4488596022129059\n",
      "Epoch: 100100 | Loss: 0.437460333108902 | Test loss: 0.448845773935318\n",
      "Epoch: 100110 | Loss: 0.4374485909938812 | Test loss: 0.4488319456577301\n",
      "Epoch: 100120 | Loss: 0.43743687868118286 | Test loss: 0.44881805777549744\n",
      "Epoch: 100130 | Loss: 0.4374251067638397 | Test loss: 0.44880422949790955\n",
      "Epoch: 100140 | Loss: 0.4374133050441742 | Test loss: 0.44879037141799927\n",
      "Epoch: 100150 | Loss: 0.43740159273147583 | Test loss: 0.4487765431404114\n",
      "Epoch: 100160 | Loss: 0.4373898208141327 | Test loss: 0.4487627148628235\n",
      "Epoch: 100170 | Loss: 0.43737801909446716 | Test loss: 0.4487488865852356\n",
      "Epoch: 100180 | Loss: 0.4373663067817688 | Test loss: 0.44873499870300293\n",
      "Epoch: 100190 | Loss: 0.43735453486442566 | Test loss: 0.44872117042541504\n",
      "Epoch: 100200 | Loss: 0.43734273314476013 | Test loss: 0.44870731234550476\n",
      "Epoch: 100210 | Loss: 0.43733102083206177 | Test loss: 0.44869348406791687\n",
      "Epoch: 100220 | Loss: 0.43731924891471863 | Test loss: 0.4486795961856842\n",
      "Epoch: 100230 | Loss: 0.4373075067996979 | Test loss: 0.4486657679080963\n",
      "Epoch: 100240 | Loss: 0.43729573488235474 | Test loss: 0.44865188002586365\n",
      "Epoch: 100250 | Loss: 0.437283992767334 | Test loss: 0.44863805174827576\n",
      "Epoch: 100260 | Loss: 0.43727216124534607 | Test loss: 0.44862422347068787\n",
      "Epoch: 100270 | Loss: 0.4372604489326477 | Test loss: 0.4486103951931\n",
      "Epoch: 100280 | Loss: 0.43724870681762695 | Test loss: 0.4485965371131897\n",
      "Epoch: 100290 | Loss: 0.4372369349002838 | Test loss: 0.4485827088356018\n",
      "Epoch: 100300 | Loss: 0.4372251629829407 | Test loss: 0.44856882095336914\n",
      "Epoch: 100310 | Loss: 0.4372134208679199 | Test loss: 0.44855499267578125\n",
      "Epoch: 100320 | Loss: 0.4372016489505768 | Test loss: 0.44854116439819336\n",
      "Epoch: 100330 | Loss: 0.43718987703323364 | Test loss: 0.44852733612060547\n",
      "Epoch: 100340 | Loss: 0.4371781349182129 | Test loss: 0.4485134780406952\n",
      "Epoch: 100350 | Loss: 0.4371664226055145 | Test loss: 0.4484996497631073\n",
      "Epoch: 100360 | Loss: 0.4371545910835266 | Test loss: 0.44848576188087463\n",
      "Epoch: 100370 | Loss: 0.43714284896850586 | Test loss: 0.44847193360328674\n",
      "Epoch: 100380 | Loss: 0.4371311366558075 | Test loss: 0.44845810532569885\n",
      "Epoch: 100390 | Loss: 0.4371193051338196 | Test loss: 0.4484442174434662\n",
      "Epoch: 100400 | Loss: 0.43710756301879883 | Test loss: 0.4484303891658783\n",
      "Epoch: 100410 | Loss: 0.43709585070610046 | Test loss: 0.448416531085968\n",
      "Epoch: 100420 | Loss: 0.43708401918411255 | Test loss: 0.4484027028083801\n",
      "Epoch: 100430 | Loss: 0.4370722770690918 | Test loss: 0.44838887453079224\n",
      "Epoch: 100440 | Loss: 0.43706056475639343 | Test loss: 0.44837498664855957\n",
      "Epoch: 100450 | Loss: 0.4370487630367279 | Test loss: 0.4483611583709717\n",
      "Epoch: 100460 | Loss: 0.43703699111938477 | Test loss: 0.4483473002910614\n",
      "Epoch: 100470 | Loss: 0.4370252788066864 | Test loss: 0.4483334720134735\n",
      "Epoch: 100480 | Loss: 0.43701353669166565 | Test loss: 0.4483196437358856\n",
      "Epoch: 100490 | Loss: 0.43700170516967773 | Test loss: 0.44830575585365295\n",
      "Epoch: 100500 | Loss: 0.43698999285697937 | Test loss: 0.44829192757606506\n",
      "Epoch: 100510 | Loss: 0.43697819113731384 | Test loss: 0.4482780396938324\n",
      "Epoch: 100520 | Loss: 0.4369664192199707 | Test loss: 0.4482642114162445\n",
      "Epoch: 100530 | Loss: 0.43695470690727234 | Test loss: 0.4482503831386566\n",
      "Epoch: 100540 | Loss: 0.4369429051876068 | Test loss: 0.4482365548610687\n",
      "Epoch: 100550 | Loss: 0.43693113327026367 | Test loss: 0.44822269678115845\n",
      "Epoch: 100560 | Loss: 0.4369194209575653 | Test loss: 0.44820886850357056\n",
      "Epoch: 100570 | Loss: 0.43690767884254456 | Test loss: 0.4481949806213379\n",
      "Epoch: 100580 | Loss: 0.4368959069252014 | Test loss: 0.44818115234375\n",
      "Epoch: 100590 | Loss: 0.4368841350078583 | Test loss: 0.4481673240661621\n",
      "Epoch: 100600 | Loss: 0.4368723928928375 | Test loss: 0.44815346598625183\n",
      "Epoch: 100610 | Loss: 0.4368606209754944 | Test loss: 0.44813963770866394\n",
      "Epoch: 100620 | Loss: 0.43684884905815125 | Test loss: 0.4481257498264313\n",
      "Epoch: 100630 | Loss: 0.4368371069431305 | Test loss: 0.4481119215488434\n",
      "Epoch: 100640 | Loss: 0.43682533502578735 | Test loss: 0.4480980932712555\n",
      "Epoch: 100650 | Loss: 0.4368135631084442 | Test loss: 0.4480842053890228\n",
      "Epoch: 100660 | Loss: 0.43680182099342346 | Test loss: 0.44807037711143494\n",
      "Epoch: 100670 | Loss: 0.4367900490760803 | Test loss: 0.44805651903152466\n",
      "Epoch: 100680 | Loss: 0.4367782771587372 | Test loss: 0.44804269075393677\n",
      "Epoch: 100690 | Loss: 0.43676653504371643 | Test loss: 0.4480288624763489\n",
      "Epoch: 100700 | Loss: 0.4367547929286957 | Test loss: 0.4480149745941162\n",
      "Epoch: 100710 | Loss: 0.43674299120903015 | Test loss: 0.4480011463165283\n",
      "Epoch: 100720 | Loss: 0.4367312490940094 | Test loss: 0.44798731803894043\n",
      "Epoch: 100730 | Loss: 0.43671947717666626 | Test loss: 0.44797345995903015\n",
      "Epoch: 100740 | Loss: 0.4367077350616455 | Test loss: 0.44795963168144226\n",
      "Epoch: 100750 | Loss: 0.43669596314430237 | Test loss: 0.44794580340385437\n",
      "Epoch: 100760 | Loss: 0.4366842210292816 | Test loss: 0.4479319155216217\n",
      "Epoch: 100770 | Loss: 0.4366724491119385 | Test loss: 0.4479180872440338\n",
      "Epoch: 100780 | Loss: 0.43666067719459534 | Test loss: 0.44790419936180115\n",
      "Epoch: 100790 | Loss: 0.4366489350795746 | Test loss: 0.44789037108421326\n",
      "Epoch: 100800 | Loss: 0.43663716316223145 | Test loss: 0.44787654280662537\n",
      "Epoch: 100810 | Loss: 0.4366254508495331 | Test loss: 0.4478627145290375\n",
      "Epoch: 100820 | Loss: 0.43661364912986755 | Test loss: 0.4478488564491272\n",
      "Epoch: 100830 | Loss: 0.4366018772125244 | Test loss: 0.44783496856689453\n",
      "Epoch: 100840 | Loss: 0.43659016489982605 | Test loss: 0.44782114028930664\n",
      "Epoch: 100850 | Loss: 0.4365783631801605 | Test loss: 0.44780731201171875\n",
      "Epoch: 100860 | Loss: 0.43656665086746216 | Test loss: 0.44779345393180847\n",
      "Epoch: 100870 | Loss: 0.43655481934547424 | Test loss: 0.4477796256542206\n",
      "Epoch: 100880 | Loss: 0.4365430772304535 | Test loss: 0.4477657377719879\n",
      "Epoch: 100890 | Loss: 0.43653130531311035 | Test loss: 0.4477519094944\n",
      "Epoch: 100900 | Loss: 0.4365195333957672 | Test loss: 0.44773808121681213\n",
      "Epoch: 100910 | Loss: 0.43650779128074646 | Test loss: 0.44772425293922424\n",
      "Epoch: 100920 | Loss: 0.4364960789680481 | Test loss: 0.4477103650569916\n",
      "Epoch: 100930 | Loss: 0.43648430705070496 | Test loss: 0.4476965367794037\n",
      "Epoch: 100940 | Loss: 0.43647250533103943 | Test loss: 0.4476826786994934\n",
      "Epoch: 100950 | Loss: 0.43646079301834106 | Test loss: 0.4476688504219055\n",
      "Epoch: 100960 | Loss: 0.4364490211009979 | Test loss: 0.4476550221443176\n",
      "Epoch: 100970 | Loss: 0.4364372193813324 | Test loss: 0.44764119386672974\n",
      "Epoch: 100980 | Loss: 0.43642550706863403 | Test loss: 0.44762730598449707\n",
      "Epoch: 100990 | Loss: 0.4364137351512909 | Test loss: 0.4476134777069092\n",
      "Epoch: 101000 | Loss: 0.43640193343162537 | Test loss: 0.4475996196269989\n",
      "Epoch: 101010 | Loss: 0.436390221118927 | Test loss: 0.447585791349411\n",
      "Epoch: 101020 | Loss: 0.43637844920158386 | Test loss: 0.44757190346717834\n",
      "Epoch: 101030 | Loss: 0.4363667070865631 | Test loss: 0.44755807518959045\n",
      "Epoch: 101040 | Loss: 0.43635493516921997 | Test loss: 0.4475441873073578\n",
      "Epoch: 101050 | Loss: 0.4363431930541992 | Test loss: 0.4475303590297699\n",
      "Epoch: 101060 | Loss: 0.4363313615322113 | Test loss: 0.447516530752182\n",
      "Epoch: 101070 | Loss: 0.43631964921951294 | Test loss: 0.4475027024745941\n",
      "Epoch: 101080 | Loss: 0.4363079071044922 | Test loss: 0.44748884439468384\n",
      "Epoch: 101090 | Loss: 0.43629613518714905 | Test loss: 0.44747501611709595\n",
      "Epoch: 101100 | Loss: 0.4362843632698059 | Test loss: 0.4474611282348633\n",
      "Epoch: 101110 | Loss: 0.43627262115478516 | Test loss: 0.4474472999572754\n",
      "Epoch: 101120 | Loss: 0.436260849237442 | Test loss: 0.4474334716796875\n",
      "Epoch: 101130 | Loss: 0.4362490773200989 | Test loss: 0.4474196434020996\n",
      "Epoch: 101140 | Loss: 0.4362373352050781 | Test loss: 0.44740578532218933\n",
      "Epoch: 101150 | Loss: 0.43622562289237976 | Test loss: 0.44739195704460144\n",
      "Epoch: 101160 | Loss: 0.43621379137039185 | Test loss: 0.4473780691623688\n",
      "Epoch: 101170 | Loss: 0.4362020492553711 | Test loss: 0.4473642408847809\n",
      "Epoch: 101180 | Loss: 0.43619033694267273 | Test loss: 0.447350412607193\n",
      "Epoch: 101190 | Loss: 0.4361785054206848 | Test loss: 0.4473365247249603\n",
      "Epoch: 101200 | Loss: 0.43616676330566406 | Test loss: 0.44732269644737244\n",
      "Epoch: 101210 | Loss: 0.4361550509929657 | Test loss: 0.44730883836746216\n",
      "Epoch: 101220 | Loss: 0.4361432194709778 | Test loss: 0.44729501008987427\n",
      "Epoch: 101230 | Loss: 0.43613147735595703 | Test loss: 0.4472811818122864\n",
      "Epoch: 101240 | Loss: 0.43611976504325867 | Test loss: 0.4472672939300537\n",
      "Epoch: 101250 | Loss: 0.43610796332359314 | Test loss: 0.4472534656524658\n",
      "Epoch: 101260 | Loss: 0.43609619140625 | Test loss: 0.44723960757255554\n",
      "Epoch: 101270 | Loss: 0.43608447909355164 | Test loss: 0.44722577929496765\n",
      "Epoch: 101280 | Loss: 0.4360727369785309 | Test loss: 0.44721195101737976\n",
      "Epoch: 101290 | Loss: 0.43606090545654297 | Test loss: 0.4471980631351471\n",
      "Epoch: 101300 | Loss: 0.4360491931438446 | Test loss: 0.4471842348575592\n",
      "Epoch: 101310 | Loss: 0.4360373914241791 | Test loss: 0.44717034697532654\n",
      "Epoch: 101320 | Loss: 0.43602561950683594 | Test loss: 0.44715651869773865\n",
      "Epoch: 101330 | Loss: 0.4360139071941376 | Test loss: 0.44714269042015076\n",
      "Epoch: 101340 | Loss: 0.43600210547447205 | Test loss: 0.44712886214256287\n",
      "Epoch: 101350 | Loss: 0.4359903335571289 | Test loss: 0.4471150040626526\n",
      "Epoch: 101360 | Loss: 0.43597862124443054 | Test loss: 0.4471011757850647\n",
      "Epoch: 101370 | Loss: 0.4359668791294098 | Test loss: 0.44708728790283203\n",
      "Epoch: 101380 | Loss: 0.43595510721206665 | Test loss: 0.44707345962524414\n",
      "Epoch: 101390 | Loss: 0.4359433352947235 | Test loss: 0.44705963134765625\n",
      "Epoch: 101400 | Loss: 0.43593159317970276 | Test loss: 0.44704577326774597\n",
      "Epoch: 101410 | Loss: 0.4359198212623596 | Test loss: 0.4470319449901581\n",
      "Epoch: 101420 | Loss: 0.4359080493450165 | Test loss: 0.4470180571079254\n",
      "Epoch: 101430 | Loss: 0.4358963072299957 | Test loss: 0.4470042288303375\n",
      "Epoch: 101440 | Loss: 0.4358845353126526 | Test loss: 0.44699040055274963\n",
      "Epoch: 101450 | Loss: 0.43587276339530945 | Test loss: 0.44697651267051697\n",
      "Epoch: 101460 | Loss: 0.4358610212802887 | Test loss: 0.4469626843929291\n",
      "Epoch: 101470 | Loss: 0.43584924936294556 | Test loss: 0.4469488263130188\n",
      "Epoch: 101480 | Loss: 0.4358374774456024 | Test loss: 0.4469349980354309\n",
      "Epoch: 101490 | Loss: 0.43582573533058167 | Test loss: 0.446921169757843\n",
      "Epoch: 101500 | Loss: 0.4358139932155609 | Test loss: 0.44690728187561035\n",
      "Epoch: 101510 | Loss: 0.4358021914958954 | Test loss: 0.44689345359802246\n",
      "Epoch: 101520 | Loss: 0.43579044938087463 | Test loss: 0.44687962532043457\n",
      "Epoch: 101530 | Loss: 0.4357786774635315 | Test loss: 0.4468657672405243\n",
      "Epoch: 101540 | Loss: 0.43576693534851074 | Test loss: 0.4468519389629364\n",
      "Epoch: 101550 | Loss: 0.4357551634311676 | Test loss: 0.4468381106853485\n",
      "Epoch: 101560 | Loss: 0.43574342131614685 | Test loss: 0.44682422280311584\n",
      "Epoch: 101570 | Loss: 0.4357316493988037 | Test loss: 0.44681039452552795\n",
      "Epoch: 101580 | Loss: 0.43571987748146057 | Test loss: 0.4467965066432953\n",
      "Epoch: 101590 | Loss: 0.4357081353664398 | Test loss: 0.4467826783657074\n",
      "Epoch: 101600 | Loss: 0.4356963634490967 | Test loss: 0.4467688500881195\n",
      "Epoch: 101610 | Loss: 0.4356846511363983 | Test loss: 0.4467550218105316\n",
      "Epoch: 101620 | Loss: 0.4356728494167328 | Test loss: 0.44674116373062134\n",
      "Epoch: 101630 | Loss: 0.43566107749938965 | Test loss: 0.44672727584838867\n",
      "Epoch: 101640 | Loss: 0.4356493651866913 | Test loss: 0.4467134475708008\n",
      "Epoch: 101650 | Loss: 0.43563756346702576 | Test loss: 0.4466996192932129\n",
      "Epoch: 101660 | Loss: 0.4356258511543274 | Test loss: 0.4466857612133026\n",
      "Epoch: 101670 | Loss: 0.4356140196323395 | Test loss: 0.4466719329357147\n",
      "Epoch: 101680 | Loss: 0.4356022775173187 | Test loss: 0.44665804505348206\n",
      "Epoch: 101690 | Loss: 0.4355905055999756 | Test loss: 0.44664421677589417\n",
      "Epoch: 101700 | Loss: 0.43557873368263245 | Test loss: 0.4466303884983063\n",
      "Epoch: 101710 | Loss: 0.4355669915676117 | Test loss: 0.4466165602207184\n",
      "Epoch: 101720 | Loss: 0.43555527925491333 | Test loss: 0.4466026723384857\n",
      "Epoch: 101730 | Loss: 0.4355435073375702 | Test loss: 0.4465888440608978\n",
      "Epoch: 101740 | Loss: 0.43553170561790466 | Test loss: 0.44657498598098755\n",
      "Epoch: 101750 | Loss: 0.4355199933052063 | Test loss: 0.44656115770339966\n",
      "Epoch: 101760 | Loss: 0.43550822138786316 | Test loss: 0.44654732942581177\n",
      "Epoch: 101770 | Loss: 0.43549641966819763 | Test loss: 0.4465335011482239\n",
      "Epoch: 101780 | Loss: 0.43548470735549927 | Test loss: 0.4465196132659912\n",
      "Epoch: 101790 | Loss: 0.43547293543815613 | Test loss: 0.4465057849884033\n",
      "Epoch: 101800 | Loss: 0.4354611337184906 | Test loss: 0.44649192690849304\n",
      "Epoch: 101810 | Loss: 0.43544942140579224 | Test loss: 0.44647809863090515\n",
      "Epoch: 101820 | Loss: 0.4354376494884491 | Test loss: 0.4464642107486725\n",
      "Epoch: 101830 | Loss: 0.43542590737342834 | Test loss: 0.4464503824710846\n",
      "Epoch: 101840 | Loss: 0.4354141354560852 | Test loss: 0.44643649458885193\n",
      "Epoch: 101850 | Loss: 0.43540239334106445 | Test loss: 0.44642266631126404\n",
      "Epoch: 101860 | Loss: 0.43539056181907654 | Test loss: 0.44640883803367615\n",
      "Epoch: 101870 | Loss: 0.4353788495063782 | Test loss: 0.44639500975608826\n",
      "Epoch: 101880 | Loss: 0.4353671073913574 | Test loss: 0.446381151676178\n",
      "Epoch: 101890 | Loss: 0.4353553354740143 | Test loss: 0.4463673233985901\n",
      "Epoch: 101900 | Loss: 0.43534356355667114 | Test loss: 0.4463534355163574\n",
      "Epoch: 101910 | Loss: 0.4353318214416504 | Test loss: 0.44633960723876953\n",
      "Epoch: 101920 | Loss: 0.43532004952430725 | Test loss: 0.44632577896118164\n",
      "Epoch: 101930 | Loss: 0.4353082776069641 | Test loss: 0.44631195068359375\n",
      "Epoch: 101940 | Loss: 0.43529653549194336 | Test loss: 0.44629809260368347\n",
      "Epoch: 101950 | Loss: 0.435284823179245 | Test loss: 0.4462842643260956\n",
      "Epoch: 101960 | Loss: 0.4352729916572571 | Test loss: 0.4462703764438629\n",
      "Epoch: 101970 | Loss: 0.43526124954223633 | Test loss: 0.446256548166275\n",
      "Epoch: 101980 | Loss: 0.43524953722953796 | Test loss: 0.44624271988868713\n",
      "Epoch: 101990 | Loss: 0.43523770570755005 | Test loss: 0.44622883200645447\n",
      "Epoch: 102000 | Loss: 0.4352259635925293 | Test loss: 0.4462150037288666\n",
      "Epoch: 102010 | Loss: 0.43521425127983093 | Test loss: 0.4462011456489563\n",
      "Epoch: 102020 | Loss: 0.435202419757843 | Test loss: 0.4461873173713684\n",
      "Epoch: 102030 | Loss: 0.43519067764282227 | Test loss: 0.4461734890937805\n",
      "Epoch: 102040 | Loss: 0.4351789653301239 | Test loss: 0.44615960121154785\n",
      "Epoch: 102050 | Loss: 0.4351671636104584 | Test loss: 0.44614577293395996\n",
      "Epoch: 102060 | Loss: 0.43515539169311523 | Test loss: 0.4461319148540497\n",
      "Epoch: 102070 | Loss: 0.43514367938041687 | Test loss: 0.4461180865764618\n",
      "Epoch: 102080 | Loss: 0.4351319372653961 | Test loss: 0.4461042582988739\n",
      "Epoch: 102090 | Loss: 0.4351201057434082 | Test loss: 0.44609037041664124\n",
      "Epoch: 102100 | Loss: 0.43510839343070984 | Test loss: 0.44607654213905334\n",
      "Epoch: 102110 | Loss: 0.4350965917110443 | Test loss: 0.4460626542568207\n",
      "Epoch: 102120 | Loss: 0.43508481979370117 | Test loss: 0.4460488259792328\n",
      "Epoch: 102130 | Loss: 0.4350731074810028 | Test loss: 0.4460349977016449\n",
      "Epoch: 102140 | Loss: 0.4350613057613373 | Test loss: 0.446021169424057\n",
      "Epoch: 102150 | Loss: 0.43504953384399414 | Test loss: 0.44600731134414673\n",
      "Epoch: 102160 | Loss: 0.4350378215312958 | Test loss: 0.44599348306655884\n",
      "Epoch: 102170 | Loss: 0.435026079416275 | Test loss: 0.44597959518432617\n",
      "Epoch: 102180 | Loss: 0.4350143074989319 | Test loss: 0.4459657669067383\n",
      "Epoch: 102190 | Loss: 0.43500253558158875 | Test loss: 0.4459519386291504\n",
      "Epoch: 102200 | Loss: 0.434990793466568 | Test loss: 0.4459380805492401\n",
      "Epoch: 102210 | Loss: 0.43497902154922485 | Test loss: 0.4459242522716522\n",
      "Epoch: 102220 | Loss: 0.4349672496318817 | Test loss: 0.44591036438941956\n",
      "Epoch: 102230 | Loss: 0.43495550751686096 | Test loss: 0.44589653611183167\n",
      "Epoch: 102240 | Loss: 0.4349437355995178 | Test loss: 0.4458827078342438\n",
      "Epoch: 102250 | Loss: 0.4349319636821747 | Test loss: 0.4458688199520111\n",
      "Epoch: 102260 | Loss: 0.43492022156715393 | Test loss: 0.4458549916744232\n",
      "Epoch: 102270 | Loss: 0.4349084496498108 | Test loss: 0.44584113359451294\n",
      "Epoch: 102280 | Loss: 0.43489667773246765 | Test loss: 0.44582730531692505\n",
      "Epoch: 102290 | Loss: 0.4348849356174469 | Test loss: 0.44581347703933716\n",
      "Epoch: 102300 | Loss: 0.43487319350242615 | Test loss: 0.4457995891571045\n",
      "Epoch: 102310 | Loss: 0.4348613917827606 | Test loss: 0.4457857608795166\n",
      "Epoch: 102320 | Loss: 0.43484964966773987 | Test loss: 0.4457719326019287\n",
      "Epoch: 102330 | Loss: 0.43483787775039673 | Test loss: 0.44575807452201843\n",
      "Epoch: 102340 | Loss: 0.434826135635376 | Test loss: 0.44574424624443054\n",
      "Epoch: 102350 | Loss: 0.43481436371803284 | Test loss: 0.44573041796684265\n",
      "Epoch: 102360 | Loss: 0.4348026216030121 | Test loss: 0.44571653008461\n",
      "Epoch: 102370 | Loss: 0.43479084968566895 | Test loss: 0.4457027018070221\n",
      "Epoch: 102380 | Loss: 0.4347790777683258 | Test loss: 0.44568881392478943\n",
      "Epoch: 102390 | Loss: 0.43476733565330505 | Test loss: 0.44567498564720154\n",
      "Epoch: 102400 | Loss: 0.4347555637359619 | Test loss: 0.44566115736961365\n",
      "Epoch: 102410 | Loss: 0.43474385142326355 | Test loss: 0.44564732909202576\n",
      "Epoch: 102420 | Loss: 0.434732049703598 | Test loss: 0.4456334710121155\n",
      "Epoch: 102430 | Loss: 0.4347202777862549 | Test loss: 0.4456195831298828\n",
      "Epoch: 102440 | Loss: 0.4347085654735565 | Test loss: 0.4456057548522949\n",
      "Epoch: 102450 | Loss: 0.434696763753891 | Test loss: 0.44559192657470703\n",
      "Epoch: 102460 | Loss: 0.4346850514411926 | Test loss: 0.44557806849479675\n",
      "Epoch: 102470 | Loss: 0.4346732199192047 | Test loss: 0.44556424021720886\n",
      "Epoch: 102480 | Loss: 0.43466147780418396 | Test loss: 0.4455503523349762\n",
      "Epoch: 102490 | Loss: 0.4346497058868408 | Test loss: 0.4455365240573883\n",
      "Epoch: 102500 | Loss: 0.4346379339694977 | Test loss: 0.4455226957798004\n",
      "Epoch: 102510 | Loss: 0.43462619185447693 | Test loss: 0.4455088675022125\n",
      "Epoch: 102520 | Loss: 0.43461447954177856 | Test loss: 0.44549497961997986\n",
      "Epoch: 102530 | Loss: 0.4346027076244354 | Test loss: 0.44548115134239197\n",
      "Epoch: 102540 | Loss: 0.4345909059047699 | Test loss: 0.4454672932624817\n",
      "Epoch: 102550 | Loss: 0.43457919359207153 | Test loss: 0.4454534649848938\n",
      "Epoch: 102560 | Loss: 0.4345674216747284 | Test loss: 0.4454396367073059\n",
      "Epoch: 102570 | Loss: 0.43455561995506287 | Test loss: 0.445425808429718\n",
      "Epoch: 102580 | Loss: 0.4345439076423645 | Test loss: 0.44541192054748535\n",
      "Epoch: 102590 | Loss: 0.43453213572502136 | Test loss: 0.44539809226989746\n",
      "Epoch: 102600 | Loss: 0.43452033400535583 | Test loss: 0.4453842341899872\n",
      "Epoch: 102610 | Loss: 0.43450862169265747 | Test loss: 0.4453704059123993\n",
      "Epoch: 102620 | Loss: 0.43449684977531433 | Test loss: 0.4453565180301666\n",
      "Epoch: 102630 | Loss: 0.4344851076602936 | Test loss: 0.44534268975257874\n",
      "Epoch: 102640 | Loss: 0.43447333574295044 | Test loss: 0.44532880187034607\n",
      "Epoch: 102650 | Loss: 0.4344615936279297 | Test loss: 0.4453149735927582\n",
      "Epoch: 102660 | Loss: 0.4344497621059418 | Test loss: 0.4453011453151703\n",
      "Epoch: 102670 | Loss: 0.4344380497932434 | Test loss: 0.4452873170375824\n",
      "Epoch: 102680 | Loss: 0.43442630767822266 | Test loss: 0.4452734589576721\n",
      "Epoch: 102690 | Loss: 0.4344145357608795 | Test loss: 0.44525963068008423\n",
      "Epoch: 102700 | Loss: 0.4344027638435364 | Test loss: 0.44524574279785156\n",
      "Epoch: 102710 | Loss: 0.4343910217285156 | Test loss: 0.44523191452026367\n",
      "Epoch: 102720 | Loss: 0.4343792498111725 | Test loss: 0.4452180862426758\n",
      "Epoch: 102730 | Loss: 0.43436747789382935 | Test loss: 0.4452042579650879\n",
      "Epoch: 102740 | Loss: 0.4343557357788086 | Test loss: 0.4451903998851776\n",
      "Epoch: 102750 | Loss: 0.43434402346611023 | Test loss: 0.4451765716075897\n",
      "Epoch: 102760 | Loss: 0.4343321919441223 | Test loss: 0.44516268372535706\n",
      "Epoch: 102770 | Loss: 0.43432044982910156 | Test loss: 0.44514885544776917\n",
      "Epoch: 102780 | Loss: 0.4343087375164032 | Test loss: 0.4451350271701813\n",
      "Epoch: 102790 | Loss: 0.4342969059944153 | Test loss: 0.4451211392879486\n",
      "Epoch: 102800 | Loss: 0.43428516387939453 | Test loss: 0.4451073110103607\n",
      "Epoch: 102810 | Loss: 0.43427345156669617 | Test loss: 0.44509345293045044\n",
      "Epoch: 102820 | Loss: 0.43426162004470825 | Test loss: 0.44507962465286255\n",
      "Epoch: 102830 | Loss: 0.4342498779296875 | Test loss: 0.44506579637527466\n",
      "Epoch: 102840 | Loss: 0.43423816561698914 | Test loss: 0.445051908493042\n",
      "Epoch: 102850 | Loss: 0.4342263638973236 | Test loss: 0.4450380802154541\n",
      "Epoch: 102860 | Loss: 0.43421459197998047 | Test loss: 0.4450242221355438\n",
      "Epoch: 102870 | Loss: 0.4342028796672821 | Test loss: 0.44501039385795593\n",
      "Epoch: 102880 | Loss: 0.43419113755226135 | Test loss: 0.44499656558036804\n",
      "Epoch: 102890 | Loss: 0.43417930603027344 | Test loss: 0.4449826776981354\n",
      "Epoch: 102900 | Loss: 0.4341675937175751 | Test loss: 0.4449688494205475\n",
      "Epoch: 102910 | Loss: 0.43415579199790955 | Test loss: 0.4449549615383148\n",
      "Epoch: 102920 | Loss: 0.4341440200805664 | Test loss: 0.44494113326072693\n",
      "Epoch: 102930 | Loss: 0.43413230776786804 | Test loss: 0.44492730498313904\n",
      "Epoch: 102940 | Loss: 0.4341205060482025 | Test loss: 0.44491347670555115\n",
      "Epoch: 102950 | Loss: 0.4341087341308594 | Test loss: 0.44489961862564087\n",
      "Epoch: 102960 | Loss: 0.434097021818161 | Test loss: 0.444885790348053\n",
      "Epoch: 102970 | Loss: 0.43408527970314026 | Test loss: 0.4448719024658203\n",
      "Epoch: 102980 | Loss: 0.4340735077857971 | Test loss: 0.4448580741882324\n",
      "Epoch: 102990 | Loss: 0.434061735868454 | Test loss: 0.44484424591064453\n",
      "Epoch: 103000 | Loss: 0.4340499937534332 | Test loss: 0.44483038783073425\n",
      "Epoch: 103010 | Loss: 0.4340382218360901 | Test loss: 0.44481655955314636\n",
      "Epoch: 103020 | Loss: 0.43402644991874695 | Test loss: 0.4448026716709137\n",
      "Epoch: 103030 | Loss: 0.4340147078037262 | Test loss: 0.4447888433933258\n",
      "Epoch: 103040 | Loss: 0.43400293588638306 | Test loss: 0.4447750151157379\n",
      "Epoch: 103050 | Loss: 0.4339911639690399 | Test loss: 0.44476112723350525\n",
      "Epoch: 103060 | Loss: 0.43397942185401917 | Test loss: 0.44474729895591736\n",
      "Epoch: 103070 | Loss: 0.433967649936676 | Test loss: 0.4447334408760071\n",
      "Epoch: 103080 | Loss: 0.4339558780193329 | Test loss: 0.4447196125984192\n",
      "Epoch: 103090 | Loss: 0.43394413590431213 | Test loss: 0.4447057843208313\n",
      "Epoch: 103100 | Loss: 0.4339323937892914 | Test loss: 0.44469189643859863\n",
      "Epoch: 103110 | Loss: 0.43392059206962585 | Test loss: 0.44467806816101074\n",
      "Epoch: 103120 | Loss: 0.4339088499546051 | Test loss: 0.44466423988342285\n",
      "Epoch: 103130 | Loss: 0.43389710783958435 | Test loss: 0.4446503818035126\n",
      "Epoch: 103140 | Loss: 0.4338853359222412 | Test loss: 0.4446365535259247\n",
      "Epoch: 103150 | Loss: 0.43387356400489807 | Test loss: 0.4446227252483368\n",
      "Epoch: 103160 | Loss: 0.4338618218898773 | Test loss: 0.4446088373661041\n",
      "Epoch: 103170 | Loss: 0.4338500499725342 | Test loss: 0.44459500908851624\n",
      "Epoch: 103180 | Loss: 0.43383827805519104 | Test loss: 0.44458112120628357\n",
      "Epoch: 103190 | Loss: 0.4338265359401703 | Test loss: 0.4445672929286957\n",
      "Epoch: 103200 | Loss: 0.43381476402282715 | Test loss: 0.4445534646511078\n",
      "Epoch: 103210 | Loss: 0.4338030517101288 | Test loss: 0.4445396363735199\n",
      "Epoch: 103220 | Loss: 0.43379124999046326 | Test loss: 0.4445257782936096\n",
      "Epoch: 103230 | Loss: 0.4337795376777649 | Test loss: 0.44451189041137695\n",
      "Epoch: 103240 | Loss: 0.43376776576042175 | Test loss: 0.44449806213378906\n",
      "Epoch: 103250 | Loss: 0.4337559640407562 | Test loss: 0.44448423385620117\n",
      "Epoch: 103260 | Loss: 0.43374425172805786 | Test loss: 0.4444703757762909\n",
      "Epoch: 103270 | Loss: 0.43373242020606995 | Test loss: 0.444456547498703\n",
      "Epoch: 103280 | Loss: 0.4337206780910492 | Test loss: 0.44444265961647034\n",
      "Epoch: 103290 | Loss: 0.43370890617370605 | Test loss: 0.44442883133888245\n",
      "Epoch: 103300 | Loss: 0.4336971342563629 | Test loss: 0.44441500306129456\n",
      "Epoch: 103310 | Loss: 0.43368539214134216 | Test loss: 0.44440117478370667\n",
      "Epoch: 103320 | Loss: 0.4336736798286438 | Test loss: 0.444387286901474\n",
      "Epoch: 103330 | Loss: 0.43366190791130066 | Test loss: 0.4443734586238861\n",
      "Epoch: 103340 | Loss: 0.43365010619163513 | Test loss: 0.44435960054397583\n",
      "Epoch: 103350 | Loss: 0.43363839387893677 | Test loss: 0.44434577226638794\n",
      "Epoch: 103360 | Loss: 0.43362662196159363 | Test loss: 0.44433194398880005\n",
      "Epoch: 103370 | Loss: 0.4336148202419281 | Test loss: 0.44431811571121216\n",
      "Epoch: 103380 | Loss: 0.43360310792922974 | Test loss: 0.4443042278289795\n",
      "Epoch: 103390 | Loss: 0.4335913360118866 | Test loss: 0.4442903995513916\n",
      "Epoch: 103400 | Loss: 0.43357953429222107 | Test loss: 0.4442765414714813\n",
      "Epoch: 103410 | Loss: 0.4335678219795227 | Test loss: 0.44426271319389343\n",
      "Epoch: 103420 | Loss: 0.43355605006217957 | Test loss: 0.44424882531166077\n",
      "Epoch: 103430 | Loss: 0.4335443079471588 | Test loss: 0.4442349970340729\n",
      "Epoch: 103440 | Loss: 0.4335325360298157 | Test loss: 0.4442211091518402\n",
      "Epoch: 103450 | Loss: 0.4335207939147949 | Test loss: 0.4442072808742523\n",
      "Epoch: 103460 | Loss: 0.433508962392807 | Test loss: 0.44419345259666443\n",
      "Epoch: 103470 | Loss: 0.43349725008010864 | Test loss: 0.44417962431907654\n",
      "Epoch: 103480 | Loss: 0.4334855079650879 | Test loss: 0.44416576623916626\n",
      "Epoch: 103490 | Loss: 0.43347373604774475 | Test loss: 0.44415193796157837\n",
      "Epoch: 103500 | Loss: 0.4334619641304016 | Test loss: 0.4441380500793457\n",
      "Epoch: 103510 | Loss: 0.43345022201538086 | Test loss: 0.4441242218017578\n",
      "Epoch: 103520 | Loss: 0.4334384500980377 | Test loss: 0.4441103935241699\n",
      "Epoch: 103530 | Loss: 0.4334266781806946 | Test loss: 0.44409656524658203\n",
      "Epoch: 103540 | Loss: 0.43341493606567383 | Test loss: 0.44408270716667175\n",
      "Epoch: 103550 | Loss: 0.43340322375297546 | Test loss: 0.44406887888908386\n",
      "Epoch: 103560 | Loss: 0.43339139223098755 | Test loss: 0.4440549910068512\n",
      "Epoch: 103570 | Loss: 0.4333796501159668 | Test loss: 0.4440411627292633\n",
      "Epoch: 103580 | Loss: 0.43336793780326843 | Test loss: 0.4440273344516754\n",
      "Epoch: 103590 | Loss: 0.4333561062812805 | Test loss: 0.44401344656944275\n",
      "Epoch: 103600 | Loss: 0.43334436416625977 | Test loss: 0.44399961829185486\n",
      "Epoch: 103610 | Loss: 0.4333326518535614 | Test loss: 0.4439857602119446\n",
      "Epoch: 103620 | Loss: 0.4333208203315735 | Test loss: 0.4439719319343567\n",
      "Epoch: 103630 | Loss: 0.43330907821655273 | Test loss: 0.4439581036567688\n",
      "Epoch: 103640 | Loss: 0.43329736590385437 | Test loss: 0.44394421577453613\n",
      "Epoch: 103650 | Loss: 0.43328556418418884 | Test loss: 0.44393038749694824\n",
      "Epoch: 103660 | Loss: 0.4332737922668457 | Test loss: 0.44391652941703796\n",
      "Epoch: 103670 | Loss: 0.43326207995414734 | Test loss: 0.4439027011394501\n",
      "Epoch: 103680 | Loss: 0.4332503378391266 | Test loss: 0.4438888728618622\n",
      "Epoch: 103690 | Loss: 0.43323850631713867 | Test loss: 0.4438749849796295\n",
      "Epoch: 103700 | Loss: 0.4332267940044403 | Test loss: 0.4438611567020416\n",
      "Epoch: 103710 | Loss: 0.4332149922847748 | Test loss: 0.44384726881980896\n",
      "Epoch: 103720 | Loss: 0.43320322036743164 | Test loss: 0.44383344054222107\n",
      "Epoch: 103730 | Loss: 0.4331915080547333 | Test loss: 0.4438196122646332\n",
      "Epoch: 103740 | Loss: 0.43317970633506775 | Test loss: 0.4438057839870453\n",
      "Epoch: 103750 | Loss: 0.4331679344177246 | Test loss: 0.443791925907135\n",
      "Epoch: 103760 | Loss: 0.43315622210502625 | Test loss: 0.4437780976295471\n",
      "Epoch: 103770 | Loss: 0.4331444799900055 | Test loss: 0.44376420974731445\n",
      "Epoch: 103780 | Loss: 0.43313270807266235 | Test loss: 0.44375038146972656\n",
      "Epoch: 103790 | Loss: 0.4331209361553192 | Test loss: 0.44373655319213867\n",
      "Epoch: 103800 | Loss: 0.43310919404029846 | Test loss: 0.4437226951122284\n",
      "Epoch: 103810 | Loss: 0.4330974221229553 | Test loss: 0.4437088668346405\n",
      "Epoch: 103820 | Loss: 0.4330856502056122 | Test loss: 0.44369497895240784\n",
      "Epoch: 103830 | Loss: 0.43307390809059143 | Test loss: 0.44368115067481995\n",
      "Epoch: 103840 | Loss: 0.4330621361732483 | Test loss: 0.44366732239723206\n",
      "Epoch: 103850 | Loss: 0.43305036425590515 | Test loss: 0.4436534345149994\n",
      "Epoch: 103860 | Loss: 0.4330386221408844 | Test loss: 0.4436396062374115\n",
      "Epoch: 103870 | Loss: 0.43302685022354126 | Test loss: 0.4436257481575012\n",
      "Epoch: 103880 | Loss: 0.4330150783061981 | Test loss: 0.44361191987991333\n",
      "Epoch: 103890 | Loss: 0.43300333619117737 | Test loss: 0.44359809160232544\n",
      "Epoch: 103900 | Loss: 0.4329915940761566 | Test loss: 0.4435842037200928\n",
      "Epoch: 103910 | Loss: 0.4329797923564911 | Test loss: 0.4435703754425049\n",
      "Epoch: 103920 | Loss: 0.43296805024147034 | Test loss: 0.443556547164917\n",
      "Epoch: 103930 | Loss: 0.4329563081264496 | Test loss: 0.4435426890850067\n",
      "Epoch: 103940 | Loss: 0.43294453620910645 | Test loss: 0.4435288608074188\n",
      "Epoch: 103950 | Loss: 0.4329327642917633 | Test loss: 0.44351503252983093\n",
      "Epoch: 103960 | Loss: 0.43292102217674255 | Test loss: 0.44350114464759827\n",
      "Epoch: 103970 | Loss: 0.4329092502593994 | Test loss: 0.4434873163700104\n",
      "Epoch: 103980 | Loss: 0.4328974783420563 | Test loss: 0.4434734284877777\n",
      "Epoch: 103990 | Loss: 0.4328857362270355 | Test loss: 0.4434596002101898\n",
      "Epoch: 104000 | Loss: 0.4328739643096924 | Test loss: 0.44344577193260193\n",
      "Epoch: 104010 | Loss: 0.432862251996994 | Test loss: 0.44343194365501404\n",
      "Epoch: 104020 | Loss: 0.4328504502773285 | Test loss: 0.44341808557510376\n",
      "Epoch: 104030 | Loss: 0.4328387379646301 | Test loss: 0.4434041976928711\n",
      "Epoch: 104040 | Loss: 0.432826966047287 | Test loss: 0.4433903694152832\n",
      "Epoch: 104050 | Loss: 0.43281516432762146 | Test loss: 0.4433765411376953\n",
      "Epoch: 104060 | Loss: 0.4328034520149231 | Test loss: 0.44336268305778503\n",
      "Epoch: 104070 | Loss: 0.4327916204929352 | Test loss: 0.44334885478019714\n",
      "Epoch: 104080 | Loss: 0.43277987837791443 | Test loss: 0.4433349668979645\n",
      "Epoch: 104090 | Loss: 0.4327681064605713 | Test loss: 0.4433211386203766\n",
      "Epoch: 104100 | Loss: 0.43275633454322815 | Test loss: 0.4433073103427887\n",
      "Epoch: 104110 | Loss: 0.4327445924282074 | Test loss: 0.4432934820652008\n",
      "Epoch: 104120 | Loss: 0.43273288011550903 | Test loss: 0.44327959418296814\n",
      "Epoch: 104130 | Loss: 0.4327211081981659 | Test loss: 0.44326576590538025\n",
      "Epoch: 104140 | Loss: 0.43270930647850037 | Test loss: 0.44325190782546997\n",
      "Epoch: 104150 | Loss: 0.432697594165802 | Test loss: 0.4432380795478821\n",
      "Epoch: 104160 | Loss: 0.43268582224845886 | Test loss: 0.4432242512702942\n",
      "Epoch: 104170 | Loss: 0.43267402052879333 | Test loss: 0.4432104229927063\n",
      "Epoch: 104180 | Loss: 0.43266230821609497 | Test loss: 0.44319653511047363\n",
      "Epoch: 104190 | Loss: 0.43265053629875183 | Test loss: 0.44318270683288574\n",
      "Epoch: 104200 | Loss: 0.4326387345790863 | Test loss: 0.44316884875297546\n",
      "Epoch: 104210 | Loss: 0.43262702226638794 | Test loss: 0.4431550204753876\n",
      "Epoch: 104220 | Loss: 0.4326152503490448 | Test loss: 0.4431411325931549\n",
      "Epoch: 104230 | Loss: 0.43260350823402405 | Test loss: 0.443127304315567\n",
      "Epoch: 104240 | Loss: 0.4325917363166809 | Test loss: 0.44311341643333435\n",
      "Epoch: 104250 | Loss: 0.43257999420166016 | Test loss: 0.44309958815574646\n",
      "Epoch: 104260 | Loss: 0.43256816267967224 | Test loss: 0.44308575987815857\n",
      "Epoch: 104270 | Loss: 0.4325564503669739 | Test loss: 0.4430719316005707\n",
      "Epoch: 104280 | Loss: 0.4325447082519531 | Test loss: 0.4430580735206604\n",
      "Epoch: 104290 | Loss: 0.43253293633461 | Test loss: 0.4430442452430725\n",
      "Epoch: 104300 | Loss: 0.43252116441726685 | Test loss: 0.44303035736083984\n",
      "Epoch: 104310 | Loss: 0.4325094223022461 | Test loss: 0.44301652908325195\n",
      "Epoch: 104320 | Loss: 0.43249765038490295 | Test loss: 0.44300270080566406\n",
      "Epoch: 104330 | Loss: 0.4324858784675598 | Test loss: 0.44298887252807617\n",
      "Epoch: 104340 | Loss: 0.43247413635253906 | Test loss: 0.4429750144481659\n",
      "Epoch: 104350 | Loss: 0.4324624240398407 | Test loss: 0.442961186170578\n",
      "Epoch: 104360 | Loss: 0.4324505925178528 | Test loss: 0.44294729828834534\n",
      "Epoch: 104370 | Loss: 0.43243885040283203 | Test loss: 0.44293347001075745\n",
      "Epoch: 104380 | Loss: 0.43242713809013367 | Test loss: 0.44291964173316956\n",
      "Epoch: 104390 | Loss: 0.43241530656814575 | Test loss: 0.4429057538509369\n",
      "Epoch: 104400 | Loss: 0.432403564453125 | Test loss: 0.442891925573349\n",
      "Epoch: 104410 | Loss: 0.43239185214042664 | Test loss: 0.4428780674934387\n",
      "Epoch: 104420 | Loss: 0.4323800206184387 | Test loss: 0.44286423921585083\n",
      "Epoch: 104430 | Loss: 0.43236827850341797 | Test loss: 0.44285041093826294\n",
      "Epoch: 104440 | Loss: 0.4323565661907196 | Test loss: 0.4428365230560303\n",
      "Epoch: 104450 | Loss: 0.4323447644710541 | Test loss: 0.4428226947784424\n",
      "Epoch: 104460 | Loss: 0.43233299255371094 | Test loss: 0.4428088366985321\n",
      "Epoch: 104470 | Loss: 0.4323212802410126 | Test loss: 0.4427950084209442\n",
      "Epoch: 104480 | Loss: 0.4323095381259918 | Test loss: 0.4427811801433563\n",
      "Epoch: 104490 | Loss: 0.4322977066040039 | Test loss: 0.44276729226112366\n",
      "Epoch: 104500 | Loss: 0.43228599429130554 | Test loss: 0.44275346398353577\n",
      "Epoch: 104510 | Loss: 0.43227419257164 | Test loss: 0.4427395761013031\n",
      "Epoch: 104520 | Loss: 0.4322624206542969 | Test loss: 0.4427257478237152\n",
      "Epoch: 104530 | Loss: 0.4322507083415985 | Test loss: 0.4427119195461273\n",
      "Epoch: 104540 | Loss: 0.432238906621933 | Test loss: 0.44269809126853943\n",
      "Epoch: 104550 | Loss: 0.43222713470458984 | Test loss: 0.44268423318862915\n",
      "Epoch: 104560 | Loss: 0.4322154223918915 | Test loss: 0.44267040491104126\n",
      "Epoch: 104570 | Loss: 0.4322036802768707 | Test loss: 0.4426565170288086\n",
      "Epoch: 104580 | Loss: 0.4321919083595276 | Test loss: 0.4426426887512207\n",
      "Epoch: 104590 | Loss: 0.43218013644218445 | Test loss: 0.4426288604736328\n",
      "Epoch: 104600 | Loss: 0.4321683943271637 | Test loss: 0.44261500239372253\n",
      "Epoch: 104610 | Loss: 0.43215662240982056 | Test loss: 0.44260117411613464\n",
      "Epoch: 104620 | Loss: 0.4321448504924774 | Test loss: 0.442587286233902\n",
      "Epoch: 104630 | Loss: 0.43213310837745667 | Test loss: 0.4425734579563141\n",
      "Epoch: 104640 | Loss: 0.4321213364601135 | Test loss: 0.4425596296787262\n",
      "Epoch: 104650 | Loss: 0.4321095645427704 | Test loss: 0.44254574179649353\n",
      "Epoch: 104660 | Loss: 0.43209782242774963 | Test loss: 0.44253191351890564\n",
      "Epoch: 104670 | Loss: 0.4320860505104065 | Test loss: 0.44251805543899536\n",
      "Epoch: 104680 | Loss: 0.43207427859306335 | Test loss: 0.44250422716140747\n",
      "Epoch: 104690 | Loss: 0.4320625364780426 | Test loss: 0.4424903988838196\n",
      "Epoch: 104700 | Loss: 0.43205079436302185 | Test loss: 0.4424765110015869\n",
      "Epoch: 104710 | Loss: 0.4320389926433563 | Test loss: 0.442462682723999\n",
      "Epoch: 104720 | Loss: 0.43202725052833557 | Test loss: 0.44244885444641113\n",
      "Epoch: 104730 | Loss: 0.4320155084133148 | Test loss: 0.44243499636650085\n",
      "Epoch: 104740 | Loss: 0.4320037364959717 | Test loss: 0.44242116808891296\n",
      "Epoch: 104750 | Loss: 0.43199196457862854 | Test loss: 0.4424073398113251\n",
      "Epoch: 104760 | Loss: 0.4319802224636078 | Test loss: 0.4423934519290924\n",
      "Epoch: 104770 | Loss: 0.43196845054626465 | Test loss: 0.4423796236515045\n",
      "Epoch: 104780 | Loss: 0.4319566786289215 | Test loss: 0.44236573576927185\n",
      "Epoch: 104790 | Loss: 0.43194493651390076 | Test loss: 0.44235190749168396\n",
      "Epoch: 104800 | Loss: 0.4319331645965576 | Test loss: 0.44233807921409607\n",
      "Epoch: 104810 | Loss: 0.43192145228385925 | Test loss: 0.4423242509365082\n",
      "Epoch: 104820 | Loss: 0.4319096505641937 | Test loss: 0.4423103928565979\n",
      "Epoch: 104830 | Loss: 0.43189793825149536 | Test loss: 0.44229650497436523\n",
      "Epoch: 104840 | Loss: 0.4318861663341522 | Test loss: 0.44228267669677734\n",
      "Epoch: 104850 | Loss: 0.4318743646144867 | Test loss: 0.44226884841918945\n",
      "Epoch: 104860 | Loss: 0.43186265230178833 | Test loss: 0.4422549903392792\n",
      "Epoch: 104870 | Loss: 0.4318508207798004 | Test loss: 0.4422411620616913\n",
      "Epoch: 104880 | Loss: 0.43183907866477966 | Test loss: 0.4422272741794586\n",
      "Epoch: 104890 | Loss: 0.4318273067474365 | Test loss: 0.4422134459018707\n",
      "Epoch: 104900 | Loss: 0.4318155348300934 | Test loss: 0.44219961762428284\n",
      "Epoch: 104910 | Loss: 0.43180379271507263 | Test loss: 0.44218578934669495\n",
      "Epoch: 104920 | Loss: 0.43179208040237427 | Test loss: 0.4421719014644623\n",
      "Epoch: 104930 | Loss: 0.43178030848503113 | Test loss: 0.4421580731868744\n",
      "Epoch: 104940 | Loss: 0.4317685067653656 | Test loss: 0.4421442151069641\n",
      "Epoch: 104950 | Loss: 0.43175679445266724 | Test loss: 0.4421303868293762\n",
      "Epoch: 104960 | Loss: 0.4317450225353241 | Test loss: 0.44211655855178833\n",
      "Epoch: 104970 | Loss: 0.43173322081565857 | Test loss: 0.44210273027420044\n",
      "Epoch: 104980 | Loss: 0.4317215085029602 | Test loss: 0.4420888423919678\n",
      "Epoch: 104990 | Loss: 0.43170973658561707 | Test loss: 0.4420750141143799\n",
      "Epoch: 105000 | Loss: 0.43169793486595154 | Test loss: 0.4420611560344696\n",
      "Epoch: 105010 | Loss: 0.4316862225532532 | Test loss: 0.4420473277568817\n",
      "Epoch: 105020 | Loss: 0.43167445063591003 | Test loss: 0.44203343987464905\n",
      "Epoch: 105030 | Loss: 0.4316627085208893 | Test loss: 0.44201961159706116\n",
      "Epoch: 105040 | Loss: 0.43165093660354614 | Test loss: 0.4420057237148285\n",
      "Epoch: 105050 | Loss: 0.4316391944885254 | Test loss: 0.4419918954372406\n",
      "Epoch: 105060 | Loss: 0.4316273629665375 | Test loss: 0.4419780671596527\n",
      "Epoch: 105070 | Loss: 0.4316156506538391 | Test loss: 0.4419642388820648\n",
      "Epoch: 105080 | Loss: 0.43160390853881836 | Test loss: 0.44195038080215454\n",
      "Epoch: 105090 | Loss: 0.4315921366214752 | Test loss: 0.44193655252456665\n",
      "Epoch: 105100 | Loss: 0.4315803647041321 | Test loss: 0.441922664642334\n",
      "Epoch: 105110 | Loss: 0.43156862258911133 | Test loss: 0.4419088363647461\n",
      "Epoch: 105120 | Loss: 0.4315568506717682 | Test loss: 0.4418950080871582\n",
      "Epoch: 105130 | Loss: 0.43154507875442505 | Test loss: 0.4418811798095703\n",
      "Epoch: 105140 | Loss: 0.4315333366394043 | Test loss: 0.44186732172966003\n",
      "Epoch: 105150 | Loss: 0.43152162432670593 | Test loss: 0.44185349345207214\n",
      "Epoch: 105160 | Loss: 0.431509792804718 | Test loss: 0.4418396055698395\n",
      "Epoch: 105170 | Loss: 0.43149805068969727 | Test loss: 0.4418257772922516\n",
      "Epoch: 105180 | Loss: 0.4314863383769989 | Test loss: 0.4418119490146637\n",
      "Epoch: 105190 | Loss: 0.431474506855011 | Test loss: 0.44179806113243103\n",
      "Epoch: 105200 | Loss: 0.43146276473999023 | Test loss: 0.44178423285484314\n",
      "Epoch: 105210 | Loss: 0.43145105242729187 | Test loss: 0.44177037477493286\n",
      "Epoch: 105220 | Loss: 0.43143922090530396 | Test loss: 0.44175654649734497\n",
      "Epoch: 105230 | Loss: 0.4314274787902832 | Test loss: 0.4417427182197571\n",
      "Epoch: 105240 | Loss: 0.43141576647758484 | Test loss: 0.4417288303375244\n",
      "Epoch: 105250 | Loss: 0.4314039647579193 | Test loss: 0.4417150020599365\n",
      "Epoch: 105260 | Loss: 0.43139219284057617 | Test loss: 0.44170114398002625\n",
      "Epoch: 105270 | Loss: 0.4313804805278778 | Test loss: 0.44168731570243835\n",
      "Epoch: 105280 | Loss: 0.43136873841285706 | Test loss: 0.44167348742485046\n",
      "Epoch: 105290 | Loss: 0.43135690689086914 | Test loss: 0.4416595995426178\n",
      "Epoch: 105300 | Loss: 0.4313451945781708 | Test loss: 0.4416457712650299\n",
      "Epoch: 105310 | Loss: 0.43133339285850525 | Test loss: 0.44163188338279724\n",
      "Epoch: 105320 | Loss: 0.4313216209411621 | Test loss: 0.44161805510520935\n",
      "Epoch: 105330 | Loss: 0.43130990862846375 | Test loss: 0.44160422682762146\n",
      "Epoch: 105340 | Loss: 0.4312981069087982 | Test loss: 0.44159039855003357\n",
      "Epoch: 105350 | Loss: 0.4312863349914551 | Test loss: 0.4415765404701233\n",
      "Epoch: 105360 | Loss: 0.4312746226787567 | Test loss: 0.4415627121925354\n",
      "Epoch: 105370 | Loss: 0.43126288056373596 | Test loss: 0.44154882431030273\n",
      "Epoch: 105380 | Loss: 0.4312511086463928 | Test loss: 0.44153499603271484\n",
      "Epoch: 105390 | Loss: 0.4312393367290497 | Test loss: 0.44152116775512695\n",
      "Epoch: 105400 | Loss: 0.43122759461402893 | Test loss: 0.4415073096752167\n",
      "Epoch: 105410 | Loss: 0.4312158226966858 | Test loss: 0.4414934813976288\n",
      "Epoch: 105420 | Loss: 0.43120405077934265 | Test loss: 0.4414795935153961\n",
      "Epoch: 105430 | Loss: 0.4311923086643219 | Test loss: 0.4414657652378082\n",
      "Epoch: 105440 | Loss: 0.43118053674697876 | Test loss: 0.44145193696022034\n",
      "Epoch: 105450 | Loss: 0.4311687648296356 | Test loss: 0.44143804907798767\n",
      "Epoch: 105460 | Loss: 0.43115702271461487 | Test loss: 0.4414242208003998\n",
      "Epoch: 105470 | Loss: 0.43114525079727173 | Test loss: 0.4414103627204895\n",
      "Epoch: 105480 | Loss: 0.4311334788799286 | Test loss: 0.4413965344429016\n",
      "Epoch: 105490 | Loss: 0.43112173676490784 | Test loss: 0.4413827061653137\n",
      "Epoch: 105500 | Loss: 0.4311099946498871 | Test loss: 0.44136881828308105\n",
      "Epoch: 105510 | Loss: 0.43109819293022156 | Test loss: 0.44135499000549316\n",
      "Epoch: 105520 | Loss: 0.4310864508152008 | Test loss: 0.4413411617279053\n",
      "Epoch: 105530 | Loss: 0.43107470870018005 | Test loss: 0.441327303647995\n",
      "Epoch: 105540 | Loss: 0.4310629367828369 | Test loss: 0.4413134753704071\n",
      "Epoch: 105550 | Loss: 0.4310511648654938 | Test loss: 0.4412996470928192\n",
      "Epoch: 105560 | Loss: 0.431039422750473 | Test loss: 0.44128575921058655\n",
      "Epoch: 105570 | Loss: 0.4310276508331299 | Test loss: 0.44127193093299866\n",
      "Epoch: 105580 | Loss: 0.43101587891578674 | Test loss: 0.441258043050766\n",
      "Epoch: 105590 | Loss: 0.431004136800766 | Test loss: 0.4412442147731781\n",
      "Epoch: 105600 | Loss: 0.43099236488342285 | Test loss: 0.4412303864955902\n",
      "Epoch: 105610 | Loss: 0.4309806525707245 | Test loss: 0.4412165582180023\n",
      "Epoch: 105620 | Loss: 0.43096885085105896 | Test loss: 0.44120270013809204\n",
      "Epoch: 105630 | Loss: 0.4309571385383606 | Test loss: 0.4411888122558594\n",
      "Epoch: 105640 | Loss: 0.43094536662101746 | Test loss: 0.4411749839782715\n",
      "Epoch: 105650 | Loss: 0.43093356490135193 | Test loss: 0.4411611557006836\n",
      "Epoch: 105660 | Loss: 0.43092185258865356 | Test loss: 0.4411472976207733\n",
      "Epoch: 105670 | Loss: 0.43091002106666565 | Test loss: 0.4411334693431854\n",
      "Epoch: 105680 | Loss: 0.4308982789516449 | Test loss: 0.44111958146095276\n",
      "Epoch: 105690 | Loss: 0.43088650703430176 | Test loss: 0.44110575318336487\n",
      "Epoch: 105700 | Loss: 0.4308747351169586 | Test loss: 0.441091924905777\n",
      "Epoch: 105710 | Loss: 0.43086299300193787 | Test loss: 0.4410780966281891\n",
      "Epoch: 105720 | Loss: 0.4308512806892395 | Test loss: 0.4410642087459564\n",
      "Epoch: 105730 | Loss: 0.43083950877189636 | Test loss: 0.44105038046836853\n",
      "Epoch: 105740 | Loss: 0.43082770705223083 | Test loss: 0.44103652238845825\n",
      "Epoch: 105750 | Loss: 0.43081599473953247 | Test loss: 0.44102269411087036\n",
      "Epoch: 105760 | Loss: 0.43080422282218933 | Test loss: 0.44100886583328247\n",
      "Epoch: 105770 | Loss: 0.4307924211025238 | Test loss: 0.4409950375556946\n",
      "Epoch: 105780 | Loss: 0.43078070878982544 | Test loss: 0.4409811496734619\n",
      "Epoch: 105790 | Loss: 0.4307689368724823 | Test loss: 0.440967321395874\n",
      "Epoch: 105800 | Loss: 0.4307571351528168 | Test loss: 0.44095346331596375\n",
      "Epoch: 105810 | Loss: 0.4307454228401184 | Test loss: 0.44093963503837585\n",
      "Epoch: 105820 | Loss: 0.43073365092277527 | Test loss: 0.4409257471561432\n",
      "Epoch: 105830 | Loss: 0.4307219088077545 | Test loss: 0.4409119188785553\n",
      "Epoch: 105840 | Loss: 0.4307101368904114 | Test loss: 0.44089803099632263\n",
      "Epoch: 105850 | Loss: 0.4306983947753906 | Test loss: 0.44088420271873474\n",
      "Epoch: 105860 | Loss: 0.4306865632534027 | Test loss: 0.44087037444114685\n",
      "Epoch: 105870 | Loss: 0.43067485094070435 | Test loss: 0.44085654616355896\n",
      "Epoch: 105880 | Loss: 0.4306631088256836 | Test loss: 0.4408426880836487\n",
      "Epoch: 105890 | Loss: 0.43065133690834045 | Test loss: 0.4408288598060608\n",
      "Epoch: 105900 | Loss: 0.4306395649909973 | Test loss: 0.4408149719238281\n",
      "Epoch: 105910 | Loss: 0.43062782287597656 | Test loss: 0.44080114364624023\n",
      "Epoch: 105920 | Loss: 0.4306160509586334 | Test loss: 0.44078731536865234\n",
      "Epoch: 105930 | Loss: 0.4306042790412903 | Test loss: 0.44077348709106445\n",
      "Epoch: 105940 | Loss: 0.43059253692626953 | Test loss: 0.4407596290111542\n",
      "Epoch: 105950 | Loss: 0.43058082461357117 | Test loss: 0.4407458007335663\n",
      "Epoch: 105960 | Loss: 0.43056899309158325 | Test loss: 0.4407319128513336\n",
      "Epoch: 105970 | Loss: 0.4305572509765625 | Test loss: 0.4407180845737457\n",
      "Epoch: 105980 | Loss: 0.43054553866386414 | Test loss: 0.44070425629615784\n",
      "Epoch: 105990 | Loss: 0.4305337071418762 | Test loss: 0.44069036841392517\n",
      "Epoch: 106000 | Loss: 0.43052196502685547 | Test loss: 0.4406765401363373\n",
      "Epoch: 106010 | Loss: 0.4305102527141571 | Test loss: 0.440662682056427\n",
      "Epoch: 106020 | Loss: 0.4304984211921692 | Test loss: 0.4406488537788391\n",
      "Epoch: 106030 | Loss: 0.43048667907714844 | Test loss: 0.4406350255012512\n",
      "Epoch: 106040 | Loss: 0.4304749667644501 | Test loss: 0.44062113761901855\n",
      "Epoch: 106050 | Loss: 0.43046316504478455 | Test loss: 0.44060730934143066\n",
      "Epoch: 106060 | Loss: 0.4304513931274414 | Test loss: 0.4405934512615204\n",
      "Epoch: 106070 | Loss: 0.43043968081474304 | Test loss: 0.4405796229839325\n",
      "Epoch: 106080 | Loss: 0.4304279386997223 | Test loss: 0.4405657947063446\n",
      "Epoch: 106090 | Loss: 0.4304161071777344 | Test loss: 0.44055190682411194\n",
      "Epoch: 106100 | Loss: 0.430404394865036 | Test loss: 0.44053807854652405\n",
      "Epoch: 106110 | Loss: 0.4303925931453705 | Test loss: 0.4405241906642914\n",
      "Epoch: 106120 | Loss: 0.43038082122802734 | Test loss: 0.4405103623867035\n",
      "Epoch: 106130 | Loss: 0.430369108915329 | Test loss: 0.4404965341091156\n",
      "Epoch: 106140 | Loss: 0.43035730719566345 | Test loss: 0.4404827058315277\n",
      "Epoch: 106150 | Loss: 0.4303455352783203 | Test loss: 0.44046884775161743\n",
      "Epoch: 106160 | Loss: 0.43033382296562195 | Test loss: 0.44045501947402954\n",
      "Epoch: 106170 | Loss: 0.4303220808506012 | Test loss: 0.4404411315917969\n",
      "Epoch: 106180 | Loss: 0.43031030893325806 | Test loss: 0.440427303314209\n",
      "Epoch: 106190 | Loss: 0.4302985370159149 | Test loss: 0.4404134750366211\n",
      "Epoch: 106200 | Loss: 0.43028679490089417 | Test loss: 0.4403996169567108\n",
      "Epoch: 106210 | Loss: 0.430275022983551 | Test loss: 0.4403857886791229\n",
      "Epoch: 106220 | Loss: 0.4302632510662079 | Test loss: 0.44037190079689026\n",
      "Epoch: 106230 | Loss: 0.43025150895118713 | Test loss: 0.44035807251930237\n",
      "Epoch: 106240 | Loss: 0.430239737033844 | Test loss: 0.4403442442417145\n",
      "Epoch: 106250 | Loss: 0.43022796511650085 | Test loss: 0.4403303563594818\n",
      "Epoch: 106260 | Loss: 0.4302162230014801 | Test loss: 0.4403165280818939\n",
      "Epoch: 106270 | Loss: 0.43020445108413696 | Test loss: 0.44030267000198364\n",
      "Epoch: 106280 | Loss: 0.4301926791667938 | Test loss: 0.44028884172439575\n",
      "Epoch: 106290 | Loss: 0.43018093705177307 | Test loss: 0.44027501344680786\n",
      "Epoch: 106300 | Loss: 0.4301691949367523 | Test loss: 0.4402611255645752\n",
      "Epoch: 106310 | Loss: 0.4301573932170868 | Test loss: 0.4402472972869873\n",
      "Epoch: 106320 | Loss: 0.43014565110206604 | Test loss: 0.4402334690093994\n",
      "Epoch: 106330 | Loss: 0.4301339089870453 | Test loss: 0.44021961092948914\n",
      "Epoch: 106340 | Loss: 0.43012213706970215 | Test loss: 0.44020578265190125\n",
      "Epoch: 106350 | Loss: 0.430110365152359 | Test loss: 0.44019195437431335\n",
      "Epoch: 106360 | Loss: 0.43009862303733826 | Test loss: 0.4401780664920807\n",
      "Epoch: 106370 | Loss: 0.4300868511199951 | Test loss: 0.4401642382144928\n",
      "Epoch: 106380 | Loss: 0.430075079202652 | Test loss: 0.44015035033226013\n",
      "Epoch: 106390 | Loss: 0.4300633370876312 | Test loss: 0.44013652205467224\n",
      "Epoch: 106400 | Loss: 0.4300515651702881 | Test loss: 0.44012269377708435\n",
      "Epoch: 106410 | Loss: 0.4300398528575897 | Test loss: 0.44010886549949646\n",
      "Epoch: 106420 | Loss: 0.4300280511379242 | Test loss: 0.4400950074195862\n",
      "Epoch: 106430 | Loss: 0.43001633882522583 | Test loss: 0.4400811195373535\n",
      "Epoch: 106440 | Loss: 0.4300045669078827 | Test loss: 0.4400672912597656\n",
      "Epoch: 106450 | Loss: 0.42999276518821716 | Test loss: 0.44005346298217773\n",
      "Epoch: 106460 | Loss: 0.4299810528755188 | Test loss: 0.44003960490226746\n",
      "Epoch: 106470 | Loss: 0.4299692213535309 | Test loss: 0.44002577662467957\n",
      "Epoch: 106480 | Loss: 0.42995747923851013 | Test loss: 0.4400118887424469\n",
      "Epoch: 106490 | Loss: 0.429945707321167 | Test loss: 0.439998060464859\n",
      "Epoch: 106500 | Loss: 0.42993393540382385 | Test loss: 0.4399842321872711\n",
      "Epoch: 106510 | Loss: 0.4299221932888031 | Test loss: 0.4399704039096832\n",
      "Epoch: 106520 | Loss: 0.42991048097610474 | Test loss: 0.43995651602745056\n",
      "Epoch: 106530 | Loss: 0.4298987090587616 | Test loss: 0.43994268774986267\n",
      "Epoch: 106540 | Loss: 0.42988690733909607 | Test loss: 0.4399288296699524\n",
      "Epoch: 106550 | Loss: 0.4298751950263977 | Test loss: 0.4399150013923645\n",
      "Epoch: 106560 | Loss: 0.42986342310905457 | Test loss: 0.4399011731147766\n",
      "Epoch: 106570 | Loss: 0.42985162138938904 | Test loss: 0.4398873448371887\n",
      "Epoch: 106580 | Loss: 0.4298399090766907 | Test loss: 0.43987345695495605\n",
      "Epoch: 106590 | Loss: 0.42982813715934753 | Test loss: 0.43985962867736816\n",
      "Epoch: 106600 | Loss: 0.429816335439682 | Test loss: 0.4398457705974579\n",
      "Epoch: 106610 | Loss: 0.42980462312698364 | Test loss: 0.43983194231987\n",
      "Epoch: 106620 | Loss: 0.4297928512096405 | Test loss: 0.43981805443763733\n",
      "Epoch: 106630 | Loss: 0.42978110909461975 | Test loss: 0.43980422616004944\n",
      "Epoch: 106640 | Loss: 0.4297693371772766 | Test loss: 0.4397903382778168\n",
      "Epoch: 106650 | Loss: 0.42975759506225586 | Test loss: 0.4397765100002289\n",
      "Epoch: 106660 | Loss: 0.42974576354026794 | Test loss: 0.439762681722641\n",
      "Epoch: 106670 | Loss: 0.4297340512275696 | Test loss: 0.4397488534450531\n",
      "Epoch: 106680 | Loss: 0.42972230911254883 | Test loss: 0.4397349953651428\n",
      "Epoch: 106690 | Loss: 0.4297105371952057 | Test loss: 0.43972116708755493\n",
      "Epoch: 106700 | Loss: 0.42969876527786255 | Test loss: 0.43970727920532227\n",
      "Epoch: 106710 | Loss: 0.4296870231628418 | Test loss: 0.4396934509277344\n",
      "Epoch: 106720 | Loss: 0.42967525124549866 | Test loss: 0.4396796226501465\n",
      "Epoch: 106730 | Loss: 0.4296634793281555 | Test loss: 0.4396657943725586\n",
      "Epoch: 106740 | Loss: 0.42965173721313477 | Test loss: 0.4396519362926483\n",
      "Epoch: 106750 | Loss: 0.4296400249004364 | Test loss: 0.4396381080150604\n",
      "Epoch: 106760 | Loss: 0.4296281933784485 | Test loss: 0.43962422013282776\n",
      "Epoch: 106770 | Loss: 0.42961645126342773 | Test loss: 0.43961039185523987\n",
      "Epoch: 106780 | Loss: 0.42960473895072937 | Test loss: 0.439596563577652\n",
      "Epoch: 106790 | Loss: 0.42959290742874146 | Test loss: 0.4395826756954193\n",
      "Epoch: 106800 | Loss: 0.4295811653137207 | Test loss: 0.4395688474178314\n",
      "Epoch: 106810 | Loss: 0.42956945300102234 | Test loss: 0.43955498933792114\n",
      "Epoch: 106820 | Loss: 0.4295576214790344 | Test loss: 0.43954116106033325\n",
      "Epoch: 106830 | Loss: 0.42954587936401367 | Test loss: 0.43952733278274536\n",
      "Epoch: 106840 | Loss: 0.4295341670513153 | Test loss: 0.4395134449005127\n",
      "Epoch: 106850 | Loss: 0.4295223653316498 | Test loss: 0.4394996166229248\n",
      "Epoch: 106860 | Loss: 0.42951059341430664 | Test loss: 0.4394857585430145\n",
      "Epoch: 106870 | Loss: 0.4294988811016083 | Test loss: 0.43947193026542664\n",
      "Epoch: 106880 | Loss: 0.4294871389865875 | Test loss: 0.43945810198783875\n",
      "Epoch: 106890 | Loss: 0.4294753074645996 | Test loss: 0.4394442141056061\n",
      "Epoch: 106900 | Loss: 0.42946359515190125 | Test loss: 0.4394303858280182\n",
      "Epoch: 106910 | Loss: 0.4294517934322357 | Test loss: 0.4394164979457855\n",
      "Epoch: 106920 | Loss: 0.4294400215148926 | Test loss: 0.43940266966819763\n",
      "Epoch: 106930 | Loss: 0.4294283092021942 | Test loss: 0.43938884139060974\n",
      "Epoch: 106940 | Loss: 0.4294165074825287 | Test loss: 0.43937501311302185\n",
      "Epoch: 106950 | Loss: 0.42940473556518555 | Test loss: 0.4393611550331116\n",
      "Epoch: 106960 | Loss: 0.4293930232524872 | Test loss: 0.4393473267555237\n",
      "Epoch: 106970 | Loss: 0.42938128113746643 | Test loss: 0.439333438873291\n",
      "Epoch: 106980 | Loss: 0.4293695092201233 | Test loss: 0.4393196105957031\n",
      "Epoch: 106990 | Loss: 0.42935773730278015 | Test loss: 0.43930578231811523\n",
      "Epoch: 107000 | Loss: 0.4293459951877594 | Test loss: 0.43929192423820496\n",
      "Epoch: 107010 | Loss: 0.42933422327041626 | Test loss: 0.43927809596061707\n",
      "Epoch: 107020 | Loss: 0.4293224513530731 | Test loss: 0.4392642080783844\n",
      "Epoch: 107030 | Loss: 0.42931070923805237 | Test loss: 0.4392503798007965\n",
      "Epoch: 107040 | Loss: 0.42929893732070923 | Test loss: 0.4392365515232086\n",
      "Epoch: 107050 | Loss: 0.4292871654033661 | Test loss: 0.43922266364097595\n",
      "Epoch: 107060 | Loss: 0.42927542328834534 | Test loss: 0.43920883536338806\n",
      "Epoch: 107070 | Loss: 0.4292636513710022 | Test loss: 0.4391949772834778\n",
      "Epoch: 107080 | Loss: 0.42925187945365906 | Test loss: 0.4391811490058899\n",
      "Epoch: 107090 | Loss: 0.4292401373386383 | Test loss: 0.439167320728302\n",
      "Epoch: 107100 | Loss: 0.42922839522361755 | Test loss: 0.43915343284606934\n",
      "Epoch: 107110 | Loss: 0.429216593503952 | Test loss: 0.43913960456848145\n",
      "Epoch: 107120 | Loss: 0.4292048513889313 | Test loss: 0.43912577629089355\n",
      "Epoch: 107130 | Loss: 0.4291931092739105 | Test loss: 0.4391119182109833\n",
      "Epoch: 107140 | Loss: 0.4291813373565674 | Test loss: 0.4390980899333954\n",
      "Epoch: 107150 | Loss: 0.42916956543922424 | Test loss: 0.4390842616558075\n",
      "Epoch: 107160 | Loss: 0.4291578233242035 | Test loss: 0.43907037377357483\n",
      "Epoch: 107170 | Loss: 0.42914605140686035 | Test loss: 0.43905654549598694\n",
      "Epoch: 107180 | Loss: 0.4291342794895172 | Test loss: 0.4390426576137543\n",
      "Epoch: 107190 | Loss: 0.42912253737449646 | Test loss: 0.4390288293361664\n",
      "Epoch: 107200 | Loss: 0.4291107654571533 | Test loss: 0.4390150010585785\n",
      "Epoch: 107210 | Loss: 0.42909905314445496 | Test loss: 0.4390011727809906\n",
      "Epoch: 107220 | Loss: 0.42908725142478943 | Test loss: 0.4389873147010803\n",
      "Epoch: 107230 | Loss: 0.42907553911209106 | Test loss: 0.43897342681884766\n",
      "Epoch: 107240 | Loss: 0.4290637671947479 | Test loss: 0.43895959854125977\n",
      "Epoch: 107250 | Loss: 0.4290519654750824 | Test loss: 0.4389457702636719\n",
      "Epoch: 107260 | Loss: 0.42904025316238403 | Test loss: 0.4389319121837616\n",
      "Epoch: 107270 | Loss: 0.4290284216403961 | Test loss: 0.4389180839061737\n",
      "Epoch: 107280 | Loss: 0.42901667952537537 | Test loss: 0.43890419602394104\n",
      "Epoch: 107290 | Loss: 0.4290049076080322 | Test loss: 0.43889036774635315\n",
      "Epoch: 107300 | Loss: 0.4289931356906891 | Test loss: 0.43887653946876526\n",
      "Epoch: 107310 | Loss: 0.42898139357566833 | Test loss: 0.43886271119117737\n",
      "Epoch: 107320 | Loss: 0.42896968126296997 | Test loss: 0.4388488233089447\n",
      "Epoch: 107330 | Loss: 0.42895790934562683 | Test loss: 0.4388349950313568\n",
      "Epoch: 107340 | Loss: 0.4289461076259613 | Test loss: 0.43882113695144653\n",
      "Epoch: 107350 | Loss: 0.42893439531326294 | Test loss: 0.43880730867385864\n",
      "Epoch: 107360 | Loss: 0.4289226233959198 | Test loss: 0.43879348039627075\n",
      "Epoch: 107370 | Loss: 0.4289108216762543 | Test loss: 0.43877965211868286\n",
      "Epoch: 107380 | Loss: 0.4288991093635559 | Test loss: 0.4387657642364502\n",
      "Epoch: 107390 | Loss: 0.42888733744621277 | Test loss: 0.4387519359588623\n",
      "Epoch: 107400 | Loss: 0.42887553572654724 | Test loss: 0.438738077878952\n",
      "Epoch: 107410 | Loss: 0.4288638234138489 | Test loss: 0.43872424960136414\n",
      "Epoch: 107420 | Loss: 0.42885205149650574 | Test loss: 0.43871036171913147\n",
      "Epoch: 107430 | Loss: 0.428840309381485 | Test loss: 0.4386965334415436\n",
      "Epoch: 107440 | Loss: 0.42882853746414185 | Test loss: 0.4386826455593109\n",
      "Epoch: 107450 | Loss: 0.4288167953491211 | Test loss: 0.438668817281723\n",
      "Epoch: 107460 | Loss: 0.4288049638271332 | Test loss: 0.43865498900413513\n",
      "Epoch: 107470 | Loss: 0.4287932515144348 | Test loss: 0.43864116072654724\n",
      "Epoch: 107480 | Loss: 0.42878150939941406 | Test loss: 0.43862730264663696\n",
      "Epoch: 107490 | Loss: 0.4287697374820709 | Test loss: 0.4386134743690491\n",
      "Epoch: 107500 | Loss: 0.4287579655647278 | Test loss: 0.4385995864868164\n",
      "Epoch: 107510 | Loss: 0.42874622344970703 | Test loss: 0.4385857582092285\n",
      "Epoch: 107520 | Loss: 0.4287344515323639 | Test loss: 0.4385719299316406\n",
      "Epoch: 107530 | Loss: 0.42872267961502075 | Test loss: 0.43855810165405273\n",
      "Epoch: 107540 | Loss: 0.4287109375 | Test loss: 0.43854424357414246\n",
      "Epoch: 107550 | Loss: 0.42869922518730164 | Test loss: 0.43853041529655457\n",
      "Epoch: 107560 | Loss: 0.4286873936653137 | Test loss: 0.4385165274143219\n",
      "Epoch: 107570 | Loss: 0.42867565155029297 | Test loss: 0.438502699136734\n",
      "Epoch: 107580 | Loss: 0.4286639392375946 | Test loss: 0.4384888708591461\n",
      "Epoch: 107590 | Loss: 0.4286521077156067 | Test loss: 0.43847498297691345\n",
      "Epoch: 107600 | Loss: 0.42864036560058594 | Test loss: 0.43846115469932556\n",
      "Epoch: 107610 | Loss: 0.4286286532878876 | Test loss: 0.4384472966194153\n",
      "Epoch: 107620 | Loss: 0.42861682176589966 | Test loss: 0.4384334683418274\n",
      "Epoch: 107630 | Loss: 0.4286050796508789 | Test loss: 0.4384196400642395\n",
      "Epoch: 107640 | Loss: 0.42859336733818054 | Test loss: 0.43840575218200684\n",
      "Epoch: 107650 | Loss: 0.428581565618515 | Test loss: 0.43839192390441895\n",
      "Epoch: 107660 | Loss: 0.4285697937011719 | Test loss: 0.43837806582450867\n",
      "Epoch: 107670 | Loss: 0.4285580813884735 | Test loss: 0.4383642375469208\n",
      "Epoch: 107680 | Loss: 0.42854633927345276 | Test loss: 0.4383504092693329\n",
      "Epoch: 107690 | Loss: 0.42853450775146484 | Test loss: 0.4383365213871002\n",
      "Epoch: 107700 | Loss: 0.4285227954387665 | Test loss: 0.43832269310951233\n",
      "Epoch: 107710 | Loss: 0.42851099371910095 | Test loss: 0.43830880522727966\n",
      "Epoch: 107720 | Loss: 0.4284992218017578 | Test loss: 0.4382949769496918\n",
      "Epoch: 107730 | Loss: 0.42848750948905945 | Test loss: 0.4382811486721039\n",
      "Epoch: 107740 | Loss: 0.4284757077693939 | Test loss: 0.438267320394516\n",
      "Epoch: 107750 | Loss: 0.4284639358520508 | Test loss: 0.4382534623146057\n",
      "Epoch: 107760 | Loss: 0.4284522235393524 | Test loss: 0.4382396340370178\n",
      "Epoch: 107770 | Loss: 0.42844048142433167 | Test loss: 0.43822574615478516\n",
      "Epoch: 107780 | Loss: 0.4284287095069885 | Test loss: 0.43821191787719727\n",
      "Epoch: 107790 | Loss: 0.4284169375896454 | Test loss: 0.4381980895996094\n",
      "Epoch: 107800 | Loss: 0.42840519547462463 | Test loss: 0.4381842315196991\n",
      "Epoch: 107810 | Loss: 0.4283934235572815 | Test loss: 0.4381704032421112\n",
      "Epoch: 107820 | Loss: 0.42838165163993835 | Test loss: 0.43815651535987854\n",
      "Epoch: 107830 | Loss: 0.4283699095249176 | Test loss: 0.43814268708229065\n",
      "Epoch: 107840 | Loss: 0.42835813760757446 | Test loss: 0.43812885880470276\n",
      "Epoch: 107850 | Loss: 0.4283463656902313 | Test loss: 0.4381149709224701\n",
      "Epoch: 107860 | Loss: 0.42833462357521057 | Test loss: 0.4381011426448822\n",
      "Epoch: 107870 | Loss: 0.42832285165786743 | Test loss: 0.4380872845649719\n",
      "Epoch: 107880 | Loss: 0.4283110797405243 | Test loss: 0.43807345628738403\n",
      "Epoch: 107890 | Loss: 0.42829933762550354 | Test loss: 0.43805962800979614\n",
      "Epoch: 107900 | Loss: 0.4282875955104828 | Test loss: 0.4380457401275635\n",
      "Epoch: 107910 | Loss: 0.42827579379081726 | Test loss: 0.4380319118499756\n",
      "Epoch: 107920 | Loss: 0.4282640516757965 | Test loss: 0.4380180835723877\n",
      "Epoch: 107930 | Loss: 0.42825230956077576 | Test loss: 0.4380042254924774\n",
      "Epoch: 107940 | Loss: 0.4282405376434326 | Test loss: 0.4379903972148895\n",
      "Epoch: 107950 | Loss: 0.4282287657260895 | Test loss: 0.43797656893730164\n",
      "Epoch: 107960 | Loss: 0.4282170236110687 | Test loss: 0.43796268105506897\n",
      "Epoch: 107970 | Loss: 0.4282052516937256 | Test loss: 0.4379488527774811\n",
      "Epoch: 107980 | Loss: 0.42819347977638245 | Test loss: 0.4379349648952484\n",
      "Epoch: 107990 | Loss: 0.4281817376613617 | Test loss: 0.4379211366176605\n",
      "Epoch: 108000 | Loss: 0.42816996574401855 | Test loss: 0.43790730834007263\n",
      "Epoch: 108010 | Loss: 0.4281582534313202 | Test loss: 0.43789348006248474\n",
      "Epoch: 108020 | Loss: 0.42814645171165466 | Test loss: 0.43787962198257446\n",
      "Epoch: 108030 | Loss: 0.4281347393989563 | Test loss: 0.4378657341003418\n",
      "Epoch: 108040 | Loss: 0.42812296748161316 | Test loss: 0.4378519058227539\n",
      "Epoch: 108050 | Loss: 0.42811116576194763 | Test loss: 0.437838077545166\n",
      "Epoch: 108060 | Loss: 0.42809945344924927 | Test loss: 0.43782421946525574\n",
      "Epoch: 108070 | Loss: 0.42808762192726135 | Test loss: 0.43781039118766785\n",
      "Epoch: 108080 | Loss: 0.4280758798122406 | Test loss: 0.4377965033054352\n",
      "Epoch: 108090 | Loss: 0.42806410789489746 | Test loss: 0.4377826750278473\n",
      "Epoch: 108100 | Loss: 0.4280523359775543 | Test loss: 0.4377688467502594\n",
      "Epoch: 108110 | Loss: 0.42804059386253357 | Test loss: 0.4377550184726715\n",
      "Epoch: 108120 | Loss: 0.4280288815498352 | Test loss: 0.43774113059043884\n",
      "Epoch: 108130 | Loss: 0.42801710963249207 | Test loss: 0.43772730231285095\n",
      "Epoch: 108140 | Loss: 0.42800530791282654 | Test loss: 0.4377134442329407\n",
      "Epoch: 108150 | Loss: 0.4279935956001282 | Test loss: 0.4376996159553528\n",
      "Epoch: 108160 | Loss: 0.42798182368278503 | Test loss: 0.4376857876777649\n",
      "Epoch: 108170 | Loss: 0.4279700219631195 | Test loss: 0.437671959400177\n",
      "Epoch: 108180 | Loss: 0.42795830965042114 | Test loss: 0.43765807151794434\n",
      "Epoch: 108190 | Loss: 0.427946537733078 | Test loss: 0.43764424324035645\n",
      "Epoch: 108200 | Loss: 0.4279347360134125 | Test loss: 0.43763038516044617\n",
      "Epoch: 108210 | Loss: 0.4279230237007141 | Test loss: 0.4376165568828583\n",
      "Epoch: 108220 | Loss: 0.42791125178337097 | Test loss: 0.4376026690006256\n",
      "Epoch: 108230 | Loss: 0.4278995096683502 | Test loss: 0.4375888407230377\n",
      "Epoch: 108240 | Loss: 0.4278877377510071 | Test loss: 0.43757495284080505\n",
      "Epoch: 108250 | Loss: 0.42787599563598633 | Test loss: 0.43756112456321716\n",
      "Epoch: 108260 | Loss: 0.4278641641139984 | Test loss: 0.4375472962856293\n",
      "Epoch: 108270 | Loss: 0.42785245180130005 | Test loss: 0.4375334680080414\n",
      "Epoch: 108280 | Loss: 0.4278407096862793 | Test loss: 0.4375196099281311\n",
      "Epoch: 108290 | Loss: 0.42782893776893616 | Test loss: 0.4375057816505432\n",
      "Epoch: 108300 | Loss: 0.427817165851593 | Test loss: 0.43749189376831055\n",
      "Epoch: 108310 | Loss: 0.42780542373657227 | Test loss: 0.43747806549072266\n",
      "Epoch: 108320 | Loss: 0.4277936518192291 | Test loss: 0.43746423721313477\n",
      "Epoch: 108330 | Loss: 0.427781879901886 | Test loss: 0.4374504089355469\n",
      "Epoch: 108340 | Loss: 0.42777013778686523 | Test loss: 0.4374365508556366\n",
      "Epoch: 108350 | Loss: 0.42775842547416687 | Test loss: 0.4374227225780487\n",
      "Epoch: 108360 | Loss: 0.42774659395217896 | Test loss: 0.43740883469581604\n",
      "Epoch: 108370 | Loss: 0.4277348518371582 | Test loss: 0.43739500641822815\n",
      "Epoch: 108380 | Loss: 0.42772313952445984 | Test loss: 0.43738117814064026\n",
      "Epoch: 108390 | Loss: 0.4277113080024719 | Test loss: 0.4373672902584076\n",
      "Epoch: 108400 | Loss: 0.42769956588745117 | Test loss: 0.4373534619808197\n",
      "Epoch: 108410 | Loss: 0.4276878535747528 | Test loss: 0.4373396039009094\n",
      "Epoch: 108420 | Loss: 0.4276760220527649 | Test loss: 0.43732577562332153\n",
      "Epoch: 108430 | Loss: 0.42766427993774414 | Test loss: 0.43731194734573364\n",
      "Epoch: 108440 | Loss: 0.4276525676250458 | Test loss: 0.437298059463501\n",
      "Epoch: 108450 | Loss: 0.42764076590538025 | Test loss: 0.4372842311859131\n",
      "Epoch: 108460 | Loss: 0.4276289939880371 | Test loss: 0.4372703731060028\n",
      "Epoch: 108470 | Loss: 0.42761728167533875 | Test loss: 0.4372565448284149\n",
      "Epoch: 108480 | Loss: 0.427605539560318 | Test loss: 0.437242716550827\n",
      "Epoch: 108490 | Loss: 0.4275937080383301 | Test loss: 0.43722882866859436\n",
      "Epoch: 108500 | Loss: 0.4275819957256317 | Test loss: 0.43721500039100647\n",
      "Epoch: 108510 | Loss: 0.4275701940059662 | Test loss: 0.4372011125087738\n",
      "Epoch: 108520 | Loss: 0.42755842208862305 | Test loss: 0.4371872842311859\n",
      "Epoch: 108530 | Loss: 0.4275467097759247 | Test loss: 0.437173455953598\n",
      "Epoch: 108540 | Loss: 0.42753490805625916 | Test loss: 0.43715962767601013\n",
      "Epoch: 108550 | Loss: 0.427523136138916 | Test loss: 0.43714576959609985\n",
      "Epoch: 108560 | Loss: 0.42751142382621765 | Test loss: 0.43713194131851196\n",
      "Epoch: 108570 | Loss: 0.4274996817111969 | Test loss: 0.4371180534362793\n",
      "Epoch: 108580 | Loss: 0.42748790979385376 | Test loss: 0.4371042251586914\n",
      "Epoch: 108590 | Loss: 0.4274761378765106 | Test loss: 0.4370903968811035\n",
      "Epoch: 108600 | Loss: 0.42746439576148987 | Test loss: 0.43707653880119324\n",
      "Epoch: 108610 | Loss: 0.42745262384414673 | Test loss: 0.43706271052360535\n",
      "Epoch: 108620 | Loss: 0.4274408519268036 | Test loss: 0.4370488226413727\n",
      "Epoch: 108630 | Loss: 0.42742910981178284 | Test loss: 0.4370349943637848\n",
      "Epoch: 108640 | Loss: 0.4274173378944397 | Test loss: 0.4370211660861969\n",
      "Epoch: 108650 | Loss: 0.42740556597709656 | Test loss: 0.43700727820396423\n",
      "Epoch: 108660 | Loss: 0.4273938238620758 | Test loss: 0.43699344992637634\n",
      "Epoch: 108670 | Loss: 0.42738205194473267 | Test loss: 0.43697959184646606\n",
      "Epoch: 108680 | Loss: 0.4273702800273895 | Test loss: 0.4369657635688782\n",
      "Epoch: 108690 | Loss: 0.4273585379123688 | Test loss: 0.4369519352912903\n",
      "Epoch: 108700 | Loss: 0.427346795797348 | Test loss: 0.4369380474090576\n",
      "Epoch: 108710 | Loss: 0.4273349940776825 | Test loss: 0.4369242191314697\n",
      "Epoch: 108720 | Loss: 0.42732325196266174 | Test loss: 0.43691039085388184\n",
      "Epoch: 108730 | Loss: 0.427311509847641 | Test loss: 0.43689653277397156\n",
      "Epoch: 108740 | Loss: 0.42729973793029785 | Test loss: 0.43688270449638367\n",
      "Epoch: 108750 | Loss: 0.4272879660129547 | Test loss: 0.4368688762187958\n",
      "Epoch: 108760 | Loss: 0.42727622389793396 | Test loss: 0.4368549883365631\n",
      "Epoch: 108770 | Loss: 0.4272644519805908 | Test loss: 0.4368411600589752\n",
      "Epoch: 108780 | Loss: 0.4272526800632477 | Test loss: 0.43682727217674255\n",
      "Epoch: 108790 | Loss: 0.42724093794822693 | Test loss: 0.43681344389915466\n",
      "Epoch: 108800 | Loss: 0.4272291660308838 | Test loss: 0.4367996156215668\n",
      "Epoch: 108810 | Loss: 0.4272174537181854 | Test loss: 0.4367857873439789\n",
      "Epoch: 108820 | Loss: 0.4272056519985199 | Test loss: 0.4367719292640686\n",
      "Epoch: 108830 | Loss: 0.42719393968582153 | Test loss: 0.43675804138183594\n",
      "Epoch: 108840 | Loss: 0.4271821677684784 | Test loss: 0.43674421310424805\n",
      "Epoch: 108850 | Loss: 0.42717036604881287 | Test loss: 0.43673038482666016\n",
      "Epoch: 108860 | Loss: 0.4271586537361145 | Test loss: 0.4367165267467499\n",
      "Epoch: 108870 | Loss: 0.4271468222141266 | Test loss: 0.436702698469162\n",
      "Epoch: 108880 | Loss: 0.42713508009910583 | Test loss: 0.4366888105869293\n",
      "Epoch: 108890 | Loss: 0.4271233081817627 | Test loss: 0.43667498230934143\n",
      "Epoch: 108900 | Loss: 0.42711153626441956 | Test loss: 0.43666115403175354\n",
      "Epoch: 108910 | Loss: 0.4270997941493988 | Test loss: 0.43664732575416565\n",
      "Epoch: 108920 | Loss: 0.42708808183670044 | Test loss: 0.436633437871933\n",
      "Epoch: 108930 | Loss: 0.4270763099193573 | Test loss: 0.4366196095943451\n",
      "Epoch: 108940 | Loss: 0.4270645081996918 | Test loss: 0.4366057515144348\n",
      "Epoch: 108950 | Loss: 0.4270527958869934 | Test loss: 0.4365919232368469\n",
      "Epoch: 108960 | Loss: 0.42704102396965027 | Test loss: 0.43657809495925903\n",
      "Epoch: 108970 | Loss: 0.42702922224998474 | Test loss: 0.43656426668167114\n",
      "Epoch: 108980 | Loss: 0.4270175099372864 | Test loss: 0.4365503787994385\n",
      "Epoch: 108990 | Loss: 0.42700573801994324 | Test loss: 0.4365365505218506\n",
      "Epoch: 109000 | Loss: 0.4269939363002777 | Test loss: 0.4365226924419403\n",
      "Epoch: 109010 | Loss: 0.42698222398757935 | Test loss: 0.4365088641643524\n",
      "Epoch: 109020 | Loss: 0.4269704520702362 | Test loss: 0.43649497628211975\n",
      "Epoch: 109030 | Loss: 0.42695870995521545 | Test loss: 0.43648114800453186\n",
      "Epoch: 109040 | Loss: 0.4269469380378723 | Test loss: 0.4364672601222992\n",
      "Epoch: 109050 | Loss: 0.42693519592285156 | Test loss: 0.4364534318447113\n",
      "Epoch: 109060 | Loss: 0.42692336440086365 | Test loss: 0.4364396035671234\n",
      "Epoch: 109070 | Loss: 0.4269116520881653 | Test loss: 0.4364257752895355\n",
      "Epoch: 109080 | Loss: 0.42689990997314453 | Test loss: 0.43641191720962524\n",
      "Epoch: 109090 | Loss: 0.4268881380558014 | Test loss: 0.43639808893203735\n",
      "Epoch: 109100 | Loss: 0.42687636613845825 | Test loss: 0.4363842010498047\n",
      "Epoch: 109110 | Loss: 0.4268646240234375 | Test loss: 0.4363703727722168\n",
      "Epoch: 109120 | Loss: 0.42685285210609436 | Test loss: 0.4363565444946289\n",
      "Epoch: 109130 | Loss: 0.4268410801887512 | Test loss: 0.436342716217041\n",
      "Epoch: 109140 | Loss: 0.42682933807373047 | Test loss: 0.43632885813713074\n",
      "Epoch: 109150 | Loss: 0.4268176257610321 | Test loss: 0.43631502985954285\n",
      "Epoch: 109160 | Loss: 0.4268057942390442 | Test loss: 0.4363011419773102\n",
      "Epoch: 109170 | Loss: 0.42679405212402344 | Test loss: 0.4362873136997223\n",
      "Epoch: 109180 | Loss: 0.4267823398113251 | Test loss: 0.4362734854221344\n",
      "Epoch: 109190 | Loss: 0.42677050828933716 | Test loss: 0.43625959753990173\n",
      "Epoch: 109200 | Loss: 0.4267587661743164 | Test loss: 0.43624576926231384\n",
      "Epoch: 109210 | Loss: 0.42674705386161804 | Test loss: 0.43623191118240356\n",
      "Epoch: 109220 | Loss: 0.4267352223396301 | Test loss: 0.4362180829048157\n",
      "Epoch: 109230 | Loss: 0.4267234802246094 | Test loss: 0.4362042546272278\n",
      "Epoch: 109240 | Loss: 0.426711767911911 | Test loss: 0.4361903667449951\n",
      "Epoch: 109250 | Loss: 0.4266999661922455 | Test loss: 0.4361765384674072\n",
      "Epoch: 109260 | Loss: 0.42668819427490234 | Test loss: 0.43616268038749695\n",
      "Epoch: 109270 | Loss: 0.426676481962204 | Test loss: 0.43614885210990906\n",
      "Epoch: 109280 | Loss: 0.4266647398471832 | Test loss: 0.43613502383232117\n",
      "Epoch: 109290 | Loss: 0.4266529083251953 | Test loss: 0.4361211359500885\n",
      "Epoch: 109300 | Loss: 0.42664119601249695 | Test loss: 0.4361073076725006\n",
      "Epoch: 109310 | Loss: 0.4266293942928314 | Test loss: 0.43609341979026794\n",
      "Epoch: 109320 | Loss: 0.4266176223754883 | Test loss: 0.43607959151268005\n",
      "Epoch: 109330 | Loss: 0.4266059100627899 | Test loss: 0.43606576323509216\n",
      "Epoch: 109340 | Loss: 0.4265941083431244 | Test loss: 0.4360519349575043\n",
      "Epoch: 109350 | Loss: 0.42658233642578125 | Test loss: 0.436038076877594\n",
      "Epoch: 109360 | Loss: 0.4265706241130829 | Test loss: 0.4360242486000061\n",
      "Epoch: 109370 | Loss: 0.42655888199806213 | Test loss: 0.43601036071777344\n",
      "Epoch: 109380 | Loss: 0.426547110080719 | Test loss: 0.43599653244018555\n",
      "Epoch: 109390 | Loss: 0.42653533816337585 | Test loss: 0.43598270416259766\n",
      "Epoch: 109400 | Loss: 0.4265235960483551 | Test loss: 0.4359688460826874\n",
      "Epoch: 109410 | Loss: 0.42651182413101196 | Test loss: 0.4359550178050995\n",
      "Epoch: 109420 | Loss: 0.4265000522136688 | Test loss: 0.4359411299228668\n",
      "Epoch: 109430 | Loss: 0.42648831009864807 | Test loss: 0.43592730164527893\n",
      "Epoch: 109440 | Loss: 0.42647653818130493 | Test loss: 0.43591347336769104\n",
      "Epoch: 109450 | Loss: 0.4264647662639618 | Test loss: 0.4358995854854584\n",
      "Epoch: 109460 | Loss: 0.42645302414894104 | Test loss: 0.4358857572078705\n",
      "Epoch: 109470 | Loss: 0.4264412522315979 | Test loss: 0.4358718991279602\n",
      "Epoch: 109480 | Loss: 0.42642948031425476 | Test loss: 0.4358580708503723\n",
      "Epoch: 109490 | Loss: 0.426417738199234 | Test loss: 0.4358442425727844\n",
      "Epoch: 109500 | Loss: 0.42640599608421326 | Test loss: 0.43583035469055176\n",
      "Epoch: 109510 | Loss: 0.42639419436454773 | Test loss: 0.43581652641296387\n",
      "Epoch: 109520 | Loss: 0.426382452249527 | Test loss: 0.435802698135376\n",
      "Epoch: 109530 | Loss: 0.4263707101345062 | Test loss: 0.4357888400554657\n",
      "Epoch: 109540 | Loss: 0.4263589382171631 | Test loss: 0.4357750117778778\n",
      "Epoch: 109550 | Loss: 0.42634716629981995 | Test loss: 0.4357611835002899\n",
      "Epoch: 109560 | Loss: 0.4263354241847992 | Test loss: 0.43574729561805725\n",
      "Epoch: 109570 | Loss: 0.42632365226745605 | Test loss: 0.43573346734046936\n",
      "Epoch: 109580 | Loss: 0.4263118803501129 | Test loss: 0.4357195794582367\n",
      "Epoch: 109590 | Loss: 0.42630013823509216 | Test loss: 0.4357057511806488\n",
      "Epoch: 109600 | Loss: 0.426288366317749 | Test loss: 0.4356919229030609\n",
      "Epoch: 109610 | Loss: 0.42627665400505066 | Test loss: 0.435678094625473\n",
      "Epoch: 109620 | Loss: 0.42626485228538513 | Test loss: 0.43566423654556274\n",
      "Epoch: 109630 | Loss: 0.42625313997268677 | Test loss: 0.4356503486633301\n",
      "Epoch: 109640 | Loss: 0.42624136805534363 | Test loss: 0.4356365203857422\n",
      "Epoch: 109650 | Loss: 0.4262295663356781 | Test loss: 0.4356226921081543\n",
      "Epoch: 109660 | Loss: 0.42621785402297974 | Test loss: 0.435608834028244\n",
      "Epoch: 109670 | Loss: 0.4262060225009918 | Test loss: 0.43559500575065613\n",
      "Epoch: 109680 | Loss: 0.42619428038597107 | Test loss: 0.43558111786842346\n",
      "Epoch: 109690 | Loss: 0.42618250846862793 | Test loss: 0.43556728959083557\n",
      "Epoch: 109700 | Loss: 0.4261707365512848 | Test loss: 0.4355534613132477\n",
      "Epoch: 109710 | Loss: 0.42615899443626404 | Test loss: 0.4355396330356598\n",
      "Epoch: 109720 | Loss: 0.4261472821235657 | Test loss: 0.4355257451534271\n",
      "Epoch: 109730 | Loss: 0.42613551020622253 | Test loss: 0.43551191687583923\n",
      "Epoch: 109740 | Loss: 0.426123708486557 | Test loss: 0.43549805879592896\n",
      "Epoch: 109750 | Loss: 0.42611199617385864 | Test loss: 0.43548423051834106\n",
      "Epoch: 109760 | Loss: 0.4261002242565155 | Test loss: 0.4354704022407532\n",
      "Epoch: 109770 | Loss: 0.42608842253685 | Test loss: 0.4354565739631653\n",
      "Epoch: 109780 | Loss: 0.4260767102241516 | Test loss: 0.4354426860809326\n",
      "Epoch: 109790 | Loss: 0.42606493830680847 | Test loss: 0.4354288578033447\n",
      "Epoch: 109800 | Loss: 0.42605313658714294 | Test loss: 0.43541499972343445\n",
      "Epoch: 109810 | Loss: 0.4260414242744446 | Test loss: 0.43540117144584656\n",
      "Epoch: 109820 | Loss: 0.42602965235710144 | Test loss: 0.4353872835636139\n",
      "Epoch: 109830 | Loss: 0.4260179102420807 | Test loss: 0.435373455286026\n",
      "Epoch: 109840 | Loss: 0.42600613832473755 | Test loss: 0.43535956740379333\n",
      "Epoch: 109850 | Loss: 0.4259943962097168 | Test loss: 0.43534573912620544\n",
      "Epoch: 109860 | Loss: 0.4259825646877289 | Test loss: 0.43533191084861755\n",
      "Epoch: 109870 | Loss: 0.4259708523750305 | Test loss: 0.43531808257102966\n",
      "Epoch: 109880 | Loss: 0.42595911026000977 | Test loss: 0.4353042244911194\n",
      "Epoch: 109890 | Loss: 0.4259473383426666 | Test loss: 0.4352903962135315\n",
      "Epoch: 109900 | Loss: 0.4259355664253235 | Test loss: 0.43527650833129883\n",
      "Epoch: 109910 | Loss: 0.42592382431030273 | Test loss: 0.43526268005371094\n",
      "Epoch: 109920 | Loss: 0.4259120523929596 | Test loss: 0.43524885177612305\n",
      "Epoch: 109930 | Loss: 0.42590028047561646 | Test loss: 0.43523502349853516\n",
      "Epoch: 109940 | Loss: 0.4258885383605957 | Test loss: 0.4352211654186249\n",
      "Epoch: 109950 | Loss: 0.42587682604789734 | Test loss: 0.435207337141037\n",
      "Epoch: 109960 | Loss: 0.4258649945259094 | Test loss: 0.4351934492588043\n",
      "Epoch: 109970 | Loss: 0.42585325241088867 | Test loss: 0.43517962098121643\n",
      "Epoch: 109980 | Loss: 0.4258415400981903 | Test loss: 0.43516579270362854\n",
      "Epoch: 109990 | Loss: 0.4258297085762024 | Test loss: 0.4351519048213959\n",
      "Epoch: 110000 | Loss: 0.42581796646118164 | Test loss: 0.435138076543808\n",
      "Epoch: 110010 | Loss: 0.4258062541484833 | Test loss: 0.4351242184638977\n",
      "Epoch: 110020 | Loss: 0.42579442262649536 | Test loss: 0.4351103901863098\n",
      "Epoch: 110030 | Loss: 0.4257826805114746 | Test loss: 0.4350965619087219\n",
      "Epoch: 110040 | Loss: 0.42577096819877625 | Test loss: 0.43508267402648926\n",
      "Epoch: 110050 | Loss: 0.4257591664791107 | Test loss: 0.43506884574890137\n",
      "Epoch: 110060 | Loss: 0.4257473945617676 | Test loss: 0.4350549876689911\n",
      "Epoch: 110070 | Loss: 0.4257356822490692 | Test loss: 0.4350411593914032\n",
      "Epoch: 110080 | Loss: 0.42572394013404846 | Test loss: 0.4350273311138153\n",
      "Epoch: 110090 | Loss: 0.42571210861206055 | Test loss: 0.43501344323158264\n",
      "Epoch: 110100 | Loss: 0.4257003962993622 | Test loss: 0.43499961495399475\n",
      "Epoch: 110110 | Loss: 0.42568859457969666 | Test loss: 0.4349857270717621\n",
      "Epoch: 110120 | Loss: 0.4256768226623535 | Test loss: 0.4349718987941742\n",
      "Epoch: 110130 | Loss: 0.42566511034965515 | Test loss: 0.4349580705165863\n",
      "Epoch: 110140 | Loss: 0.4256533086299896 | Test loss: 0.4349442422389984\n",
      "Epoch: 110150 | Loss: 0.4256415367126465 | Test loss: 0.43493038415908813\n",
      "Epoch: 110160 | Loss: 0.4256298243999481 | Test loss: 0.43491655588150024\n",
      "Epoch: 110170 | Loss: 0.42561808228492737 | Test loss: 0.4349026679992676\n",
      "Epoch: 110180 | Loss: 0.42560631036758423 | Test loss: 0.4348888397216797\n",
      "Epoch: 110190 | Loss: 0.4255945384502411 | Test loss: 0.4348750114440918\n",
      "Epoch: 110200 | Loss: 0.42558279633522034 | Test loss: 0.4348611533641815\n",
      "Epoch: 110210 | Loss: 0.4255710244178772 | Test loss: 0.43484732508659363\n",
      "Epoch: 110220 | Loss: 0.42555925250053406 | Test loss: 0.43483343720436096\n",
      "Epoch: 110230 | Loss: 0.4255475103855133 | Test loss: 0.43481960892677307\n",
      "Epoch: 110240 | Loss: 0.42553573846817017 | Test loss: 0.4348057806491852\n",
      "Epoch: 110250 | Loss: 0.425523966550827 | Test loss: 0.4347918927669525\n",
      "Epoch: 110260 | Loss: 0.4255122244358063 | Test loss: 0.4347780644893646\n",
      "Epoch: 110270 | Loss: 0.42550045251846313 | Test loss: 0.43476420640945435\n",
      "Epoch: 110280 | Loss: 0.42548868060112 | Test loss: 0.43475037813186646\n",
      "Epoch: 110290 | Loss: 0.42547693848609924 | Test loss: 0.43473654985427856\n",
      "Epoch: 110300 | Loss: 0.4254651963710785 | Test loss: 0.4347226619720459\n",
      "Epoch: 110310 | Loss: 0.42545339465141296 | Test loss: 0.434708833694458\n",
      "Epoch: 110320 | Loss: 0.4254416525363922 | Test loss: 0.4346950054168701\n",
      "Epoch: 110330 | Loss: 0.42542991042137146 | Test loss: 0.43468114733695984\n",
      "Epoch: 110340 | Loss: 0.4254181385040283 | Test loss: 0.43466731905937195\n",
      "Epoch: 110350 | Loss: 0.4254063665866852 | Test loss: 0.43465349078178406\n",
      "Epoch: 110360 | Loss: 0.42539462447166443 | Test loss: 0.4346396028995514\n",
      "Epoch: 110370 | Loss: 0.4253828525543213 | Test loss: 0.4346257746219635\n",
      "Epoch: 110380 | Loss: 0.42537108063697815 | Test loss: 0.43461188673973083\n",
      "Epoch: 110390 | Loss: 0.4253593385219574 | Test loss: 0.43459805846214294\n",
      "Epoch: 110400 | Loss: 0.42534756660461426 | Test loss: 0.43458423018455505\n",
      "Epoch: 110410 | Loss: 0.4253358542919159 | Test loss: 0.43457040190696716\n",
      "Epoch: 110420 | Loss: 0.42532405257225037 | Test loss: 0.4345565438270569\n",
      "Epoch: 110430 | Loss: 0.425312340259552 | Test loss: 0.4345426559448242\n",
      "Epoch: 110440 | Loss: 0.42530056834220886 | Test loss: 0.43452882766723633\n",
      "Epoch: 110450 | Loss: 0.42528876662254333 | Test loss: 0.43451499938964844\n",
      "Epoch: 110460 | Loss: 0.42527705430984497 | Test loss: 0.43450114130973816\n",
      "Epoch: 110470 | Loss: 0.42526522278785706 | Test loss: 0.43448731303215027\n",
      "Epoch: 110480 | Loss: 0.4252534806728363 | Test loss: 0.4344734251499176\n",
      "Epoch: 110490 | Loss: 0.42524170875549316 | Test loss: 0.4344595968723297\n",
      "Epoch: 110500 | Loss: 0.42522993683815 | Test loss: 0.4344457685947418\n",
      "Epoch: 110510 | Loss: 0.4252181947231293 | Test loss: 0.43443194031715393\n",
      "Epoch: 110520 | Loss: 0.4252064824104309 | Test loss: 0.43441805243492126\n",
      "Epoch: 110530 | Loss: 0.42519471049308777 | Test loss: 0.4344042241573334\n",
      "Epoch: 110540 | Loss: 0.42518290877342224 | Test loss: 0.4343903660774231\n",
      "Epoch: 110550 | Loss: 0.4251711964607239 | Test loss: 0.4343765377998352\n",
      "Epoch: 110560 | Loss: 0.42515942454338074 | Test loss: 0.4343627095222473\n",
      "Epoch: 110570 | Loss: 0.4251476228237152 | Test loss: 0.4343488812446594\n",
      "Epoch: 110580 | Loss: 0.42513591051101685 | Test loss: 0.43433499336242676\n",
      "Epoch: 110590 | Loss: 0.4251241385936737 | Test loss: 0.43432116508483887\n",
      "Epoch: 110600 | Loss: 0.4251123368740082 | Test loss: 0.4343073070049286\n",
      "Epoch: 110610 | Loss: 0.4251006245613098 | Test loss: 0.4342934787273407\n",
      "Epoch: 110620 | Loss: 0.4250888526439667 | Test loss: 0.43427959084510803\n",
      "Epoch: 110630 | Loss: 0.4250771105289459 | Test loss: 0.43426576256752014\n",
      "Epoch: 110640 | Loss: 0.4250653386116028 | Test loss: 0.4342518746852875\n",
      "Epoch: 110650 | Loss: 0.42505359649658203 | Test loss: 0.4342380464076996\n",
      "Epoch: 110660 | Loss: 0.4250417649745941 | Test loss: 0.4342242181301117\n",
      "Epoch: 110670 | Loss: 0.42503005266189575 | Test loss: 0.4342103898525238\n",
      "Epoch: 110680 | Loss: 0.425018310546875 | Test loss: 0.4341965317726135\n",
      "Epoch: 110690 | Loss: 0.42500653862953186 | Test loss: 0.43418270349502563\n",
      "Epoch: 110700 | Loss: 0.4249947667121887 | Test loss: 0.43416881561279297\n",
      "Epoch: 110710 | Loss: 0.42498302459716797 | Test loss: 0.4341549873352051\n",
      "Epoch: 110720 | Loss: 0.42497125267982483 | Test loss: 0.4341411590576172\n",
      "Epoch: 110730 | Loss: 0.4249594807624817 | Test loss: 0.4341273307800293\n",
      "Epoch: 110740 | Loss: 0.42494773864746094 | Test loss: 0.434113472700119\n",
      "Epoch: 110750 | Loss: 0.4249360263347626 | Test loss: 0.43409964442253113\n",
      "Epoch: 110760 | Loss: 0.42492419481277466 | Test loss: 0.43408575654029846\n",
      "Epoch: 110770 | Loss: 0.4249124526977539 | Test loss: 0.43407192826271057\n",
      "Epoch: 110780 | Loss: 0.42490074038505554 | Test loss: 0.4340580999851227\n",
      "Epoch: 110790 | Loss: 0.4248889088630676 | Test loss: 0.43404421210289\n",
      "Epoch: 110800 | Loss: 0.4248771667480469 | Test loss: 0.4340303838253021\n",
      "Epoch: 110810 | Loss: 0.4248654544353485 | Test loss: 0.43401652574539185\n",
      "Epoch: 110820 | Loss: 0.4248536229133606 | Test loss: 0.43400269746780396\n",
      "Epoch: 110830 | Loss: 0.42484188079833984 | Test loss: 0.43398886919021606\n",
      "Epoch: 110840 | Loss: 0.4248301684856415 | Test loss: 0.4339749813079834\n",
      "Epoch: 110850 | Loss: 0.42481836676597595 | Test loss: 0.4339611530303955\n",
      "Epoch: 110860 | Loss: 0.4248065948486328 | Test loss: 0.43394729495048523\n",
      "Epoch: 110870 | Loss: 0.42479488253593445 | Test loss: 0.43393346667289734\n",
      "Epoch: 110880 | Loss: 0.4247831404209137 | Test loss: 0.43391963839530945\n",
      "Epoch: 110890 | Loss: 0.4247713088989258 | Test loss: 0.4339057505130768\n",
      "Epoch: 110900 | Loss: 0.4247595965862274 | Test loss: 0.4338919222354889\n",
      "Epoch: 110910 | Loss: 0.4247477948665619 | Test loss: 0.4338780343532562\n",
      "Epoch: 110920 | Loss: 0.42473602294921875 | Test loss: 0.43386420607566833\n",
      "Epoch: 110930 | Loss: 0.4247243106365204 | Test loss: 0.43385037779808044\n",
      "Epoch: 110940 | Loss: 0.42471250891685486 | Test loss: 0.43383654952049255\n",
      "Epoch: 110950 | Loss: 0.4247007369995117 | Test loss: 0.4338226914405823\n",
      "Epoch: 110960 | Loss: 0.42468902468681335 | Test loss: 0.4338088631629944\n",
      "Epoch: 110970 | Loss: 0.4246772825717926 | Test loss: 0.4337949752807617\n",
      "Epoch: 110980 | Loss: 0.42466551065444946 | Test loss: 0.43378114700317383\n",
      "Epoch: 110990 | Loss: 0.4246537387371063 | Test loss: 0.43376731872558594\n",
      "Epoch: 111000 | Loss: 0.42464199662208557 | Test loss: 0.43375346064567566\n",
      "Epoch: 111010 | Loss: 0.42463022470474243 | Test loss: 0.43373963236808777\n",
      "Epoch: 111020 | Loss: 0.4246184527873993 | Test loss: 0.4337257444858551\n",
      "Epoch: 111030 | Loss: 0.42460671067237854 | Test loss: 0.4337119162082672\n",
      "Epoch: 111040 | Loss: 0.4245949387550354 | Test loss: 0.4336980879306793\n",
      "Epoch: 111050 | Loss: 0.42458316683769226 | Test loss: 0.43368420004844666\n",
      "Epoch: 111060 | Loss: 0.4245714247226715 | Test loss: 0.43367037177085876\n",
      "Epoch: 111070 | Loss: 0.42455965280532837 | Test loss: 0.4336565136909485\n",
      "Epoch: 111080 | Loss: 0.42454788088798523 | Test loss: 0.4336426854133606\n",
      "Epoch: 111090 | Loss: 0.4245361387729645 | Test loss: 0.4336288571357727\n",
      "Epoch: 111100 | Loss: 0.4245243966579437 | Test loss: 0.43361496925354004\n",
      "Epoch: 111110 | Loss: 0.4245125949382782 | Test loss: 0.43360114097595215\n",
      "Epoch: 111120 | Loss: 0.42450085282325745 | Test loss: 0.43358731269836426\n",
      "Epoch: 111130 | Loss: 0.4244891107082367 | Test loss: 0.433573454618454\n",
      "Epoch: 111140 | Loss: 0.42447733879089355 | Test loss: 0.4335596263408661\n",
      "Epoch: 111150 | Loss: 0.4244655668735504 | Test loss: 0.4335457980632782\n",
      "Epoch: 111160 | Loss: 0.42445382475852966 | Test loss: 0.43353191018104553\n",
      "Epoch: 111170 | Loss: 0.4244420528411865 | Test loss: 0.43351808190345764\n",
      "Epoch: 111180 | Loss: 0.4244302809238434 | Test loss: 0.433504194021225\n",
      "Epoch: 111190 | Loss: 0.42441853880882263 | Test loss: 0.4334903657436371\n",
      "Epoch: 111200 | Loss: 0.4244067668914795 | Test loss: 0.4334765374660492\n",
      "Epoch: 111210 | Loss: 0.42439505457878113 | Test loss: 0.4334627091884613\n",
      "Epoch: 111220 | Loss: 0.4243832528591156 | Test loss: 0.433448851108551\n",
      "Epoch: 111230 | Loss: 0.42437154054641724 | Test loss: 0.43343496322631836\n",
      "Epoch: 111240 | Loss: 0.4243597686290741 | Test loss: 0.43342113494873047\n",
      "Epoch: 111250 | Loss: 0.42434796690940857 | Test loss: 0.4334073066711426\n",
      "Epoch: 111260 | Loss: 0.4243362545967102 | Test loss: 0.4333934485912323\n",
      "Epoch: 111270 | Loss: 0.4243244230747223 | Test loss: 0.4333796203136444\n",
      "Epoch: 111280 | Loss: 0.42431268095970154 | Test loss: 0.43336573243141174\n",
      "Epoch: 111290 | Loss: 0.4243009090423584 | Test loss: 0.43335190415382385\n",
      "Epoch: 111300 | Loss: 0.42428913712501526 | Test loss: 0.43333807587623596\n",
      "Epoch: 111310 | Loss: 0.4242773950099945 | Test loss: 0.43332424759864807\n",
      "Epoch: 111320 | Loss: 0.42426568269729614 | Test loss: 0.4333103597164154\n",
      "Epoch: 111330 | Loss: 0.424253910779953 | Test loss: 0.4332965314388275\n",
      "Epoch: 111340 | Loss: 0.4242421090602875 | Test loss: 0.43328267335891724\n",
      "Epoch: 111350 | Loss: 0.4242303967475891 | Test loss: 0.43326884508132935\n",
      "Epoch: 111360 | Loss: 0.42421862483024597 | Test loss: 0.43325501680374146\n",
      "Epoch: 111370 | Loss: 0.42420682311058044 | Test loss: 0.43324118852615356\n",
      "Epoch: 111380 | Loss: 0.4241951107978821 | Test loss: 0.4332273006439209\n",
      "Epoch: 111390 | Loss: 0.42418333888053894 | Test loss: 0.433213472366333\n",
      "Epoch: 111400 | Loss: 0.4241715371608734 | Test loss: 0.43319961428642273\n",
      "Epoch: 111410 | Loss: 0.42415982484817505 | Test loss: 0.43318578600883484\n",
      "Epoch: 111420 | Loss: 0.4241480529308319 | Test loss: 0.4331718981266022\n",
      "Epoch: 111430 | Loss: 0.42413631081581116 | Test loss: 0.4331580698490143\n",
      "Epoch: 111440 | Loss: 0.424124538898468 | Test loss: 0.4331441819667816\n",
      "Epoch: 111450 | Loss: 0.42411279678344727 | Test loss: 0.4331303536891937\n",
      "Epoch: 111460 | Loss: 0.42410096526145935 | Test loss: 0.43311652541160583\n",
      "Epoch: 111470 | Loss: 0.424089252948761 | Test loss: 0.43310269713401794\n",
      "Epoch: 111480 | Loss: 0.42407751083374023 | Test loss: 0.43308883905410767\n",
      "Epoch: 111490 | Loss: 0.4240657389163971 | Test loss: 0.4330750107765198\n",
      "Epoch: 111500 | Loss: 0.42405396699905396 | Test loss: 0.4330611228942871\n",
      "Epoch: 111510 | Loss: 0.4240422248840332 | Test loss: 0.4330472946166992\n",
      "Epoch: 111520 | Loss: 0.42403045296669006 | Test loss: 0.43303346633911133\n",
      "Epoch: 111530 | Loss: 0.4240186810493469 | Test loss: 0.43301963806152344\n",
      "Epoch: 111540 | Loss: 0.42400693893432617 | Test loss: 0.43300577998161316\n",
      "Epoch: 111550 | Loss: 0.4239952266216278 | Test loss: 0.43299195170402527\n",
      "Epoch: 111560 | Loss: 0.4239833950996399 | Test loss: 0.4329780638217926\n",
      "Epoch: 111570 | Loss: 0.42397165298461914 | Test loss: 0.4329642355442047\n",
      "Epoch: 111580 | Loss: 0.4239599406719208 | Test loss: 0.4329504072666168\n",
      "Epoch: 111590 | Loss: 0.42394810914993286 | Test loss: 0.43293651938438416\n",
      "Epoch: 111600 | Loss: 0.4239363670349121 | Test loss: 0.43292269110679626\n",
      "Epoch: 111610 | Loss: 0.42392465472221375 | Test loss: 0.432908833026886\n",
      "Epoch: 111620 | Loss: 0.42391282320022583 | Test loss: 0.4328950047492981\n",
      "Epoch: 111630 | Loss: 0.4239010810852051 | Test loss: 0.4328811764717102\n",
      "Epoch: 111640 | Loss: 0.4238893687725067 | Test loss: 0.43286728858947754\n",
      "Epoch: 111650 | Loss: 0.4238775670528412 | Test loss: 0.43285346031188965\n",
      "Epoch: 111660 | Loss: 0.42386579513549805 | Test loss: 0.43283960223197937\n",
      "Epoch: 111670 | Loss: 0.4238540828227997 | Test loss: 0.4328257739543915\n",
      "Epoch: 111680 | Loss: 0.42384234070777893 | Test loss: 0.4328119456768036\n",
      "Epoch: 111690 | Loss: 0.423830509185791 | Test loss: 0.4327980577945709\n",
      "Epoch: 111700 | Loss: 0.42381879687309265 | Test loss: 0.43278422951698303\n",
      "Epoch: 111710 | Loss: 0.4238069951534271 | Test loss: 0.43277034163475037\n",
      "Epoch: 111720 | Loss: 0.423795223236084 | Test loss: 0.4327565133571625\n",
      "Epoch: 111730 | Loss: 0.4237835109233856 | Test loss: 0.4327426850795746\n",
      "Epoch: 111740 | Loss: 0.4237717092037201 | Test loss: 0.4327288568019867\n",
      "Epoch: 111750 | Loss: 0.42375993728637695 | Test loss: 0.4327149987220764\n",
      "Epoch: 111760 | Loss: 0.4237482249736786 | Test loss: 0.4327011704444885\n",
      "Epoch: 111770 | Loss: 0.42373648285865784 | Test loss: 0.43268728256225586\n",
      "Epoch: 111780 | Loss: 0.4237247109413147 | Test loss: 0.43267345428466797\n",
      "Epoch: 111790 | Loss: 0.42371293902397156 | Test loss: 0.4326596260070801\n",
      "Epoch: 111800 | Loss: 0.4237011969089508 | Test loss: 0.4326457679271698\n",
      "Epoch: 111810 | Loss: 0.42368942499160767 | Test loss: 0.4326319396495819\n",
      "Epoch: 111820 | Loss: 0.4236776530742645 | Test loss: 0.43261805176734924\n",
      "Epoch: 111830 | Loss: 0.4236659109592438 | Test loss: 0.43260422348976135\n",
      "Epoch: 111840 | Loss: 0.42365413904190063 | Test loss: 0.43259039521217346\n",
      "Epoch: 111850 | Loss: 0.4236423671245575 | Test loss: 0.4325765073299408\n",
      "Epoch: 111860 | Loss: 0.42363062500953674 | Test loss: 0.4325626790523529\n",
      "Epoch: 111870 | Loss: 0.4236188530921936 | Test loss: 0.4325488209724426\n",
      "Epoch: 111880 | Loss: 0.42360708117485046 | Test loss: 0.43253499269485474\n",
      "Epoch: 111890 | Loss: 0.4235953390598297 | Test loss: 0.43252116441726685\n",
      "Epoch: 111900 | Loss: 0.42358359694480896 | Test loss: 0.4325072765350342\n",
      "Epoch: 111910 | Loss: 0.42357179522514343 | Test loss: 0.4324934482574463\n",
      "Epoch: 111920 | Loss: 0.4235600531101227 | Test loss: 0.4324796199798584\n",
      "Epoch: 111930 | Loss: 0.42354831099510193 | Test loss: 0.4324657618999481\n",
      "Epoch: 111940 | Loss: 0.4235365390777588 | Test loss: 0.43245193362236023\n",
      "Epoch: 111950 | Loss: 0.42352476716041565 | Test loss: 0.43243810534477234\n",
      "Epoch: 111960 | Loss: 0.4235130250453949 | Test loss: 0.4324242174625397\n",
      "Epoch: 111970 | Loss: 0.42350125312805176 | Test loss: 0.4324103891849518\n",
      "Epoch: 111980 | Loss: 0.4234894812107086 | Test loss: 0.4323965013027191\n",
      "Epoch: 111990 | Loss: 0.42347773909568787 | Test loss: 0.4323826730251312\n",
      "Epoch: 112000 | Loss: 0.4234659671783447 | Test loss: 0.43236884474754333\n",
      "Epoch: 112010 | Loss: 0.42345425486564636 | Test loss: 0.43235501646995544\n",
      "Epoch: 112020 | Loss: 0.42344245314598083 | Test loss: 0.43234115839004517\n",
      "Epoch: 112030 | Loss: 0.42343074083328247 | Test loss: 0.4323272705078125\n",
      "Epoch: 112040 | Loss: 0.42341896891593933 | Test loss: 0.4323134422302246\n",
      "Epoch: 112050 | Loss: 0.4234071671962738 | Test loss: 0.4322996139526367\n",
      "Epoch: 112060 | Loss: 0.42339545488357544 | Test loss: 0.43228575587272644\n",
      "Epoch: 112070 | Loss: 0.4233836233615875 | Test loss: 0.43227192759513855\n",
      "Epoch: 112080 | Loss: 0.4233718812465668 | Test loss: 0.4322580397129059\n",
      "Epoch: 112090 | Loss: 0.42336010932922363 | Test loss: 0.432244211435318\n",
      "Epoch: 112100 | Loss: 0.4233483374118805 | Test loss: 0.4322303831577301\n",
      "Epoch: 112110 | Loss: 0.42333659529685974 | Test loss: 0.4322165548801422\n",
      "Epoch: 112120 | Loss: 0.4233248829841614 | Test loss: 0.43220266699790955\n",
      "Epoch: 112130 | Loss: 0.42331311106681824 | Test loss: 0.43218883872032166\n",
      "Epoch: 112140 | Loss: 0.4233013093471527 | Test loss: 0.4321749806404114\n",
      "Epoch: 112150 | Loss: 0.42328959703445435 | Test loss: 0.4321611523628235\n",
      "Epoch: 112160 | Loss: 0.4232778251171112 | Test loss: 0.4321473240852356\n",
      "Epoch: 112170 | Loss: 0.4232660233974457 | Test loss: 0.4321334958076477\n",
      "Epoch: 112180 | Loss: 0.4232543110847473 | Test loss: 0.43211960792541504\n",
      "Epoch: 112190 | Loss: 0.4232425391674042 | Test loss: 0.43210577964782715\n",
      "Epoch: 112200 | Loss: 0.42323073744773865 | Test loss: 0.43209192156791687\n",
      "Epoch: 112210 | Loss: 0.4232190251350403 | Test loss: 0.432078093290329\n",
      "Epoch: 112220 | Loss: 0.42320725321769714 | Test loss: 0.4320642054080963\n",
      "Epoch: 112230 | Loss: 0.4231955111026764 | Test loss: 0.4320503771305084\n",
      "Epoch: 112240 | Loss: 0.42318373918533325 | Test loss: 0.43203648924827576\n",
      "Epoch: 112250 | Loss: 0.4231719970703125 | Test loss: 0.43202266097068787\n",
      "Epoch: 112260 | Loss: 0.4231601655483246 | Test loss: 0.4320088326931\n",
      "Epoch: 112270 | Loss: 0.4231484532356262 | Test loss: 0.4319950044155121\n",
      "Epoch: 112280 | Loss: 0.42313671112060547 | Test loss: 0.4319811463356018\n",
      "Epoch: 112290 | Loss: 0.42312493920326233 | Test loss: 0.4319673180580139\n",
      "Epoch: 112300 | Loss: 0.4231131672859192 | Test loss: 0.43195343017578125\n",
      "Epoch: 112310 | Loss: 0.42310142517089844 | Test loss: 0.43193960189819336\n",
      "Epoch: 112320 | Loss: 0.4230896532535553 | Test loss: 0.43192577362060547\n",
      "Epoch: 112330 | Loss: 0.42307788133621216 | Test loss: 0.4319119453430176\n",
      "Epoch: 112340 | Loss: 0.4230661392211914 | Test loss: 0.4318980872631073\n",
      "Epoch: 112350 | Loss: 0.42305442690849304 | Test loss: 0.4318842589855194\n",
      "Epoch: 112360 | Loss: 0.4230425953865051 | Test loss: 0.43187037110328674\n",
      "Epoch: 112370 | Loss: 0.4230308532714844 | Test loss: 0.43185654282569885\n",
      "Epoch: 112380 | Loss: 0.423019140958786 | Test loss: 0.43184271454811096\n",
      "Epoch: 112390 | Loss: 0.4230073094367981 | Test loss: 0.4318288266658783\n",
      "Epoch: 112400 | Loss: 0.42299556732177734 | Test loss: 0.4318149983882904\n",
      "Epoch: 112410 | Loss: 0.422983855009079 | Test loss: 0.4318011403083801\n",
      "Epoch: 112420 | Loss: 0.42297202348709106 | Test loss: 0.43178731203079224\n",
      "Epoch: 112430 | Loss: 0.4229602813720703 | Test loss: 0.43177348375320435\n",
      "Epoch: 112440 | Loss: 0.42294856905937195 | Test loss: 0.4317595958709717\n",
      "Epoch: 112450 | Loss: 0.4229367673397064 | Test loss: 0.4317457675933838\n",
      "Epoch: 112460 | Loss: 0.4229249954223633 | Test loss: 0.4317319095134735\n",
      "Epoch: 112470 | Loss: 0.4229132831096649 | Test loss: 0.4317180812358856\n",
      "Epoch: 112480 | Loss: 0.42290154099464417 | Test loss: 0.43170425295829773\n",
      "Epoch: 112490 | Loss: 0.42288970947265625 | Test loss: 0.43169036507606506\n",
      "Epoch: 112500 | Loss: 0.4228779971599579 | Test loss: 0.4316765367984772\n",
      "Epoch: 112510 | Loss: 0.42286619544029236 | Test loss: 0.4316626489162445\n",
      "Epoch: 112520 | Loss: 0.4228544235229492 | Test loss: 0.4316488206386566\n",
      "Epoch: 112530 | Loss: 0.42284271121025085 | Test loss: 0.4316349923610687\n",
      "Epoch: 112540 | Loss: 0.4228309094905853 | Test loss: 0.43162116408348083\n",
      "Epoch: 112550 | Loss: 0.4228191375732422 | Test loss: 0.43160730600357056\n",
      "Epoch: 112560 | Loss: 0.4228074252605438 | Test loss: 0.43159347772598267\n",
      "Epoch: 112570 | Loss: 0.42279568314552307 | Test loss: 0.43157958984375\n",
      "Epoch: 112580 | Loss: 0.42278391122817993 | Test loss: 0.4315657615661621\n",
      "Epoch: 112590 | Loss: 0.4227721393108368 | Test loss: 0.4315519332885742\n",
      "Epoch: 112600 | Loss: 0.42276039719581604 | Test loss: 0.43153807520866394\n",
      "Epoch: 112610 | Loss: 0.4227486252784729 | Test loss: 0.43152424693107605\n",
      "Epoch: 112620 | Loss: 0.42273685336112976 | Test loss: 0.4315103590488434\n",
      "Epoch: 112630 | Loss: 0.422725111246109 | Test loss: 0.4314965307712555\n",
      "Epoch: 112640 | Loss: 0.42271333932876587 | Test loss: 0.4314827024936676\n",
      "Epoch: 112650 | Loss: 0.42270156741142273 | Test loss: 0.43146881461143494\n",
      "Epoch: 112660 | Loss: 0.422689825296402 | Test loss: 0.43145498633384705\n",
      "Epoch: 112670 | Loss: 0.42267805337905884 | Test loss: 0.43144112825393677\n",
      "Epoch: 112680 | Loss: 0.4226662814617157 | Test loss: 0.4314272999763489\n",
      "Epoch: 112690 | Loss: 0.42265453934669495 | Test loss: 0.431413471698761\n",
      "Epoch: 112700 | Loss: 0.4226427972316742 | Test loss: 0.4313995838165283\n",
      "Epoch: 112710 | Loss: 0.42263099551200867 | Test loss: 0.43138575553894043\n",
      "Epoch: 112720 | Loss: 0.4226192533969879 | Test loss: 0.43137192726135254\n",
      "Epoch: 112730 | Loss: 0.42260751128196716 | Test loss: 0.43135806918144226\n",
      "Epoch: 112740 | Loss: 0.422595739364624 | Test loss: 0.43134424090385437\n",
      "Epoch: 112750 | Loss: 0.4225839674472809 | Test loss: 0.4313304126262665\n",
      "Epoch: 112760 | Loss: 0.42257222533226013 | Test loss: 0.4313165247440338\n",
      "Epoch: 112770 | Loss: 0.422560453414917 | Test loss: 0.4313026964664459\n",
      "Epoch: 112780 | Loss: 0.42254868149757385 | Test loss: 0.43128880858421326\n",
      "Epoch: 112790 | Loss: 0.4225369393825531 | Test loss: 0.43127498030662537\n",
      "Epoch: 112800 | Loss: 0.42252516746520996 | Test loss: 0.4312611520290375\n",
      "Epoch: 112810 | Loss: 0.4225134551525116 | Test loss: 0.4312473237514496\n",
      "Epoch: 112820 | Loss: 0.42250165343284607 | Test loss: 0.4312334656715393\n",
      "Epoch: 112830 | Loss: 0.4224899411201477 | Test loss: 0.43121957778930664\n",
      "Epoch: 112840 | Loss: 0.42247816920280457 | Test loss: 0.43120574951171875\n",
      "Epoch: 112850 | Loss: 0.42246636748313904 | Test loss: 0.43119192123413086\n",
      "Epoch: 112860 | Loss: 0.4224546551704407 | Test loss: 0.4311780631542206\n",
      "Epoch: 112870 | Loss: 0.42244282364845276 | Test loss: 0.4311642348766327\n",
      "Epoch: 112880 | Loss: 0.422431081533432 | Test loss: 0.4311503469944\n",
      "Epoch: 112890 | Loss: 0.42241930961608887 | Test loss: 0.43113651871681213\n",
      "Epoch: 112900 | Loss: 0.4224075376987457 | Test loss: 0.43112269043922424\n",
      "Epoch: 112910 | Loss: 0.422395795583725 | Test loss: 0.43110886216163635\n",
      "Epoch: 112920 | Loss: 0.4223840832710266 | Test loss: 0.4310949742794037\n",
      "Epoch: 112930 | Loss: 0.42237231135368347 | Test loss: 0.4310811460018158\n",
      "Epoch: 112940 | Loss: 0.42236050963401794 | Test loss: 0.4310672879219055\n",
      "Epoch: 112950 | Loss: 0.4223487973213196 | Test loss: 0.4310534596443176\n",
      "Epoch: 112960 | Loss: 0.42233702540397644 | Test loss: 0.43103963136672974\n",
      "Epoch: 112970 | Loss: 0.4223252236843109 | Test loss: 0.43102580308914185\n",
      "Epoch: 112980 | Loss: 0.42231351137161255 | Test loss: 0.4310119152069092\n",
      "Epoch: 112990 | Loss: 0.4223017394542694 | Test loss: 0.4309980869293213\n",
      "Epoch: 113000 | Loss: 0.4222899377346039 | Test loss: 0.430984228849411\n",
      "Epoch: 113010 | Loss: 0.4222782254219055 | Test loss: 0.4309704005718231\n",
      "Epoch: 113020 | Loss: 0.4222664535045624 | Test loss: 0.43095651268959045\n",
      "Epoch: 113030 | Loss: 0.4222547113895416 | Test loss: 0.43094268441200256\n",
      "Epoch: 113040 | Loss: 0.4222429394721985 | Test loss: 0.4309287965297699\n",
      "Epoch: 113050 | Loss: 0.42223119735717773 | Test loss: 0.430914968252182\n",
      "Epoch: 113060 | Loss: 0.4222193658351898 | Test loss: 0.4309011399745941\n",
      "Epoch: 113070 | Loss: 0.42220765352249146 | Test loss: 0.4308873116970062\n",
      "Epoch: 113080 | Loss: 0.4221959114074707 | Test loss: 0.43087345361709595\n",
      "Epoch: 113090 | Loss: 0.42218413949012756 | Test loss: 0.43085962533950806\n",
      "Epoch: 113100 | Loss: 0.4221723675727844 | Test loss: 0.4308457374572754\n",
      "Epoch: 113110 | Loss: 0.42216062545776367 | Test loss: 0.4308319091796875\n",
      "Epoch: 113120 | Loss: 0.42214885354042053 | Test loss: 0.4308180809020996\n",
      "Epoch: 113130 | Loss: 0.4221370816230774 | Test loss: 0.4308042526245117\n",
      "Epoch: 113140 | Loss: 0.42212533950805664 | Test loss: 0.43079039454460144\n",
      "Epoch: 113150 | Loss: 0.4221136271953583 | Test loss: 0.43077656626701355\n",
      "Epoch: 113160 | Loss: 0.42210179567337036 | Test loss: 0.4307626783847809\n",
      "Epoch: 113170 | Loss: 0.4220900535583496 | Test loss: 0.430748850107193\n",
      "Epoch: 113180 | Loss: 0.42207834124565125 | Test loss: 0.4307350218296051\n",
      "Epoch: 113190 | Loss: 0.42206650972366333 | Test loss: 0.43072113394737244\n",
      "Epoch: 113200 | Loss: 0.4220547676086426 | Test loss: 0.43070730566978455\n",
      "Epoch: 113210 | Loss: 0.4220430552959442 | Test loss: 0.43069344758987427\n",
      "Epoch: 113220 | Loss: 0.4220312237739563 | Test loss: 0.4306796193122864\n",
      "Epoch: 113230 | Loss: 0.42201948165893555 | Test loss: 0.4306657910346985\n",
      "Epoch: 113240 | Loss: 0.4220077693462372 | Test loss: 0.4306519031524658\n",
      "Epoch: 113250 | Loss: 0.42199596762657166 | Test loss: 0.43063807487487793\n",
      "Epoch: 113260 | Loss: 0.4219841957092285 | Test loss: 0.43062421679496765\n",
      "Epoch: 113270 | Loss: 0.42197248339653015 | Test loss: 0.43061038851737976\n",
      "Epoch: 113280 | Loss: 0.4219607412815094 | Test loss: 0.43059656023979187\n",
      "Epoch: 113290 | Loss: 0.4219489097595215 | Test loss: 0.4305826723575592\n",
      "Epoch: 113300 | Loss: 0.4219371974468231 | Test loss: 0.4305688440799713\n",
      "Epoch: 113310 | Loss: 0.4219253957271576 | Test loss: 0.43055495619773865\n",
      "Epoch: 113320 | Loss: 0.42191362380981445 | Test loss: 0.43054112792015076\n",
      "Epoch: 113330 | Loss: 0.4219019114971161 | Test loss: 0.43052729964256287\n",
      "Epoch: 113340 | Loss: 0.42189010977745056 | Test loss: 0.430513471364975\n",
      "Epoch: 113350 | Loss: 0.4218783378601074 | Test loss: 0.4304996132850647\n",
      "Epoch: 113360 | Loss: 0.42186662554740906 | Test loss: 0.4304857850074768\n",
      "Epoch: 113370 | Loss: 0.4218548834323883 | Test loss: 0.43047189712524414\n",
      "Epoch: 113380 | Loss: 0.42184311151504517 | Test loss: 0.43045806884765625\n",
      "Epoch: 113390 | Loss: 0.421831339597702 | Test loss: 0.43044424057006836\n",
      "Epoch: 113400 | Loss: 0.4218195974826813 | Test loss: 0.4304303824901581\n",
      "Epoch: 113410 | Loss: 0.42180782556533813 | Test loss: 0.4304165542125702\n",
      "Epoch: 113420 | Loss: 0.421796053647995 | Test loss: 0.4304026663303375\n",
      "Epoch: 113430 | Loss: 0.42178431153297424 | Test loss: 0.43038883805274963\n",
      "Epoch: 113440 | Loss: 0.4217725396156311 | Test loss: 0.43037500977516174\n",
      "Epoch: 113450 | Loss: 0.42176076769828796 | Test loss: 0.4303611218929291\n",
      "Epoch: 113460 | Loss: 0.4217490255832672 | Test loss: 0.4303472936153412\n",
      "Epoch: 113470 | Loss: 0.4217372536659241 | Test loss: 0.4303334355354309\n",
      "Epoch: 113480 | Loss: 0.42172548174858093 | Test loss: 0.430319607257843\n",
      "Epoch: 113490 | Loss: 0.4217137396335602 | Test loss: 0.4303057789802551\n",
      "Epoch: 113500 | Loss: 0.42170199751853943 | Test loss: 0.43029189109802246\n",
      "Epoch: 113510 | Loss: 0.4216901957988739 | Test loss: 0.43027806282043457\n",
      "Epoch: 113520 | Loss: 0.42167845368385315 | Test loss: 0.4302642345428467\n",
      "Epoch: 113530 | Loss: 0.4216667115688324 | Test loss: 0.4302503764629364\n",
      "Epoch: 113540 | Loss: 0.42165493965148926 | Test loss: 0.4302365481853485\n",
      "Epoch: 113550 | Loss: 0.4216431677341461 | Test loss: 0.4302227199077606\n",
      "Epoch: 113560 | Loss: 0.42163142561912537 | Test loss: 0.43020883202552795\n",
      "Epoch: 113570 | Loss: 0.4216196537017822 | Test loss: 0.43019500374794006\n",
      "Epoch: 113580 | Loss: 0.4216078817844391 | Test loss: 0.4301811158657074\n",
      "Epoch: 113590 | Loss: 0.42159613966941833 | Test loss: 0.4301672875881195\n",
      "Epoch: 113600 | Loss: 0.4215843677520752 | Test loss: 0.4301534593105316\n",
      "Epoch: 113610 | Loss: 0.42157265543937683 | Test loss: 0.4301396310329437\n",
      "Epoch: 113620 | Loss: 0.4215608537197113 | Test loss: 0.43012577295303345\n",
      "Epoch: 113630 | Loss: 0.42154914140701294 | Test loss: 0.4301118850708008\n",
      "Epoch: 113640 | Loss: 0.4215373694896698 | Test loss: 0.4300980567932129\n",
      "Epoch: 113650 | Loss: 0.4215255677700043 | Test loss: 0.430084228515625\n",
      "Epoch: 113660 | Loss: 0.4215138554573059 | Test loss: 0.4300703704357147\n",
      "Epoch: 113670 | Loss: 0.421502023935318 | Test loss: 0.43005654215812683\n",
      "Epoch: 113680 | Loss: 0.42149028182029724 | Test loss: 0.43004265427589417\n",
      "Epoch: 113690 | Loss: 0.4214785099029541 | Test loss: 0.4300288259983063\n",
      "Epoch: 113700 | Loss: 0.42146673798561096 | Test loss: 0.4300149977207184\n",
      "Epoch: 113710 | Loss: 0.4214549958705902 | Test loss: 0.4300011694431305\n",
      "Epoch: 113720 | Loss: 0.42144328355789185 | Test loss: 0.4299872815608978\n",
      "Epoch: 113730 | Loss: 0.4214315116405487 | Test loss: 0.42997345328330994\n",
      "Epoch: 113740 | Loss: 0.4214197099208832 | Test loss: 0.42995959520339966\n",
      "Epoch: 113750 | Loss: 0.4214079976081848 | Test loss: 0.42994576692581177\n",
      "Epoch: 113760 | Loss: 0.4213962256908417 | Test loss: 0.4299319386482239\n",
      "Epoch: 113770 | Loss: 0.42138442397117615 | Test loss: 0.429918110370636\n",
      "Epoch: 113780 | Loss: 0.4213727116584778 | Test loss: 0.4299042224884033\n",
      "Epoch: 113790 | Loss: 0.42136093974113464 | Test loss: 0.42989039421081543\n",
      "Epoch: 113800 | Loss: 0.4213491380214691 | Test loss: 0.42987653613090515\n",
      "Epoch: 113810 | Loss: 0.42133742570877075 | Test loss: 0.42986270785331726\n",
      "Epoch: 113820 | Loss: 0.4213256537914276 | Test loss: 0.4298488199710846\n",
      "Epoch: 113830 | Loss: 0.42131391167640686 | Test loss: 0.4298349916934967\n",
      "Epoch: 113840 | Loss: 0.4213021397590637 | Test loss: 0.42982110381126404\n",
      "Epoch: 113850 | Loss: 0.42129039764404297 | Test loss: 0.42980727553367615\n",
      "Epoch: 113860 | Loss: 0.42127856612205505 | Test loss: 0.42979344725608826\n",
      "Epoch: 113870 | Loss: 0.4212668538093567 | Test loss: 0.42977961897850037\n",
      "Epoch: 113880 | Loss: 0.42125511169433594 | Test loss: 0.4297657608985901\n",
      "Epoch: 113890 | Loss: 0.4212433397769928 | Test loss: 0.4297519326210022\n",
      "Epoch: 113900 | Loss: 0.42123156785964966 | Test loss: 0.42973804473876953\n",
      "Epoch: 113910 | Loss: 0.4212198257446289 | Test loss: 0.42972421646118164\n",
      "Epoch: 113920 | Loss: 0.42120805382728577 | Test loss: 0.42971038818359375\n",
      "Epoch: 113930 | Loss: 0.4211962819099426 | Test loss: 0.42969655990600586\n",
      "Epoch: 113940 | Loss: 0.4211845397949219 | Test loss: 0.4296827018260956\n",
      "Epoch: 113950 | Loss: 0.4211728274822235 | Test loss: 0.4296688735485077\n",
      "Epoch: 113960 | Loss: 0.4211609959602356 | Test loss: 0.429654985666275\n",
      "Epoch: 113970 | Loss: 0.42114925384521484 | Test loss: 0.42964115738868713\n",
      "Epoch: 113980 | Loss: 0.4211375415325165 | Test loss: 0.42962732911109924\n",
      "Epoch: 113990 | Loss: 0.42112571001052856 | Test loss: 0.4296134412288666\n",
      "Epoch: 114000 | Loss: 0.4211139678955078 | Test loss: 0.4295996129512787\n",
      "Epoch: 114010 | Loss: 0.42110225558280945 | Test loss: 0.4295857548713684\n",
      "Epoch: 114020 | Loss: 0.42109042406082153 | Test loss: 0.4295719265937805\n",
      "Epoch: 114030 | Loss: 0.4210786819458008 | Test loss: 0.4295580983161926\n",
      "Epoch: 114040 | Loss: 0.4210669696331024 | Test loss: 0.42954421043395996\n",
      "Epoch: 114050 | Loss: 0.4210551679134369 | Test loss: 0.42953038215637207\n",
      "Epoch: 114060 | Loss: 0.42104339599609375 | Test loss: 0.4295165240764618\n",
      "Epoch: 114070 | Loss: 0.4210316836833954 | Test loss: 0.4295026957988739\n",
      "Epoch: 114080 | Loss: 0.42101994156837463 | Test loss: 0.429488867521286\n",
      "Epoch: 114090 | Loss: 0.4210081100463867 | Test loss: 0.42947497963905334\n",
      "Epoch: 114100 | Loss: 0.42099639773368835 | Test loss: 0.42946115136146545\n",
      "Epoch: 114110 | Loss: 0.4209845960140228 | Test loss: 0.4294472634792328\n",
      "Epoch: 114120 | Loss: 0.4209728240966797 | Test loss: 0.4294334352016449\n",
      "Epoch: 114130 | Loss: 0.4209611117839813 | Test loss: 0.429419606924057\n",
      "Epoch: 114140 | Loss: 0.4209493100643158 | Test loss: 0.4294057786464691\n",
      "Epoch: 114150 | Loss: 0.42093753814697266 | Test loss: 0.42939192056655884\n",
      "Epoch: 114160 | Loss: 0.4209258258342743 | Test loss: 0.42937809228897095\n",
      "Epoch: 114170 | Loss: 0.42091408371925354 | Test loss: 0.4293642044067383\n",
      "Epoch: 114180 | Loss: 0.4209023118019104 | Test loss: 0.4293503761291504\n",
      "Epoch: 114190 | Loss: 0.42089053988456726 | Test loss: 0.4293365478515625\n",
      "Epoch: 114200 | Loss: 0.4208787977695465 | Test loss: 0.4293226897716522\n",
      "Epoch: 114210 | Loss: 0.42086702585220337 | Test loss: 0.42930886149406433\n",
      "Epoch: 114220 | Loss: 0.42085525393486023 | Test loss: 0.42929497361183167\n",
      "Epoch: 114230 | Loss: 0.4208435118198395 | Test loss: 0.4292811453342438\n",
      "Epoch: 114240 | Loss: 0.42083173990249634 | Test loss: 0.4292673170566559\n",
      "Epoch: 114250 | Loss: 0.4208199679851532 | Test loss: 0.4292534291744232\n",
      "Epoch: 114260 | Loss: 0.42080822587013245 | Test loss: 0.4292396008968353\n",
      "Epoch: 114270 | Loss: 0.4207964539527893 | Test loss: 0.42922574281692505\n",
      "Epoch: 114280 | Loss: 0.42078468203544617 | Test loss: 0.42921191453933716\n",
      "Epoch: 114290 | Loss: 0.4207729399204254 | Test loss: 0.42919808626174927\n",
      "Epoch: 114300 | Loss: 0.42076119780540466 | Test loss: 0.4291841983795166\n",
      "Epoch: 114310 | Loss: 0.42074939608573914 | Test loss: 0.4291703701019287\n",
      "Epoch: 114320 | Loss: 0.4207376539707184 | Test loss: 0.4291565418243408\n",
      "Epoch: 114330 | Loss: 0.42072591185569763 | Test loss: 0.42914268374443054\n",
      "Epoch: 114340 | Loss: 0.4207141399383545 | Test loss: 0.42912885546684265\n",
      "Epoch: 114350 | Loss: 0.42070236802101135 | Test loss: 0.42911502718925476\n",
      "Epoch: 114360 | Loss: 0.4206906259059906 | Test loss: 0.4291011393070221\n",
      "Epoch: 114370 | Loss: 0.42067885398864746 | Test loss: 0.4290873110294342\n",
      "Epoch: 114380 | Loss: 0.4206670820713043 | Test loss: 0.42907342314720154\n",
      "Epoch: 114390 | Loss: 0.42065533995628357 | Test loss: 0.42905959486961365\n",
      "Epoch: 114400 | Loss: 0.42064356803894043 | Test loss: 0.42904576659202576\n",
      "Epoch: 114410 | Loss: 0.42063185572624207 | Test loss: 0.42903193831443787\n",
      "Epoch: 114420 | Loss: 0.42062005400657654 | Test loss: 0.4290180802345276\n",
      "Epoch: 114430 | Loss: 0.4206083416938782 | Test loss: 0.4290041923522949\n",
      "Epoch: 114440 | Loss: 0.42059656977653503 | Test loss: 0.42899036407470703\n",
      "Epoch: 114450 | Loss: 0.4205847680568695 | Test loss: 0.42897653579711914\n",
      "Epoch: 114460 | Loss: 0.42057305574417114 | Test loss: 0.42896267771720886\n",
      "Epoch: 114470 | Loss: 0.4205612242221832 | Test loss: 0.42894884943962097\n",
      "Epoch: 114480 | Loss: 0.4205494821071625 | Test loss: 0.4289349615573883\n",
      "Epoch: 114490 | Loss: 0.42053771018981934 | Test loss: 0.4289211332798004\n",
      "Epoch: 114500 | Loss: 0.4205259382724762 | Test loss: 0.4289073050022125\n",
      "Epoch: 114510 | Loss: 0.42051419615745544 | Test loss: 0.42889347672462463\n",
      "Epoch: 114520 | Loss: 0.4205024838447571 | Test loss: 0.42887958884239197\n",
      "Epoch: 114530 | Loss: 0.42049071192741394 | Test loss: 0.4288657605648041\n",
      "Epoch: 114540 | Loss: 0.4204789102077484 | Test loss: 0.4288519024848938\n",
      "Epoch: 114550 | Loss: 0.42046719789505005 | Test loss: 0.4288380742073059\n",
      "Epoch: 114560 | Loss: 0.4204554259777069 | Test loss: 0.428824245929718\n",
      "Epoch: 114570 | Loss: 0.4204436242580414 | Test loss: 0.4288104176521301\n",
      "Epoch: 114580 | Loss: 0.420431911945343 | Test loss: 0.42879652976989746\n",
      "Epoch: 114590 | Loss: 0.4204201400279999 | Test loss: 0.42878270149230957\n",
      "Epoch: 114600 | Loss: 0.42040833830833435 | Test loss: 0.4287688434123993\n",
      "Epoch: 114610 | Loss: 0.420396625995636 | Test loss: 0.4287550151348114\n",
      "Epoch: 114620 | Loss: 0.42038485407829285 | Test loss: 0.42874112725257874\n",
      "Epoch: 114630 | Loss: 0.4203731119632721 | Test loss: 0.42872729897499084\n",
      "Epoch: 114640 | Loss: 0.42036134004592896 | Test loss: 0.4287134110927582\n",
      "Epoch: 114650 | Loss: 0.4203495979309082 | Test loss: 0.4286995828151703\n",
      "Epoch: 114660 | Loss: 0.4203377664089203 | Test loss: 0.4286857545375824\n",
      "Epoch: 114670 | Loss: 0.4203260540962219 | Test loss: 0.4286719262599945\n",
      "Epoch: 114680 | Loss: 0.42031431198120117 | Test loss: 0.42865806818008423\n",
      "Epoch: 114690 | Loss: 0.42030254006385803 | Test loss: 0.42864423990249634\n",
      "Epoch: 114700 | Loss: 0.4202907681465149 | Test loss: 0.42863035202026367\n",
      "Epoch: 114710 | Loss: 0.42027902603149414 | Test loss: 0.4286165237426758\n",
      "Epoch: 114720 | Loss: 0.420267254114151 | Test loss: 0.4286026954650879\n",
      "Epoch: 114730 | Loss: 0.42025548219680786 | Test loss: 0.4285888671875\n",
      "Epoch: 114740 | Loss: 0.4202437400817871 | Test loss: 0.4285750091075897\n",
      "Epoch: 114750 | Loss: 0.42023202776908875 | Test loss: 0.42856118083000183\n",
      "Epoch: 114760 | Loss: 0.42022019624710083 | Test loss: 0.42854729294776917\n",
      "Epoch: 114770 | Loss: 0.4202084541320801 | Test loss: 0.4285334646701813\n",
      "Epoch: 114780 | Loss: 0.4201967418193817 | Test loss: 0.4285196363925934\n",
      "Epoch: 114790 | Loss: 0.4201849102973938 | Test loss: 0.4285057485103607\n",
      "Epoch: 114800 | Loss: 0.42017316818237305 | Test loss: 0.4284919202327728\n",
      "Epoch: 114810 | Loss: 0.4201614558696747 | Test loss: 0.42847806215286255\n",
      "Epoch: 114820 | Loss: 0.42014962434768677 | Test loss: 0.42846423387527466\n",
      "Epoch: 114830 | Loss: 0.420137882232666 | Test loss: 0.42845040559768677\n",
      "Epoch: 114840 | Loss: 0.42012616991996765 | Test loss: 0.4284365177154541\n",
      "Epoch: 114850 | Loss: 0.4201143682003021 | Test loss: 0.4284226894378662\n",
      "Epoch: 114860 | Loss: 0.420102596282959 | Test loss: 0.42840883135795593\n",
      "Epoch: 114870 | Loss: 0.4200908839702606 | Test loss: 0.42839500308036804\n",
      "Epoch: 114880 | Loss: 0.42007914185523987 | Test loss: 0.42838117480278015\n",
      "Epoch: 114890 | Loss: 0.42006731033325195 | Test loss: 0.4283672869205475\n",
      "Epoch: 114900 | Loss: 0.4200555980205536 | Test loss: 0.4283534586429596\n",
      "Epoch: 114910 | Loss: 0.42004379630088806 | Test loss: 0.42833957076072693\n",
      "Epoch: 114920 | Loss: 0.4200320243835449 | Test loss: 0.42832574248313904\n",
      "Epoch: 114930 | Loss: 0.42002031207084656 | Test loss: 0.42831191420555115\n",
      "Epoch: 114940 | Loss: 0.42000851035118103 | Test loss: 0.42829808592796326\n",
      "Epoch: 114950 | Loss: 0.4199967384338379 | Test loss: 0.428284227848053\n",
      "Epoch: 114960 | Loss: 0.4199850261211395 | Test loss: 0.4282703995704651\n",
      "Epoch: 114970 | Loss: 0.4199732840061188 | Test loss: 0.4282565116882324\n",
      "Epoch: 114980 | Loss: 0.41996151208877563 | Test loss: 0.42824268341064453\n",
      "Epoch: 114990 | Loss: 0.4199497401714325 | Test loss: 0.42822885513305664\n",
      "Epoch: 115000 | Loss: 0.41993799805641174 | Test loss: 0.42821499705314636\n",
      "Epoch: 115010 | Loss: 0.4199262261390686 | Test loss: 0.42820116877555847\n",
      "Epoch: 115020 | Loss: 0.41991445422172546 | Test loss: 0.4281872808933258\n",
      "Epoch: 115030 | Loss: 0.4199027121067047 | Test loss: 0.4281734526157379\n",
      "Epoch: 115040 | Loss: 0.4198909401893616 | Test loss: 0.42815962433815\n",
      "Epoch: 115050 | Loss: 0.41987916827201843 | Test loss: 0.42814573645591736\n",
      "Epoch: 115060 | Loss: 0.4198674261569977 | Test loss: 0.42813190817832947\n",
      "Epoch: 115070 | Loss: 0.41985565423965454 | Test loss: 0.4281180500984192\n",
      "Epoch: 115080 | Loss: 0.4198438823223114 | Test loss: 0.4281042218208313\n",
      "Epoch: 115090 | Loss: 0.41983214020729065 | Test loss: 0.4280903935432434\n",
      "Epoch: 115100 | Loss: 0.4198203980922699 | Test loss: 0.42807650566101074\n",
      "Epoch: 115110 | Loss: 0.41980859637260437 | Test loss: 0.42806267738342285\n",
      "Epoch: 115120 | Loss: 0.4197968542575836 | Test loss: 0.42804884910583496\n",
      "Epoch: 115130 | Loss: 0.41978511214256287 | Test loss: 0.4280349910259247\n",
      "Epoch: 115140 | Loss: 0.4197733402252197 | Test loss: 0.4280211627483368\n",
      "Epoch: 115150 | Loss: 0.4197615683078766 | Test loss: 0.4280073344707489\n",
      "Epoch: 115160 | Loss: 0.41974982619285583 | Test loss: 0.42799344658851624\n",
      "Epoch: 115170 | Loss: 0.4197380542755127 | Test loss: 0.42797961831092834\n",
      "Epoch: 115180 | Loss: 0.41972628235816956 | Test loss: 0.4279657304286957\n",
      "Epoch: 115190 | Loss: 0.4197145104408264 | Test loss: 0.4279519021511078\n",
      "Epoch: 115200 | Loss: 0.41970276832580566 | Test loss: 0.4279380738735199\n",
      "Epoch: 115210 | Loss: 0.4196910560131073 | Test loss: 0.427924245595932\n",
      "Epoch: 115220 | Loss: 0.4196792542934418 | Test loss: 0.42791038751602173\n",
      "Epoch: 115230 | Loss: 0.4196675419807434 | Test loss: 0.42789649963378906\n",
      "Epoch: 115240 | Loss: 0.41965577006340027 | Test loss: 0.42788267135620117\n",
      "Epoch: 115250 | Loss: 0.41964396834373474 | Test loss: 0.4278688430786133\n",
      "Epoch: 115260 | Loss: 0.4196322560310364 | Test loss: 0.427854984998703\n",
      "Epoch: 115270 | Loss: 0.41962042450904846 | Test loss: 0.4278411567211151\n",
      "Epoch: 115280 | Loss: 0.4196086823940277 | Test loss: 0.42782726883888245\n",
      "Epoch: 115290 | Loss: 0.41959691047668457 | Test loss: 0.42781344056129456\n",
      "Epoch: 115300 | Loss: 0.41958513855934143 | Test loss: 0.42779961228370667\n",
      "Epoch: 115310 | Loss: 0.4195733964443207 | Test loss: 0.4277857840061188\n",
      "Epoch: 115320 | Loss: 0.4195616841316223 | Test loss: 0.4277718961238861\n",
      "Epoch: 115330 | Loss: 0.4195499122142792 | Test loss: 0.4277580678462982\n",
      "Epoch: 115340 | Loss: 0.41953811049461365 | Test loss: 0.42774420976638794\n",
      "Epoch: 115350 | Loss: 0.4195263981819153 | Test loss: 0.42773038148880005\n",
      "Epoch: 115360 | Loss: 0.41951462626457214 | Test loss: 0.42771655321121216\n",
      "Epoch: 115370 | Loss: 0.4195028245449066 | Test loss: 0.42770272493362427\n",
      "Epoch: 115380 | Loss: 0.41949111223220825 | Test loss: 0.4276888370513916\n",
      "Epoch: 115390 | Loss: 0.4194793403148651 | Test loss: 0.4276750087738037\n",
      "Epoch: 115400 | Loss: 0.4194675385951996 | Test loss: 0.42766115069389343\n",
      "Epoch: 115410 | Loss: 0.4194558262825012 | Test loss: 0.42764732241630554\n",
      "Epoch: 115420 | Loss: 0.4194440543651581 | Test loss: 0.4276334345340729\n",
      "Epoch: 115430 | Loss: 0.41943231225013733 | Test loss: 0.427619606256485\n",
      "Epoch: 115440 | Loss: 0.4194205403327942 | Test loss: 0.4276057183742523\n",
      "Epoch: 115450 | Loss: 0.41940879821777344 | Test loss: 0.42759189009666443\n",
      "Epoch: 115460 | Loss: 0.4193969666957855 | Test loss: 0.42757806181907654\n",
      "Epoch: 115470 | Loss: 0.41938525438308716 | Test loss: 0.42756423354148865\n",
      "Epoch: 115480 | Loss: 0.4193735122680664 | Test loss: 0.42755037546157837\n",
      "Epoch: 115490 | Loss: 0.41936174035072327 | Test loss: 0.4275365471839905\n",
      "Epoch: 115500 | Loss: 0.4193499684333801 | Test loss: 0.4275226593017578\n",
      "Epoch: 115510 | Loss: 0.4193382263183594 | Test loss: 0.4275088310241699\n",
      "Epoch: 115520 | Loss: 0.41932645440101624 | Test loss: 0.42749500274658203\n",
      "Epoch: 115530 | Loss: 0.4193146824836731 | Test loss: 0.42748117446899414\n",
      "Epoch: 115540 | Loss: 0.41930294036865234 | Test loss: 0.42746731638908386\n",
      "Epoch: 115550 | Loss: 0.419291228055954 | Test loss: 0.42745348811149597\n",
      "Epoch: 115560 | Loss: 0.41927939653396606 | Test loss: 0.4274396002292633\n",
      "Epoch: 115570 | Loss: 0.4192676544189453 | Test loss: 0.4274257719516754\n",
      "Epoch: 115580 | Loss: 0.41925594210624695 | Test loss: 0.4274119436740875\n",
      "Epoch: 115590 | Loss: 0.41924411058425903 | Test loss: 0.42739805579185486\n",
      "Epoch: 115600 | Loss: 0.4192323684692383 | Test loss: 0.42738422751426697\n",
      "Epoch: 115610 | Loss: 0.4192206561565399 | Test loss: 0.4273703694343567\n",
      "Epoch: 115620 | Loss: 0.419208824634552 | Test loss: 0.4273565411567688\n",
      "Epoch: 115630 | Loss: 0.41919708251953125 | Test loss: 0.4273427128791809\n",
      "Epoch: 115640 | Loss: 0.4191853702068329 | Test loss: 0.42732882499694824\n",
      "Epoch: 115650 | Loss: 0.41917356848716736 | Test loss: 0.42731499671936035\n",
      "Epoch: 115660 | Loss: 0.4191617965698242 | Test loss: 0.4273011386394501\n",
      "Epoch: 115670 | Loss: 0.41915008425712585 | Test loss: 0.4272873103618622\n",
      "Epoch: 115680 | Loss: 0.4191383421421051 | Test loss: 0.4272734820842743\n",
      "Epoch: 115690 | Loss: 0.4191265106201172 | Test loss: 0.4272595942020416\n",
      "Epoch: 115700 | Loss: 0.4191147983074188 | Test loss: 0.42724576592445374\n",
      "Epoch: 115710 | Loss: 0.4191029965877533 | Test loss: 0.42723187804222107\n",
      "Epoch: 115720 | Loss: 0.41909122467041016 | Test loss: 0.4272180497646332\n",
      "Epoch: 115730 | Loss: 0.4190795123577118 | Test loss: 0.4272042214870453\n",
      "Epoch: 115740 | Loss: 0.41906771063804626 | Test loss: 0.4271903932094574\n",
      "Epoch: 115750 | Loss: 0.4190559387207031 | Test loss: 0.4271765351295471\n",
      "Epoch: 115760 | Loss: 0.41904422640800476 | Test loss: 0.42716270685195923\n",
      "Epoch: 115770 | Loss: 0.419032484292984 | Test loss: 0.42714881896972656\n",
      "Epoch: 115780 | Loss: 0.41902071237564087 | Test loss: 0.42713499069213867\n",
      "Epoch: 115790 | Loss: 0.41900894045829773 | Test loss: 0.4271211624145508\n",
      "Epoch: 115800 | Loss: 0.418997198343277 | Test loss: 0.4271073043346405\n",
      "Epoch: 115810 | Loss: 0.41898542642593384 | Test loss: 0.4270934760570526\n",
      "Epoch: 115820 | Loss: 0.4189736545085907 | Test loss: 0.42707958817481995\n",
      "Epoch: 115830 | Loss: 0.41896191239356995 | Test loss: 0.42706575989723206\n",
      "Epoch: 115840 | Loss: 0.4189501404762268 | Test loss: 0.42705193161964417\n",
      "Epoch: 115850 | Loss: 0.41893836855888367 | Test loss: 0.4270380437374115\n",
      "Epoch: 115860 | Loss: 0.4189266264438629 | Test loss: 0.4270242154598236\n",
      "Epoch: 115870 | Loss: 0.4189148545265198 | Test loss: 0.42701035737991333\n",
      "Epoch: 115880 | Loss: 0.41890308260917664 | Test loss: 0.42699652910232544\n",
      "Epoch: 115890 | Loss: 0.4188913404941559 | Test loss: 0.42698270082473755\n",
      "Epoch: 115900 | Loss: 0.41887959837913513 | Test loss: 0.4269688129425049\n",
      "Epoch: 115910 | Loss: 0.4188677966594696 | Test loss: 0.426954984664917\n",
      "Epoch: 115920 | Loss: 0.41885605454444885 | Test loss: 0.4269411563873291\n",
      "Epoch: 115930 | Loss: 0.4188443124294281 | Test loss: 0.4269272983074188\n",
      "Epoch: 115940 | Loss: 0.41883254051208496 | Test loss: 0.42691347002983093\n",
      "Epoch: 115950 | Loss: 0.4188207685947418 | Test loss: 0.42689964175224304\n",
      "Epoch: 115960 | Loss: 0.41880902647972107 | Test loss: 0.4268857538700104\n",
      "Epoch: 115970 | Loss: 0.41879725456237793 | Test loss: 0.4268719255924225\n",
      "Epoch: 115980 | Loss: 0.4187854826450348 | Test loss: 0.4268580377101898\n",
      "Epoch: 115990 | Loss: 0.41877371072769165 | Test loss: 0.42684420943260193\n",
      "Epoch: 116000 | Loss: 0.4187619686126709 | Test loss: 0.42683038115501404\n",
      "Epoch: 116010 | Loss: 0.41875025629997253 | Test loss: 0.42681655287742615\n",
      "Epoch: 116020 | Loss: 0.418738454580307 | Test loss: 0.42680269479751587\n",
      "Epoch: 116030 | Loss: 0.41872674226760864 | Test loss: 0.4267888069152832\n",
      "Epoch: 116040 | Loss: 0.4187149703502655 | Test loss: 0.4267749786376953\n",
      "Epoch: 116050 | Loss: 0.4187031686306 | Test loss: 0.4267611503601074\n",
      "Epoch: 116060 | Loss: 0.4186914563179016 | Test loss: 0.42674729228019714\n",
      "Epoch: 116070 | Loss: 0.4186796247959137 | Test loss: 0.42673346400260925\n",
      "Epoch: 116080 | Loss: 0.41866788268089294 | Test loss: 0.4267195761203766\n",
      "Epoch: 116090 | Loss: 0.4186561107635498 | Test loss: 0.4267057478427887\n",
      "Epoch: 116100 | Loss: 0.41864433884620667 | Test loss: 0.4266919195652008\n",
      "Epoch: 116110 | Loss: 0.4186325967311859 | Test loss: 0.4266780912876129\n",
      "Epoch: 116120 | Loss: 0.41862088441848755 | Test loss: 0.42666420340538025\n",
      "Epoch: 116130 | Loss: 0.4186091125011444 | Test loss: 0.42665037512779236\n",
      "Epoch: 116140 | Loss: 0.4185973107814789 | Test loss: 0.4266365170478821\n",
      "Epoch: 116150 | Loss: 0.4185855984687805 | Test loss: 0.4266226887702942\n",
      "Epoch: 116160 | Loss: 0.4185738265514374 | Test loss: 0.4266088604927063\n",
      "Epoch: 116170 | Loss: 0.41856202483177185 | Test loss: 0.4265950322151184\n",
      "Epoch: 116180 | Loss: 0.4185503125190735 | Test loss: 0.42658114433288574\n",
      "Epoch: 116190 | Loss: 0.41853854060173035 | Test loss: 0.42656731605529785\n",
      "Epoch: 116200 | Loss: 0.4185267388820648 | Test loss: 0.4265534579753876\n",
      "Epoch: 116210 | Loss: 0.41851502656936646 | Test loss: 0.4265396296977997\n",
      "Epoch: 116220 | Loss: 0.4185032546520233 | Test loss: 0.426525741815567\n",
      "Epoch: 116230 | Loss: 0.41849151253700256 | Test loss: 0.4265119135379791\n",
      "Epoch: 116240 | Loss: 0.4184797406196594 | Test loss: 0.42649802565574646\n",
      "Epoch: 116250 | Loss: 0.41846799850463867 | Test loss: 0.42648419737815857\n",
      "Epoch: 116260 | Loss: 0.41845616698265076 | Test loss: 0.4264703691005707\n",
      "Epoch: 116270 | Loss: 0.4184444546699524 | Test loss: 0.4264565408229828\n",
      "Epoch: 116280 | Loss: 0.41843271255493164 | Test loss: 0.4264426827430725\n",
      "Epoch: 116290 | Loss: 0.4184209406375885 | Test loss: 0.4264288544654846\n",
      "Epoch: 116300 | Loss: 0.41840916872024536 | Test loss: 0.42641496658325195\n",
      "Epoch: 116310 | Loss: 0.4183974266052246 | Test loss: 0.42640113830566406\n",
      "Epoch: 116320 | Loss: 0.41838565468788147 | Test loss: 0.42638731002807617\n",
      "Epoch: 116330 | Loss: 0.41837388277053833 | Test loss: 0.4263734817504883\n",
      "Epoch: 116340 | Loss: 0.4183621406555176 | Test loss: 0.426359623670578\n",
      "Epoch: 116350 | Loss: 0.4183504283428192 | Test loss: 0.4263457953929901\n",
      "Epoch: 116360 | Loss: 0.4183385968208313 | Test loss: 0.42633190751075745\n",
      "Epoch: 116370 | Loss: 0.41832685470581055 | Test loss: 0.42631807923316956\n",
      "Epoch: 116380 | Loss: 0.4183151423931122 | Test loss: 0.42630425095558167\n",
      "Epoch: 116390 | Loss: 0.41830331087112427 | Test loss: 0.426290363073349\n",
      "Epoch: 116400 | Loss: 0.4182915687561035 | Test loss: 0.4262765347957611\n",
      "Epoch: 116410 | Loss: 0.41827985644340515 | Test loss: 0.42626267671585083\n",
      "Epoch: 116420 | Loss: 0.41826802492141724 | Test loss: 0.42624884843826294\n",
      "Epoch: 116430 | Loss: 0.4182562828063965 | Test loss: 0.42623502016067505\n",
      "Epoch: 116440 | Loss: 0.4182445704936981 | Test loss: 0.4262211322784424\n",
      "Epoch: 116450 | Loss: 0.4182327687740326 | Test loss: 0.4262073040008545\n",
      "Epoch: 116460 | Loss: 0.41822099685668945 | Test loss: 0.4261934459209442\n",
      "Epoch: 116470 | Loss: 0.4182092845439911 | Test loss: 0.4261796176433563\n",
      "Epoch: 116480 | Loss: 0.41819754242897034 | Test loss: 0.42616578936576843\n",
      "Epoch: 116490 | Loss: 0.4181857109069824 | Test loss: 0.42615190148353577\n",
      "Epoch: 116500 | Loss: 0.41817399859428406 | Test loss: 0.4261380732059479\n",
      "Epoch: 116510 | Loss: 0.41816219687461853 | Test loss: 0.4261241853237152\n",
      "Epoch: 116520 | Loss: 0.4181504249572754 | Test loss: 0.4261103570461273\n",
      "Epoch: 116530 | Loss: 0.418138712644577 | Test loss: 0.42609652876853943\n",
      "Epoch: 116540 | Loss: 0.4181269109249115 | Test loss: 0.42608270049095154\n",
      "Epoch: 116550 | Loss: 0.41811513900756836 | Test loss: 0.42606884241104126\n",
      "Epoch: 116560 | Loss: 0.41810342669487 | Test loss: 0.42605501413345337\n",
      "Epoch: 116570 | Loss: 0.41809168457984924 | Test loss: 0.4260411262512207\n",
      "Epoch: 116580 | Loss: 0.4180799126625061 | Test loss: 0.4260272979736328\n",
      "Epoch: 116590 | Loss: 0.41806814074516296 | Test loss: 0.4260134696960449\n",
      "Epoch: 116600 | Loss: 0.4180563986301422 | Test loss: 0.42599961161613464\n",
      "Epoch: 116610 | Loss: 0.4180446267127991 | Test loss: 0.42598578333854675\n",
      "Epoch: 116620 | Loss: 0.41803285479545593 | Test loss: 0.4259718954563141\n",
      "Epoch: 116630 | Loss: 0.4180211126804352 | Test loss: 0.4259580671787262\n",
      "Epoch: 116640 | Loss: 0.41800934076309204 | Test loss: 0.4259442389011383\n",
      "Epoch: 116650 | Loss: 0.4179975688457489 | Test loss: 0.42593035101890564\n",
      "Epoch: 116660 | Loss: 0.41798582673072815 | Test loss: 0.42591652274131775\n",
      "Epoch: 116670 | Loss: 0.417974054813385 | Test loss: 0.42590266466140747\n",
      "Epoch: 116680 | Loss: 0.41796228289604187 | Test loss: 0.4258888363838196\n",
      "Epoch: 116690 | Loss: 0.4179505407810211 | Test loss: 0.4258750081062317\n",
      "Epoch: 116700 | Loss: 0.41793879866600037 | Test loss: 0.425861120223999\n",
      "Epoch: 116710 | Loss: 0.41792699694633484 | Test loss: 0.42584729194641113\n",
      "Epoch: 116720 | Loss: 0.4179152548313141 | Test loss: 0.42583346366882324\n",
      "Epoch: 116730 | Loss: 0.41790351271629333 | Test loss: 0.42581960558891296\n",
      "Epoch: 116740 | Loss: 0.4178917407989502 | Test loss: 0.4258057773113251\n",
      "Epoch: 116750 | Loss: 0.41787996888160706 | Test loss: 0.4257919490337372\n",
      "Epoch: 116760 | Loss: 0.4178682267665863 | Test loss: 0.4257780611515045\n",
      "Epoch: 116770 | Loss: 0.41785645484924316 | Test loss: 0.4257642328739166\n",
      "Epoch: 116780 | Loss: 0.4178446829319 | Test loss: 0.42575034499168396\n",
      "Epoch: 116790 | Loss: 0.4178329110145569 | Test loss: 0.42573651671409607\n",
      "Epoch: 116800 | Loss: 0.41782116889953613 | Test loss: 0.4257226884365082\n",
      "Epoch: 116810 | Loss: 0.41780945658683777 | Test loss: 0.4257088601589203\n",
      "Epoch: 116820 | Loss: 0.41779765486717224 | Test loss: 0.42569500207901\n",
      "Epoch: 116830 | Loss: 0.4177859425544739 | Test loss: 0.42568111419677734\n",
      "Epoch: 116840 | Loss: 0.41777417063713074 | Test loss: 0.42566728591918945\n",
      "Epoch: 116850 | Loss: 0.4177623689174652 | Test loss: 0.42565345764160156\n",
      "Epoch: 116860 | Loss: 0.41775065660476685 | Test loss: 0.4256395995616913\n",
      "Epoch: 116870 | Loss: 0.41773882508277893 | Test loss: 0.4256257712841034\n",
      "Epoch: 116880 | Loss: 0.4177270829677582 | Test loss: 0.4256118834018707\n",
      "Epoch: 116890 | Loss: 0.41771531105041504 | Test loss: 0.42559805512428284\n",
      "Epoch: 116900 | Loss: 0.4177035391330719 | Test loss: 0.42558422684669495\n",
      "Epoch: 116910 | Loss: 0.41769179701805115 | Test loss: 0.42557039856910706\n",
      "Epoch: 116920 | Loss: 0.4176800847053528 | Test loss: 0.4255565106868744\n",
      "Epoch: 116930 | Loss: 0.41766831278800964 | Test loss: 0.4255426824092865\n",
      "Epoch: 116940 | Loss: 0.4176565110683441 | Test loss: 0.4255288243293762\n",
      "Epoch: 116950 | Loss: 0.41764479875564575 | Test loss: 0.42551499605178833\n",
      "Epoch: 116960 | Loss: 0.4176330268383026 | Test loss: 0.42550116777420044\n",
      "Epoch: 116970 | Loss: 0.4176212251186371 | Test loss: 0.42548733949661255\n",
      "Epoch: 116980 | Loss: 0.4176095128059387 | Test loss: 0.4254734516143799\n",
      "Epoch: 116990 | Loss: 0.4175977408885956 | Test loss: 0.425459623336792\n",
      "Epoch: 117000 | Loss: 0.41758593916893005 | Test loss: 0.4254457652568817\n",
      "Epoch: 117010 | Loss: 0.4175742268562317 | Test loss: 0.4254319369792938\n",
      "Epoch: 117020 | Loss: 0.41756245493888855 | Test loss: 0.42541804909706116\n",
      "Epoch: 117030 | Loss: 0.4175507128238678 | Test loss: 0.42540422081947327\n",
      "Epoch: 117040 | Loss: 0.41753894090652466 | Test loss: 0.4253903329372406\n",
      "Epoch: 117050 | Loss: 0.4175271987915039 | Test loss: 0.4253765046596527\n",
      "Epoch: 117060 | Loss: 0.417515367269516 | Test loss: 0.4253626763820648\n",
      "Epoch: 117070 | Loss: 0.4175036549568176 | Test loss: 0.42534884810447693\n",
      "Epoch: 117080 | Loss: 0.4174919128417969 | Test loss: 0.42533499002456665\n",
      "Epoch: 117090 | Loss: 0.41748014092445374 | Test loss: 0.42532116174697876\n",
      "Epoch: 117100 | Loss: 0.4174683690071106 | Test loss: 0.4253072738647461\n",
      "Epoch: 117110 | Loss: 0.41745662689208984 | Test loss: 0.4252934455871582\n",
      "Epoch: 117120 | Loss: 0.4174448549747467 | Test loss: 0.4252796173095703\n",
      "Epoch: 117130 | Loss: 0.41743308305740356 | Test loss: 0.4252657890319824\n",
      "Epoch: 117140 | Loss: 0.4174213409423828 | Test loss: 0.42525193095207214\n",
      "Epoch: 117150 | Loss: 0.41740962862968445 | Test loss: 0.42523810267448425\n",
      "Epoch: 117160 | Loss: 0.41739779710769653 | Test loss: 0.4252242147922516\n",
      "Epoch: 117170 | Loss: 0.4173860549926758 | Test loss: 0.4252103865146637\n",
      "Epoch: 117180 | Loss: 0.4173743426799774 | Test loss: 0.4251965582370758\n",
      "Epoch: 117190 | Loss: 0.4173625111579895 | Test loss: 0.42518267035484314\n",
      "Epoch: 117200 | Loss: 0.41735076904296875 | Test loss: 0.42516884207725525\n",
      "Epoch: 117210 | Loss: 0.4173390567302704 | Test loss: 0.42515498399734497\n",
      "Epoch: 117220 | Loss: 0.41732722520828247 | Test loss: 0.4251411557197571\n",
      "Epoch: 117230 | Loss: 0.4173154830932617 | Test loss: 0.4251273274421692\n",
      "Epoch: 117240 | Loss: 0.41730377078056335 | Test loss: 0.4251134395599365\n",
      "Epoch: 117250 | Loss: 0.4172919690608978 | Test loss: 0.42509961128234863\n",
      "Epoch: 117260 | Loss: 0.4172801971435547 | Test loss: 0.42508575320243835\n",
      "Epoch: 117270 | Loss: 0.4172684848308563 | Test loss: 0.42507192492485046\n",
      "Epoch: 117280 | Loss: 0.41725674271583557 | Test loss: 0.4250580966472626\n",
      "Epoch: 117290 | Loss: 0.41724491119384766 | Test loss: 0.4250442087650299\n",
      "Epoch: 117300 | Loss: 0.4172331988811493 | Test loss: 0.425030380487442\n",
      "Epoch: 117310 | Loss: 0.41722139716148376 | Test loss: 0.42501649260520935\n",
      "Epoch: 117320 | Loss: 0.4172096252441406 | Test loss: 0.42500266432762146\n",
      "Epoch: 117330 | Loss: 0.41719791293144226 | Test loss: 0.42498883605003357\n",
      "Epoch: 117340 | Loss: 0.41718611121177673 | Test loss: 0.4249750077724457\n",
      "Epoch: 117350 | Loss: 0.4171743392944336 | Test loss: 0.4249611496925354\n",
      "Epoch: 117360 | Loss: 0.41716262698173523 | Test loss: 0.4249473214149475\n",
      "Epoch: 117370 | Loss: 0.4171508848667145 | Test loss: 0.42493343353271484\n",
      "Epoch: 117380 | Loss: 0.41713911294937134 | Test loss: 0.42491960525512695\n",
      "Epoch: 117390 | Loss: 0.4171273410320282 | Test loss: 0.42490577697753906\n",
      "Epoch: 117400 | Loss: 0.41711559891700745 | Test loss: 0.4248919188976288\n",
      "Epoch: 117410 | Loss: 0.4171038269996643 | Test loss: 0.4248780906200409\n",
      "Epoch: 117420 | Loss: 0.41709205508232117 | Test loss: 0.4248642027378082\n",
      "Epoch: 117430 | Loss: 0.4170803129673004 | Test loss: 0.42485037446022034\n",
      "Epoch: 117440 | Loss: 0.4170685410499573 | Test loss: 0.42483654618263245\n",
      "Epoch: 117450 | Loss: 0.41705676913261414 | Test loss: 0.4248226583003998\n",
      "Epoch: 117460 | Loss: 0.4170450270175934 | Test loss: 0.4248088300228119\n",
      "Epoch: 117470 | Loss: 0.41703325510025024 | Test loss: 0.4247949719429016\n",
      "Epoch: 117480 | Loss: 0.4170214831829071 | Test loss: 0.4247811436653137\n",
      "Epoch: 117490 | Loss: 0.41700974106788635 | Test loss: 0.42476731538772583\n",
      "Epoch: 117500 | Loss: 0.4169979989528656 | Test loss: 0.42475342750549316\n",
      "Epoch: 117510 | Loss: 0.4169861972332001 | Test loss: 0.4247395992279053\n",
      "Epoch: 117520 | Loss: 0.4169744551181793 | Test loss: 0.4247257709503174\n",
      "Epoch: 117530 | Loss: 0.41696271300315857 | Test loss: 0.4247119128704071\n",
      "Epoch: 117540 | Loss: 0.41695094108581543 | Test loss: 0.4246980845928192\n",
      "Epoch: 117550 | Loss: 0.4169391691684723 | Test loss: 0.4246842563152313\n",
      "Epoch: 117560 | Loss: 0.41692742705345154 | Test loss: 0.42467036843299866\n",
      "Epoch: 117570 | Loss: 0.4169156551361084 | Test loss: 0.42465654015541077\n",
      "Epoch: 117580 | Loss: 0.41690388321876526 | Test loss: 0.4246426522731781\n",
      "Epoch: 117590 | Loss: 0.4168921113014221 | Test loss: 0.4246288239955902\n",
      "Epoch: 117600 | Loss: 0.41688036918640137 | Test loss: 0.4246149957180023\n",
      "Epoch: 117610 | Loss: 0.416868656873703 | Test loss: 0.42460116744041443\n",
      "Epoch: 117620 | Loss: 0.4168568551540375 | Test loss: 0.42458730936050415\n",
      "Epoch: 117630 | Loss: 0.4168451428413391 | Test loss: 0.4245734214782715\n",
      "Epoch: 117640 | Loss: 0.41683337092399597 | Test loss: 0.4245595932006836\n",
      "Epoch: 117650 | Loss: 0.41682156920433044 | Test loss: 0.4245457649230957\n",
      "Epoch: 117660 | Loss: 0.4168098568916321 | Test loss: 0.4245319068431854\n",
      "Epoch: 117670 | Loss: 0.41679802536964417 | Test loss: 0.42451807856559753\n",
      "Epoch: 117680 | Loss: 0.4167862832546234 | Test loss: 0.42450419068336487\n",
      "Epoch: 117690 | Loss: 0.4167745113372803 | Test loss: 0.424490362405777\n",
      "Epoch: 117700 | Loss: 0.41676273941993713 | Test loss: 0.4244765341281891\n",
      "Epoch: 117710 | Loss: 0.4167509973049164 | Test loss: 0.4244627058506012\n",
      "Epoch: 117720 | Loss: 0.416739284992218 | Test loss: 0.42444881796836853\n",
      "Epoch: 117730 | Loss: 0.4167275130748749 | Test loss: 0.42443498969078064\n",
      "Epoch: 117740 | Loss: 0.41671571135520935 | Test loss: 0.42442113161087036\n",
      "Epoch: 117750 | Loss: 0.416703999042511 | Test loss: 0.42440730333328247\n",
      "Epoch: 117760 | Loss: 0.41669222712516785 | Test loss: 0.4243934750556946\n",
      "Epoch: 117770 | Loss: 0.4166804254055023 | Test loss: 0.4243796467781067\n",
      "Epoch: 117780 | Loss: 0.41666871309280396 | Test loss: 0.424365758895874\n",
      "Epoch: 117790 | Loss: 0.4166569411754608 | Test loss: 0.42435193061828613\n",
      "Epoch: 117800 | Loss: 0.4166451394557953 | Test loss: 0.42433807253837585\n",
      "Epoch: 117810 | Loss: 0.4166334271430969 | Test loss: 0.42432424426078796\n",
      "Epoch: 117820 | Loss: 0.4166216552257538 | Test loss: 0.4243103563785553\n",
      "Epoch: 117830 | Loss: 0.41660991311073303 | Test loss: 0.4242965281009674\n",
      "Epoch: 117840 | Loss: 0.4165981411933899 | Test loss: 0.42428264021873474\n",
      "Epoch: 117850 | Loss: 0.41658639907836914 | Test loss: 0.42426881194114685\n",
      "Epoch: 117860 | Loss: 0.4165745675563812 | Test loss: 0.42425498366355896\n",
      "Epoch: 117870 | Loss: 0.41656285524368286 | Test loss: 0.42424115538597107\n",
      "Epoch: 117880 | Loss: 0.4165511131286621 | Test loss: 0.4242272973060608\n",
      "Epoch: 117890 | Loss: 0.41653934121131897 | Test loss: 0.4242134690284729\n",
      "Epoch: 117900 | Loss: 0.41652756929397583 | Test loss: 0.42419958114624023\n",
      "Epoch: 117910 | Loss: 0.4165158271789551 | Test loss: 0.42418575286865234\n",
      "Epoch: 117920 | Loss: 0.41650405526161194 | Test loss: 0.42417192459106445\n",
      "Epoch: 117930 | Loss: 0.4164922833442688 | Test loss: 0.42415809631347656\n",
      "Epoch: 117940 | Loss: 0.41648054122924805 | Test loss: 0.4241442382335663\n",
      "Epoch: 117950 | Loss: 0.4164688289165497 | Test loss: 0.4241304099559784\n",
      "Epoch: 117960 | Loss: 0.41645699739456177 | Test loss: 0.4241165220737457\n",
      "Epoch: 117970 | Loss: 0.416445255279541 | Test loss: 0.42410269379615784\n",
      "Epoch: 117980 | Loss: 0.41643354296684265 | Test loss: 0.42408886551856995\n",
      "Epoch: 117990 | Loss: 0.41642171144485474 | Test loss: 0.4240749776363373\n",
      "Epoch: 118000 | Loss: 0.416409969329834 | Test loss: 0.4240611493587494\n",
      "Epoch: 118010 | Loss: 0.4163982570171356 | Test loss: 0.4240472912788391\n",
      "Epoch: 118020 | Loss: 0.4163864254951477 | Test loss: 0.4240334630012512\n",
      "Epoch: 118030 | Loss: 0.41637468338012695 | Test loss: 0.42401963472366333\n",
      "Epoch: 118040 | Loss: 0.4163629710674286 | Test loss: 0.42400574684143066\n",
      "Epoch: 118050 | Loss: 0.41635116934776306 | Test loss: 0.4239919185638428\n",
      "Epoch: 118060 | Loss: 0.4163393974304199 | Test loss: 0.4239780604839325\n",
      "Epoch: 118070 | Loss: 0.41632768511772156 | Test loss: 0.4239642322063446\n",
      "Epoch: 118080 | Loss: 0.4163159430027008 | Test loss: 0.4239504039287567\n",
      "Epoch: 118090 | Loss: 0.4163041114807129 | Test loss: 0.42393651604652405\n",
      "Epoch: 118100 | Loss: 0.4162923991680145 | Test loss: 0.42392268776893616\n",
      "Epoch: 118110 | Loss: 0.416280597448349 | Test loss: 0.4239087998867035\n",
      "Epoch: 118120 | Loss: 0.41626882553100586 | Test loss: 0.4238949716091156\n",
      "Epoch: 118130 | Loss: 0.4162571132183075 | Test loss: 0.4238811433315277\n",
      "Epoch: 118140 | Loss: 0.41624531149864197 | Test loss: 0.4238673150539398\n",
      "Epoch: 118150 | Loss: 0.41623353958129883 | Test loss: 0.42385345697402954\n",
      "Epoch: 118160 | Loss: 0.41622182726860046 | Test loss: 0.42383962869644165\n",
      "Epoch: 118170 | Loss: 0.4162100851535797 | Test loss: 0.423825740814209\n",
      "Epoch: 118180 | Loss: 0.4161983132362366 | Test loss: 0.4238119125366211\n",
      "Epoch: 118190 | Loss: 0.41618654131889343 | Test loss: 0.4237980842590332\n",
      "Epoch: 118200 | Loss: 0.4161747992038727 | Test loss: 0.4237842261791229\n",
      "Epoch: 118210 | Loss: 0.41616302728652954 | Test loss: 0.42377039790153503\n",
      "Epoch: 118220 | Loss: 0.4161512553691864 | Test loss: 0.42375651001930237\n",
      "Epoch: 118230 | Loss: 0.41613951325416565 | Test loss: 0.4237426817417145\n",
      "Epoch: 118240 | Loss: 0.4161277413368225 | Test loss: 0.4237288534641266\n",
      "Epoch: 118250 | Loss: 0.41611596941947937 | Test loss: 0.4237149655818939\n",
      "Epoch: 118260 | Loss: 0.4161042273044586 | Test loss: 0.42370113730430603\n",
      "Epoch: 118270 | Loss: 0.4160924553871155 | Test loss: 0.42368727922439575\n",
      "Epoch: 118280 | Loss: 0.41608068346977234 | Test loss: 0.42367345094680786\n",
      "Epoch: 118290 | Loss: 0.4160689413547516 | Test loss: 0.42365962266921997\n",
      "Epoch: 118300 | Loss: 0.41605719923973083 | Test loss: 0.4236457347869873\n",
      "Epoch: 118310 | Loss: 0.4160453975200653 | Test loss: 0.4236319065093994\n",
      "Epoch: 118320 | Loss: 0.41603365540504456 | Test loss: 0.4236180782318115\n",
      "Epoch: 118330 | Loss: 0.4160219132900238 | Test loss: 0.42360422015190125\n",
      "Epoch: 118340 | Loss: 0.41601014137268066 | Test loss: 0.42359039187431335\n",
      "Epoch: 118350 | Loss: 0.4159983694553375 | Test loss: 0.42357656359672546\n",
      "Epoch: 118360 | Loss: 0.4159866273403168 | Test loss: 0.4235626757144928\n",
      "Epoch: 118370 | Loss: 0.41597485542297363 | Test loss: 0.4235488474369049\n",
      "Epoch: 118380 | Loss: 0.4159630835056305 | Test loss: 0.42353495955467224\n",
      "Epoch: 118390 | Loss: 0.41595131158828735 | Test loss: 0.42352113127708435\n",
      "Epoch: 118400 | Loss: 0.4159395694732666 | Test loss: 0.42350730299949646\n",
      "Epoch: 118410 | Loss: 0.41592785716056824 | Test loss: 0.42349347472190857\n",
      "Epoch: 118420 | Loss: 0.4159160554409027 | Test loss: 0.4234796166419983\n",
      "Epoch: 118430 | Loss: 0.41590434312820435 | Test loss: 0.4234657287597656\n",
      "Epoch: 118440 | Loss: 0.4158925712108612 | Test loss: 0.42345190048217773\n",
      "Epoch: 118450 | Loss: 0.4158807694911957 | Test loss: 0.42343807220458984\n",
      "Epoch: 118460 | Loss: 0.4158690571784973 | Test loss: 0.42342421412467957\n",
      "Epoch: 118470 | Loss: 0.4158572256565094 | Test loss: 0.4234103858470917\n",
      "Epoch: 118480 | Loss: 0.41584548354148865 | Test loss: 0.423396497964859\n",
      "Epoch: 118490 | Loss: 0.4158337116241455 | Test loss: 0.4233826696872711\n",
      "Epoch: 118500 | Loss: 0.41582193970680237 | Test loss: 0.4233688414096832\n",
      "Epoch: 118510 | Loss: 0.4158101975917816 | Test loss: 0.42335501313209534\n",
      "Epoch: 118520 | Loss: 0.41579848527908325 | Test loss: 0.42334112524986267\n",
      "Epoch: 118530 | Loss: 0.4157867133617401 | Test loss: 0.4233272969722748\n",
      "Epoch: 118540 | Loss: 0.4157749116420746 | Test loss: 0.4233134388923645\n",
      "Epoch: 118550 | Loss: 0.4157631993293762 | Test loss: 0.4232996106147766\n",
      "Epoch: 118560 | Loss: 0.4157514274120331 | Test loss: 0.4232857823371887\n",
      "Epoch: 118570 | Loss: 0.41573962569236755 | Test loss: 0.42327195405960083\n",
      "Epoch: 118580 | Loss: 0.4157279133796692 | Test loss: 0.42325806617736816\n",
      "Epoch: 118590 | Loss: 0.41571614146232605 | Test loss: 0.4232442378997803\n",
      "Epoch: 118600 | Loss: 0.4157043397426605 | Test loss: 0.42323037981987\n",
      "Epoch: 118610 | Loss: 0.41569262742996216 | Test loss: 0.4232165515422821\n",
      "Epoch: 118620 | Loss: 0.415680855512619 | Test loss: 0.42320266366004944\n",
      "Epoch: 118630 | Loss: 0.41566911339759827 | Test loss: 0.42318883538246155\n",
      "Epoch: 118640 | Loss: 0.4156573414802551 | Test loss: 0.4231749475002289\n",
      "Epoch: 118650 | Loss: 0.4156455993652344 | Test loss: 0.423161119222641\n",
      "Epoch: 118660 | Loss: 0.41563376784324646 | Test loss: 0.4231472909450531\n",
      "Epoch: 118670 | Loss: 0.4156220555305481 | Test loss: 0.4231334626674652\n",
      "Epoch: 118680 | Loss: 0.41561031341552734 | Test loss: 0.42311960458755493\n",
      "Epoch: 118690 | Loss: 0.4155985414981842 | Test loss: 0.42310577630996704\n",
      "Epoch: 118700 | Loss: 0.41558676958084106 | Test loss: 0.4230918884277344\n",
      "Epoch: 118710 | Loss: 0.4155750274658203 | Test loss: 0.4230780601501465\n",
      "Epoch: 118720 | Loss: 0.4155632555484772 | Test loss: 0.4230642318725586\n",
      "Epoch: 118730 | Loss: 0.41555148363113403 | Test loss: 0.4230504035949707\n",
      "Epoch: 118740 | Loss: 0.4155397415161133 | Test loss: 0.4230365455150604\n",
      "Epoch: 118750 | Loss: 0.4155280292034149 | Test loss: 0.42302271723747253\n",
      "Epoch: 118760 | Loss: 0.415516197681427 | Test loss: 0.42300882935523987\n",
      "Epoch: 118770 | Loss: 0.41550445556640625 | Test loss: 0.422995001077652\n",
      "Epoch: 118780 | Loss: 0.4154927432537079 | Test loss: 0.4229811728000641\n",
      "Epoch: 118790 | Loss: 0.41548091173171997 | Test loss: 0.4229672849178314\n",
      "Epoch: 118800 | Loss: 0.4154691696166992 | Test loss: 0.42295345664024353\n",
      "Epoch: 118810 | Loss: 0.41545745730400085 | Test loss: 0.42293959856033325\n",
      "Epoch: 118820 | Loss: 0.41544562578201294 | Test loss: 0.42292577028274536\n",
      "Epoch: 118830 | Loss: 0.4154338836669922 | Test loss: 0.42291194200515747\n",
      "Epoch: 118840 | Loss: 0.4154221713542938 | Test loss: 0.4228980541229248\n",
      "Epoch: 118850 | Loss: 0.4154103696346283 | Test loss: 0.4228842258453369\n",
      "Epoch: 118860 | Loss: 0.41539859771728516 | Test loss: 0.42287036776542664\n",
      "Epoch: 118870 | Loss: 0.4153868854045868 | Test loss: 0.42285653948783875\n",
      "Epoch: 118880 | Loss: 0.41537514328956604 | Test loss: 0.42284271121025085\n",
      "Epoch: 118890 | Loss: 0.4153633117675781 | Test loss: 0.4228288233280182\n",
      "Epoch: 118900 | Loss: 0.41535159945487976 | Test loss: 0.4228149950504303\n",
      "Epoch: 118910 | Loss: 0.41533979773521423 | Test loss: 0.42280110716819763\n",
      "Epoch: 118920 | Loss: 0.4153280258178711 | Test loss: 0.42278727889060974\n",
      "Epoch: 118930 | Loss: 0.41531631350517273 | Test loss: 0.42277345061302185\n",
      "Epoch: 118940 | Loss: 0.4153045117855072 | Test loss: 0.42275962233543396\n",
      "Epoch: 118950 | Loss: 0.41529273986816406 | Test loss: 0.4227457642555237\n",
      "Epoch: 118960 | Loss: 0.4152810275554657 | Test loss: 0.4227319359779358\n",
      "Epoch: 118970 | Loss: 0.41526928544044495 | Test loss: 0.4227180480957031\n",
      "Epoch: 118980 | Loss: 0.4152575135231018 | Test loss: 0.42270421981811523\n",
      "Epoch: 118990 | Loss: 0.41524574160575867 | Test loss: 0.42269039154052734\n",
      "Epoch: 119000 | Loss: 0.4152339994907379 | Test loss: 0.42267653346061707\n",
      "Epoch: 119010 | Loss: 0.4152222275733948 | Test loss: 0.4226627051830292\n",
      "Epoch: 119020 | Loss: 0.41521045565605164 | Test loss: 0.4226488173007965\n",
      "Epoch: 119030 | Loss: 0.4151987135410309 | Test loss: 0.4226349890232086\n",
      "Epoch: 119040 | Loss: 0.41518694162368774 | Test loss: 0.4226211607456207\n",
      "Epoch: 119050 | Loss: 0.4151751697063446 | Test loss: 0.42260727286338806\n",
      "Epoch: 119060 | Loss: 0.41516342759132385 | Test loss: 0.42259344458580017\n",
      "Epoch: 119070 | Loss: 0.4151516556739807 | Test loss: 0.4225795865058899\n",
      "Epoch: 119080 | Loss: 0.4151398837566376 | Test loss: 0.422565758228302\n",
      "Epoch: 119090 | Loss: 0.4151281416416168 | Test loss: 0.4225519299507141\n",
      "Epoch: 119100 | Loss: 0.41511639952659607 | Test loss: 0.42253804206848145\n",
      "Epoch: 119110 | Loss: 0.41510459780693054 | Test loss: 0.42252421379089355\n",
      "Epoch: 119120 | Loss: 0.4150928556919098 | Test loss: 0.42251038551330566\n",
      "Epoch: 119130 | Loss: 0.41508111357688904 | Test loss: 0.4224965274333954\n",
      "Epoch: 119140 | Loss: 0.4150693416595459 | Test loss: 0.4224826991558075\n",
      "Epoch: 119150 | Loss: 0.41505756974220276 | Test loss: 0.4224688708782196\n",
      "Epoch: 119160 | Loss: 0.415045827627182 | Test loss: 0.42245498299598694\n",
      "Epoch: 119170 | Loss: 0.41503405570983887 | Test loss: 0.42244115471839905\n",
      "Epoch: 119180 | Loss: 0.4150222837924957 | Test loss: 0.4224272668361664\n",
      "Epoch: 119190 | Loss: 0.4150105118751526 | Test loss: 0.4224134385585785\n",
      "Epoch: 119200 | Loss: 0.41499876976013184 | Test loss: 0.4223996102809906\n",
      "Epoch: 119210 | Loss: 0.41498705744743347 | Test loss: 0.4223857820034027\n",
      "Epoch: 119220 | Loss: 0.41497525572776794 | Test loss: 0.42237192392349243\n",
      "Epoch: 119230 | Loss: 0.4149635434150696 | Test loss: 0.42235803604125977\n",
      "Epoch: 119240 | Loss: 0.41495177149772644 | Test loss: 0.4223442077636719\n",
      "Epoch: 119250 | Loss: 0.4149399697780609 | Test loss: 0.422330379486084\n",
      "Epoch: 119260 | Loss: 0.41492825746536255 | Test loss: 0.4223165214061737\n",
      "Epoch: 119270 | Loss: 0.41491642594337463 | Test loss: 0.4223026931285858\n",
      "Epoch: 119280 | Loss: 0.4149046838283539 | Test loss: 0.42228880524635315\n",
      "Epoch: 119290 | Loss: 0.41489291191101074 | Test loss: 0.42227497696876526\n",
      "Epoch: 119300 | Loss: 0.4148811399936676 | Test loss: 0.42226114869117737\n",
      "Epoch: 119310 | Loss: 0.41486939787864685 | Test loss: 0.4222473204135895\n",
      "Epoch: 119320 | Loss: 0.4148576855659485 | Test loss: 0.4222334325313568\n",
      "Epoch: 119330 | Loss: 0.41484591364860535 | Test loss: 0.4222196042537689\n",
      "Epoch: 119340 | Loss: 0.4148341119289398 | Test loss: 0.42220574617385864\n",
      "Epoch: 119350 | Loss: 0.41482239961624146 | Test loss: 0.42219191789627075\n",
      "Epoch: 119360 | Loss: 0.4148106276988983 | Test loss: 0.42217808961868286\n",
      "Epoch: 119370 | Loss: 0.4147988259792328 | Test loss: 0.42216426134109497\n",
      "Epoch: 119380 | Loss: 0.4147871136665344 | Test loss: 0.4221503734588623\n",
      "Epoch: 119390 | Loss: 0.4147753417491913 | Test loss: 0.4221365451812744\n",
      "Epoch: 119400 | Loss: 0.41476354002952576 | Test loss: 0.42212268710136414\n",
      "Epoch: 119410 | Loss: 0.4147518277168274 | Test loss: 0.42210885882377625\n",
      "Epoch: 119420 | Loss: 0.41474005579948425 | Test loss: 0.4220949709415436\n",
      "Epoch: 119430 | Loss: 0.4147283136844635 | Test loss: 0.4220811426639557\n",
      "Epoch: 119440 | Loss: 0.41471654176712036 | Test loss: 0.422067254781723\n",
      "Epoch: 119450 | Loss: 0.4147047996520996 | Test loss: 0.42205342650413513\n",
      "Epoch: 119460 | Loss: 0.4146929681301117 | Test loss: 0.42203959822654724\n",
      "Epoch: 119470 | Loss: 0.41468125581741333 | Test loss: 0.42202576994895935\n",
      "Epoch: 119480 | Loss: 0.4146695137023926 | Test loss: 0.4220119118690491\n",
      "Epoch: 119490 | Loss: 0.41465774178504944 | Test loss: 0.4219980835914612\n",
      "Epoch: 119500 | Loss: 0.4146459698677063 | Test loss: 0.4219841957092285\n",
      "Epoch: 119510 | Loss: 0.41463422775268555 | Test loss: 0.4219703674316406\n",
      "Epoch: 119520 | Loss: 0.4146224558353424 | Test loss: 0.42195653915405273\n",
      "Epoch: 119530 | Loss: 0.41461068391799927 | Test loss: 0.42194271087646484\n",
      "Epoch: 119540 | Loss: 0.4145989418029785 | Test loss: 0.42192885279655457\n",
      "Epoch: 119550 | Loss: 0.41458722949028015 | Test loss: 0.4219150245189667\n",
      "Epoch: 119560 | Loss: 0.41457539796829224 | Test loss: 0.421901136636734\n",
      "Epoch: 119570 | Loss: 0.4145636558532715 | Test loss: 0.4218873083591461\n",
      "Epoch: 119580 | Loss: 0.4145519435405731 | Test loss: 0.4218734800815582\n",
      "Epoch: 119590 | Loss: 0.4145401120185852 | Test loss: 0.42185959219932556\n",
      "Epoch: 119600 | Loss: 0.41452836990356445 | Test loss: 0.42184576392173767\n",
      "Epoch: 119610 | Loss: 0.4145166575908661 | Test loss: 0.4218319058418274\n",
      "Epoch: 119620 | Loss: 0.4145048260688782 | Test loss: 0.4218180775642395\n",
      "Epoch: 119630 | Loss: 0.4144930839538574 | Test loss: 0.4218042492866516\n",
      "Epoch: 119640 | Loss: 0.41448137164115906 | Test loss: 0.42179036140441895\n",
      "Epoch: 119650 | Loss: 0.41446956992149353 | Test loss: 0.42177653312683105\n",
      "Epoch: 119660 | Loss: 0.4144577980041504 | Test loss: 0.4217626750469208\n",
      "Epoch: 119670 | Loss: 0.414446085691452 | Test loss: 0.4217488467693329\n",
      "Epoch: 119680 | Loss: 0.4144343435764313 | Test loss: 0.421735018491745\n",
      "Epoch: 119690 | Loss: 0.41442251205444336 | Test loss: 0.42172113060951233\n",
      "Epoch: 119700 | Loss: 0.414410799741745 | Test loss: 0.42170730233192444\n",
      "Epoch: 119710 | Loss: 0.41439899802207947 | Test loss: 0.4216934144496918\n",
      "Epoch: 119720 | Loss: 0.41438722610473633 | Test loss: 0.4216795861721039\n",
      "Epoch: 119730 | Loss: 0.41437551379203796 | Test loss: 0.421665757894516\n",
      "Epoch: 119740 | Loss: 0.41436371207237244 | Test loss: 0.4216519296169281\n",
      "Epoch: 119750 | Loss: 0.4143519401550293 | Test loss: 0.4216380715370178\n",
      "Epoch: 119760 | Loss: 0.41434022784233093 | Test loss: 0.42162424325942993\n",
      "Epoch: 119770 | Loss: 0.4143284857273102 | Test loss: 0.42161035537719727\n",
      "Epoch: 119780 | Loss: 0.41431671380996704 | Test loss: 0.4215965270996094\n",
      "Epoch: 119790 | Loss: 0.4143049418926239 | Test loss: 0.4215826988220215\n",
      "Epoch: 119800 | Loss: 0.41429319977760315 | Test loss: 0.4215688407421112\n",
      "Epoch: 119810 | Loss: 0.41428142786026 | Test loss: 0.4215550124645233\n",
      "Epoch: 119820 | Loss: 0.41426965594291687 | Test loss: 0.42154112458229065\n",
      "Epoch: 119830 | Loss: 0.4142579138278961 | Test loss: 0.42152729630470276\n",
      "Epoch: 119840 | Loss: 0.414246141910553 | Test loss: 0.42151346802711487\n",
      "Epoch: 119850 | Loss: 0.41423436999320984 | Test loss: 0.4214995801448822\n",
      "Epoch: 119860 | Loss: 0.4142226278781891 | Test loss: 0.4214857518672943\n",
      "Epoch: 119870 | Loss: 0.41421085596084595 | Test loss: 0.42147189378738403\n",
      "Epoch: 119880 | Loss: 0.4141990840435028 | Test loss: 0.42145806550979614\n",
      "Epoch: 119890 | Loss: 0.41418734192848206 | Test loss: 0.42144423723220825\n",
      "Epoch: 119900 | Loss: 0.4141755998134613 | Test loss: 0.4214303493499756\n",
      "Epoch: 119910 | Loss: 0.4141637980937958 | Test loss: 0.4214165210723877\n",
      "Epoch: 119920 | Loss: 0.414152055978775 | Test loss: 0.4214026927947998\n",
      "Epoch: 119930 | Loss: 0.4141403138637543 | Test loss: 0.4213888347148895\n",
      "Epoch: 119940 | Loss: 0.41412854194641113 | Test loss: 0.42137500643730164\n",
      "Epoch: 119950 | Loss: 0.414116770029068 | Test loss: 0.42136117815971375\n",
      "Epoch: 119960 | Loss: 0.41410502791404724 | Test loss: 0.4213472902774811\n",
      "Epoch: 119970 | Loss: 0.4140932559967041 | Test loss: 0.4213334619998932\n",
      "Epoch: 119980 | Loss: 0.41408148407936096 | Test loss: 0.4213195741176605\n",
      "Epoch: 119990 | Loss: 0.4140697121620178 | Test loss: 0.42130574584007263\n",
      "Epoch: 120000 | Loss: 0.41405797004699707 | Test loss: 0.42129191756248474\n",
      "Epoch: 120010 | Loss: 0.4140462577342987 | Test loss: 0.42127808928489685\n",
      "Epoch: 120020 | Loss: 0.4140344560146332 | Test loss: 0.4212642312049866\n",
      "Epoch: 120030 | Loss: 0.4140227437019348 | Test loss: 0.4212503433227539\n",
      "Epoch: 120040 | Loss: 0.4140109717845917 | Test loss: 0.421236515045166\n",
      "Epoch: 120050 | Loss: 0.41399917006492615 | Test loss: 0.4212226867675781\n",
      "Epoch: 120060 | Loss: 0.4139874577522278 | Test loss: 0.42120882868766785\n",
      "Epoch: 120070 | Loss: 0.41397562623023987 | Test loss: 0.42119500041007996\n",
      "Epoch: 120080 | Loss: 0.4139638841152191 | Test loss: 0.4211811125278473\n",
      "Epoch: 120090 | Loss: 0.413952112197876 | Test loss: 0.4211672842502594\n",
      "Epoch: 120100 | Loss: 0.41394034028053284 | Test loss: 0.4211534559726715\n",
      "Epoch: 120110 | Loss: 0.4139285981655121 | Test loss: 0.4211396276950836\n",
      "Epoch: 120120 | Loss: 0.4139168858528137 | Test loss: 0.42112573981285095\n",
      "Epoch: 120130 | Loss: 0.4139051139354706 | Test loss: 0.42111191153526306\n",
      "Epoch: 120140 | Loss: 0.41389331221580505 | Test loss: 0.4210980534553528\n",
      "Epoch: 120150 | Loss: 0.4138815999031067 | Test loss: 0.4210842251777649\n",
      "Epoch: 120160 | Loss: 0.41386982798576355 | Test loss: 0.421070396900177\n",
      "Epoch: 120170 | Loss: 0.413858026266098 | Test loss: 0.4210565686225891\n",
      "Epoch: 120180 | Loss: 0.41384631395339966 | Test loss: 0.42104268074035645\n",
      "Epoch: 120190 | Loss: 0.4138345420360565 | Test loss: 0.42102885246276855\n",
      "Epoch: 120200 | Loss: 0.413822740316391 | Test loss: 0.4210149943828583\n",
      "Epoch: 120210 | Loss: 0.4138110280036926 | Test loss: 0.4210011661052704\n",
      "Epoch: 120220 | Loss: 0.4137992560863495 | Test loss: 0.4209872782230377\n",
      "Epoch: 120230 | Loss: 0.41378751397132874 | Test loss: 0.42097344994544983\n",
      "Epoch: 120240 | Loss: 0.4137757420539856 | Test loss: 0.42095956206321716\n",
      "Epoch: 120250 | Loss: 0.41376399993896484 | Test loss: 0.4209457337856293\n",
      "Epoch: 120260 | Loss: 0.41375216841697693 | Test loss: 0.4209319055080414\n",
      "Epoch: 120270 | Loss: 0.41374045610427856 | Test loss: 0.4209180772304535\n",
      "Epoch: 120280 | Loss: 0.4137287139892578 | Test loss: 0.4209042191505432\n",
      "Epoch: 120290 | Loss: 0.4137169420719147 | Test loss: 0.4208903908729553\n",
      "Epoch: 120300 | Loss: 0.41370517015457153 | Test loss: 0.42087650299072266\n",
      "Epoch: 120310 | Loss: 0.4136934280395508 | Test loss: 0.42086267471313477\n",
      "Epoch: 120320 | Loss: 0.41368165612220764 | Test loss: 0.4208488464355469\n",
      "Epoch: 120330 | Loss: 0.4136698842048645 | Test loss: 0.420835018157959\n",
      "Epoch: 120340 | Loss: 0.41365814208984375 | Test loss: 0.4208211600780487\n",
      "Epoch: 120350 | Loss: 0.4136464297771454 | Test loss: 0.4208073318004608\n",
      "Epoch: 120360 | Loss: 0.41363459825515747 | Test loss: 0.42079344391822815\n",
      "Epoch: 120370 | Loss: 0.4136228561401367 | Test loss: 0.42077961564064026\n",
      "Epoch: 120380 | Loss: 0.41361114382743835 | Test loss: 0.42076578736305237\n",
      "Epoch: 120390 | Loss: 0.41359931230545044 | Test loss: 0.4207518994808197\n",
      "Epoch: 120400 | Loss: 0.4135875701904297 | Test loss: 0.4207380712032318\n",
      "Epoch: 120410 | Loss: 0.4135758578777313 | Test loss: 0.42072421312332153\n",
      "Epoch: 120420 | Loss: 0.4135640263557434 | Test loss: 0.42071038484573364\n",
      "Epoch: 120430 | Loss: 0.41355228424072266 | Test loss: 0.42069655656814575\n",
      "Epoch: 120440 | Loss: 0.4135405719280243 | Test loss: 0.4206826686859131\n",
      "Epoch: 120450 | Loss: 0.41352877020835876 | Test loss: 0.4206688404083252\n",
      "Epoch: 120460 | Loss: 0.4135169982910156 | Test loss: 0.4206549823284149\n",
      "Epoch: 120470 | Loss: 0.41350528597831726 | Test loss: 0.420641154050827\n",
      "Epoch: 120480 | Loss: 0.4134935438632965 | Test loss: 0.42062732577323914\n",
      "Epoch: 120490 | Loss: 0.4134817123413086 | Test loss: 0.42061343789100647\n",
      "Epoch: 120500 | Loss: 0.41347000002861023 | Test loss: 0.4205996096134186\n",
      "Epoch: 120510 | Loss: 0.4134581983089447 | Test loss: 0.4205857217311859\n",
      "Epoch: 120520 | Loss: 0.41344642639160156 | Test loss: 0.420571893453598\n",
      "Epoch: 120530 | Loss: 0.4134347140789032 | Test loss: 0.42055806517601013\n",
      "Epoch: 120540 | Loss: 0.41342291235923767 | Test loss: 0.42054423689842224\n",
      "Epoch: 120550 | Loss: 0.41341114044189453 | Test loss: 0.42053037881851196\n",
      "Epoch: 120560 | Loss: 0.41339942812919617 | Test loss: 0.4205165505409241\n",
      "Epoch: 120570 | Loss: 0.4133876860141754 | Test loss: 0.4205026626586914\n",
      "Epoch: 120580 | Loss: 0.4133759140968323 | Test loss: 0.4204888343811035\n",
      "Epoch: 120590 | Loss: 0.41336414217948914 | Test loss: 0.4204750061035156\n",
      "Epoch: 120600 | Loss: 0.4133524000644684 | Test loss: 0.42046114802360535\n",
      "Epoch: 120610 | Loss: 0.41334062814712524 | Test loss: 0.42044731974601746\n",
      "Epoch: 120620 | Loss: 0.4133288562297821 | Test loss: 0.4204334318637848\n",
      "Epoch: 120630 | Loss: 0.41331711411476135 | Test loss: 0.4204196035861969\n",
      "Epoch: 120640 | Loss: 0.4133053421974182 | Test loss: 0.420405775308609\n",
      "Epoch: 120650 | Loss: 0.4132935702800751 | Test loss: 0.42039188742637634\n",
      "Epoch: 120660 | Loss: 0.4132818281650543 | Test loss: 0.42037805914878845\n",
      "Epoch: 120670 | Loss: 0.4132700562477112 | Test loss: 0.4203642010688782\n",
      "Epoch: 120680 | Loss: 0.41325828433036804 | Test loss: 0.4203503727912903\n",
      "Epoch: 120690 | Loss: 0.4132465422153473 | Test loss: 0.4203365445137024\n",
      "Epoch: 120700 | Loss: 0.41323480010032654 | Test loss: 0.4203226566314697\n",
      "Epoch: 120710 | Loss: 0.413222998380661 | Test loss: 0.42030882835388184\n",
      "Epoch: 120720 | Loss: 0.41321125626564026 | Test loss: 0.42029500007629395\n",
      "Epoch: 120730 | Loss: 0.4131995141506195 | Test loss: 0.42028114199638367\n",
      "Epoch: 120740 | Loss: 0.41318774223327637 | Test loss: 0.4202673137187958\n",
      "Epoch: 120750 | Loss: 0.4131759703159332 | Test loss: 0.4202534854412079\n",
      "Epoch: 120760 | Loss: 0.4131642282009125 | Test loss: 0.4202395975589752\n",
      "Epoch: 120770 | Loss: 0.41315245628356934 | Test loss: 0.42022576928138733\n",
      "Epoch: 120780 | Loss: 0.4131406843662262 | Test loss: 0.42021188139915466\n",
      "Epoch: 120790 | Loss: 0.41312891244888306 | Test loss: 0.4201980531215668\n",
      "Epoch: 120800 | Loss: 0.4131171703338623 | Test loss: 0.4201842248439789\n",
      "Epoch: 120810 | Loss: 0.41310545802116394 | Test loss: 0.420170396566391\n",
      "Epoch: 120820 | Loss: 0.4130936563014984 | Test loss: 0.4201565384864807\n",
      "Epoch: 120830 | Loss: 0.41308194398880005 | Test loss: 0.42014265060424805\n",
      "Epoch: 120840 | Loss: 0.4130701720714569 | Test loss: 0.42012882232666016\n",
      "Epoch: 120850 | Loss: 0.4130583703517914 | Test loss: 0.42011499404907227\n",
      "Epoch: 120860 | Loss: 0.413046658039093 | Test loss: 0.420101135969162\n",
      "Epoch: 120870 | Loss: 0.4130348265171051 | Test loss: 0.4200873076915741\n",
      "Epoch: 120880 | Loss: 0.41302308440208435 | Test loss: 0.42007341980934143\n",
      "Epoch: 120890 | Loss: 0.4130113124847412 | Test loss: 0.42005959153175354\n",
      "Epoch: 120900 | Loss: 0.41299954056739807 | Test loss: 0.42004576325416565\n",
      "Epoch: 120910 | Loss: 0.4129877984523773 | Test loss: 0.42003193497657776\n",
      "Epoch: 120920 | Loss: 0.41297608613967896 | Test loss: 0.4200180470943451\n",
      "Epoch: 120930 | Loss: 0.4129643142223358 | Test loss: 0.4200042188167572\n",
      "Epoch: 120940 | Loss: 0.4129525125026703 | Test loss: 0.4199903607368469\n",
      "Epoch: 120950 | Loss: 0.4129408001899719 | Test loss: 0.41997653245925903\n",
      "Epoch: 120960 | Loss: 0.4129290282726288 | Test loss: 0.41996270418167114\n",
      "Epoch: 120970 | Loss: 0.41291722655296326 | Test loss: 0.41994887590408325\n",
      "Epoch: 120980 | Loss: 0.4129055142402649 | Test loss: 0.4199349880218506\n",
      "Epoch: 120990 | Loss: 0.41289374232292175 | Test loss: 0.4199211597442627\n",
      "Epoch: 121000 | Loss: 0.4128819406032562 | Test loss: 0.4199073016643524\n",
      "Epoch: 121010 | Loss: 0.41287022829055786 | Test loss: 0.4198934733867645\n",
      "Epoch: 121020 | Loss: 0.4128584563732147 | Test loss: 0.41987958550453186\n",
      "Epoch: 121030 | Loss: 0.41284671425819397 | Test loss: 0.41986575722694397\n",
      "Epoch: 121040 | Loss: 0.41283494234085083 | Test loss: 0.4198518693447113\n",
      "Epoch: 121050 | Loss: 0.4128232002258301 | Test loss: 0.4198380410671234\n",
      "Epoch: 121060 | Loss: 0.41281136870384216 | Test loss: 0.4198242127895355\n",
      "Epoch: 121070 | Loss: 0.4127996563911438 | Test loss: 0.41981038451194763\n",
      "Epoch: 121080 | Loss: 0.41278791427612305 | Test loss: 0.41979652643203735\n",
      "Epoch: 121090 | Loss: 0.4127761423587799 | Test loss: 0.41978269815444946\n",
      "Epoch: 121100 | Loss: 0.41276437044143677 | Test loss: 0.4197688102722168\n",
      "Epoch: 121110 | Loss: 0.412752628326416 | Test loss: 0.4197549819946289\n",
      "Epoch: 121120 | Loss: 0.4127408564090729 | Test loss: 0.419741153717041\n",
      "Epoch: 121130 | Loss: 0.41272908449172974 | Test loss: 0.4197273254394531\n",
      "Epoch: 121140 | Loss: 0.412717342376709 | Test loss: 0.41971346735954285\n",
      "Epoch: 121150 | Loss: 0.4127056300640106 | Test loss: 0.41969963908195496\n",
      "Epoch: 121160 | Loss: 0.4126937985420227 | Test loss: 0.4196857511997223\n",
      "Epoch: 121170 | Loss: 0.41268205642700195 | Test loss: 0.4196719229221344\n",
      "Epoch: 121180 | Loss: 0.4126703441143036 | Test loss: 0.4196580946445465\n",
      "Epoch: 121190 | Loss: 0.4126585125923157 | Test loss: 0.41964420676231384\n",
      "Epoch: 121200 | Loss: 0.4126467704772949 | Test loss: 0.41963037848472595\n",
      "Epoch: 121210 | Loss: 0.41263505816459656 | Test loss: 0.4196165204048157\n",
      "Epoch: 121220 | Loss: 0.41262322664260864 | Test loss: 0.4196026921272278\n",
      "Epoch: 121230 | Loss: 0.4126114845275879 | Test loss: 0.4195888638496399\n",
      "Epoch: 121240 | Loss: 0.4125997722148895 | Test loss: 0.4195749759674072\n",
      "Epoch: 121250 | Loss: 0.412587970495224 | Test loss: 0.41956114768981934\n",
      "Epoch: 121260 | Loss: 0.41257619857788086 | Test loss: 0.41954728960990906\n",
      "Epoch: 121270 | Loss: 0.4125644862651825 | Test loss: 0.41953346133232117\n",
      "Epoch: 121280 | Loss: 0.41255274415016174 | Test loss: 0.4195196330547333\n",
      "Epoch: 121290 | Loss: 0.41254091262817383 | Test loss: 0.4195057451725006\n",
      "Epoch: 121300 | Loss: 0.41252920031547546 | Test loss: 0.4194919168949127\n",
      "Epoch: 121310 | Loss: 0.41251739859580994 | Test loss: 0.41947802901268005\n",
      "Epoch: 121320 | Loss: 0.4125056266784668 | Test loss: 0.41946420073509216\n",
      "Epoch: 121330 | Loss: 0.41249391436576843 | Test loss: 0.4194503724575043\n",
      "Epoch: 121340 | Loss: 0.4124821126461029 | Test loss: 0.4194365441799164\n",
      "Epoch: 121350 | Loss: 0.41247034072875977 | Test loss: 0.4194226861000061\n",
      "Epoch: 121360 | Loss: 0.4124586284160614 | Test loss: 0.4194088578224182\n",
      "Epoch: 121370 | Loss: 0.41244688630104065 | Test loss: 0.41939496994018555\n",
      "Epoch: 121380 | Loss: 0.4124351143836975 | Test loss: 0.41938114166259766\n",
      "Epoch: 121390 | Loss: 0.41242334246635437 | Test loss: 0.41936731338500977\n",
      "Epoch: 121400 | Loss: 0.4124116003513336 | Test loss: 0.4193534553050995\n",
      "Epoch: 121410 | Loss: 0.4123998284339905 | Test loss: 0.4193396270275116\n",
      "Epoch: 121420 | Loss: 0.41238805651664734 | Test loss: 0.41932573914527893\n",
      "Epoch: 121430 | Loss: 0.4123763144016266 | Test loss: 0.41931191086769104\n",
      "Epoch: 121440 | Loss: 0.41236454248428345 | Test loss: 0.41929808259010315\n",
      "Epoch: 121450 | Loss: 0.4123527705669403 | Test loss: 0.4192841947078705\n",
      "Epoch: 121460 | Loss: 0.41234102845191956 | Test loss: 0.4192703664302826\n",
      "Epoch: 121470 | Loss: 0.4123292565345764 | Test loss: 0.4192565083503723\n",
      "Epoch: 121480 | Loss: 0.4123174846172333 | Test loss: 0.4192426800727844\n",
      "Epoch: 121490 | Loss: 0.4123057425022125 | Test loss: 0.41922885179519653\n",
      "Epoch: 121500 | Loss: 0.4122940003871918 | Test loss: 0.41921496391296387\n",
      "Epoch: 121510 | Loss: 0.41228219866752625 | Test loss: 0.419201135635376\n",
      "Epoch: 121520 | Loss: 0.4122704565525055 | Test loss: 0.4191873073577881\n",
      "Epoch: 121530 | Loss: 0.41225871443748474 | Test loss: 0.4191734492778778\n",
      "Epoch: 121540 | Loss: 0.4122469425201416 | Test loss: 0.4191596210002899\n",
      "Epoch: 121550 | Loss: 0.41223517060279846 | Test loss: 0.419145792722702\n",
      "Epoch: 121560 | Loss: 0.4122234284877777 | Test loss: 0.41913190484046936\n",
      "Epoch: 121570 | Loss: 0.41221165657043457 | Test loss: 0.41911807656288147\n",
      "Epoch: 121580 | Loss: 0.41219988465309143 | Test loss: 0.4191041886806488\n",
      "Epoch: 121590 | Loss: 0.4121881127357483 | Test loss: 0.4190903604030609\n",
      "Epoch: 121600 | Loss: 0.41217637062072754 | Test loss: 0.419076532125473\n",
      "Epoch: 121610 | Loss: 0.4121646583080292 | Test loss: 0.41906270384788513\n",
      "Epoch: 121620 | Loss: 0.41215285658836365 | Test loss: 0.41904884576797485\n",
      "Epoch: 121630 | Loss: 0.4121411442756653 | Test loss: 0.4190349578857422\n",
      "Epoch: 121640 | Loss: 0.41212937235832214 | Test loss: 0.4190211296081543\n",
      "Epoch: 121650 | Loss: 0.4121175706386566 | Test loss: 0.4190073013305664\n",
      "Epoch: 121660 | Loss: 0.41210585832595825 | Test loss: 0.41899344325065613\n",
      "Epoch: 121670 | Loss: 0.41209402680397034 | Test loss: 0.41897961497306824\n",
      "Epoch: 121680 | Loss: 0.4120822846889496 | Test loss: 0.41896572709083557\n",
      "Epoch: 121690 | Loss: 0.41207051277160645 | Test loss: 0.4189518988132477\n",
      "Epoch: 121700 | Loss: 0.4120587408542633 | Test loss: 0.4189380705356598\n",
      "Epoch: 121710 | Loss: 0.41204699873924255 | Test loss: 0.4189242422580719\n",
      "Epoch: 121720 | Loss: 0.4120352864265442 | Test loss: 0.41891035437583923\n",
      "Epoch: 121730 | Loss: 0.41202351450920105 | Test loss: 0.41889652609825134\n",
      "Epoch: 121740 | Loss: 0.4120117127895355 | Test loss: 0.41888266801834106\n",
      "Epoch: 121750 | Loss: 0.41200000047683716 | Test loss: 0.4188688397407532\n",
      "Epoch: 121760 | Loss: 0.411988228559494 | Test loss: 0.4188550114631653\n",
      "Epoch: 121770 | Loss: 0.4119764268398285 | Test loss: 0.4188411831855774\n",
      "Epoch: 121780 | Loss: 0.4119647145271301 | Test loss: 0.4188272953033447\n",
      "Epoch: 121790 | Loss: 0.411952942609787 | Test loss: 0.41881346702575684\n",
      "Epoch: 121800 | Loss: 0.41194114089012146 | Test loss: 0.41879960894584656\n",
      "Epoch: 121810 | Loss: 0.4119294285774231 | Test loss: 0.41878578066825867\n",
      "Epoch: 121820 | Loss: 0.41191765666007996 | Test loss: 0.418771892786026\n",
      "Epoch: 121830 | Loss: 0.4119059145450592 | Test loss: 0.4187580645084381\n",
      "Epoch: 121840 | Loss: 0.41189414262771606 | Test loss: 0.41874417662620544\n",
      "Epoch: 121850 | Loss: 0.4118824005126953 | Test loss: 0.41873034834861755\n",
      "Epoch: 121860 | Loss: 0.4118705689907074 | Test loss: 0.41871652007102966\n",
      "Epoch: 121870 | Loss: 0.41185885667800903 | Test loss: 0.4187026917934418\n",
      "Epoch: 121880 | Loss: 0.4118471145629883 | Test loss: 0.4186888337135315\n",
      "Epoch: 121890 | Loss: 0.41183534264564514 | Test loss: 0.4186750054359436\n",
      "Epoch: 121900 | Loss: 0.411823570728302 | Test loss: 0.41866111755371094\n",
      "Epoch: 121910 | Loss: 0.41181182861328125 | Test loss: 0.41864728927612305\n",
      "Epoch: 121920 | Loss: 0.4118000566959381 | Test loss: 0.41863346099853516\n",
      "Epoch: 121930 | Loss: 0.41178828477859497 | Test loss: 0.41861963272094727\n",
      "Epoch: 121940 | Loss: 0.4117765426635742 | Test loss: 0.418605774641037\n",
      "Epoch: 121950 | Loss: 0.41176483035087585 | Test loss: 0.4185919463634491\n",
      "Epoch: 121960 | Loss: 0.41175299882888794 | Test loss: 0.41857805848121643\n",
      "Epoch: 121970 | Loss: 0.4117412567138672 | Test loss: 0.41856423020362854\n",
      "Epoch: 121980 | Loss: 0.4117295444011688 | Test loss: 0.41855040192604065\n",
      "Epoch: 121990 | Loss: 0.4117177128791809 | Test loss: 0.418536514043808\n",
      "Epoch: 122000 | Loss: 0.41170597076416016 | Test loss: 0.4185226857662201\n",
      "Epoch: 122010 | Loss: 0.4116942584514618 | Test loss: 0.4185088276863098\n",
      "Epoch: 122020 | Loss: 0.4116824269294739 | Test loss: 0.4184949994087219\n",
      "Epoch: 122030 | Loss: 0.4116706848144531 | Test loss: 0.41848117113113403\n",
      "Epoch: 122040 | Loss: 0.41165897250175476 | Test loss: 0.41846728324890137\n",
      "Epoch: 122050 | Loss: 0.41164717078208923 | Test loss: 0.4184534549713135\n",
      "Epoch: 122060 | Loss: 0.4116353988647461 | Test loss: 0.4184395968914032\n",
      "Epoch: 122070 | Loss: 0.41162368655204773 | Test loss: 0.4184257686138153\n",
      "Epoch: 122080 | Loss: 0.411611944437027 | Test loss: 0.4184119403362274\n",
      "Epoch: 122090 | Loss: 0.41160011291503906 | Test loss: 0.41839805245399475\n",
      "Epoch: 122100 | Loss: 0.4115884006023407 | Test loss: 0.41838422417640686\n",
      "Epoch: 122110 | Loss: 0.41157659888267517 | Test loss: 0.4183703362941742\n",
      "Epoch: 122120 | Loss: 0.41156482696533203 | Test loss: 0.4183565080165863\n",
      "Epoch: 122130 | Loss: 0.41155311465263367 | Test loss: 0.4183426797389984\n",
      "Epoch: 122140 | Loss: 0.41154131293296814 | Test loss: 0.4183288514614105\n",
      "Epoch: 122150 | Loss: 0.411529541015625 | Test loss: 0.41831499338150024\n",
      "Epoch: 122160 | Loss: 0.41151782870292664 | Test loss: 0.41830116510391235\n",
      "Epoch: 122170 | Loss: 0.4115060865879059 | Test loss: 0.4182872772216797\n",
      "Epoch: 122180 | Loss: 0.41149431467056274 | Test loss: 0.4182734489440918\n",
      "Epoch: 122190 | Loss: 0.4114825427532196 | Test loss: 0.4182596206665039\n",
      "Epoch: 122200 | Loss: 0.41147080063819885 | Test loss: 0.41824576258659363\n",
      "Epoch: 122210 | Loss: 0.4114590287208557 | Test loss: 0.41823193430900574\n",
      "Epoch: 122220 | Loss: 0.4114472568035126 | Test loss: 0.41821804642677307\n",
      "Epoch: 122230 | Loss: 0.4114355146884918 | Test loss: 0.4182042181491852\n",
      "Epoch: 122240 | Loss: 0.4114237427711487 | Test loss: 0.4181903898715973\n",
      "Epoch: 122250 | Loss: 0.41141197085380554 | Test loss: 0.4181765019893646\n",
      "Epoch: 122260 | Loss: 0.4114002287387848 | Test loss: 0.41816267371177673\n",
      "Epoch: 122270 | Loss: 0.41138845682144165 | Test loss: 0.41814881563186646\n",
      "Epoch: 122280 | Loss: 0.4113766849040985 | Test loss: 0.41813498735427856\n",
      "Epoch: 122290 | Loss: 0.41136494278907776 | Test loss: 0.4181211590766907\n",
      "Epoch: 122300 | Loss: 0.411353200674057 | Test loss: 0.418107271194458\n",
      "Epoch: 122310 | Loss: 0.4113413989543915 | Test loss: 0.4180934429168701\n",
      "Epoch: 122320 | Loss: 0.4113296568393707 | Test loss: 0.4180796146392822\n",
      "Epoch: 122330 | Loss: 0.41131791472435 | Test loss: 0.41806575655937195\n",
      "Epoch: 122340 | Loss: 0.41130614280700684 | Test loss: 0.41805192828178406\n",
      "Epoch: 122350 | Loss: 0.4112943708896637 | Test loss: 0.41803810000419617\n",
      "Epoch: 122360 | Loss: 0.41128262877464294 | Test loss: 0.4180242121219635\n",
      "Epoch: 122370 | Loss: 0.4112708568572998 | Test loss: 0.4180103838443756\n",
      "Epoch: 122380 | Loss: 0.41125908493995667 | Test loss: 0.41799649596214294\n",
      "Epoch: 122390 | Loss: 0.4112473130226135 | Test loss: 0.41798266768455505\n",
      "Epoch: 122400 | Loss: 0.4112355709075928 | Test loss: 0.41796883940696716\n",
      "Epoch: 122410 | Loss: 0.4112238585948944 | Test loss: 0.4179550111293793\n",
      "Epoch: 122420 | Loss: 0.4112120568752289 | Test loss: 0.417941153049469\n",
      "Epoch: 122430 | Loss: 0.4112003445625305 | Test loss: 0.41792726516723633\n",
      "Epoch: 122440 | Loss: 0.4111885726451874 | Test loss: 0.41791343688964844\n",
      "Epoch: 122450 | Loss: 0.41117677092552185 | Test loss: 0.41789960861206055\n",
      "Epoch: 122460 | Loss: 0.4111650586128235 | Test loss: 0.41788575053215027\n",
      "Epoch: 122470 | Loss: 0.41115322709083557 | Test loss: 0.4178719222545624\n",
      "Epoch: 122480 | Loss: 0.4111414849758148 | Test loss: 0.4178580343723297\n",
      "Epoch: 122490 | Loss: 0.4111297130584717 | Test loss: 0.4178442060947418\n",
      "Epoch: 122500 | Loss: 0.41111794114112854 | Test loss: 0.41783037781715393\n",
      "Epoch: 122510 | Loss: 0.4111061990261078 | Test loss: 0.41781654953956604\n",
      "Epoch: 122520 | Loss: 0.4110944867134094 | Test loss: 0.4178026616573334\n",
      "Epoch: 122530 | Loss: 0.4110827147960663 | Test loss: 0.4177888333797455\n",
      "Epoch: 122540 | Loss: 0.41107091307640076 | Test loss: 0.4177749752998352\n",
      "Epoch: 122550 | Loss: 0.4110592007637024 | Test loss: 0.4177611470222473\n",
      "Epoch: 122560 | Loss: 0.41104742884635925 | Test loss: 0.4177473187446594\n",
      "Epoch: 122570 | Loss: 0.4110356271266937 | Test loss: 0.41773349046707153\n",
      "Epoch: 122580 | Loss: 0.41102391481399536 | Test loss: 0.41771960258483887\n",
      "Epoch: 122590 | Loss: 0.4110121428966522 | Test loss: 0.417705774307251\n",
      "Epoch: 122600 | Loss: 0.4110003411769867 | Test loss: 0.4176919162273407\n",
      "Epoch: 122610 | Loss: 0.41098862886428833 | Test loss: 0.4176780879497528\n",
      "Epoch: 122620 | Loss: 0.4109768569469452 | Test loss: 0.41766420006752014\n",
      "Epoch: 122630 | Loss: 0.41096511483192444 | Test loss: 0.41765037178993225\n",
      "Epoch: 122640 | Loss: 0.4109533429145813 | Test loss: 0.4176364839076996\n",
      "Epoch: 122650 | Loss: 0.41094160079956055 | Test loss: 0.4176226556301117\n",
      "Epoch: 122660 | Loss: 0.41092976927757263 | Test loss: 0.4176088273525238\n",
      "Epoch: 122670 | Loss: 0.41091805696487427 | Test loss: 0.4175949990749359\n",
      "Epoch: 122680 | Loss: 0.4109063148498535 | Test loss: 0.41758114099502563\n",
      "Epoch: 122690 | Loss: 0.4108945429325104 | Test loss: 0.41756731271743774\n",
      "Epoch: 122700 | Loss: 0.41088277101516724 | Test loss: 0.4175534248352051\n",
      "Epoch: 122710 | Loss: 0.4108710289001465 | Test loss: 0.4175395965576172\n",
      "Epoch: 122720 | Loss: 0.41085925698280334 | Test loss: 0.4175257682800293\n",
      "Epoch: 122730 | Loss: 0.4108474850654602 | Test loss: 0.4175119400024414\n",
      "Epoch: 122740 | Loss: 0.41083574295043945 | Test loss: 0.41749808192253113\n",
      "Epoch: 122750 | Loss: 0.4108240306377411 | Test loss: 0.41748425364494324\n",
      "Epoch: 122760 | Loss: 0.4108121991157532 | Test loss: 0.41747036576271057\n",
      "Epoch: 122770 | Loss: 0.4108004570007324 | Test loss: 0.4174565374851227\n",
      "Epoch: 122780 | Loss: 0.41078874468803406 | Test loss: 0.4174427092075348\n",
      "Epoch: 122790 | Loss: 0.41077691316604614 | Test loss: 0.4174288213253021\n",
      "Epoch: 122800 | Loss: 0.4107651710510254 | Test loss: 0.41741499304771423\n",
      "Epoch: 122810 | Loss: 0.410753458738327 | Test loss: 0.41740113496780396\n",
      "Epoch: 122820 | Loss: 0.4107416272163391 | Test loss: 0.41738730669021606\n",
      "Epoch: 122830 | Loss: 0.41072988510131836 | Test loss: 0.4173734784126282\n",
      "Epoch: 122840 | Loss: 0.41071817278862 | Test loss: 0.4173595905303955\n",
      "Epoch: 122850 | Loss: 0.41070637106895447 | Test loss: 0.4173457622528076\n",
      "Epoch: 122860 | Loss: 0.41069459915161133 | Test loss: 0.41733190417289734\n",
      "Epoch: 122870 | Loss: 0.41068288683891296 | Test loss: 0.41731807589530945\n",
      "Epoch: 122880 | Loss: 0.4106711447238922 | Test loss: 0.41730424761772156\n",
      "Epoch: 122890 | Loss: 0.4106593132019043 | Test loss: 0.4172903597354889\n",
      "Epoch: 122900 | Loss: 0.41064760088920593 | Test loss: 0.417276531457901\n",
      "Epoch: 122910 | Loss: 0.4106357991695404 | Test loss: 0.41726264357566833\n",
      "Epoch: 122920 | Loss: 0.41062402725219727 | Test loss: 0.41724881529808044\n",
      "Epoch: 122930 | Loss: 0.4106123149394989 | Test loss: 0.41723498702049255\n",
      "Epoch: 122940 | Loss: 0.4106005132198334 | Test loss: 0.41722115874290466\n",
      "Epoch: 122950 | Loss: 0.41058874130249023 | Test loss: 0.4172073006629944\n",
      "Epoch: 122960 | Loss: 0.41057702898979187 | Test loss: 0.4171934723854065\n",
      "Epoch: 122970 | Loss: 0.4105652868747711 | Test loss: 0.41717958450317383\n",
      "Epoch: 122980 | Loss: 0.410553514957428 | Test loss: 0.41716575622558594\n",
      "Epoch: 122990 | Loss: 0.41054174304008484 | Test loss: 0.41715192794799805\n",
      "Epoch: 123000 | Loss: 0.4105300009250641 | Test loss: 0.41713806986808777\n",
      "Epoch: 123010 | Loss: 0.41051822900772095 | Test loss: 0.4171242415904999\n",
      "Epoch: 123020 | Loss: 0.4105064570903778 | Test loss: 0.4171103537082672\n",
      "Epoch: 123030 | Loss: 0.41049471497535706 | Test loss: 0.4170965254306793\n",
      "Epoch: 123040 | Loss: 0.4104829430580139 | Test loss: 0.41708269715309143\n",
      "Epoch: 123050 | Loss: 0.4104711711406708 | Test loss: 0.41706880927085876\n",
      "Epoch: 123060 | Loss: 0.41045942902565 | Test loss: 0.4170549809932709\n",
      "Epoch: 123070 | Loss: 0.4104476571083069 | Test loss: 0.4170411229133606\n",
      "Epoch: 123080 | Loss: 0.41043588519096375 | Test loss: 0.4170272946357727\n",
      "Epoch: 123090 | Loss: 0.410424143075943 | Test loss: 0.4170134663581848\n",
      "Epoch: 123100 | Loss: 0.41041240096092224 | Test loss: 0.41699957847595215\n",
      "Epoch: 123110 | Loss: 0.4104005992412567 | Test loss: 0.41698575019836426\n",
      "Epoch: 123120 | Loss: 0.41038885712623596 | Test loss: 0.41697192192077637\n",
      "Epoch: 123130 | Loss: 0.4103771150112152 | Test loss: 0.4169580638408661\n",
      "Epoch: 123140 | Loss: 0.41036534309387207 | Test loss: 0.4169442355632782\n",
      "Epoch: 123150 | Loss: 0.41035357117652893 | Test loss: 0.4169304072856903\n",
      "Epoch: 123160 | Loss: 0.4103418290615082 | Test loss: 0.41691651940345764\n",
      "Epoch: 123170 | Loss: 0.41033005714416504 | Test loss: 0.41690269112586975\n",
      "Epoch: 123180 | Loss: 0.4103182852268219 | Test loss: 0.4168888032436371\n",
      "Epoch: 123190 | Loss: 0.41030651330947876 | Test loss: 0.4168749749660492\n",
      "Epoch: 123200 | Loss: 0.410294771194458 | Test loss: 0.4168611466884613\n",
      "Epoch: 123210 | Loss: 0.41028305888175964 | Test loss: 0.4168473184108734\n",
      "Epoch: 123220 | Loss: 0.4102712571620941 | Test loss: 0.41683346033096313\n",
      "Epoch: 123230 | Loss: 0.41025954484939575 | Test loss: 0.41681957244873047\n",
      "Epoch: 123240 | Loss: 0.4102477729320526 | Test loss: 0.4168057441711426\n",
      "Epoch: 123250 | Loss: 0.4102359712123871 | Test loss: 0.4167919158935547\n",
      "Epoch: 123260 | Loss: 0.4102242588996887 | Test loss: 0.4167780578136444\n",
      "Epoch: 123270 | Loss: 0.4102124273777008 | Test loss: 0.4167642295360565\n",
      "Epoch: 123280 | Loss: 0.41020068526268005 | Test loss: 0.41675034165382385\n",
      "Epoch: 123290 | Loss: 0.4101889133453369 | Test loss: 0.41673651337623596\n",
      "Epoch: 123300 | Loss: 0.4101771414279938 | Test loss: 0.41672268509864807\n",
      "Epoch: 123310 | Loss: 0.410165399312973 | Test loss: 0.4167088568210602\n",
      "Epoch: 123320 | Loss: 0.41015368700027466 | Test loss: 0.4166949689388275\n",
      "Epoch: 123330 | Loss: 0.4101419150829315 | Test loss: 0.4166811406612396\n",
      "Epoch: 123340 | Loss: 0.410130113363266 | Test loss: 0.41666728258132935\n",
      "Epoch: 123350 | Loss: 0.4101184010505676 | Test loss: 0.41665345430374146\n",
      "Epoch: 123360 | Loss: 0.4101066291332245 | Test loss: 0.41663962602615356\n",
      "Epoch: 123370 | Loss: 0.41009482741355896 | Test loss: 0.4166257977485657\n",
      "Epoch: 123380 | Loss: 0.4100831151008606 | Test loss: 0.416611909866333\n",
      "Epoch: 123390 | Loss: 0.41007134318351746 | Test loss: 0.4165980815887451\n",
      "Epoch: 123400 | Loss: 0.41005954146385193 | Test loss: 0.41658422350883484\n",
      "Epoch: 123410 | Loss: 0.41004782915115356 | Test loss: 0.41657039523124695\n",
      "Epoch: 123420 | Loss: 0.4100360572338104 | Test loss: 0.4165565073490143\n",
      "Epoch: 123430 | Loss: 0.4100243151187897 | Test loss: 0.4165426790714264\n",
      "Epoch: 123440 | Loss: 0.41001254320144653 | Test loss: 0.4165287911891937\n",
      "Epoch: 123450 | Loss: 0.4100008010864258 | Test loss: 0.41651496291160583\n",
      "Epoch: 123460 | Loss: 0.40998896956443787 | Test loss: 0.41650113463401794\n",
      "Epoch: 123470 | Loss: 0.4099772572517395 | Test loss: 0.41648730635643005\n",
      "Epoch: 123480 | Loss: 0.40996551513671875 | Test loss: 0.4164734482765198\n",
      "Epoch: 123490 | Loss: 0.4099537432193756 | Test loss: 0.4164596199989319\n",
      "Epoch: 123500 | Loss: 0.40994197130203247 | Test loss: 0.4164457321166992\n",
      "Epoch: 123510 | Loss: 0.4099302291870117 | Test loss: 0.41643190383911133\n",
      "Epoch: 123520 | Loss: 0.4099184572696686 | Test loss: 0.41641807556152344\n",
      "Epoch: 123530 | Loss: 0.40990668535232544 | Test loss: 0.41640424728393555\n",
      "Epoch: 123540 | Loss: 0.4098949432373047 | Test loss: 0.41639038920402527\n",
      "Epoch: 123550 | Loss: 0.4098832309246063 | Test loss: 0.4163765609264374\n",
      "Epoch: 123560 | Loss: 0.4098713994026184 | Test loss: 0.4163626730442047\n",
      "Epoch: 123570 | Loss: 0.40985965728759766 | Test loss: 0.4163488447666168\n",
      "Epoch: 123580 | Loss: 0.4098479449748993 | Test loss: 0.41633501648902893\n",
      "Epoch: 123590 | Loss: 0.4098361134529114 | Test loss: 0.41632112860679626\n",
      "Epoch: 123600 | Loss: 0.4098243713378906 | Test loss: 0.4163073003292084\n",
      "Epoch: 123610 | Loss: 0.40981265902519226 | Test loss: 0.4162934422492981\n",
      "Epoch: 123620 | Loss: 0.40980082750320435 | Test loss: 0.4162796139717102\n",
      "Epoch: 123630 | Loss: 0.4097890853881836 | Test loss: 0.4162657856941223\n",
      "Epoch: 123640 | Loss: 0.40977737307548523 | Test loss: 0.41625189781188965\n",
      "Epoch: 123650 | Loss: 0.4097655713558197 | Test loss: 0.41623806953430176\n",
      "Epoch: 123660 | Loss: 0.40975379943847656 | Test loss: 0.4162242114543915\n",
      "Epoch: 123670 | Loss: 0.4097420871257782 | Test loss: 0.4162103831768036\n",
      "Epoch: 123680 | Loss: 0.40973034501075745 | Test loss: 0.4161965548992157\n",
      "Epoch: 123690 | Loss: 0.40971851348876953 | Test loss: 0.41618266701698303\n",
      "Epoch: 123700 | Loss: 0.40970680117607117 | Test loss: 0.41616883873939514\n",
      "Epoch: 123710 | Loss: 0.40969499945640564 | Test loss: 0.4161549508571625\n",
      "Epoch: 123720 | Loss: 0.4096832275390625 | Test loss: 0.4161411225795746\n",
      "Epoch: 123730 | Loss: 0.40967151522636414 | Test loss: 0.4161272943019867\n",
      "Epoch: 123740 | Loss: 0.4096597135066986 | Test loss: 0.4161134660243988\n",
      "Epoch: 123750 | Loss: 0.40964794158935547 | Test loss: 0.4160996079444885\n",
      "Epoch: 123760 | Loss: 0.4096362292766571 | Test loss: 0.41608577966690063\n",
      "Epoch: 123770 | Loss: 0.40962448716163635 | Test loss: 0.41607189178466797\n",
      "Epoch: 123780 | Loss: 0.4096127152442932 | Test loss: 0.4160580635070801\n",
      "Epoch: 123790 | Loss: 0.4096009433269501 | Test loss: 0.4160442352294922\n",
      "Epoch: 123800 | Loss: 0.4095892012119293 | Test loss: 0.4160303771495819\n",
      "Epoch: 123810 | Loss: 0.4095774292945862 | Test loss: 0.416016548871994\n",
      "Epoch: 123820 | Loss: 0.40956565737724304 | Test loss: 0.41600266098976135\n",
      "Epoch: 123830 | Loss: 0.4095539152622223 | Test loss: 0.41598883271217346\n",
      "Epoch: 123840 | Loss: 0.40954214334487915 | Test loss: 0.41597500443458557\n",
      "Epoch: 123850 | Loss: 0.409530371427536 | Test loss: 0.4159611165523529\n",
      "Epoch: 123860 | Loss: 0.40951862931251526 | Test loss: 0.415947288274765\n",
      "Epoch: 123870 | Loss: 0.4095068573951721 | Test loss: 0.41593343019485474\n",
      "Epoch: 123880 | Loss: 0.409495085477829 | Test loss: 0.41591960191726685\n",
      "Epoch: 123890 | Loss: 0.4094833433628082 | Test loss: 0.41590577363967896\n",
      "Epoch: 123900 | Loss: 0.4094716012477875 | Test loss: 0.4158918857574463\n",
      "Epoch: 123910 | Loss: 0.40945979952812195 | Test loss: 0.4158780574798584\n",
      "Epoch: 123920 | Loss: 0.4094480574131012 | Test loss: 0.4158642292022705\n",
      "Epoch: 123930 | Loss: 0.40943631529808044 | Test loss: 0.41585037112236023\n",
      "Epoch: 123940 | Loss: 0.4094245433807373 | Test loss: 0.41583654284477234\n",
      "Epoch: 123950 | Loss: 0.40941277146339417 | Test loss: 0.41582271456718445\n",
      "Epoch: 123960 | Loss: 0.4094010293483734 | Test loss: 0.4158088266849518\n",
      "Epoch: 123970 | Loss: 0.4093892574310303 | Test loss: 0.4157949984073639\n",
      "Epoch: 123980 | Loss: 0.40937748551368713 | Test loss: 0.4157811105251312\n",
      "Epoch: 123990 | Loss: 0.409365713596344 | Test loss: 0.41576728224754333\n",
      "Epoch: 124000 | Loss: 0.40935397148132324 | Test loss: 0.41575345396995544\n",
      "Epoch: 124010 | Loss: 0.4093422591686249 | Test loss: 0.41573962569236755\n",
      "Epoch: 124020 | Loss: 0.40933045744895935 | Test loss: 0.4157257676124573\n",
      "Epoch: 124030 | Loss: 0.409318745136261 | Test loss: 0.4157118797302246\n",
      "Epoch: 124040 | Loss: 0.40930697321891785 | Test loss: 0.4156980514526367\n",
      "Epoch: 124050 | Loss: 0.4092951714992523 | Test loss: 0.41568422317504883\n",
      "Epoch: 124060 | Loss: 0.40928345918655396 | Test loss: 0.41567036509513855\n",
      "Epoch: 124070 | Loss: 0.40927162766456604 | Test loss: 0.41565653681755066\n",
      "Epoch: 124080 | Loss: 0.4092598855495453 | Test loss: 0.415642648935318\n",
      "Epoch: 124090 | Loss: 0.40924811363220215 | Test loss: 0.4156288206577301\n",
      "Epoch: 124100 | Loss: 0.409236341714859 | Test loss: 0.4156149923801422\n",
      "Epoch: 124110 | Loss: 0.40922459959983826 | Test loss: 0.4156011641025543\n",
      "Epoch: 124120 | Loss: 0.4092128872871399 | Test loss: 0.41558727622032166\n",
      "Epoch: 124130 | Loss: 0.40920111536979675 | Test loss: 0.41557344794273376\n",
      "Epoch: 124140 | Loss: 0.4091893136501312 | Test loss: 0.4155595898628235\n",
      "Epoch: 124150 | Loss: 0.40917760133743286 | Test loss: 0.4155457615852356\n",
      "Epoch: 124160 | Loss: 0.4091658294200897 | Test loss: 0.4155319333076477\n",
      "Epoch: 124170 | Loss: 0.4091540277004242 | Test loss: 0.4155181050300598\n",
      "Epoch: 124180 | Loss: 0.40914231538772583 | Test loss: 0.41550421714782715\n",
      "Epoch: 124190 | Loss: 0.4091305434703827 | Test loss: 0.41549038887023926\n",
      "Epoch: 124200 | Loss: 0.40911874175071716 | Test loss: 0.415476530790329\n",
      "Epoch: 124210 | Loss: 0.4091070294380188 | Test loss: 0.4154627025127411\n",
      "Epoch: 124220 | Loss: 0.40909525752067566 | Test loss: 0.4154488146305084\n",
      "Epoch: 124230 | Loss: 0.4090835154056549 | Test loss: 0.41543498635292053\n",
      "Epoch: 124240 | Loss: 0.40907174348831177 | Test loss: 0.41542109847068787\n",
      "Epoch: 124250 | Loss: 0.409060001373291 | Test loss: 0.4154072701931\n",
      "Epoch: 124260 | Loss: 0.4090481698513031 | Test loss: 0.4153934419155121\n",
      "Epoch: 124270 | Loss: 0.40903645753860474 | Test loss: 0.4153796136379242\n",
      "Epoch: 124280 | Loss: 0.409024715423584 | Test loss: 0.4153657555580139\n",
      "Epoch: 124290 | Loss: 0.40901294350624084 | Test loss: 0.415351927280426\n",
      "Epoch: 124300 | Loss: 0.4090011715888977 | Test loss: 0.41533803939819336\n",
      "Epoch: 124310 | Loss: 0.40898942947387695 | Test loss: 0.41532421112060547\n",
      "Epoch: 124320 | Loss: 0.4089776575565338 | Test loss: 0.4153103828430176\n",
      "Epoch: 124330 | Loss: 0.4089658856391907 | Test loss: 0.4152965545654297\n",
      "Epoch: 124340 | Loss: 0.4089541435241699 | Test loss: 0.4152826964855194\n",
      "Epoch: 124350 | Loss: 0.40894243121147156 | Test loss: 0.4152688682079315\n",
      "Epoch: 124360 | Loss: 0.40893059968948364 | Test loss: 0.41525498032569885\n",
      "Epoch: 124370 | Loss: 0.4089188575744629 | Test loss: 0.41524115204811096\n",
      "Epoch: 124380 | Loss: 0.4089071452617645 | Test loss: 0.41522732377052307\n",
      "Epoch: 124390 | Loss: 0.4088953137397766 | Test loss: 0.4152134358882904\n",
      "Epoch: 124400 | Loss: 0.40888357162475586 | Test loss: 0.4151996076107025\n",
      "Epoch: 124410 | Loss: 0.4088718593120575 | Test loss: 0.41518574953079224\n",
      "Epoch: 124420 | Loss: 0.4088600277900696 | Test loss: 0.41517192125320435\n",
      "Epoch: 124430 | Loss: 0.40884828567504883 | Test loss: 0.41515809297561646\n",
      "Epoch: 124440 | Loss: 0.40883657336235046 | Test loss: 0.4151442050933838\n",
      "Epoch: 124450 | Loss: 0.40882477164268494 | Test loss: 0.4151303768157959\n",
      "Epoch: 124460 | Loss: 0.4088129997253418 | Test loss: 0.4151165187358856\n",
      "Epoch: 124470 | Loss: 0.40880128741264343 | Test loss: 0.41510269045829773\n",
      "Epoch: 124480 | Loss: 0.4087895452976227 | Test loss: 0.41508886218070984\n",
      "Epoch: 124490 | Loss: 0.40877771377563477 | Test loss: 0.4150749742984772\n",
      "Epoch: 124500 | Loss: 0.4087660014629364 | Test loss: 0.4150611460208893\n",
      "Epoch: 124510 | Loss: 0.4087541997432709 | Test loss: 0.4150472581386566\n",
      "Epoch: 124520 | Loss: 0.40874242782592773 | Test loss: 0.4150334298610687\n",
      "Epoch: 124530 | Loss: 0.40873071551322937 | Test loss: 0.41501960158348083\n",
      "Epoch: 124540 | Loss: 0.40871891379356384 | Test loss: 0.41500577330589294\n",
      "Epoch: 124550 | Loss: 0.4087071418762207 | Test loss: 0.41499191522598267\n",
      "Epoch: 124560 | Loss: 0.40869542956352234 | Test loss: 0.4149780869483948\n",
      "Epoch: 124570 | Loss: 0.4086836874485016 | Test loss: 0.4149641990661621\n",
      "Epoch: 124580 | Loss: 0.40867191553115845 | Test loss: 0.4149503707885742\n",
      "Epoch: 124590 | Loss: 0.4086601436138153 | Test loss: 0.41493654251098633\n",
      "Epoch: 124600 | Loss: 0.40864840149879456 | Test loss: 0.41492268443107605\n",
      "Epoch: 124610 | Loss: 0.4086366295814514 | Test loss: 0.41490885615348816\n",
      "Epoch: 124620 | Loss: 0.4086248576641083 | Test loss: 0.4148949682712555\n",
      "Epoch: 124630 | Loss: 0.4086131155490875 | Test loss: 0.4148811399936676\n",
      "Epoch: 124640 | Loss: 0.4086013436317444 | Test loss: 0.4148673117160797\n",
      "Epoch: 124650 | Loss: 0.40858957171440125 | Test loss: 0.41485342383384705\n",
      "Epoch: 124660 | Loss: 0.4085778295993805 | Test loss: 0.41483959555625916\n",
      "Epoch: 124670 | Loss: 0.40856605768203735 | Test loss: 0.4148257374763489\n",
      "Epoch: 124680 | Loss: 0.4085542857646942 | Test loss: 0.414811909198761\n",
      "Epoch: 124690 | Loss: 0.40854254364967346 | Test loss: 0.4147980809211731\n",
      "Epoch: 124700 | Loss: 0.4085308015346527 | Test loss: 0.41478419303894043\n",
      "Epoch: 124710 | Loss: 0.4085189998149872 | Test loss: 0.41477036476135254\n",
      "Epoch: 124720 | Loss: 0.40850725769996643 | Test loss: 0.41475653648376465\n",
      "Epoch: 124730 | Loss: 0.4084955155849457 | Test loss: 0.41474267840385437\n",
      "Epoch: 124740 | Loss: 0.40848374366760254 | Test loss: 0.4147288501262665\n",
      "Epoch: 124750 | Loss: 0.4084719717502594 | Test loss: 0.4147150218486786\n",
      "Epoch: 124760 | Loss: 0.40846022963523865 | Test loss: 0.4147011339664459\n",
      "Epoch: 124770 | Loss: 0.4084484577178955 | Test loss: 0.41468730568885803\n",
      "Epoch: 124780 | Loss: 0.40843668580055237 | Test loss: 0.41467341780662537\n",
      "Epoch: 124790 | Loss: 0.40842491388320923 | Test loss: 0.4146595895290375\n",
      "Epoch: 124800 | Loss: 0.4084131717681885 | Test loss: 0.4146457612514496\n",
      "Epoch: 124810 | Loss: 0.4084014594554901 | Test loss: 0.4146319329738617\n",
      "Epoch: 124820 | Loss: 0.4083896577358246 | Test loss: 0.4146180748939514\n",
      "Epoch: 124830 | Loss: 0.4083779454231262 | Test loss: 0.41460418701171875\n",
      "Epoch: 124840 | Loss: 0.4083661735057831 | Test loss: 0.41459035873413086\n",
      "Epoch: 124850 | Loss: 0.40835437178611755 | Test loss: 0.41457653045654297\n",
      "Epoch: 124860 | Loss: 0.4083426594734192 | Test loss: 0.4145626723766327\n",
      "Epoch: 124870 | Loss: 0.4083308279514313 | Test loss: 0.4145488440990448\n",
      "Epoch: 124880 | Loss: 0.4083190858364105 | Test loss: 0.41453495621681213\n",
      "Epoch: 124890 | Loss: 0.4083073139190674 | Test loss: 0.41452112793922424\n",
      "Epoch: 124900 | Loss: 0.40829554200172424 | Test loss: 0.41450729966163635\n",
      "Epoch: 124910 | Loss: 0.4082837998867035 | Test loss: 0.41449347138404846\n",
      "Epoch: 124920 | Loss: 0.4082720875740051 | Test loss: 0.4144795835018158\n",
      "Epoch: 124930 | Loss: 0.408260315656662 | Test loss: 0.4144657552242279\n",
      "Epoch: 124940 | Loss: 0.40824851393699646 | Test loss: 0.4144518971443176\n",
      "Epoch: 124950 | Loss: 0.4082368016242981 | Test loss: 0.41443806886672974\n",
      "Epoch: 124960 | Loss: 0.40822502970695496 | Test loss: 0.41442424058914185\n",
      "Epoch: 124970 | Loss: 0.40821322798728943 | Test loss: 0.41441041231155396\n",
      "Epoch: 124980 | Loss: 0.40820151567459106 | Test loss: 0.4143965244293213\n",
      "Epoch: 124990 | Loss: 0.4081897437572479 | Test loss: 0.4143826961517334\n",
      "Epoch: 125000 | Loss: 0.4081779420375824 | Test loss: 0.4143688380718231\n",
      "Epoch: 125010 | Loss: 0.40816622972488403 | Test loss: 0.41435500979423523\n",
      "Epoch: 125020 | Loss: 0.4081544578075409 | Test loss: 0.41434112191200256\n",
      "Epoch: 125030 | Loss: 0.40814271569252014 | Test loss: 0.4143272936344147\n",
      "Epoch: 125040 | Loss: 0.408130943775177 | Test loss: 0.414313405752182\n",
      "Epoch: 125050 | Loss: 0.40811920166015625 | Test loss: 0.4142995774745941\n",
      "Epoch: 125060 | Loss: 0.40810737013816833 | Test loss: 0.4142857491970062\n",
      "Epoch: 125070 | Loss: 0.40809565782546997 | Test loss: 0.41427192091941833\n",
      "Epoch: 125080 | Loss: 0.4080839157104492 | Test loss: 0.41425806283950806\n",
      "Epoch: 125090 | Loss: 0.4080721437931061 | Test loss: 0.41424423456192017\n",
      "Epoch: 125100 | Loss: 0.40806037187576294 | Test loss: 0.4142303466796875\n",
      "Epoch: 125110 | Loss: 0.4080486297607422 | Test loss: 0.4142165184020996\n",
      "Epoch: 125120 | Loss: 0.40803685784339905 | Test loss: 0.4142026901245117\n",
      "Epoch: 125130 | Loss: 0.4080250859260559 | Test loss: 0.41418886184692383\n",
      "Epoch: 125140 | Loss: 0.40801334381103516 | Test loss: 0.41417500376701355\n",
      "Epoch: 125150 | Loss: 0.4080016314983368 | Test loss: 0.41416117548942566\n",
      "Epoch: 125160 | Loss: 0.4079897999763489 | Test loss: 0.414147287607193\n",
      "Epoch: 125170 | Loss: 0.4079780578613281 | Test loss: 0.4141334593296051\n",
      "Epoch: 125180 | Loss: 0.40796634554862976 | Test loss: 0.4141196310520172\n",
      "Epoch: 125190 | Loss: 0.40795451402664185 | Test loss: 0.41410574316978455\n",
      "Epoch: 125200 | Loss: 0.4079427719116211 | Test loss: 0.41409191489219666\n",
      "Epoch: 125210 | Loss: 0.40793105959892273 | Test loss: 0.4140780568122864\n",
      "Epoch: 125220 | Loss: 0.4079192280769348 | Test loss: 0.4140642285346985\n",
      "Epoch: 125230 | Loss: 0.40790748596191406 | Test loss: 0.4140504002571106\n",
      "Epoch: 125240 | Loss: 0.4078957736492157 | Test loss: 0.41403651237487793\n",
      "Epoch: 125250 | Loss: 0.40788397192955017 | Test loss: 0.41402268409729004\n",
      "Epoch: 125260 | Loss: 0.40787220001220703 | Test loss: 0.41400882601737976\n",
      "Epoch: 125270 | Loss: 0.40786048769950867 | Test loss: 0.41399499773979187\n",
      "Epoch: 125280 | Loss: 0.4078487455844879 | Test loss: 0.413981169462204\n",
      "Epoch: 125290 | Loss: 0.4078369140625 | Test loss: 0.4139672815799713\n",
      "Epoch: 125300 | Loss: 0.40782520174980164 | Test loss: 0.4139534533023834\n",
      "Epoch: 125310 | Loss: 0.4078134000301361 | Test loss: 0.41393956542015076\n",
      "Epoch: 125320 | Loss: 0.40780162811279297 | Test loss: 0.41392573714256287\n",
      "Epoch: 125330 | Loss: 0.4077899158000946 | Test loss: 0.413911908864975\n",
      "Epoch: 125340 | Loss: 0.4077781140804291 | Test loss: 0.4138980805873871\n",
      "Epoch: 125350 | Loss: 0.40776634216308594 | Test loss: 0.4138842225074768\n",
      "Epoch: 125360 | Loss: 0.4077546298503876 | Test loss: 0.4138703942298889\n",
      "Epoch: 125370 | Loss: 0.4077428877353668 | Test loss: 0.41385650634765625\n",
      "Epoch: 125380 | Loss: 0.4077311158180237 | Test loss: 0.41384267807006836\n",
      "Epoch: 125390 | Loss: 0.40771934390068054 | Test loss: 0.41382884979248047\n",
      "Epoch: 125400 | Loss: 0.4077076017856598 | Test loss: 0.4138149917125702\n",
      "Epoch: 125410 | Loss: 0.40769582986831665 | Test loss: 0.4138011634349823\n",
      "Epoch: 125420 | Loss: 0.4076840579509735 | Test loss: 0.41378727555274963\n",
      "Epoch: 125430 | Loss: 0.40767231583595276 | Test loss: 0.41377344727516174\n",
      "Epoch: 125440 | Loss: 0.4076605439186096 | Test loss: 0.41375961899757385\n",
      "Epoch: 125450 | Loss: 0.4076487720012665 | Test loss: 0.4137457311153412\n",
      "Epoch: 125460 | Loss: 0.4076370298862457 | Test loss: 0.4137319028377533\n",
      "Epoch: 125470 | Loss: 0.4076252579689026 | Test loss: 0.413718044757843\n",
      "Epoch: 125480 | Loss: 0.40761348605155945 | Test loss: 0.4137042164802551\n",
      "Epoch: 125490 | Loss: 0.4076017439365387 | Test loss: 0.41369038820266724\n",
      "Epoch: 125500 | Loss: 0.40759000182151794 | Test loss: 0.41367650032043457\n",
      "Epoch: 125510 | Loss: 0.4075782001018524 | Test loss: 0.4136626720428467\n",
      "Epoch: 125520 | Loss: 0.40756645798683167 | Test loss: 0.4136488437652588\n",
      "Epoch: 125530 | Loss: 0.4075547158718109 | Test loss: 0.4136349856853485\n",
      "Epoch: 125540 | Loss: 0.4075429439544678 | Test loss: 0.4136211574077606\n",
      "Epoch: 125550 | Loss: 0.40753117203712463 | Test loss: 0.41360732913017273\n",
      "Epoch: 125560 | Loss: 0.4075194299221039 | Test loss: 0.41359344124794006\n",
      "Epoch: 125570 | Loss: 0.40750765800476074 | Test loss: 0.4135796129703522\n",
      "Epoch: 125580 | Loss: 0.4074958860874176 | Test loss: 0.4135657250881195\n",
      "Epoch: 125590 | Loss: 0.40748411417007446 | Test loss: 0.4135518968105316\n",
      "Epoch: 125600 | Loss: 0.4074723720550537 | Test loss: 0.4135380685329437\n",
      "Epoch: 125610 | Loss: 0.40746065974235535 | Test loss: 0.41352424025535583\n",
      "Epoch: 125620 | Loss: 0.4074488580226898 | Test loss: 0.41351038217544556\n",
      "Epoch: 125630 | Loss: 0.40743714570999146 | Test loss: 0.4134964942932129\n",
      "Epoch: 125640 | Loss: 0.4074253737926483 | Test loss: 0.413482666015625\n",
      "Epoch: 125650 | Loss: 0.4074135720729828 | Test loss: 0.4134688377380371\n",
      "Epoch: 125660 | Loss: 0.4074018597602844 | Test loss: 0.41345497965812683\n",
      "Epoch: 125670 | Loss: 0.4073900282382965 | Test loss: 0.41344115138053894\n",
      "Epoch: 125680 | Loss: 0.40737828612327576 | Test loss: 0.4134272634983063\n",
      "Epoch: 125690 | Loss: 0.4073665142059326 | Test loss: 0.4134134352207184\n",
      "Epoch: 125700 | Loss: 0.4073547422885895 | Test loss: 0.4133996069431305\n",
      "Epoch: 125710 | Loss: 0.4073430001735687 | Test loss: 0.4133857786655426\n",
      "Epoch: 125720 | Loss: 0.40733128786087036 | Test loss: 0.41337189078330994\n",
      "Epoch: 125730 | Loss: 0.4073195159435272 | Test loss: 0.41335806250572205\n",
      "Epoch: 125740 | Loss: 0.4073077142238617 | Test loss: 0.41334420442581177\n",
      "Epoch: 125750 | Loss: 0.40729600191116333 | Test loss: 0.4133303761482239\n",
      "Epoch: 125760 | Loss: 0.4072842299938202 | Test loss: 0.413316547870636\n",
      "Epoch: 125770 | Loss: 0.40727242827415466 | Test loss: 0.4133027195930481\n",
      "Epoch: 125780 | Loss: 0.4072607159614563 | Test loss: 0.41328883171081543\n",
      "Epoch: 125790 | Loss: 0.40724894404411316 | Test loss: 0.41327500343322754\n",
      "Epoch: 125800 | Loss: 0.40723714232444763 | Test loss: 0.41326114535331726\n",
      "Epoch: 125810 | Loss: 0.40722543001174927 | Test loss: 0.41324731707572937\n",
      "Epoch: 125820 | Loss: 0.40721365809440613 | Test loss: 0.4132334291934967\n",
      "Epoch: 125830 | Loss: 0.4072019159793854 | Test loss: 0.4132196009159088\n",
      "Epoch: 125840 | Loss: 0.40719014406204224 | Test loss: 0.41320571303367615\n",
      "Epoch: 125850 | Loss: 0.4071784019470215 | Test loss: 0.41319188475608826\n",
      "Epoch: 125860 | Loss: 0.40716657042503357 | Test loss: 0.41317805647850037\n",
      "Epoch: 125870 | Loss: 0.4071548581123352 | Test loss: 0.4131642282009125\n",
      "Epoch: 125880 | Loss: 0.40714311599731445 | Test loss: 0.4131503701210022\n",
      "Epoch: 125890 | Loss: 0.4071313440799713 | Test loss: 0.4131365418434143\n",
      "Epoch: 125900 | Loss: 0.4071195721626282 | Test loss: 0.41312265396118164\n",
      "Epoch: 125910 | Loss: 0.4071078300476074 | Test loss: 0.41310882568359375\n",
      "Epoch: 125920 | Loss: 0.4070960581302643 | Test loss: 0.41309499740600586\n",
      "Epoch: 125930 | Loss: 0.40708428621292114 | Test loss: 0.41308116912841797\n",
      "Epoch: 125940 | Loss: 0.4070725440979004 | Test loss: 0.4130673110485077\n",
      "Epoch: 125950 | Loss: 0.407060831785202 | Test loss: 0.4130534827709198\n",
      "Epoch: 125960 | Loss: 0.4070490002632141 | Test loss: 0.41303959488868713\n",
      "Epoch: 125970 | Loss: 0.40703725814819336 | Test loss: 0.41302576661109924\n",
      "Epoch: 125980 | Loss: 0.407025545835495 | Test loss: 0.41301193833351135\n",
      "Epoch: 125990 | Loss: 0.4070137143135071 | Test loss: 0.4129980504512787\n",
      "Epoch: 126000 | Loss: 0.40700197219848633 | Test loss: 0.4129842221736908\n",
      "Epoch: 126010 | Loss: 0.40699025988578796 | Test loss: 0.4129703640937805\n",
      "Epoch: 126020 | Loss: 0.40697842836380005 | Test loss: 0.4129565358161926\n",
      "Epoch: 126030 | Loss: 0.4069666862487793 | Test loss: 0.41294270753860474\n",
      "Epoch: 126040 | Loss: 0.40695497393608093 | Test loss: 0.41292881965637207\n",
      "Epoch: 126050 | Loss: 0.4069431722164154 | Test loss: 0.4129149913787842\n",
      "Epoch: 126060 | Loss: 0.40693140029907227 | Test loss: 0.4129011332988739\n",
      "Epoch: 126070 | Loss: 0.4069196879863739 | Test loss: 0.412887305021286\n",
      "Epoch: 126080 | Loss: 0.40690794587135315 | Test loss: 0.4128734767436981\n",
      "Epoch: 126090 | Loss: 0.40689611434936523 | Test loss: 0.41285958886146545\n",
      "Epoch: 126100 | Loss: 0.40688440203666687 | Test loss: 0.41284576058387756\n",
      "Epoch: 126110 | Loss: 0.40687260031700134 | Test loss: 0.4128318727016449\n",
      "Epoch: 126120 | Loss: 0.4068608283996582 | Test loss: 0.412818044424057\n",
      "Epoch: 126130 | Loss: 0.40684911608695984 | Test loss: 0.4128042161464691\n",
      "Epoch: 126140 | Loss: 0.4068373143672943 | Test loss: 0.4127903878688812\n",
      "Epoch: 126150 | Loss: 0.40682554244995117 | Test loss: 0.41277652978897095\n",
      "Epoch: 126160 | Loss: 0.4068138301372528 | Test loss: 0.41276270151138306\n",
      "Epoch: 126170 | Loss: 0.40680208802223206 | Test loss: 0.4127488136291504\n",
      "Epoch: 126180 | Loss: 0.4067903161048889 | Test loss: 0.4127349853515625\n",
      "Epoch: 126190 | Loss: 0.4067785441875458 | Test loss: 0.4127211570739746\n",
      "Epoch: 126200 | Loss: 0.406766802072525 | Test loss: 0.41270729899406433\n",
      "Epoch: 126210 | Loss: 0.4067550301551819 | Test loss: 0.41269347071647644\n",
      "Epoch: 126220 | Loss: 0.40674325823783875 | Test loss: 0.4126795828342438\n",
      "Epoch: 126230 | Loss: 0.406731516122818 | Test loss: 0.4126657545566559\n",
      "Epoch: 126240 | Loss: 0.40671974420547485 | Test loss: 0.412651926279068\n",
      "Epoch: 126250 | Loss: 0.4067079722881317 | Test loss: 0.4126380383968353\n",
      "Epoch: 126260 | Loss: 0.40669623017311096 | Test loss: 0.41262421011924744\n",
      "Epoch: 126270 | Loss: 0.4066844582557678 | Test loss: 0.41261035203933716\n",
      "Epoch: 126280 | Loss: 0.4066726863384247 | Test loss: 0.41259652376174927\n",
      "Epoch: 126290 | Loss: 0.40666094422340393 | Test loss: 0.4125826954841614\n",
      "Epoch: 126300 | Loss: 0.4066492021083832 | Test loss: 0.4125688076019287\n",
      "Epoch: 126310 | Loss: 0.40663740038871765 | Test loss: 0.4125549793243408\n",
      "Epoch: 126320 | Loss: 0.4066256582736969 | Test loss: 0.41254115104675293\n",
      "Epoch: 126330 | Loss: 0.40661391615867615 | Test loss: 0.41252729296684265\n",
      "Epoch: 126340 | Loss: 0.406602144241333 | Test loss: 0.41251346468925476\n",
      "Epoch: 126350 | Loss: 0.40659037232398987 | Test loss: 0.41249963641166687\n",
      "Epoch: 126360 | Loss: 0.4065786302089691 | Test loss: 0.4124857485294342\n",
      "Epoch: 126370 | Loss: 0.406566858291626 | Test loss: 0.4124719202518463\n",
      "Epoch: 126380 | Loss: 0.40655508637428284 | Test loss: 0.41245803236961365\n",
      "Epoch: 126390 | Loss: 0.4065433144569397 | Test loss: 0.41244420409202576\n",
      "Epoch: 126400 | Loss: 0.40653157234191895 | Test loss: 0.41243037581443787\n",
      "Epoch: 126410 | Loss: 0.4065198600292206 | Test loss: 0.41241654753685\n",
      "Epoch: 126420 | Loss: 0.40650805830955505 | Test loss: 0.4124026894569397\n",
      "Epoch: 126430 | Loss: 0.4064963459968567 | Test loss: 0.41238880157470703\n",
      "Epoch: 126440 | Loss: 0.40648457407951355 | Test loss: 0.41237497329711914\n",
      "Epoch: 126450 | Loss: 0.406472772359848 | Test loss: 0.41236114501953125\n",
      "Epoch: 126460 | Loss: 0.40646106004714966 | Test loss: 0.41234728693962097\n",
      "Epoch: 126470 | Loss: 0.40644922852516174 | Test loss: 0.4123334586620331\n",
      "Epoch: 126480 | Loss: 0.406437486410141 | Test loss: 0.4123195707798004\n",
      "Epoch: 126490 | Loss: 0.40642571449279785 | Test loss: 0.4123057425022125\n",
      "Epoch: 126500 | Loss: 0.4064139425754547 | Test loss: 0.41229191422462463\n",
      "Epoch: 126510 | Loss: 0.40640220046043396 | Test loss: 0.41227808594703674\n",
      "Epoch: 126520 | Loss: 0.4063904881477356 | Test loss: 0.4122641980648041\n",
      "Epoch: 126530 | Loss: 0.40637871623039246 | Test loss: 0.4122503697872162\n",
      "Epoch: 126540 | Loss: 0.40636691451072693 | Test loss: 0.4122365117073059\n",
      "Epoch: 126550 | Loss: 0.40635520219802856 | Test loss: 0.412222683429718\n",
      "Epoch: 126560 | Loss: 0.4063434302806854 | Test loss: 0.4122088551521301\n",
      "Epoch: 126570 | Loss: 0.4063316285610199 | Test loss: 0.41219502687454224\n",
      "Epoch: 126580 | Loss: 0.40631991624832153 | Test loss: 0.41218113899230957\n",
      "Epoch: 126590 | Loss: 0.4063081443309784 | Test loss: 0.4121673107147217\n",
      "Epoch: 126600 | Loss: 0.40629634261131287 | Test loss: 0.4121534526348114\n",
      "Epoch: 126610 | Loss: 0.4062846302986145 | Test loss: 0.4121396243572235\n",
      "Epoch: 126620 | Loss: 0.40627285838127136 | Test loss: 0.41212573647499084\n",
      "Epoch: 126630 | Loss: 0.4062611162662506 | Test loss: 0.41211190819740295\n",
      "Epoch: 126640 | Loss: 0.40624934434890747 | Test loss: 0.4120980203151703\n",
      "Epoch: 126650 | Loss: 0.4062376022338867 | Test loss: 0.4120841920375824\n",
      "Epoch: 126660 | Loss: 0.4062257707118988 | Test loss: 0.4120703637599945\n",
      "Epoch: 126670 | Loss: 0.40621405839920044 | Test loss: 0.4120565354824066\n",
      "Epoch: 126680 | Loss: 0.4062023162841797 | Test loss: 0.41204267740249634\n",
      "Epoch: 126690 | Loss: 0.40619054436683655 | Test loss: 0.41202884912490845\n",
      "Epoch: 126700 | Loss: 0.4061787724494934 | Test loss: 0.4120149612426758\n",
      "Epoch: 126710 | Loss: 0.40616703033447266 | Test loss: 0.4120011329650879\n",
      "Epoch: 126720 | Loss: 0.4061552584171295 | Test loss: 0.4119873046875\n",
      "Epoch: 126730 | Loss: 0.4061434864997864 | Test loss: 0.4119734764099121\n",
      "Epoch: 126740 | Loss: 0.4061317443847656 | Test loss: 0.41195961833000183\n",
      "Epoch: 126750 | Loss: 0.40612003207206726 | Test loss: 0.41194579005241394\n",
      "Epoch: 126760 | Loss: 0.40610820055007935 | Test loss: 0.4119319021701813\n",
      "Epoch: 126770 | Loss: 0.4060964584350586 | Test loss: 0.4119180738925934\n",
      "Epoch: 126780 | Loss: 0.40608474612236023 | Test loss: 0.4119042456150055\n",
      "Epoch: 126790 | Loss: 0.4060729146003723 | Test loss: 0.4118903577327728\n",
      "Epoch: 126800 | Loss: 0.40606117248535156 | Test loss: 0.41187652945518494\n",
      "Epoch: 126810 | Loss: 0.4060494601726532 | Test loss: 0.41186267137527466\n",
      "Epoch: 126820 | Loss: 0.4060376286506653 | Test loss: 0.41184884309768677\n",
      "Epoch: 126830 | Loss: 0.40602588653564453 | Test loss: 0.4118350148200989\n",
      "Epoch: 126840 | Loss: 0.40601417422294617 | Test loss: 0.4118211269378662\n",
      "Epoch: 126850 | Loss: 0.40600237250328064 | Test loss: 0.4118072986602783\n",
      "Epoch: 126860 | Loss: 0.4059906005859375 | Test loss: 0.41179344058036804\n",
      "Epoch: 126870 | Loss: 0.40597888827323914 | Test loss: 0.41177961230278015\n",
      "Epoch: 126880 | Loss: 0.4059671461582184 | Test loss: 0.41176578402519226\n",
      "Epoch: 126890 | Loss: 0.40595531463623047 | Test loss: 0.4117518961429596\n",
      "Epoch: 126900 | Loss: 0.4059436023235321 | Test loss: 0.4117380678653717\n",
      "Epoch: 126910 | Loss: 0.4059318006038666 | Test loss: 0.41172417998313904\n",
      "Epoch: 126920 | Loss: 0.40592002868652344 | Test loss: 0.41171035170555115\n",
      "Epoch: 126930 | Loss: 0.4059083163738251 | Test loss: 0.41169652342796326\n",
      "Epoch: 126940 | Loss: 0.40589651465415955 | Test loss: 0.41168269515037537\n",
      "Epoch: 126950 | Loss: 0.4058847427368164 | Test loss: 0.4116688370704651\n",
      "Epoch: 126960 | Loss: 0.40587303042411804 | Test loss: 0.4116550087928772\n",
      "Epoch: 126970 | Loss: 0.4058612883090973 | Test loss: 0.41164112091064453\n",
      "Epoch: 126980 | Loss: 0.40584951639175415 | Test loss: 0.41162729263305664\n",
      "Epoch: 126990 | Loss: 0.405837744474411 | Test loss: 0.41161346435546875\n",
      "Epoch: 127000 | Loss: 0.40582600235939026 | Test loss: 0.41159960627555847\n",
      "Epoch: 127010 | Loss: 0.4058142304420471 | Test loss: 0.4115857779979706\n",
      "Epoch: 127020 | Loss: 0.405802458524704 | Test loss: 0.4115718901157379\n",
      "Epoch: 127030 | Loss: 0.4057907164096832 | Test loss: 0.41155806183815\n",
      "Epoch: 127040 | Loss: 0.4057789444923401 | Test loss: 0.41154423356056213\n",
      "Epoch: 127050 | Loss: 0.40576717257499695 | Test loss: 0.41153034567832947\n",
      "Epoch: 127060 | Loss: 0.4057554304599762 | Test loss: 0.4115165174007416\n",
      "Epoch: 127070 | Loss: 0.40574365854263306 | Test loss: 0.4115026593208313\n",
      "Epoch: 127080 | Loss: 0.4057318866252899 | Test loss: 0.4114888310432434\n",
      "Epoch: 127090 | Loss: 0.40572014451026917 | Test loss: 0.4114750027656555\n",
      "Epoch: 127100 | Loss: 0.4057084023952484 | Test loss: 0.41146111488342285\n",
      "Epoch: 127110 | Loss: 0.4056966006755829 | Test loss: 0.41144728660583496\n",
      "Epoch: 127120 | Loss: 0.40568485856056213 | Test loss: 0.41143345832824707\n",
      "Epoch: 127130 | Loss: 0.4056731164455414 | Test loss: 0.4114196002483368\n",
      "Epoch: 127140 | Loss: 0.40566134452819824 | Test loss: 0.4114057719707489\n",
      "Epoch: 127150 | Loss: 0.4056495726108551 | Test loss: 0.411391943693161\n",
      "Epoch: 127160 | Loss: 0.40563783049583435 | Test loss: 0.41137805581092834\n",
      "Epoch: 127170 | Loss: 0.4056260585784912 | Test loss: 0.41136422753334045\n",
      "Epoch: 127180 | Loss: 0.40561428666114807 | Test loss: 0.4113503396511078\n",
      "Epoch: 127190 | Loss: 0.40560251474380493 | Test loss: 0.4113365113735199\n",
      "Epoch: 127200 | Loss: 0.4055907726287842 | Test loss: 0.411322683095932\n",
      "Epoch: 127210 | Loss: 0.4055790603160858 | Test loss: 0.4113088548183441\n",
      "Epoch: 127220 | Loss: 0.4055672585964203 | Test loss: 0.41129499673843384\n",
      "Epoch: 127230 | Loss: 0.40555548667907715 | Test loss: 0.41128110885620117\n",
      "Epoch: 127240 | Loss: 0.4055437743663788 | Test loss: 0.4112672805786133\n",
      "Epoch: 127250 | Loss: 0.40553197264671326 | Test loss: 0.4112534523010254\n",
      "Epoch: 127260 | Loss: 0.4055202603340149 | Test loss: 0.4112395942211151\n",
      "Epoch: 127270 | Loss: 0.405508428812027 | Test loss: 0.4112257659435272\n",
      "Epoch: 127280 | Loss: 0.4054966866970062 | Test loss: 0.41121187806129456\n",
      "Epoch: 127290 | Loss: 0.4054849147796631 | Test loss: 0.41119804978370667\n",
      "Epoch: 127300 | Loss: 0.40547314286231995 | Test loss: 0.4111842215061188\n",
      "Epoch: 127310 | Loss: 0.4054614007472992 | Test loss: 0.4111703932285309\n",
      "Epoch: 127320 | Loss: 0.40544968843460083 | Test loss: 0.4111565053462982\n",
      "Epoch: 127330 | Loss: 0.4054379165172577 | Test loss: 0.4111426770687103\n",
      "Epoch: 127340 | Loss: 0.40542611479759216 | Test loss: 0.41112881898880005\n",
      "Epoch: 127350 | Loss: 0.4054144024848938 | Test loss: 0.41111499071121216\n",
      "Epoch: 127360 | Loss: 0.40540263056755066 | Test loss: 0.41110116243362427\n",
      "Epoch: 127370 | Loss: 0.40539082884788513 | Test loss: 0.4110873341560364\n",
      "Epoch: 127380 | Loss: 0.40537911653518677 | Test loss: 0.4110734462738037\n",
      "Epoch: 127390 | Loss: 0.40536734461784363 | Test loss: 0.4110596179962158\n",
      "Epoch: 127400 | Loss: 0.4053555428981781 | Test loss: 0.41104575991630554\n",
      "Epoch: 127410 | Loss: 0.40534383058547974 | Test loss: 0.41103193163871765\n",
      "Epoch: 127420 | Loss: 0.4053320586681366 | Test loss: 0.411018043756485\n",
      "Epoch: 127430 | Loss: 0.40532031655311584 | Test loss: 0.4110042154788971\n",
      "Epoch: 127440 | Loss: 0.4053085446357727 | Test loss: 0.41099032759666443\n",
      "Epoch: 127450 | Loss: 0.40529680252075195 | Test loss: 0.41097649931907654\n",
      "Epoch: 127460 | Loss: 0.40528497099876404 | Test loss: 0.41096267104148865\n",
      "Epoch: 127470 | Loss: 0.4052732586860657 | Test loss: 0.41094884276390076\n",
      "Epoch: 127480 | Loss: 0.4052615165710449 | Test loss: 0.4109349846839905\n",
      "Epoch: 127490 | Loss: 0.4052497446537018 | Test loss: 0.4109211564064026\n",
      "Epoch: 127500 | Loss: 0.40523797273635864 | Test loss: 0.4109072685241699\n",
      "Epoch: 127510 | Loss: 0.4052262306213379 | Test loss: 0.41089344024658203\n",
      "Epoch: 127520 | Loss: 0.40521445870399475 | Test loss: 0.41087961196899414\n",
      "Epoch: 127530 | Loss: 0.4052026867866516 | Test loss: 0.41086578369140625\n",
      "Epoch: 127540 | Loss: 0.40519094467163086 | Test loss: 0.41085192561149597\n",
      "Epoch: 127550 | Loss: 0.4051792323589325 | Test loss: 0.4108380973339081\n",
      "Epoch: 127560 | Loss: 0.4051674008369446 | Test loss: 0.4108242094516754\n",
      "Epoch: 127570 | Loss: 0.40515565872192383 | Test loss: 0.4108103811740875\n",
      "Epoch: 127580 | Loss: 0.40514394640922546 | Test loss: 0.41079655289649963\n",
      "Epoch: 127590 | Loss: 0.40513211488723755 | Test loss: 0.41078266501426697\n",
      "Epoch: 127600 | Loss: 0.4051203727722168 | Test loss: 0.4107688367366791\n",
      "Epoch: 127610 | Loss: 0.40510866045951843 | Test loss: 0.4107549786567688\n",
      "Epoch: 127620 | Loss: 0.4050968289375305 | Test loss: 0.4107411503791809\n",
      "Epoch: 127630 | Loss: 0.40508508682250977 | Test loss: 0.410727322101593\n",
      "Epoch: 127640 | Loss: 0.4050733745098114 | Test loss: 0.41071343421936035\n",
      "Epoch: 127650 | Loss: 0.4050615727901459 | Test loss: 0.41069960594177246\n",
      "Epoch: 127660 | Loss: 0.40504980087280273 | Test loss: 0.4106857478618622\n",
      "Epoch: 127670 | Loss: 0.40503808856010437 | Test loss: 0.4106719195842743\n",
      "Epoch: 127680 | Loss: 0.4050263464450836 | Test loss: 0.4106580913066864\n",
      "Epoch: 127690 | Loss: 0.4050145149230957 | Test loss: 0.41064420342445374\n",
      "Epoch: 127700 | Loss: 0.40500280261039734 | Test loss: 0.41063037514686584\n",
      "Epoch: 127710 | Loss: 0.4049910008907318 | Test loss: 0.4106164872646332\n",
      "Epoch: 127720 | Loss: 0.40497922897338867 | Test loss: 0.4106026589870453\n",
      "Epoch: 127730 | Loss: 0.4049675166606903 | Test loss: 0.4105888307094574\n",
      "Epoch: 127740 | Loss: 0.4049557149410248 | Test loss: 0.4105750024318695\n",
      "Epoch: 127750 | Loss: 0.40494394302368164 | Test loss: 0.41056114435195923\n",
      "Epoch: 127760 | Loss: 0.4049322307109833 | Test loss: 0.41054731607437134\n",
      "Epoch: 127770 | Loss: 0.4049204885959625 | Test loss: 0.41053342819213867\n",
      "Epoch: 127780 | Loss: 0.4049087166786194 | Test loss: 0.4105195999145508\n",
      "Epoch: 127790 | Loss: 0.40489694476127625 | Test loss: 0.4105057716369629\n",
      "Epoch: 127800 | Loss: 0.4048852026462555 | Test loss: 0.4104919135570526\n",
      "Epoch: 127810 | Loss: 0.40487343072891235 | Test loss: 0.4104780852794647\n",
      "Epoch: 127820 | Loss: 0.4048616588115692 | Test loss: 0.41046419739723206\n",
      "Epoch: 127830 | Loss: 0.40484991669654846 | Test loss: 0.41045036911964417\n",
      "Epoch: 127840 | Loss: 0.4048381447792053 | Test loss: 0.4104365408420563\n",
      "Epoch: 127850 | Loss: 0.4048263728618622 | Test loss: 0.4104226529598236\n",
      "Epoch: 127860 | Loss: 0.40481463074684143 | Test loss: 0.4104088246822357\n",
      "Epoch: 127870 | Loss: 0.4048028588294983 | Test loss: 0.41039496660232544\n",
      "Epoch: 127880 | Loss: 0.40479108691215515 | Test loss: 0.41038113832473755\n",
      "Epoch: 127890 | Loss: 0.4047793447971344 | Test loss: 0.41036731004714966\n",
      "Epoch: 127900 | Loss: 0.40476760268211365 | Test loss: 0.410353422164917\n",
      "Epoch: 127910 | Loss: 0.4047558009624481 | Test loss: 0.4103395938873291\n",
      "Epoch: 127920 | Loss: 0.40474405884742737 | Test loss: 0.4103257656097412\n",
      "Epoch: 127930 | Loss: 0.40473228693008423 | Test loss: 0.41031190752983093\n",
      "Epoch: 127940 | Loss: 0.4047205448150635 | Test loss: 0.41029807925224304\n",
      "Epoch: 127950 | Loss: 0.40470877289772034 | Test loss: 0.41028425097465515\n",
      "Epoch: 127960 | Loss: 0.4046970307826996 | Test loss: 0.4102703630924225\n",
      "Epoch: 127970 | Loss: 0.40468525886535645 | Test loss: 0.4102565348148346\n",
      "Epoch: 127980 | Loss: 0.4046734869480133 | Test loss: 0.41024264693260193\n",
      "Epoch: 127990 | Loss: 0.40466171503067017 | Test loss: 0.41022881865501404\n",
      "Epoch: 128000 | Loss: 0.4046499729156494 | Test loss: 0.41021499037742615\n",
      "Epoch: 128010 | Loss: 0.40463826060295105 | Test loss: 0.41020116209983826\n",
      "Epoch: 128020 | Loss: 0.4046264588832855 | Test loss: 0.410187304019928\n",
      "Epoch: 128030 | Loss: 0.4046146869659424 | Test loss: 0.4101734161376953\n",
      "Epoch: 128040 | Loss: 0.404602974653244 | Test loss: 0.4101595878601074\n",
      "Epoch: 128050 | Loss: 0.4045911729335785 | Test loss: 0.41014575958251953\n",
      "Epoch: 128060 | Loss: 0.4045794606208801 | Test loss: 0.41013190150260925\n",
      "Epoch: 128070 | Loss: 0.4045676290988922 | Test loss: 0.41011807322502136\n",
      "Epoch: 128080 | Loss: 0.40455588698387146 | Test loss: 0.4101041853427887\n",
      "Epoch: 128090 | Loss: 0.4045441150665283 | Test loss: 0.4100903570652008\n",
      "Epoch: 128100 | Loss: 0.4045323431491852 | Test loss: 0.4100765287876129\n",
      "Epoch: 128110 | Loss: 0.40452060103416443 | Test loss: 0.410062700510025\n",
      "Epoch: 128120 | Loss: 0.40450888872146606 | Test loss: 0.41004881262779236\n",
      "Epoch: 128130 | Loss: 0.4044971168041229 | Test loss: 0.41003498435020447\n",
      "Epoch: 128140 | Loss: 0.4044853150844574 | Test loss: 0.4100211262702942\n",
      "Epoch: 128150 | Loss: 0.40447360277175903 | Test loss: 0.4100072979927063\n",
      "Epoch: 128160 | Loss: 0.4044618308544159 | Test loss: 0.4099934697151184\n",
      "Epoch: 128170 | Loss: 0.40445002913475037 | Test loss: 0.4099796414375305\n",
      "Epoch: 128180 | Loss: 0.404438316822052 | Test loss: 0.40996575355529785\n",
      "Epoch: 128190 | Loss: 0.40442654490470886 | Test loss: 0.40995192527770996\n",
      "Epoch: 128200 | Loss: 0.40441474318504333 | Test loss: 0.4099380671977997\n",
      "Epoch: 128210 | Loss: 0.40440303087234497 | Test loss: 0.4099242389202118\n",
      "Epoch: 128220 | Loss: 0.40439125895500183 | Test loss: 0.4099103510379791\n",
      "Epoch: 128230 | Loss: 0.4043795168399811 | Test loss: 0.40989652276039124\n",
      "Epoch: 128240 | Loss: 0.40436774492263794 | Test loss: 0.40988263487815857\n",
      "Epoch: 128250 | Loss: 0.4043560028076172 | Test loss: 0.4098688066005707\n",
      "Epoch: 128260 | Loss: 0.4043441712856293 | Test loss: 0.4098549783229828\n",
      "Epoch: 128270 | Loss: 0.4043324589729309 | Test loss: 0.4098411500453949\n",
      "Epoch: 128280 | Loss: 0.40432071685791016 | Test loss: 0.4098272919654846\n",
      "Epoch: 128290 | Loss: 0.404308944940567 | Test loss: 0.40981346368789673\n",
      "Epoch: 128300 | Loss: 0.4042971730232239 | Test loss: 0.40979957580566406\n",
      "Epoch: 128310 | Loss: 0.4042854309082031 | Test loss: 0.40978574752807617\n",
      "Epoch: 128320 | Loss: 0.40427365899086 | Test loss: 0.4097719192504883\n",
      "Epoch: 128330 | Loss: 0.40426188707351685 | Test loss: 0.4097580909729004\n",
      "Epoch: 128340 | Loss: 0.4042501449584961 | Test loss: 0.4097442328929901\n",
      "Epoch: 128350 | Loss: 0.40423843264579773 | Test loss: 0.4097304046154022\n",
      "Epoch: 128360 | Loss: 0.4042266011238098 | Test loss: 0.40971651673316956\n",
      "Epoch: 128370 | Loss: 0.40421485900878906 | Test loss: 0.40970268845558167\n",
      "Epoch: 128380 | Loss: 0.4042031466960907 | Test loss: 0.4096888601779938\n",
      "Epoch: 128390 | Loss: 0.4041913151741028 | Test loss: 0.4096749722957611\n",
      "Epoch: 128400 | Loss: 0.40417957305908203 | Test loss: 0.4096611440181732\n",
      "Epoch: 128410 | Loss: 0.40416786074638367 | Test loss: 0.40964728593826294\n",
      "Epoch: 128420 | Loss: 0.40415602922439575 | Test loss: 0.40963345766067505\n",
      "Epoch: 128430 | Loss: 0.404144287109375 | Test loss: 0.40961962938308716\n",
      "Epoch: 128440 | Loss: 0.40413257479667664 | Test loss: 0.4096057415008545\n",
      "Epoch: 128450 | Loss: 0.4041207730770111 | Test loss: 0.4095919132232666\n",
      "Epoch: 128460 | Loss: 0.40410900115966797 | Test loss: 0.4095780551433563\n",
      "Epoch: 128470 | Loss: 0.4040972888469696 | Test loss: 0.40956422686576843\n",
      "Epoch: 128480 | Loss: 0.40408554673194885 | Test loss: 0.40955039858818054\n",
      "Epoch: 128490 | Loss: 0.40407371520996094 | Test loss: 0.4095365107059479\n",
      "Epoch: 128500 | Loss: 0.4040620028972626 | Test loss: 0.40952268242836\n",
      "Epoch: 128510 | Loss: 0.40405020117759705 | Test loss: 0.4095087945461273\n",
      "Epoch: 128520 | Loss: 0.4040384292602539 | Test loss: 0.40949496626853943\n",
      "Epoch: 128530 | Loss: 0.40402671694755554 | Test loss: 0.40948113799095154\n",
      "Epoch: 128540 | Loss: 0.40401491522789 | Test loss: 0.40946730971336365\n",
      "Epoch: 128550 | Loss: 0.4040031433105469 | Test loss: 0.40945345163345337\n",
      "Epoch: 128560 | Loss: 0.4039914309978485 | Test loss: 0.4094396233558655\n",
      "Epoch: 128570 | Loss: 0.40397968888282776 | Test loss: 0.4094257354736328\n",
      "Epoch: 128580 | Loss: 0.4039679169654846 | Test loss: 0.4094119071960449\n",
      "Epoch: 128590 | Loss: 0.4039561450481415 | Test loss: 0.40939807891845703\n",
      "Epoch: 128600 | Loss: 0.4039444029331207 | Test loss: 0.40938422083854675\n",
      "Epoch: 128610 | Loss: 0.4039326310157776 | Test loss: 0.40937039256095886\n",
      "Epoch: 128620 | Loss: 0.40392085909843445 | Test loss: 0.4093565046787262\n",
      "Epoch: 128630 | Loss: 0.4039091169834137 | Test loss: 0.4093426764011383\n",
      "Epoch: 128640 | Loss: 0.40389734506607056 | Test loss: 0.4093288481235504\n",
      "Epoch: 128650 | Loss: 0.4038855731487274 | Test loss: 0.40931496024131775\n",
      "Epoch: 128660 | Loss: 0.40387383103370667 | Test loss: 0.40930113196372986\n",
      "Epoch: 128670 | Loss: 0.4038620591163635 | Test loss: 0.4092872738838196\n",
      "Epoch: 128680 | Loss: 0.4038502871990204 | Test loss: 0.4092734456062317\n",
      "Epoch: 128690 | Loss: 0.40383854508399963 | Test loss: 0.4092596173286438\n",
      "Epoch: 128700 | Loss: 0.4038268029689789 | Test loss: 0.40924572944641113\n",
      "Epoch: 128710 | Loss: 0.40381500124931335 | Test loss: 0.40923190116882324\n",
      "Epoch: 128720 | Loss: 0.4038032591342926 | Test loss: 0.40921807289123535\n",
      "Epoch: 128730 | Loss: 0.40379148721694946 | Test loss: 0.4092042148113251\n",
      "Epoch: 128740 | Loss: 0.4037797451019287 | Test loss: 0.4091903865337372\n",
      "Epoch: 128750 | Loss: 0.40376797318458557 | Test loss: 0.4091765582561493\n",
      "Epoch: 128760 | Loss: 0.4037562310695648 | Test loss: 0.4091626703739166\n",
      "Epoch: 128770 | Loss: 0.4037444591522217 | Test loss: 0.40914884209632874\n",
      "Epoch: 128780 | Loss: 0.40373268723487854 | Test loss: 0.40913495421409607\n",
      "Epoch: 128790 | Loss: 0.4037209153175354 | Test loss: 0.4091211259365082\n",
      "Epoch: 128800 | Loss: 0.40370917320251465 | Test loss: 0.4091072976589203\n",
      "Epoch: 128810 | Loss: 0.4036974608898163 | Test loss: 0.4090934693813324\n",
      "Epoch: 128820 | Loss: 0.40368565917015076 | Test loss: 0.4090796113014221\n",
      "Epoch: 128830 | Loss: 0.4036738872528076 | Test loss: 0.40906572341918945\n",
      "Epoch: 128840 | Loss: 0.40366217494010925 | Test loss: 0.40905189514160156\n",
      "Epoch: 128850 | Loss: 0.4036503732204437 | Test loss: 0.40903806686401367\n",
      "Epoch: 128860 | Loss: 0.40363866090774536 | Test loss: 0.4090242087841034\n",
      "Epoch: 128870 | Loss: 0.40362682938575745 | Test loss: 0.4090103805065155\n",
      "Epoch: 128880 | Loss: 0.4036150872707367 | Test loss: 0.40899649262428284\n",
      "Epoch: 128890 | Loss: 0.40360331535339355 | Test loss: 0.40898266434669495\n",
      "Epoch: 128900 | Loss: 0.4035915434360504 | Test loss: 0.40896883606910706\n",
      "Epoch: 128910 | Loss: 0.40357980132102966 | Test loss: 0.40895500779151917\n",
      "Epoch: 128920 | Loss: 0.4035680890083313 | Test loss: 0.4089411199092865\n",
      "Epoch: 128930 | Loss: 0.40355631709098816 | Test loss: 0.4089272916316986\n",
      "Epoch: 128940 | Loss: 0.40354451537132263 | Test loss: 0.40891343355178833\n",
      "Epoch: 128950 | Loss: 0.40353280305862427 | Test loss: 0.40889960527420044\n",
      "Epoch: 128960 | Loss: 0.40352103114128113 | Test loss: 0.40888577699661255\n",
      "Epoch: 128970 | Loss: 0.4035092294216156 | Test loss: 0.40887194871902466\n",
      "Epoch: 128980 | Loss: 0.40349751710891724 | Test loss: 0.408858060836792\n",
      "Epoch: 128990 | Loss: 0.4034857451915741 | Test loss: 0.4088442325592041\n",
      "Epoch: 129000 | Loss: 0.40347394347190857 | Test loss: 0.4088303744792938\n",
      "Epoch: 129010 | Loss: 0.4034622311592102 | Test loss: 0.40881654620170593\n",
      "Epoch: 129020 | Loss: 0.40345045924186707 | Test loss: 0.40880265831947327\n",
      "Epoch: 129030 | Loss: 0.4034387171268463 | Test loss: 0.4087888300418854\n",
      "Epoch: 129040 | Loss: 0.4034269452095032 | Test loss: 0.4087749421596527\n",
      "Epoch: 129050 | Loss: 0.4034152030944824 | Test loss: 0.4087611138820648\n",
      "Epoch: 129060 | Loss: 0.4034033715724945 | Test loss: 0.40874728560447693\n",
      "Epoch: 129070 | Loss: 0.40339165925979614 | Test loss: 0.40873345732688904\n",
      "Epoch: 129080 | Loss: 0.4033799171447754 | Test loss: 0.40871959924697876\n",
      "Epoch: 129090 | Loss: 0.40336814522743225 | Test loss: 0.40870577096939087\n",
      "Epoch: 129100 | Loss: 0.4033563733100891 | Test loss: 0.4086918830871582\n",
      "Epoch: 129110 | Loss: 0.40334463119506836 | Test loss: 0.4086780548095703\n",
      "Epoch: 129120 | Loss: 0.4033328592777252 | Test loss: 0.4086642265319824\n",
      "Epoch: 129130 | Loss: 0.4033210873603821 | Test loss: 0.40865039825439453\n",
      "Epoch: 129140 | Loss: 0.40330934524536133 | Test loss: 0.40863654017448425\n",
      "Epoch: 129150 | Loss: 0.40329763293266296 | Test loss: 0.40862271189689636\n",
      "Epoch: 129160 | Loss: 0.40328580141067505 | Test loss: 0.4086088240146637\n",
      "Epoch: 129170 | Loss: 0.4032740592956543 | Test loss: 0.4085949957370758\n",
      "Epoch: 129180 | Loss: 0.40326234698295593 | Test loss: 0.4085811674594879\n",
      "Epoch: 129190 | Loss: 0.403250515460968 | Test loss: 0.40856727957725525\n",
      "Epoch: 129200 | Loss: 0.40323877334594727 | Test loss: 0.40855345129966736\n",
      "Epoch: 129210 | Loss: 0.4032270610332489 | Test loss: 0.4085395932197571\n",
      "Epoch: 129220 | Loss: 0.403215229511261 | Test loss: 0.4085257649421692\n",
      "Epoch: 129230 | Loss: 0.40320348739624023 | Test loss: 0.4085119366645813\n",
      "Epoch: 129240 | Loss: 0.40319177508354187 | Test loss: 0.40849804878234863\n",
      "Epoch: 129250 | Loss: 0.40317997336387634 | Test loss: 0.40848422050476074\n",
      "Epoch: 129260 | Loss: 0.4031682014465332 | Test loss: 0.40847036242485046\n",
      "Epoch: 129270 | Loss: 0.40315648913383484 | Test loss: 0.4084565341472626\n",
      "Epoch: 129280 | Loss: 0.4031447470188141 | Test loss: 0.4084427058696747\n",
      "Epoch: 129290 | Loss: 0.40313291549682617 | Test loss: 0.408428817987442\n",
      "Epoch: 129300 | Loss: 0.4031212031841278 | Test loss: 0.4084149897098541\n",
      "Epoch: 129310 | Loss: 0.4031094014644623 | Test loss: 0.40840110182762146\n",
      "Epoch: 129320 | Loss: 0.40309762954711914 | Test loss: 0.40838727355003357\n",
      "Epoch: 129330 | Loss: 0.4030859172344208 | Test loss: 0.4083734452724457\n",
      "Epoch: 129340 | Loss: 0.40307411551475525 | Test loss: 0.4083596169948578\n",
      "Epoch: 129350 | Loss: 0.4030623435974121 | Test loss: 0.4083457589149475\n",
      "Epoch: 129360 | Loss: 0.40305063128471375 | Test loss: 0.4083319306373596\n",
      "Epoch: 129370 | Loss: 0.403038889169693 | Test loss: 0.40831804275512695\n",
      "Epoch: 129380 | Loss: 0.40302711725234985 | Test loss: 0.40830421447753906\n",
      "Epoch: 129390 | Loss: 0.4030153453350067 | Test loss: 0.40829038619995117\n",
      "Epoch: 129400 | Loss: 0.40300360321998596 | Test loss: 0.4082765281200409\n",
      "Epoch: 129410 | Loss: 0.4029918313026428 | Test loss: 0.408262699842453\n",
      "Epoch: 129420 | Loss: 0.4029800593852997 | Test loss: 0.40824881196022034\n",
      "Epoch: 129430 | Loss: 0.40296831727027893 | Test loss: 0.40823498368263245\n",
      "Epoch: 129440 | Loss: 0.4029565453529358 | Test loss: 0.40822115540504456\n",
      "Epoch: 129450 | Loss: 0.40294477343559265 | Test loss: 0.4082072675228119\n",
      "Epoch: 129460 | Loss: 0.4029330313205719 | Test loss: 0.408193439245224\n",
      "Epoch: 129470 | Loss: 0.40292125940322876 | Test loss: 0.4081795811653137\n",
      "Epoch: 129480 | Loss: 0.4029094874858856 | Test loss: 0.40816575288772583\n",
      "Epoch: 129490 | Loss: 0.40289774537086487 | Test loss: 0.40815192461013794\n",
      "Epoch: 129500 | Loss: 0.4028860032558441 | Test loss: 0.4081380367279053\n",
      "Epoch: 129510 | Loss: 0.4028742015361786 | Test loss: 0.4081242084503174\n",
      "Epoch: 129520 | Loss: 0.40286245942115784 | Test loss: 0.4081103801727295\n",
      "Epoch: 129530 | Loss: 0.4028506875038147 | Test loss: 0.4080965220928192\n",
      "Epoch: 129540 | Loss: 0.40283894538879395 | Test loss: 0.4080826938152313\n",
      "Epoch: 129550 | Loss: 0.4028271734714508 | Test loss: 0.40806886553764343\n",
      "Epoch: 129560 | Loss: 0.40281543135643005 | Test loss: 0.40805497765541077\n",
      "Epoch: 129570 | Loss: 0.4028036594390869 | Test loss: 0.4080411493778229\n",
      "Epoch: 129580 | Loss: 0.4027918875217438 | Test loss: 0.4080272614955902\n",
      "Epoch: 129590 | Loss: 0.40278011560440063 | Test loss: 0.4080134332180023\n",
      "Epoch: 129600 | Loss: 0.4027683734893799 | Test loss: 0.40799960494041443\n",
      "Epoch: 129610 | Loss: 0.4027566611766815 | Test loss: 0.40798577666282654\n",
      "Epoch: 129620 | Loss: 0.402744859457016 | Test loss: 0.40797191858291626\n",
      "Epoch: 129630 | Loss: 0.40273308753967285 | Test loss: 0.4079580307006836\n",
      "Epoch: 129640 | Loss: 0.4027213752269745 | Test loss: 0.4079442024230957\n",
      "Epoch: 129650 | Loss: 0.40270957350730896 | Test loss: 0.4079303741455078\n",
      "Epoch: 129660 | Loss: 0.4026978611946106 | Test loss: 0.40791651606559753\n",
      "Epoch: 129670 | Loss: 0.4026860296726227 | Test loss: 0.40790268778800964\n",
      "Epoch: 129680 | Loss: 0.40267428755760193 | Test loss: 0.407888799905777\n",
      "Epoch: 129690 | Loss: 0.4026625156402588 | Test loss: 0.4078749716281891\n",
      "Epoch: 129700 | Loss: 0.40265074372291565 | Test loss: 0.4078611433506012\n",
      "Epoch: 129710 | Loss: 0.4026390016078949 | Test loss: 0.4078473150730133\n",
      "Epoch: 129720 | Loss: 0.40262728929519653 | Test loss: 0.40783342719078064\n",
      "Epoch: 129730 | Loss: 0.4026155173778534 | Test loss: 0.40781959891319275\n",
      "Epoch: 129740 | Loss: 0.40260371565818787 | Test loss: 0.40780574083328247\n",
      "Epoch: 129750 | Loss: 0.4025920033454895 | Test loss: 0.4077919125556946\n",
      "Epoch: 129760 | Loss: 0.40258023142814636 | Test loss: 0.4077780842781067\n",
      "Epoch: 129770 | Loss: 0.40256842970848083 | Test loss: 0.4077642560005188\n",
      "Epoch: 129780 | Loss: 0.40255671739578247 | Test loss: 0.40775036811828613\n",
      "Epoch: 129790 | Loss: 0.40254494547843933 | Test loss: 0.40773653984069824\n",
      "Epoch: 129800 | Loss: 0.4025331437587738 | Test loss: 0.40772268176078796\n",
      "Epoch: 129810 | Loss: 0.40252143144607544 | Test loss: 0.4077088534832001\n",
      "Epoch: 129820 | Loss: 0.4025096595287323 | Test loss: 0.4076949656009674\n",
      "Epoch: 129830 | Loss: 0.40249791741371155 | Test loss: 0.4076811373233795\n",
      "Epoch: 129840 | Loss: 0.4024861454963684 | Test loss: 0.40766724944114685\n",
      "Epoch: 129850 | Loss: 0.40247440338134766 | Test loss: 0.40765342116355896\n",
      "Epoch: 129860 | Loss: 0.40246257185935974 | Test loss: 0.40763959288597107\n",
      "Epoch: 129870 | Loss: 0.4024508595466614 | Test loss: 0.4076257646083832\n",
      "Epoch: 129880 | Loss: 0.4024391174316406 | Test loss: 0.4076119065284729\n",
      "Epoch: 129890 | Loss: 0.4024273455142975 | Test loss: 0.407598078250885\n",
      "Epoch: 129900 | Loss: 0.40241557359695435 | Test loss: 0.40758419036865234\n",
      "Epoch: 129910 | Loss: 0.4024038314819336 | Test loss: 0.40757036209106445\n",
      "Epoch: 129920 | Loss: 0.40239205956459045 | Test loss: 0.40755653381347656\n",
      "Epoch: 129930 | Loss: 0.4023802876472473 | Test loss: 0.40754270553588867\n",
      "Epoch: 129940 | Loss: 0.40236854553222656 | Test loss: 0.4075288474559784\n",
      "Epoch: 129950 | Loss: 0.4023568332195282 | Test loss: 0.4075150191783905\n",
      "Epoch: 129960 | Loss: 0.4023450016975403 | Test loss: 0.40750113129615784\n",
      "Epoch: 129970 | Loss: 0.40233325958251953 | Test loss: 0.40748730301856995\n",
      "Epoch: 129980 | Loss: 0.40232154726982117 | Test loss: 0.40747347474098206\n",
      "Epoch: 129990 | Loss: 0.40230971574783325 | Test loss: 0.4074595868587494\n",
      "Epoch: 130000 | Loss: 0.4022979736328125 | Test loss: 0.4074457585811615\n",
      "Epoch: 130010 | Loss: 0.40228626132011414 | Test loss: 0.4074319005012512\n",
      "Epoch: 130020 | Loss: 0.4022744297981262 | Test loss: 0.40741807222366333\n",
      "Epoch: 130030 | Loss: 0.40226268768310547 | Test loss: 0.40740424394607544\n",
      "Epoch: 130040 | Loss: 0.4022509753704071 | Test loss: 0.4073903560638428\n",
      "Epoch: 130050 | Loss: 0.4022391736507416 | Test loss: 0.4073765277862549\n",
      "Epoch: 130060 | Loss: 0.40222740173339844 | Test loss: 0.4073626697063446\n",
      "Epoch: 130070 | Loss: 0.4022156894207001 | Test loss: 0.4073488414287567\n",
      "Epoch: 130080 | Loss: 0.4022039473056793 | Test loss: 0.4073350131511688\n",
      "Epoch: 130090 | Loss: 0.4021921157836914 | Test loss: 0.40732112526893616\n",
      "Epoch: 130100 | Loss: 0.40218040347099304 | Test loss: 0.40730729699134827\n",
      "Epoch: 130110 | Loss: 0.4021686017513275 | Test loss: 0.4072934091091156\n",
      "Epoch: 130120 | Loss: 0.4021568298339844 | Test loss: 0.4072795808315277\n",
      "Epoch: 130130 | Loss: 0.402145117521286 | Test loss: 0.4072657525539398\n",
      "Epoch: 130140 | Loss: 0.4021333158016205 | Test loss: 0.40725192427635193\n",
      "Epoch: 130150 | Loss: 0.40212154388427734 | Test loss: 0.40723806619644165\n",
      "Epoch: 130160 | Loss: 0.402109831571579 | Test loss: 0.40722423791885376\n",
      "Epoch: 130170 | Loss: 0.4020980894565582 | Test loss: 0.4072103500366211\n",
      "Epoch: 130180 | Loss: 0.4020863175392151 | Test loss: 0.4071965217590332\n",
      "Epoch: 130190 | Loss: 0.40207454562187195 | Test loss: 0.4071826934814453\n",
      "Epoch: 130200 | Loss: 0.4020628035068512 | Test loss: 0.40716883540153503\n",
      "Epoch: 130210 | Loss: 0.40205103158950806 | Test loss: 0.40715500712394714\n",
      "Epoch: 130220 | Loss: 0.4020392596721649 | Test loss: 0.4071411192417145\n",
      "Epoch: 130230 | Loss: 0.40202751755714417 | Test loss: 0.4071272909641266\n",
      "Epoch: 130240 | Loss: 0.402015745639801 | Test loss: 0.4071134626865387\n",
      "Epoch: 130250 | Loss: 0.4020039737224579 | Test loss: 0.40709957480430603\n",
      "Epoch: 130260 | Loss: 0.40199223160743713 | Test loss: 0.40708574652671814\n",
      "Epoch: 130270 | Loss: 0.401980459690094 | Test loss: 0.40707188844680786\n",
      "Epoch: 130280 | Loss: 0.40196868777275085 | Test loss: 0.40705806016921997\n",
      "Epoch: 130290 | Loss: 0.4019569456577301 | Test loss: 0.4070442318916321\n",
      "Epoch: 130300 | Loss: 0.40194520354270935 | Test loss: 0.4070303440093994\n",
      "Epoch: 130310 | Loss: 0.4019334018230438 | Test loss: 0.4070165157318115\n",
      "Epoch: 130320 | Loss: 0.40192165970802307 | Test loss: 0.40700268745422363\n",
      "Epoch: 130330 | Loss: 0.40190988779067993 | Test loss: 0.40698882937431335\n",
      "Epoch: 130340 | Loss: 0.4018981456756592 | Test loss: 0.40697500109672546\n",
      "Epoch: 130350 | Loss: 0.40188637375831604 | Test loss: 0.4069611728191376\n",
      "Epoch: 130360 | Loss: 0.4018746316432953 | Test loss: 0.4069472849369049\n",
      "Epoch: 130370 | Loss: 0.40186285972595215 | Test loss: 0.406933456659317\n",
      "Epoch: 130380 | Loss: 0.401851087808609 | Test loss: 0.40691956877708435\n",
      "Epoch: 130390 | Loss: 0.40183931589126587 | Test loss: 0.40690574049949646\n",
      "Epoch: 130400 | Loss: 0.4018275737762451 | Test loss: 0.40689191222190857\n",
      "Epoch: 130410 | Loss: 0.40181586146354675 | Test loss: 0.4068780839443207\n",
      "Epoch: 130420 | Loss: 0.4018040597438812 | Test loss: 0.4068642258644104\n",
      "Epoch: 130430 | Loss: 0.4017922878265381 | Test loss: 0.40685033798217773\n",
      "Epoch: 130440 | Loss: 0.4017805755138397 | Test loss: 0.40683650970458984\n",
      "Epoch: 130450 | Loss: 0.4017687737941742 | Test loss: 0.40682268142700195\n",
      "Epoch: 130460 | Loss: 0.40175706148147583 | Test loss: 0.4068088233470917\n",
      "Epoch: 130470 | Loss: 0.4017452299594879 | Test loss: 0.4067949950695038\n",
      "Epoch: 130480 | Loss: 0.40173348784446716 | Test loss: 0.4067811071872711\n",
      "Epoch: 130490 | Loss: 0.401721715927124 | Test loss: 0.4067672789096832\n",
      "Epoch: 130500 | Loss: 0.4017099440097809 | Test loss: 0.40675345063209534\n",
      "Epoch: 130510 | Loss: 0.40169820189476013 | Test loss: 0.40673962235450745\n",
      "Epoch: 130520 | Loss: 0.40168648958206177 | Test loss: 0.4067257344722748\n",
      "Epoch: 130530 | Loss: 0.40167471766471863 | Test loss: 0.4067119061946869\n",
      "Epoch: 130540 | Loss: 0.4016629159450531 | Test loss: 0.4066980481147766\n",
      "Epoch: 130550 | Loss: 0.40165120363235474 | Test loss: 0.4066842198371887\n",
      "Epoch: 130560 | Loss: 0.4016394317150116 | Test loss: 0.40667039155960083\n",
      "Epoch: 130570 | Loss: 0.40162762999534607 | Test loss: 0.40665656328201294\n",
      "Epoch: 130580 | Loss: 0.4016159176826477 | Test loss: 0.4066426753997803\n",
      "Epoch: 130590 | Loss: 0.40160414576530457 | Test loss: 0.4066288471221924\n",
      "Epoch: 130600 | Loss: 0.40159234404563904 | Test loss: 0.4066149890422821\n",
      "Epoch: 130610 | Loss: 0.4015806317329407 | Test loss: 0.4066011607646942\n",
      "Epoch: 130620 | Loss: 0.40156885981559753 | Test loss: 0.40658727288246155\n",
      "Epoch: 130630 | Loss: 0.4015571177005768 | Test loss: 0.40657344460487366\n",
      "Epoch: 130640 | Loss: 0.40154534578323364 | Test loss: 0.406559556722641\n",
      "Epoch: 130650 | Loss: 0.4015336036682129 | Test loss: 0.4065457284450531\n",
      "Epoch: 130660 | Loss: 0.401521772146225 | Test loss: 0.4065319001674652\n",
      "Epoch: 130670 | Loss: 0.4015100598335266 | Test loss: 0.4065180718898773\n",
      "Epoch: 130680 | Loss: 0.40149831771850586 | Test loss: 0.40650421380996704\n",
      "Epoch: 130690 | Loss: 0.4014865458011627 | Test loss: 0.40649038553237915\n",
      "Epoch: 130700 | Loss: 0.4014747738838196 | Test loss: 0.4064764976501465\n",
      "Epoch: 130710 | Loss: 0.40146303176879883 | Test loss: 0.4064626693725586\n",
      "Epoch: 130720 | Loss: 0.4014512598514557 | Test loss: 0.4064488410949707\n",
      "Epoch: 130730 | Loss: 0.40143948793411255 | Test loss: 0.4064350128173828\n",
      "Epoch: 130740 | Loss: 0.4014277458190918 | Test loss: 0.40642115473747253\n",
      "Epoch: 130750 | Loss: 0.40141603350639343 | Test loss: 0.40640732645988464\n",
      "Epoch: 130760 | Loss: 0.4014042019844055 | Test loss: 0.406393438577652\n",
      "Epoch: 130770 | Loss: 0.40139245986938477 | Test loss: 0.4063796103000641\n",
      "Epoch: 130780 | Loss: 0.4013807475566864 | Test loss: 0.4063657820224762\n",
      "Epoch: 130790 | Loss: 0.4013689160346985 | Test loss: 0.40635189414024353\n",
      "Epoch: 130800 | Loss: 0.40135717391967773 | Test loss: 0.40633806586265564\n",
      "Epoch: 130810 | Loss: 0.40134546160697937 | Test loss: 0.40632420778274536\n",
      "Epoch: 130820 | Loss: 0.40133363008499146 | Test loss: 0.40631037950515747\n",
      "Epoch: 130830 | Loss: 0.4013218879699707 | Test loss: 0.4062965512275696\n",
      "Epoch: 130840 | Loss: 0.40131017565727234 | Test loss: 0.4062826633453369\n",
      "Epoch: 130850 | Loss: 0.4012983739376068 | Test loss: 0.406268835067749\n",
      "Epoch: 130860 | Loss: 0.40128660202026367 | Test loss: 0.40625497698783875\n",
      "Epoch: 130870 | Loss: 0.4012748897075653 | Test loss: 0.40624114871025085\n",
      "Epoch: 130880 | Loss: 0.40126314759254456 | Test loss: 0.40622732043266296\n",
      "Epoch: 130890 | Loss: 0.40125131607055664 | Test loss: 0.4062134325504303\n",
      "Epoch: 130900 | Loss: 0.4012396037578583 | Test loss: 0.4061996042728424\n",
      "Epoch: 130910 | Loss: 0.40122780203819275 | Test loss: 0.40618571639060974\n",
      "Epoch: 130920 | Loss: 0.4012160301208496 | Test loss: 0.40617188811302185\n",
      "Epoch: 130930 | Loss: 0.40120431780815125 | Test loss: 0.40615805983543396\n",
      "Epoch: 130940 | Loss: 0.4011925160884857 | Test loss: 0.40614423155784607\n",
      "Epoch: 130950 | Loss: 0.4011807441711426 | Test loss: 0.4061303734779358\n",
      "Epoch: 130960 | Loss: 0.4011690318584442 | Test loss: 0.4061165452003479\n",
      "Epoch: 130970 | Loss: 0.40115728974342346 | Test loss: 0.40610265731811523\n",
      "Epoch: 130980 | Loss: 0.4011455178260803 | Test loss: 0.40608882904052734\n",
      "Epoch: 130990 | Loss: 0.4011337459087372 | Test loss: 0.40607500076293945\n",
      "Epoch: 131000 | Loss: 0.40112200379371643 | Test loss: 0.4060611426830292\n",
      "Epoch: 131010 | Loss: 0.4011102318763733 | Test loss: 0.4060473144054413\n",
      "Epoch: 131020 | Loss: 0.40109845995903015 | Test loss: 0.4060334265232086\n",
      "Epoch: 131030 | Loss: 0.4010867178440094 | Test loss: 0.4060195982456207\n",
      "Epoch: 131040 | Loss: 0.40107494592666626 | Test loss: 0.40600576996803284\n",
      "Epoch: 131050 | Loss: 0.4010631740093231 | Test loss: 0.40599188208580017\n",
      "Epoch: 131060 | Loss: 0.40105143189430237 | Test loss: 0.4059780538082123\n",
      "Epoch: 131070 | Loss: 0.40103965997695923 | Test loss: 0.405964195728302\n",
      "Epoch: 131080 | Loss: 0.4010278880596161 | Test loss: 0.4059503674507141\n",
      "Epoch: 131090 | Loss: 0.40101614594459534 | Test loss: 0.4059365391731262\n",
      "Epoch: 131100 | Loss: 0.4010044038295746 | Test loss: 0.40592265129089355\n",
      "Epoch: 131110 | Loss: 0.40099260210990906 | Test loss: 0.40590882301330566\n",
      "Epoch: 131120 | Loss: 0.4009808599948883 | Test loss: 0.4058949947357178\n",
      "Epoch: 131130 | Loss: 0.40096908807754517 | Test loss: 0.4058811366558075\n",
      "Epoch: 131140 | Loss: 0.4009573459625244 | Test loss: 0.4058673083782196\n",
      "Epoch: 131150 | Loss: 0.4009455740451813 | Test loss: 0.4058534801006317\n",
      "Epoch: 131160 | Loss: 0.4009338319301605 | Test loss: 0.40583959221839905\n",
      "Epoch: 131170 | Loss: 0.4009220600128174 | Test loss: 0.40582576394081116\n",
      "Epoch: 131180 | Loss: 0.40091028809547424 | Test loss: 0.4058118760585785\n",
      "Epoch: 131190 | Loss: 0.4008985161781311 | Test loss: 0.4057980477809906\n",
      "Epoch: 131200 | Loss: 0.40088677406311035 | Test loss: 0.4057842195034027\n",
      "Epoch: 131210 | Loss: 0.400875061750412 | Test loss: 0.4057703912258148\n",
      "Epoch: 131220 | Loss: 0.40086326003074646 | Test loss: 0.40575653314590454\n",
      "Epoch: 131230 | Loss: 0.4008514881134033 | Test loss: 0.4057426452636719\n",
      "Epoch: 131240 | Loss: 0.40083977580070496 | Test loss: 0.405728816986084\n",
      "Epoch: 131250 | Loss: 0.40082797408103943 | Test loss: 0.4057149887084961\n",
      "Epoch: 131260 | Loss: 0.40081626176834106 | Test loss: 0.4057011306285858\n",
      "Epoch: 131270 | Loss: 0.40080443024635315 | Test loss: 0.4056873023509979\n",
      "Epoch: 131280 | Loss: 0.4007926881313324 | Test loss: 0.40567341446876526\n",
      "Epoch: 131290 | Loss: 0.40078091621398926 | Test loss: 0.40565958619117737\n",
      "Epoch: 131300 | Loss: 0.4007691442966461 | Test loss: 0.4056457579135895\n",
      "Epoch: 131310 | Loss: 0.40075740218162537 | Test loss: 0.4056319296360016\n",
      "Epoch: 131320 | Loss: 0.400745689868927 | Test loss: 0.4056180417537689\n",
      "Epoch: 131330 | Loss: 0.40073391795158386 | Test loss: 0.40560421347618103\n",
      "Epoch: 131340 | Loss: 0.40072211623191833 | Test loss: 0.40559035539627075\n",
      "Epoch: 131350 | Loss: 0.40071040391921997 | Test loss: 0.40557652711868286\n",
      "Epoch: 131360 | Loss: 0.40069863200187683 | Test loss: 0.40556269884109497\n",
      "Epoch: 131370 | Loss: 0.4006868302822113 | Test loss: 0.4055488705635071\n",
      "Epoch: 131380 | Loss: 0.40067511796951294 | Test loss: 0.4055349826812744\n",
      "Epoch: 131390 | Loss: 0.4006633460521698 | Test loss: 0.4055211544036865\n",
      "Epoch: 131400 | Loss: 0.4006515443325043 | Test loss: 0.40550729632377625\n",
      "Epoch: 131410 | Loss: 0.4006398320198059 | Test loss: 0.40549346804618835\n",
      "Epoch: 131420 | Loss: 0.40062806010246277 | Test loss: 0.4054795801639557\n",
      "Epoch: 131430 | Loss: 0.400616317987442 | Test loss: 0.4054657518863678\n",
      "Epoch: 131440 | Loss: 0.4006045460700989 | Test loss: 0.40545186400413513\n",
      "Epoch: 131450 | Loss: 0.4005928039550781 | Test loss: 0.40543803572654724\n",
      "Epoch: 131460 | Loss: 0.4005809724330902 | Test loss: 0.40542420744895935\n",
      "Epoch: 131470 | Loss: 0.40056926012039185 | Test loss: 0.40541037917137146\n",
      "Epoch: 131480 | Loss: 0.4005575180053711 | Test loss: 0.4053965210914612\n",
      "Epoch: 131490 | Loss: 0.40054574608802795 | Test loss: 0.4053826928138733\n",
      "Epoch: 131500 | Loss: 0.4005339741706848 | Test loss: 0.4053688049316406\n",
      "Epoch: 131510 | Loss: 0.40052223205566406 | Test loss: 0.40535497665405273\n",
      "Epoch: 131520 | Loss: 0.4005104601383209 | Test loss: 0.40534114837646484\n",
      "Epoch: 131530 | Loss: 0.4004986882209778 | Test loss: 0.40532732009887695\n",
      "Epoch: 131540 | Loss: 0.40048694610595703 | Test loss: 0.4053134620189667\n",
      "Epoch: 131550 | Loss: 0.40047523379325867 | Test loss: 0.4052996337413788\n",
      "Epoch: 131560 | Loss: 0.40046340227127075 | Test loss: 0.4052857458591461\n",
      "Epoch: 131570 | Loss: 0.40045166015625 | Test loss: 0.4052719175815582\n",
      "Epoch: 131580 | Loss: 0.40043994784355164 | Test loss: 0.40525808930397034\n",
      "Epoch: 131590 | Loss: 0.4004281163215637 | Test loss: 0.40524420142173767\n",
      "Epoch: 131600 | Loss: 0.40041637420654297 | Test loss: 0.4052303731441498\n",
      "Epoch: 131610 | Loss: 0.4004046618938446 | Test loss: 0.4052165150642395\n",
      "Epoch: 131620 | Loss: 0.4003928303718567 | Test loss: 0.4052026867866516\n",
      "Epoch: 131630 | Loss: 0.40038108825683594 | Test loss: 0.4051888585090637\n",
      "Epoch: 131640 | Loss: 0.4003693759441376 | Test loss: 0.40517497062683105\n",
      "Epoch: 131650 | Loss: 0.40035757422447205 | Test loss: 0.40516114234924316\n",
      "Epoch: 131660 | Loss: 0.4003458023071289 | Test loss: 0.4051472842693329\n",
      "Epoch: 131670 | Loss: 0.40033408999443054 | Test loss: 0.405133455991745\n",
      "Epoch: 131680 | Loss: 0.4003223478794098 | Test loss: 0.4051196277141571\n",
      "Epoch: 131690 | Loss: 0.4003105163574219 | Test loss: 0.40510573983192444\n",
      "Epoch: 131700 | Loss: 0.4002988040447235 | Test loss: 0.40509191155433655\n",
      "Epoch: 131710 | Loss: 0.400287002325058 | Test loss: 0.4050780236721039\n",
      "Epoch: 131720 | Loss: 0.40027523040771484 | Test loss: 0.405064195394516\n",
      "Epoch: 131730 | Loss: 0.4002635180950165 | Test loss: 0.4050503671169281\n",
      "Epoch: 131740 | Loss: 0.40025171637535095 | Test loss: 0.4050365388393402\n",
      "Epoch: 131750 | Loss: 0.4002399444580078 | Test loss: 0.40502268075942993\n",
      "Epoch: 131760 | Loss: 0.40022823214530945 | Test loss: 0.40500885248184204\n",
      "Epoch: 131770 | Loss: 0.4002164900302887 | Test loss: 0.4049949645996094\n",
      "Epoch: 131780 | Loss: 0.40020471811294556 | Test loss: 0.4049811363220215\n",
      "Epoch: 131790 | Loss: 0.4001929461956024 | Test loss: 0.4049673080444336\n",
      "Epoch: 131800 | Loss: 0.40018120408058167 | Test loss: 0.4049534499645233\n",
      "Epoch: 131810 | Loss: 0.4001694321632385 | Test loss: 0.4049396216869354\n",
      "Epoch: 131820 | Loss: 0.4001576602458954 | Test loss: 0.40492573380470276\n",
      "Epoch: 131830 | Loss: 0.40014591813087463 | Test loss: 0.40491190552711487\n",
      "Epoch: 131840 | Loss: 0.4001341462135315 | Test loss: 0.404898077249527\n",
      "Epoch: 131850 | Loss: 0.40012237429618835 | Test loss: 0.4048841893672943\n",
      "Epoch: 131860 | Loss: 0.4001106321811676 | Test loss: 0.4048703610897064\n",
      "Epoch: 131870 | Loss: 0.40009886026382446 | Test loss: 0.40485650300979614\n",
      "Epoch: 131880 | Loss: 0.4000870883464813 | Test loss: 0.40484267473220825\n",
      "Epoch: 131890 | Loss: 0.40007534623146057 | Test loss: 0.40482884645462036\n",
      "Epoch: 131900 | Loss: 0.4000636041164398 | Test loss: 0.4048149585723877\n",
      "Epoch: 131910 | Loss: 0.4000518023967743 | Test loss: 0.4048011302947998\n",
      "Epoch: 131920 | Loss: 0.40004006028175354 | Test loss: 0.4047873020172119\n",
      "Epoch: 131930 | Loss: 0.4000282883644104 | Test loss: 0.40477344393730164\n",
      "Epoch: 131940 | Loss: 0.40001654624938965 | Test loss: 0.40475961565971375\n",
      "Epoch: 131950 | Loss: 0.4000047743320465 | Test loss: 0.40474578738212585\n",
      "Epoch: 131960 | Loss: 0.39999303221702576 | Test loss: 0.4047318994998932\n",
      "Epoch: 131970 | Loss: 0.3999812602996826 | Test loss: 0.4047180712223053\n",
      "Epoch: 131980 | Loss: 0.39996951818466187 | Test loss: 0.40470418334007263\n",
      "Epoch: 131990 | Loss: 0.39995771646499634 | Test loss: 0.40469035506248474\n",
      "Epoch: 132000 | Loss: 0.3999459743499756 | Test loss: 0.40467652678489685\n",
      "Epoch: 132010 | Loss: 0.3999342620372772 | Test loss: 0.40466269850730896\n",
      "Epoch: 132020 | Loss: 0.3999224603176117 | Test loss: 0.4046488404273987\n",
      "Epoch: 132030 | Loss: 0.39991068840026855 | Test loss: 0.404634952545166\n",
      "Epoch: 132040 | Loss: 0.3998989760875702 | Test loss: 0.4046211242675781\n",
      "Epoch: 132050 | Loss: 0.39988717436790466 | Test loss: 0.40460729598999023\n",
      "Epoch: 132060 | Loss: 0.3998754322528839 | Test loss: 0.40459343791007996\n",
      "Epoch: 132070 | Loss: 0.39986366033554077 | Test loss: 0.40457960963249207\n",
      "Epoch: 132080 | Loss: 0.39985191822052 | Test loss: 0.4045657217502594\n",
      "Epoch: 132090 | Loss: 0.3998401165008545 | Test loss: 0.4045518934726715\n",
      "Epoch: 132100 | Loss: 0.39982837438583374 | Test loss: 0.4045380651950836\n",
      "Epoch: 132110 | Loss: 0.3998166024684906 | Test loss: 0.4045242369174957\n",
      "Epoch: 132120 | Loss: 0.39980486035346985 | Test loss: 0.40451034903526306\n",
      "Epoch: 132130 | Loss: 0.3997931182384491 | Test loss: 0.40449652075767517\n",
      "Epoch: 132140 | Loss: 0.39978131651878357 | Test loss: 0.4044826626777649\n",
      "Epoch: 132150 | Loss: 0.3997695744037628 | Test loss: 0.404468834400177\n",
      "Epoch: 132160 | Loss: 0.39975783228874207 | Test loss: 0.4044550061225891\n",
      "Epoch: 132170 | Loss: 0.3997460603713989 | Test loss: 0.4044411778450012\n",
      "Epoch: 132180 | Loss: 0.3997343182563782 | Test loss: 0.40442728996276855\n",
      "Epoch: 132190 | Loss: 0.39972254633903503 | Test loss: 0.40441346168518066\n",
      "Epoch: 132200 | Loss: 0.3997107744216919 | Test loss: 0.4043996036052704\n",
      "Epoch: 132210 | Loss: 0.39969903230667114 | Test loss: 0.4043857753276825\n",
      "Epoch: 132220 | Loss: 0.399687260389328 | Test loss: 0.40437188744544983\n",
      "Epoch: 132230 | Loss: 0.39967551827430725 | Test loss: 0.40435805916786194\n",
      "Epoch: 132240 | Loss: 0.3996637463569641 | Test loss: 0.4043441712856293\n",
      "Epoch: 132250 | Loss: 0.39965197443962097 | Test loss: 0.4043303430080414\n",
      "Epoch: 132260 | Loss: 0.39964020252227783 | Test loss: 0.4043165147304535\n",
      "Epoch: 132270 | Loss: 0.3996284604072571 | Test loss: 0.4043026864528656\n",
      "Epoch: 132280 | Loss: 0.39961668848991394 | Test loss: 0.4042888283729553\n",
      "Epoch: 132290 | Loss: 0.3996049463748932 | Test loss: 0.40427500009536743\n",
      "Epoch: 132300 | Loss: 0.39959320425987244 | Test loss: 0.40426111221313477\n",
      "Epoch: 132310 | Loss: 0.3995814025402069 | Test loss: 0.4042472839355469\n",
      "Epoch: 132320 | Loss: 0.39956966042518616 | Test loss: 0.404233455657959\n",
      "Epoch: 132330 | Loss: 0.399557888507843 | Test loss: 0.4042196273803711\n",
      "Epoch: 132340 | Loss: 0.39954614639282227 | Test loss: 0.4042057693004608\n",
      "Epoch: 132350 | Loss: 0.3995344042778015 | Test loss: 0.4041919410228729\n",
      "Epoch: 132360 | Loss: 0.3995226323604584 | Test loss: 0.40417805314064026\n",
      "Epoch: 132370 | Loss: 0.39951086044311523 | Test loss: 0.40416422486305237\n",
      "Epoch: 132380 | Loss: 0.3994991183280945 | Test loss: 0.4041503965854645\n",
      "Epoch: 132390 | Loss: 0.39948731660842896 | Test loss: 0.4041365087032318\n",
      "Epoch: 132400 | Loss: 0.3994755744934082 | Test loss: 0.4041226804256439\n",
      "Epoch: 132410 | Loss: 0.39946386218070984 | Test loss: 0.40410882234573364\n",
      "Epoch: 132420 | Loss: 0.3994520604610443 | Test loss: 0.40409499406814575\n",
      "Epoch: 132430 | Loss: 0.39944028854370117 | Test loss: 0.40408116579055786\n",
      "Epoch: 132440 | Loss: 0.3994285762310028 | Test loss: 0.4040672779083252\n",
      "Epoch: 132450 | Loss: 0.3994167745113373 | Test loss: 0.4040534496307373\n",
      "Epoch: 132460 | Loss: 0.39940503239631653 | Test loss: 0.404039591550827\n",
      "Epoch: 132470 | Loss: 0.3993932604789734 | Test loss: 0.40402576327323914\n",
      "Epoch: 132480 | Loss: 0.39938151836395264 | Test loss: 0.40401193499565125\n",
      "Epoch: 132490 | Loss: 0.3993697166442871 | Test loss: 0.4039980471134186\n",
      "Epoch: 132500 | Loss: 0.39935797452926636 | Test loss: 0.4039842188358307\n",
      "Epoch: 132510 | Loss: 0.3993462026119232 | Test loss: 0.403970330953598\n",
      "Epoch: 132520 | Loss: 0.39933446049690247 | Test loss: 0.40395650267601013\n",
      "Epoch: 132530 | Loss: 0.3993227183818817 | Test loss: 0.40394267439842224\n",
      "Epoch: 132540 | Loss: 0.3993109166622162 | Test loss: 0.40392884612083435\n",
      "Epoch: 132550 | Loss: 0.39929917454719543 | Test loss: 0.4039149880409241\n",
      "Epoch: 132560 | Loss: 0.3992874324321747 | Test loss: 0.4039011597633362\n",
      "Epoch: 132570 | Loss: 0.39927566051483154 | Test loss: 0.4038872718811035\n",
      "Epoch: 132580 | Loss: 0.3992639183998108 | Test loss: 0.4038734436035156\n",
      "Epoch: 132590 | Loss: 0.39925214648246765 | Test loss: 0.40385961532592773\n",
      "Epoch: 132600 | Loss: 0.3992403745651245 | Test loss: 0.40384575724601746\n",
      "Epoch: 132610 | Loss: 0.39922863245010376 | Test loss: 0.40383192896842957\n",
      "Epoch: 132620 | Loss: 0.3992168605327606 | Test loss: 0.4038180410861969\n",
      "Epoch: 132630 | Loss: 0.39920511841773987 | Test loss: 0.403804212808609\n",
      "Epoch: 132640 | Loss: 0.39919334650039673 | Test loss: 0.4037903845310211\n",
      "Epoch: 132650 | Loss: 0.3991815745830536 | Test loss: 0.40377649664878845\n",
      "Epoch: 132660 | Loss: 0.39916980266571045 | Test loss: 0.40376266837120056\n",
      "Epoch: 132670 | Loss: 0.3991580605506897 | Test loss: 0.4037488102912903\n",
      "Epoch: 132680 | Loss: 0.39914628863334656 | Test loss: 0.4037349820137024\n",
      "Epoch: 132690 | Loss: 0.3991345465183258 | Test loss: 0.4037211537361145\n",
      "Epoch: 132700 | Loss: 0.39912280440330505 | Test loss: 0.40370726585388184\n",
      "Epoch: 132710 | Loss: 0.3991110026836395 | Test loss: 0.40369343757629395\n",
      "Epoch: 132720 | Loss: 0.3990992605686188 | Test loss: 0.40367960929870605\n",
      "Epoch: 132730 | Loss: 0.39908748865127563 | Test loss: 0.4036657512187958\n",
      "Epoch: 132740 | Loss: 0.3990757465362549 | Test loss: 0.4036519229412079\n",
      "Epoch: 132750 | Loss: 0.39906400442123413 | Test loss: 0.40363809466362\n",
      "Epoch: 132760 | Loss: 0.399052232503891 | Test loss: 0.40362420678138733\n",
      "Epoch: 132770 | Loss: 0.39904046058654785 | Test loss: 0.40361037850379944\n",
      "Epoch: 132780 | Loss: 0.3990287184715271 | Test loss: 0.4035964906215668\n",
      "Epoch: 132790 | Loss: 0.3990169167518616 | Test loss: 0.4035826623439789\n",
      "Epoch: 132800 | Loss: 0.3990051746368408 | Test loss: 0.403568834066391\n",
      "Epoch: 132810 | Loss: 0.39899346232414246 | Test loss: 0.4035550057888031\n",
      "Epoch: 132820 | Loss: 0.39898166060447693 | Test loss: 0.4035411477088928\n",
      "Epoch: 132830 | Loss: 0.3989698886871338 | Test loss: 0.40352725982666016\n",
      "Epoch: 132840 | Loss: 0.3989581763744354 | Test loss: 0.40351343154907227\n",
      "Epoch: 132850 | Loss: 0.3989463746547699 | Test loss: 0.4034996032714844\n",
      "Epoch: 132860 | Loss: 0.39893463253974915 | Test loss: 0.4034857451915741\n",
      "Epoch: 132870 | Loss: 0.398922860622406 | Test loss: 0.4034719169139862\n",
      "Epoch: 132880 | Loss: 0.39891111850738525 | Test loss: 0.40345802903175354\n",
      "Epoch: 132890 | Loss: 0.3988993167877197 | Test loss: 0.40344420075416565\n",
      "Epoch: 132900 | Loss: 0.398887574672699 | Test loss: 0.40343037247657776\n",
      "Epoch: 132910 | Loss: 0.39887580275535583 | Test loss: 0.40341654419898987\n",
      "Epoch: 132920 | Loss: 0.3988640606403351 | Test loss: 0.4034026563167572\n",
      "Epoch: 132930 | Loss: 0.39885231852531433 | Test loss: 0.4033888280391693\n",
      "Epoch: 132940 | Loss: 0.3988405168056488 | Test loss: 0.40337496995925903\n",
      "Epoch: 132950 | Loss: 0.39882877469062805 | Test loss: 0.40336114168167114\n",
      "Epoch: 132960 | Loss: 0.3988170325756073 | Test loss: 0.40334731340408325\n",
      "Epoch: 132970 | Loss: 0.39880526065826416 | Test loss: 0.40333348512649536\n",
      "Epoch: 132980 | Loss: 0.3987935185432434 | Test loss: 0.4033195972442627\n",
      "Epoch: 132990 | Loss: 0.39878174662590027 | Test loss: 0.4033057689666748\n",
      "Epoch: 133000 | Loss: 0.39876997470855713 | Test loss: 0.4032919108867645\n",
      "Epoch: 133010 | Loss: 0.3987582325935364 | Test loss: 0.40327808260917664\n",
      "Epoch: 133020 | Loss: 0.39874646067619324 | Test loss: 0.40326419472694397\n",
      "Epoch: 133030 | Loss: 0.3987347185611725 | Test loss: 0.4032503664493561\n",
      "Epoch: 133040 | Loss: 0.39872294664382935 | Test loss: 0.4032364785671234\n",
      "Epoch: 133050 | Loss: 0.3987111747264862 | Test loss: 0.4032226502895355\n",
      "Epoch: 133060 | Loss: 0.39869940280914307 | Test loss: 0.40320882201194763\n",
      "Epoch: 133070 | Loss: 0.3986876606941223 | Test loss: 0.40319499373435974\n",
      "Epoch: 133080 | Loss: 0.3986758887767792 | Test loss: 0.40318113565444946\n",
      "Epoch: 133090 | Loss: 0.3986641466617584 | Test loss: 0.4031673073768616\n",
      "Epoch: 133100 | Loss: 0.39865240454673767 | Test loss: 0.4031534194946289\n",
      "Epoch: 133110 | Loss: 0.39864060282707214 | Test loss: 0.403139591217041\n",
      "Epoch: 133120 | Loss: 0.3986288607120514 | Test loss: 0.4031257629394531\n",
      "Epoch: 133130 | Loss: 0.39861708879470825 | Test loss: 0.40311193466186523\n",
      "Epoch: 133140 | Loss: 0.3986053466796875 | Test loss: 0.40309807658195496\n",
      "Epoch: 133150 | Loss: 0.39859360456466675 | Test loss: 0.40308424830436707\n",
      "Epoch: 133160 | Loss: 0.3985818326473236 | Test loss: 0.4030703604221344\n",
      "Epoch: 133170 | Loss: 0.39857006072998047 | Test loss: 0.4030565321445465\n",
      "Epoch: 133180 | Loss: 0.3985583186149597 | Test loss: 0.4030427038669586\n",
      "Epoch: 133190 | Loss: 0.3985465168952942 | Test loss: 0.40302881598472595\n",
      "Epoch: 133200 | Loss: 0.39853477478027344 | Test loss: 0.40301498770713806\n",
      "Epoch: 133210 | Loss: 0.3985230624675751 | Test loss: 0.4030011296272278\n",
      "Epoch: 133220 | Loss: 0.39851126074790955 | Test loss: 0.4029873013496399\n",
      "Epoch: 133230 | Loss: 0.3984994888305664 | Test loss: 0.402973473072052\n",
      "Epoch: 133240 | Loss: 0.39848777651786804 | Test loss: 0.40295958518981934\n",
      "Epoch: 133250 | Loss: 0.3984759747982025 | Test loss: 0.40294575691223145\n",
      "Epoch: 133260 | Loss: 0.39846423268318176 | Test loss: 0.40293189883232117\n",
      "Epoch: 133270 | Loss: 0.3984524607658386 | Test loss: 0.4029180705547333\n",
      "Epoch: 133280 | Loss: 0.39844071865081787 | Test loss: 0.4029042422771454\n",
      "Epoch: 133290 | Loss: 0.39842891693115234 | Test loss: 0.4028903543949127\n",
      "Epoch: 133300 | Loss: 0.3984171748161316 | Test loss: 0.40287652611732483\n",
      "Epoch: 133310 | Loss: 0.39840540289878845 | Test loss: 0.40286263823509216\n",
      "Epoch: 133320 | Loss: 0.3983936607837677 | Test loss: 0.4028488099575043\n",
      "Epoch: 133330 | Loss: 0.39838191866874695 | Test loss: 0.4028349816799164\n",
      "Epoch: 133340 | Loss: 0.3983701169490814 | Test loss: 0.4028211534023285\n",
      "Epoch: 133350 | Loss: 0.39835837483406067 | Test loss: 0.4028072953224182\n",
      "Epoch: 133360 | Loss: 0.3983466327190399 | Test loss: 0.4027934670448303\n",
      "Epoch: 133370 | Loss: 0.3983348608016968 | Test loss: 0.40277957916259766\n",
      "Epoch: 133380 | Loss: 0.398323118686676 | Test loss: 0.40276575088500977\n",
      "Epoch: 133390 | Loss: 0.3983113467693329 | Test loss: 0.4027519226074219\n",
      "Epoch: 133400 | Loss: 0.39829957485198975 | Test loss: 0.4027380645275116\n",
      "Epoch: 133410 | Loss: 0.398287832736969 | Test loss: 0.4027242362499237\n",
      "Epoch: 133420 | Loss: 0.39827606081962585 | Test loss: 0.40271034836769104\n",
      "Epoch: 133430 | Loss: 0.3982643187046051 | Test loss: 0.40269652009010315\n",
      "Epoch: 133440 | Loss: 0.39825254678726196 | Test loss: 0.40268269181251526\n",
      "Epoch: 133450 | Loss: 0.3982407748699188 | Test loss: 0.4026688039302826\n",
      "Epoch: 133460 | Loss: 0.3982290029525757 | Test loss: 0.4026549756526947\n",
      "Epoch: 133470 | Loss: 0.39821726083755493 | Test loss: 0.4026411175727844\n",
      "Epoch: 133480 | Loss: 0.3982054889202118 | Test loss: 0.40262728929519653\n",
      "Epoch: 133490 | Loss: 0.39819374680519104 | Test loss: 0.40261346101760864\n",
      "Epoch: 133500 | Loss: 0.3981820046901703 | Test loss: 0.402599573135376\n",
      "Epoch: 133510 | Loss: 0.39817020297050476 | Test loss: 0.4025857448577881\n",
      "Epoch: 133520 | Loss: 0.398158460855484 | Test loss: 0.4025719165802002\n",
      "Epoch: 133530 | Loss: 0.39814668893814087 | Test loss: 0.4025580585002899\n",
      "Epoch: 133540 | Loss: 0.3981349468231201 | Test loss: 0.402544230222702\n",
      "Epoch: 133550 | Loss: 0.39812320470809937 | Test loss: 0.40253040194511414\n",
      "Epoch: 133560 | Loss: 0.3981114327907562 | Test loss: 0.40251651406288147\n",
      "Epoch: 133570 | Loss: 0.3980996608734131 | Test loss: 0.4025026857852936\n",
      "Epoch: 133580 | Loss: 0.39808791875839233 | Test loss: 0.4024887979030609\n",
      "Epoch: 133590 | Loss: 0.3980761170387268 | Test loss: 0.402474969625473\n",
      "Epoch: 133600 | Loss: 0.39806437492370605 | Test loss: 0.40246114134788513\n",
      "Epoch: 133610 | Loss: 0.3980526626110077 | Test loss: 0.40244731307029724\n",
      "Epoch: 133620 | Loss: 0.39804086089134216 | Test loss: 0.40243345499038696\n",
      "Epoch: 133630 | Loss: 0.398029088973999 | Test loss: 0.4024195671081543\n",
      "Epoch: 133640 | Loss: 0.39801737666130066 | Test loss: 0.4024057388305664\n",
      "Epoch: 133650 | Loss: 0.39800557494163513 | Test loss: 0.4023919105529785\n",
      "Epoch: 133660 | Loss: 0.3979938328266144 | Test loss: 0.40237805247306824\n",
      "Epoch: 133670 | Loss: 0.39798206090927124 | Test loss: 0.40236422419548035\n",
      "Epoch: 133680 | Loss: 0.3979703187942505 | Test loss: 0.4023503363132477\n",
      "Epoch: 133690 | Loss: 0.39795851707458496 | Test loss: 0.4023365080356598\n",
      "Epoch: 133700 | Loss: 0.3979467749595642 | Test loss: 0.4023226797580719\n",
      "Epoch: 133710 | Loss: 0.39793500304222107 | Test loss: 0.402308851480484\n",
      "Epoch: 133720 | Loss: 0.3979232609272003 | Test loss: 0.40229496359825134\n",
      "Epoch: 133730 | Loss: 0.39791151881217957 | Test loss: 0.40228113532066345\n",
      "Epoch: 133740 | Loss: 0.39789971709251404 | Test loss: 0.4022672772407532\n",
      "Epoch: 133750 | Loss: 0.3978879749774933 | Test loss: 0.4022534489631653\n",
      "Epoch: 133760 | Loss: 0.39787623286247253 | Test loss: 0.4022396206855774\n",
      "Epoch: 133770 | Loss: 0.3978644609451294 | Test loss: 0.4022257924079895\n",
      "Epoch: 133780 | Loss: 0.39785271883010864 | Test loss: 0.40221190452575684\n",
      "Epoch: 133790 | Loss: 0.3978409469127655 | Test loss: 0.40219807624816895\n",
      "Epoch: 133800 | Loss: 0.39782917499542236 | Test loss: 0.40218421816825867\n",
      "Epoch: 133810 | Loss: 0.3978174328804016 | Test loss: 0.4021703898906708\n",
      "Epoch: 133820 | Loss: 0.39780566096305847 | Test loss: 0.4021565020084381\n",
      "Epoch: 133830 | Loss: 0.3977939188480377 | Test loss: 0.4021426737308502\n",
      "Epoch: 133840 | Loss: 0.3977821469306946 | Test loss: 0.40212878584861755\n",
      "Epoch: 133850 | Loss: 0.39777037501335144 | Test loss: 0.40211495757102966\n",
      "Epoch: 133860 | Loss: 0.3977586030960083 | Test loss: 0.4021011292934418\n",
      "Epoch: 133870 | Loss: 0.39774686098098755 | Test loss: 0.4020873010158539\n",
      "Epoch: 133880 | Loss: 0.3977350890636444 | Test loss: 0.4020734429359436\n",
      "Epoch: 133890 | Loss: 0.39772334694862366 | Test loss: 0.4020596146583557\n",
      "Epoch: 133900 | Loss: 0.3977116048336029 | Test loss: 0.40204572677612305\n",
      "Epoch: 133910 | Loss: 0.3976998031139374 | Test loss: 0.40203189849853516\n",
      "Epoch: 133920 | Loss: 0.3976880609989166 | Test loss: 0.40201807022094727\n",
      "Epoch: 133930 | Loss: 0.3976762890815735 | Test loss: 0.4020042419433594\n",
      "Epoch: 133940 | Loss: 0.39766454696655273 | Test loss: 0.4019903838634491\n",
      "Epoch: 133950 | Loss: 0.397652804851532 | Test loss: 0.4019765555858612\n",
      "Epoch: 133960 | Loss: 0.39764103293418884 | Test loss: 0.40196266770362854\n",
      "Epoch: 133970 | Loss: 0.3976292610168457 | Test loss: 0.40194883942604065\n",
      "Epoch: 133980 | Loss: 0.39761751890182495 | Test loss: 0.40193501114845276\n",
      "Epoch: 133990 | Loss: 0.3976057171821594 | Test loss: 0.4019211232662201\n",
      "Epoch: 134000 | Loss: 0.39759397506713867 | Test loss: 0.4019072949886322\n",
      "Epoch: 134010 | Loss: 0.3975822627544403 | Test loss: 0.4018934369087219\n",
      "Epoch: 134020 | Loss: 0.3975704610347748 | Test loss: 0.40187960863113403\n",
      "Epoch: 134030 | Loss: 0.39755868911743164 | Test loss: 0.40186578035354614\n",
      "Epoch: 134040 | Loss: 0.3975469768047333 | Test loss: 0.4018518924713135\n",
      "Epoch: 134050 | Loss: 0.39753517508506775 | Test loss: 0.4018380641937256\n",
      "Epoch: 134060 | Loss: 0.397523432970047 | Test loss: 0.4018242061138153\n",
      "Epoch: 134070 | Loss: 0.39751166105270386 | Test loss: 0.4018103778362274\n",
      "Epoch: 134080 | Loss: 0.3974999189376831 | Test loss: 0.4017965495586395\n",
      "Epoch: 134090 | Loss: 0.3974881172180176 | Test loss: 0.40178266167640686\n",
      "Epoch: 134100 | Loss: 0.3974763751029968 | Test loss: 0.40176883339881897\n",
      "Epoch: 134110 | Loss: 0.3974646031856537 | Test loss: 0.4017549455165863\n",
      "Epoch: 134120 | Loss: 0.39745286107063293 | Test loss: 0.4017411172389984\n",
      "Epoch: 134130 | Loss: 0.3974411189556122 | Test loss: 0.4017272889614105\n",
      "Epoch: 134140 | Loss: 0.39742931723594666 | Test loss: 0.40171346068382263\n",
      "Epoch: 134150 | Loss: 0.3974175751209259 | Test loss: 0.40169960260391235\n",
      "Epoch: 134160 | Loss: 0.39740583300590515 | Test loss: 0.40168577432632446\n",
      "Epoch: 134170 | Loss: 0.397394061088562 | Test loss: 0.4016718864440918\n",
      "Epoch: 134180 | Loss: 0.39738231897354126 | Test loss: 0.4016580581665039\n",
      "Epoch: 134190 | Loss: 0.3973705470561981 | Test loss: 0.401644229888916\n",
      "Epoch: 134200 | Loss: 0.397358775138855 | Test loss: 0.40163037180900574\n",
      "Epoch: 134210 | Loss: 0.39734703302383423 | Test loss: 0.40161654353141785\n",
      "Epoch: 134220 | Loss: 0.3973352611064911 | Test loss: 0.4016026556491852\n",
      "Epoch: 134230 | Loss: 0.39732351899147034 | Test loss: 0.4015888273715973\n",
      "Epoch: 134240 | Loss: 0.3973117470741272 | Test loss: 0.4015749990940094\n",
      "Epoch: 134250 | Loss: 0.39729997515678406 | Test loss: 0.40156111121177673\n",
      "Epoch: 134260 | Loss: 0.3972882032394409 | Test loss: 0.40154728293418884\n",
      "Epoch: 134270 | Loss: 0.39727646112442017 | Test loss: 0.40153342485427856\n",
      "Epoch: 134280 | Loss: 0.397264689207077 | Test loss: 0.4015195965766907\n",
      "Epoch: 134290 | Loss: 0.3972529470920563 | Test loss: 0.4015057682991028\n",
      "Epoch: 134300 | Loss: 0.3972412049770355 | Test loss: 0.4014918804168701\n",
      "Epoch: 134310 | Loss: 0.39722940325737 | Test loss: 0.4014780521392822\n",
      "Epoch: 134320 | Loss: 0.39721766114234924 | Test loss: 0.40146422386169434\n",
      "Epoch: 134330 | Loss: 0.3972058892250061 | Test loss: 0.40145036578178406\n",
      "Epoch: 134340 | Loss: 0.39719414710998535 | Test loss: 0.40143653750419617\n",
      "Epoch: 134350 | Loss: 0.3971824049949646 | Test loss: 0.4014227092266083\n",
      "Epoch: 134360 | Loss: 0.39717063307762146 | Test loss: 0.4014088213443756\n",
      "Epoch: 134370 | Loss: 0.3971588611602783 | Test loss: 0.4013949930667877\n",
      "Epoch: 134380 | Loss: 0.39714711904525757 | Test loss: 0.40138110518455505\n",
      "Epoch: 134390 | Loss: 0.39713531732559204 | Test loss: 0.40136727690696716\n",
      "Epoch: 134400 | Loss: 0.3971235752105713 | Test loss: 0.4013534486293793\n",
      "Epoch: 134410 | Loss: 0.3971118628978729 | Test loss: 0.4013396203517914\n",
      "Epoch: 134420 | Loss: 0.3971000611782074 | Test loss: 0.4013257622718811\n",
      "Epoch: 134430 | Loss: 0.39708828926086426 | Test loss: 0.40131187438964844\n",
      "Epoch: 134440 | Loss: 0.3970765769481659 | Test loss: 0.40129804611206055\n",
      "Epoch: 134450 | Loss: 0.39706477522850037 | Test loss: 0.40128421783447266\n",
      "Epoch: 134460 | Loss: 0.3970530331134796 | Test loss: 0.4012703597545624\n",
      "Epoch: 134470 | Loss: 0.3970412611961365 | Test loss: 0.4012565314769745\n",
      "Epoch: 134480 | Loss: 0.3970295190811157 | Test loss: 0.4012426435947418\n",
      "Epoch: 134490 | Loss: 0.3970177173614502 | Test loss: 0.40122881531715393\n",
      "Epoch: 134500 | Loss: 0.39700597524642944 | Test loss: 0.40121498703956604\n",
      "Epoch: 134510 | Loss: 0.3969942033290863 | Test loss: 0.40120115876197815\n",
      "Epoch: 134520 | Loss: 0.39698246121406555 | Test loss: 0.4011872708797455\n",
      "Epoch: 134530 | Loss: 0.3969707190990448 | Test loss: 0.4011734426021576\n",
      "Epoch: 134540 | Loss: 0.3969589173793793 | Test loss: 0.4011595845222473\n",
      "Epoch: 134550 | Loss: 0.3969471752643585 | Test loss: 0.4011457562446594\n",
      "Epoch: 134560 | Loss: 0.39693543314933777 | Test loss: 0.40113192796707153\n",
      "Epoch: 134570 | Loss: 0.39692366123199463 | Test loss: 0.40111809968948364\n",
      "Epoch: 134580 | Loss: 0.3969119191169739 | Test loss: 0.401104211807251\n",
      "Epoch: 134590 | Loss: 0.39690014719963074 | Test loss: 0.4010903835296631\n",
      "Epoch: 134600 | Loss: 0.3968883752822876 | Test loss: 0.4010765254497528\n",
      "Epoch: 134610 | Loss: 0.39687663316726685 | Test loss: 0.4010626971721649\n",
      "Epoch: 134620 | Loss: 0.3968648612499237 | Test loss: 0.40104880928993225\n",
      "Epoch: 134630 | Loss: 0.39685311913490295 | Test loss: 0.40103498101234436\n",
      "Epoch: 134640 | Loss: 0.3968413472175598 | Test loss: 0.4010210931301117\n",
      "Epoch: 134650 | Loss: 0.3968295753002167 | Test loss: 0.4010072648525238\n",
      "Epoch: 134660 | Loss: 0.39681780338287354 | Test loss: 0.4009934365749359\n",
      "Epoch: 134670 | Loss: 0.3968060612678528 | Test loss: 0.400979608297348\n",
      "Epoch: 134680 | Loss: 0.39679428935050964 | Test loss: 0.40096575021743774\n",
      "Epoch: 134690 | Loss: 0.3967825472354889 | Test loss: 0.40095192193984985\n",
      "Epoch: 134700 | Loss: 0.39677080512046814 | Test loss: 0.4009380340576172\n",
      "Epoch: 134710 | Loss: 0.3967590034008026 | Test loss: 0.4009242057800293\n",
      "Epoch: 134720 | Loss: 0.39674726128578186 | Test loss: 0.4009103775024414\n",
      "Epoch: 134730 | Loss: 0.3967354893684387 | Test loss: 0.4008965492248535\n",
      "Epoch: 134740 | Loss: 0.39672374725341797 | Test loss: 0.40088269114494324\n",
      "Epoch: 134750 | Loss: 0.3967120051383972 | Test loss: 0.40086886286735535\n",
      "Epoch: 134760 | Loss: 0.3967002332210541 | Test loss: 0.4008549749851227\n",
      "Epoch: 134770 | Loss: 0.39668846130371094 | Test loss: 0.4008411467075348\n",
      "Epoch: 134780 | Loss: 0.3966767191886902 | Test loss: 0.4008273184299469\n",
      "Epoch: 134790 | Loss: 0.39666491746902466 | Test loss: 0.40081343054771423\n",
      "Epoch: 134800 | Loss: 0.3966531753540039 | Test loss: 0.40079960227012634\n",
      "Epoch: 134810 | Loss: 0.39664146304130554 | Test loss: 0.40078574419021606\n",
      "Epoch: 134820 | Loss: 0.39662966132164 | Test loss: 0.4007719159126282\n",
      "Epoch: 134830 | Loss: 0.3966178894042969 | Test loss: 0.4007580876350403\n",
      "Epoch: 134840 | Loss: 0.3966061770915985 | Test loss: 0.4007441997528076\n",
      "Epoch: 134850 | Loss: 0.396594375371933 | Test loss: 0.4007303714752197\n",
      "Epoch: 134860 | Loss: 0.39658263325691223 | Test loss: 0.40071651339530945\n",
      "Epoch: 134870 | Loss: 0.3965708613395691 | Test loss: 0.40070268511772156\n",
      "Epoch: 134880 | Loss: 0.39655911922454834 | Test loss: 0.40068885684013367\n",
      "Epoch: 134890 | Loss: 0.3965473175048828 | Test loss: 0.400674968957901\n",
      "Epoch: 134900 | Loss: 0.39653557538986206 | Test loss: 0.4006611406803131\n",
      "Epoch: 134910 | Loss: 0.3965238034725189 | Test loss: 0.40064725279808044\n",
      "Epoch: 134920 | Loss: 0.39651206135749817 | Test loss: 0.40063342452049255\n",
      "Epoch: 134930 | Loss: 0.3965003192424774 | Test loss: 0.40061959624290466\n",
      "Epoch: 134940 | Loss: 0.3964885175228119 | Test loss: 0.4006057679653168\n",
      "Epoch: 134950 | Loss: 0.39647677540779114 | Test loss: 0.4005919098854065\n",
      "Epoch: 134960 | Loss: 0.3964650332927704 | Test loss: 0.4005780816078186\n",
      "Epoch: 134970 | Loss: 0.39645326137542725 | Test loss: 0.40056419372558594\n",
      "Epoch: 134980 | Loss: 0.3964415192604065 | Test loss: 0.40055036544799805\n",
      "Epoch: 134990 | Loss: 0.39642974734306335 | Test loss: 0.40053653717041016\n",
      "Epoch: 135000 | Loss: 0.3964179754257202 | Test loss: 0.4005226790904999\n",
      "Epoch: 135010 | Loss: 0.39640623331069946 | Test loss: 0.400508850812912\n",
      "Epoch: 135020 | Loss: 0.3963944613933563 | Test loss: 0.4004949629306793\n",
      "Epoch: 135030 | Loss: 0.39638271927833557 | Test loss: 0.40048113465309143\n",
      "Epoch: 135040 | Loss: 0.39637094736099243 | Test loss: 0.40046730637550354\n",
      "Epoch: 135050 | Loss: 0.3963591754436493 | Test loss: 0.4004534184932709\n",
      "Epoch: 135060 | Loss: 0.39634740352630615 | Test loss: 0.400439590215683\n",
      "Epoch: 135070 | Loss: 0.3963356614112854 | Test loss: 0.4004257321357727\n",
      "Epoch: 135080 | Loss: 0.39632388949394226 | Test loss: 0.4004119038581848\n",
      "Epoch: 135090 | Loss: 0.3963121473789215 | Test loss: 0.4003980755805969\n",
      "Epoch: 135100 | Loss: 0.39630040526390076 | Test loss: 0.40038418769836426\n",
      "Epoch: 135110 | Loss: 0.39628860354423523 | Test loss: 0.40037035942077637\n",
      "Epoch: 135120 | Loss: 0.3962768614292145 | Test loss: 0.4003565311431885\n",
      "Epoch: 135130 | Loss: 0.39626508951187134 | Test loss: 0.4003426730632782\n",
      "Epoch: 135140 | Loss: 0.3962533473968506 | Test loss: 0.4003288447856903\n",
      "Epoch: 135150 | Loss: 0.39624160528182983 | Test loss: 0.4003150165081024\n",
      "Epoch: 135160 | Loss: 0.3962298333644867 | Test loss: 0.40030112862586975\n",
      "Epoch: 135170 | Loss: 0.39621806144714355 | Test loss: 0.40028730034828186\n",
      "Epoch: 135180 | Loss: 0.3962063193321228 | Test loss: 0.4002734124660492\n",
      "Epoch: 135190 | Loss: 0.3961945176124573 | Test loss: 0.4002595841884613\n",
      "Epoch: 135200 | Loss: 0.3961827754974365 | Test loss: 0.4002457559108734\n",
      "Epoch: 135210 | Loss: 0.39617106318473816 | Test loss: 0.4002319276332855\n",
      "Epoch: 135220 | Loss: 0.39615926146507263 | Test loss: 0.40021806955337524\n",
      "Epoch: 135230 | Loss: 0.3961474895477295 | Test loss: 0.4002041816711426\n",
      "Epoch: 135240 | Loss: 0.39613577723503113 | Test loss: 0.4001903533935547\n",
      "Epoch: 135250 | Loss: 0.3961239755153656 | Test loss: 0.4001765251159668\n",
      "Epoch: 135260 | Loss: 0.39611223340034485 | Test loss: 0.4001626670360565\n",
      "Epoch: 135270 | Loss: 0.3961004614830017 | Test loss: 0.40014883875846863\n",
      "Epoch: 135280 | Loss: 0.39608871936798096 | Test loss: 0.40013495087623596\n",
      "Epoch: 135290 | Loss: 0.39607691764831543 | Test loss: 0.40012112259864807\n",
      "Epoch: 135300 | Loss: 0.3960651755332947 | Test loss: 0.4001072943210602\n",
      "Epoch: 135310 | Loss: 0.39605340361595154 | Test loss: 0.4000934660434723\n",
      "Epoch: 135320 | Loss: 0.3960416615009308 | Test loss: 0.4000795781612396\n",
      "Epoch: 135330 | Loss: 0.39602991938591003 | Test loss: 0.40006574988365173\n",
      "Epoch: 135340 | Loss: 0.3960181176662445 | Test loss: 0.40005189180374146\n",
      "Epoch: 135350 | Loss: 0.39600637555122375 | Test loss: 0.40003806352615356\n",
      "Epoch: 135360 | Loss: 0.395994633436203 | Test loss: 0.4000242352485657\n",
      "Epoch: 135370 | Loss: 0.39598286151885986 | Test loss: 0.4000104069709778\n",
      "Epoch: 135380 | Loss: 0.3959711194038391 | Test loss: 0.3999965190887451\n",
      "Epoch: 135390 | Loss: 0.39595934748649597 | Test loss: 0.3999826908111572\n",
      "Epoch: 135400 | Loss: 0.39594757556915283 | Test loss: 0.39996883273124695\n",
      "Epoch: 135410 | Loss: 0.3959358334541321 | Test loss: 0.39995500445365906\n",
      "Epoch: 135420 | Loss: 0.39592406153678894 | Test loss: 0.3999411165714264\n",
      "Epoch: 135430 | Loss: 0.3959123194217682 | Test loss: 0.3999272882938385\n",
      "Epoch: 135440 | Loss: 0.39590054750442505 | Test loss: 0.3999134302139282\n",
      "Epoch: 135450 | Loss: 0.3958887755870819 | Test loss: 0.39989957213401794\n",
      "Epoch: 135460 | Loss: 0.39587700366973877 | Test loss: 0.39988574385643005\n",
      "Epoch: 135470 | Loss: 0.395865261554718 | Test loss: 0.39987191557884216\n",
      "Epoch: 135480 | Loss: 0.3958534896373749 | Test loss: 0.3998580574989319\n",
      "Epoch: 135490 | Loss: 0.3958417475223541 | Test loss: 0.399844229221344\n",
      "Epoch: 135500 | Loss: 0.3958300054073334 | Test loss: 0.39983034133911133\n",
      "Epoch: 135510 | Loss: 0.39581820368766785 | Test loss: 0.39981651306152344\n",
      "Epoch: 135520 | Loss: 0.3958064615726471 | Test loss: 0.39980268478393555\n",
      "Epoch: 135530 | Loss: 0.39579468965530396 | Test loss: 0.39978882670402527\n",
      "Epoch: 135540 | Loss: 0.3957829475402832 | Test loss: 0.3997749984264374\n",
      "Epoch: 135550 | Loss: 0.39577120542526245 | Test loss: 0.3997611701488495\n",
      "Epoch: 135560 | Loss: 0.3957594335079193 | Test loss: 0.3997472822666168\n",
      "Epoch: 135570 | Loss: 0.39574766159057617 | Test loss: 0.39973345398902893\n",
      "Epoch: 135580 | Loss: 0.3957359194755554 | Test loss: 0.39971959590911865\n",
      "Epoch: 135590 | Loss: 0.3957241177558899 | Test loss: 0.3997057378292084\n",
      "Epoch: 135600 | Loss: 0.39571237564086914 | Test loss: 0.3996919095516205\n",
      "Epoch: 135610 | Loss: 0.3957006633281708 | Test loss: 0.3996780812740326\n",
      "Epoch: 135620 | Loss: 0.39568886160850525 | Test loss: 0.3996642231941223\n",
      "Epoch: 135630 | Loss: 0.3956770896911621 | Test loss: 0.39965036511421204\n",
      "Epoch: 135640 | Loss: 0.39566537737846375 | Test loss: 0.39963650703430176\n",
      "Epoch: 135650 | Loss: 0.3956535756587982 | Test loss: 0.39962267875671387\n",
      "Epoch: 135660 | Loss: 0.39564183354377747 | Test loss: 0.3996088206768036\n",
      "Epoch: 135670 | Loss: 0.3956300616264343 | Test loss: 0.3995949923992157\n",
      "Epoch: 135680 | Loss: 0.3956183195114136 | Test loss: 0.3995811343193054\n",
      "Epoch: 135690 | Loss: 0.39560651779174805 | Test loss: 0.39956727623939514\n",
      "Epoch: 135700 | Loss: 0.3955947756767273 | Test loss: 0.39955344796180725\n",
      "Epoch: 135710 | Loss: 0.39558300375938416 | Test loss: 0.399539589881897\n",
      "Epoch: 135720 | Loss: 0.3955712616443634 | Test loss: 0.3995257318019867\n",
      "Epoch: 135730 | Loss: 0.39555951952934265 | Test loss: 0.3995119035243988\n",
      "Epoch: 135740 | Loss: 0.3955477178096771 | Test loss: 0.3994980752468109\n",
      "Epoch: 135750 | Loss: 0.39553597569465637 | Test loss: 0.39948421716690063\n",
      "Epoch: 135760 | Loss: 0.3955242335796356 | Test loss: 0.39947038888931274\n",
      "Epoch: 135770 | Loss: 0.3955124616622925 | Test loss: 0.39945653080940247\n",
      "Epoch: 135780 | Loss: 0.39550071954727173 | Test loss: 0.3994426727294922\n",
      "Epoch: 135790 | Loss: 0.3954889476299286 | Test loss: 0.3994288444519043\n",
      "Epoch: 135800 | Loss: 0.39547717571258545 | Test loss: 0.399414986371994\n",
      "Epoch: 135810 | Loss: 0.3954654335975647 | Test loss: 0.39940115809440613\n",
      "Epoch: 135820 | Loss: 0.39545366168022156 | Test loss: 0.39938727021217346\n",
      "Epoch: 135830 | Loss: 0.3954419195652008 | Test loss: 0.39937344193458557\n",
      "Epoch: 135840 | Loss: 0.39543014764785767 | Test loss: 0.3993595838546753\n",
      "Epoch: 135850 | Loss: 0.3954183757305145 | Test loss: 0.399345725774765\n",
      "Epoch: 135860 | Loss: 0.3954066038131714 | Test loss: 0.3993318974971771\n",
      "Epoch: 135870 | Loss: 0.39539486169815063 | Test loss: 0.39931806921958923\n",
      "Epoch: 135880 | Loss: 0.3953830897808075 | Test loss: 0.39930421113967896\n",
      "Epoch: 135890 | Loss: 0.39537134766578674 | Test loss: 0.39929038286209106\n",
      "Epoch: 135900 | Loss: 0.395359605550766 | Test loss: 0.3992764949798584\n",
      "Epoch: 135910 | Loss: 0.39534780383110046 | Test loss: 0.3992626667022705\n",
      "Epoch: 135920 | Loss: 0.3953360617160797 | Test loss: 0.3992488384246826\n",
      "Epoch: 135930 | Loss: 0.3953242897987366 | Test loss: 0.39923498034477234\n",
      "Epoch: 135940 | Loss: 0.3953125476837158 | Test loss: 0.39922115206718445\n",
      "Epoch: 135950 | Loss: 0.39530080556869507 | Test loss: 0.39920732378959656\n",
      "Epoch: 135960 | Loss: 0.39528903365135193 | Test loss: 0.3991934359073639\n",
      "Epoch: 135970 | Loss: 0.3952772617340088 | Test loss: 0.399179607629776\n",
      "Epoch: 135980 | Loss: 0.39526551961898804 | Test loss: 0.3991657495498657\n",
      "Epoch: 135990 | Loss: 0.3952537178993225 | Test loss: 0.39915189146995544\n",
      "Epoch: 136000 | Loss: 0.39524197578430176 | Test loss: 0.39913806319236755\n",
      "Epoch: 136010 | Loss: 0.3952302634716034 | Test loss: 0.39912423491477966\n",
      "Epoch: 136020 | Loss: 0.39521846175193787 | Test loss: 0.3991103768348694\n",
      "Epoch: 136030 | Loss: 0.3952066898345947 | Test loss: 0.3990965187549591\n",
      "Epoch: 136040 | Loss: 0.39519497752189636 | Test loss: 0.39908266067504883\n",
      "Epoch: 136050 | Loss: 0.39518317580223083 | Test loss: 0.39906883239746094\n",
      "Epoch: 136060 | Loss: 0.3951714336872101 | Test loss: 0.39905497431755066\n",
      "Epoch: 136070 | Loss: 0.39515966176986694 | Test loss: 0.39904114603996277\n",
      "Epoch: 136080 | Loss: 0.3951479196548462 | Test loss: 0.3990272879600525\n",
      "Epoch: 136090 | Loss: 0.39513611793518066 | Test loss: 0.3990134298801422\n",
      "Epoch: 136100 | Loss: 0.3951243758201599 | Test loss: 0.3989996016025543\n",
      "Epoch: 136110 | Loss: 0.3951126039028168 | Test loss: 0.39898574352264404\n",
      "Epoch: 136120 | Loss: 0.395100861787796 | Test loss: 0.39897188544273376\n",
      "Epoch: 136130 | Loss: 0.39508911967277527 | Test loss: 0.3989580571651459\n",
      "Epoch: 136140 | Loss: 0.39507731795310974 | Test loss: 0.398944228887558\n",
      "Epoch: 136150 | Loss: 0.395065575838089 | Test loss: 0.3989303708076477\n",
      "Epoch: 136160 | Loss: 0.39505383372306824 | Test loss: 0.3989165425300598\n",
      "Epoch: 136170 | Loss: 0.3950420618057251 | Test loss: 0.39890268445014954\n",
      "Epoch: 136180 | Loss: 0.39503031969070435 | Test loss: 0.39888882637023926\n",
      "Epoch: 136190 | Loss: 0.3950185477733612 | Test loss: 0.39887499809265137\n",
      "Epoch: 136200 | Loss: 0.39500677585601807 | Test loss: 0.3988611400127411\n",
      "Epoch: 136210 | Loss: 0.3949950337409973 | Test loss: 0.3988473117351532\n",
      "Epoch: 136220 | Loss: 0.3949832618236542 | Test loss: 0.39883342385292053\n",
      "Epoch: 136230 | Loss: 0.3949715197086334 | Test loss: 0.39881959557533264\n",
      "Epoch: 136240 | Loss: 0.3949597477912903 | Test loss: 0.39880573749542236\n",
      "Epoch: 136250 | Loss: 0.39494797587394714 | Test loss: 0.3987918794155121\n",
      "Epoch: 136260 | Loss: 0.394936203956604 | Test loss: 0.3987780511379242\n",
      "Epoch: 136270 | Loss: 0.39492446184158325 | Test loss: 0.3987642228603363\n",
      "Epoch: 136280 | Loss: 0.3949126899242401 | Test loss: 0.398750364780426\n",
      "Epoch: 136290 | Loss: 0.39490094780921936 | Test loss: 0.39873653650283813\n",
      "Epoch: 136300 | Loss: 0.3948892056941986 | Test loss: 0.39872264862060547\n",
      "Epoch: 136310 | Loss: 0.3948774039745331 | Test loss: 0.3987088203430176\n",
      "Epoch: 136320 | Loss: 0.39486566185951233 | Test loss: 0.3986949920654297\n",
      "Epoch: 136330 | Loss: 0.3948538899421692 | Test loss: 0.3986811339855194\n",
      "Epoch: 136340 | Loss: 0.39484214782714844 | Test loss: 0.3986673057079315\n",
      "Epoch: 136350 | Loss: 0.3948304057121277 | Test loss: 0.39865347743034363\n",
      "Epoch: 136360 | Loss: 0.39481863379478455 | Test loss: 0.39863958954811096\n",
      "Epoch: 136370 | Loss: 0.3948068618774414 | Test loss: 0.39862576127052307\n",
      "Epoch: 136380 | Loss: 0.39479511976242065 | Test loss: 0.3986119031906128\n",
      "Epoch: 136390 | Loss: 0.3947833180427551 | Test loss: 0.3985980451107025\n",
      "Epoch: 136400 | Loss: 0.3947715759277344 | Test loss: 0.3985842168331146\n",
      "Epoch: 136410 | Loss: 0.394759863615036 | Test loss: 0.39857038855552673\n",
      "Epoch: 136420 | Loss: 0.3947480618953705 | Test loss: 0.39855653047561646\n",
      "Epoch: 136430 | Loss: 0.39473628997802734 | Test loss: 0.3985426723957062\n",
      "Epoch: 136440 | Loss: 0.394724577665329 | Test loss: 0.3985288143157959\n",
      "Epoch: 136450 | Loss: 0.39471277594566345 | Test loss: 0.398514986038208\n",
      "Epoch: 136460 | Loss: 0.3947010338306427 | Test loss: 0.39850112795829773\n",
      "Epoch: 136470 | Loss: 0.39468926191329956 | Test loss: 0.39848729968070984\n",
      "Epoch: 136480 | Loss: 0.3946775197982788 | Test loss: 0.39847344160079956\n",
      "Epoch: 136490 | Loss: 0.3946657180786133 | Test loss: 0.3984595835208893\n",
      "Epoch: 136500 | Loss: 0.39465397596359253 | Test loss: 0.3984457552433014\n",
      "Epoch: 136510 | Loss: 0.3946422040462494 | Test loss: 0.3984318971633911\n",
      "Epoch: 136520 | Loss: 0.39463046193122864 | Test loss: 0.39841803908348083\n",
      "Epoch: 136530 | Loss: 0.3946187198162079 | Test loss: 0.39840421080589294\n",
      "Epoch: 136540 | Loss: 0.39460691809654236 | Test loss: 0.39839038252830505\n",
      "Epoch: 136550 | Loss: 0.3945951759815216 | Test loss: 0.3983765244483948\n",
      "Epoch: 136560 | Loss: 0.39458343386650085 | Test loss: 0.3983626961708069\n",
      "Epoch: 136570 | Loss: 0.3945716619491577 | Test loss: 0.3983488380908966\n",
      "Epoch: 136580 | Loss: 0.39455991983413696 | Test loss: 0.39833498001098633\n",
      "Epoch: 136590 | Loss: 0.3945481479167938 | Test loss: 0.39832115173339844\n",
      "Epoch: 136600 | Loss: 0.3945363759994507 | Test loss: 0.39830729365348816\n",
      "Epoch: 136610 | Loss: 0.39452463388442993 | Test loss: 0.39829346537590027\n",
      "Epoch: 136620 | Loss: 0.3945128619670868 | Test loss: 0.3982795774936676\n",
      "Epoch: 136630 | Loss: 0.39450111985206604 | Test loss: 0.3982657492160797\n",
      "Epoch: 136640 | Loss: 0.3944893479347229 | Test loss: 0.39825189113616943\n",
      "Epoch: 136650 | Loss: 0.39447757601737976 | Test loss: 0.39823803305625916\n",
      "Epoch: 136660 | Loss: 0.3944658041000366 | Test loss: 0.39822420477867126\n",
      "Epoch: 136670 | Loss: 0.39445406198501587 | Test loss: 0.3982103765010834\n",
      "Epoch: 136680 | Loss: 0.39444229006767273 | Test loss: 0.3981965184211731\n",
      "Epoch: 136690 | Loss: 0.394430547952652 | Test loss: 0.3981826901435852\n",
      "Epoch: 136700 | Loss: 0.3944188058376312 | Test loss: 0.39816880226135254\n",
      "Epoch: 136710 | Loss: 0.3944070041179657 | Test loss: 0.39815497398376465\n",
      "Epoch: 136720 | Loss: 0.39439526200294495 | Test loss: 0.39814114570617676\n",
      "Epoch: 136730 | Loss: 0.3943834900856018 | Test loss: 0.3981272876262665\n",
      "Epoch: 136740 | Loss: 0.39437174797058105 | Test loss: 0.3981134593486786\n",
      "Epoch: 136750 | Loss: 0.3943600058555603 | Test loss: 0.3980996310710907\n",
      "Epoch: 136760 | Loss: 0.39434823393821716 | Test loss: 0.39808574318885803\n",
      "Epoch: 136770 | Loss: 0.394336462020874 | Test loss: 0.39807191491127014\n",
      "Epoch: 136780 | Loss: 0.39432471990585327 | Test loss: 0.39805805683135986\n",
      "Epoch: 136790 | Loss: 0.39431291818618774 | Test loss: 0.3980441987514496\n",
      "Epoch: 136800 | Loss: 0.394301176071167 | Test loss: 0.3980303704738617\n",
      "Epoch: 136810 | Loss: 0.39428946375846863 | Test loss: 0.3980165421962738\n",
      "Epoch: 136820 | Loss: 0.3942776620388031 | Test loss: 0.3980026841163635\n",
      "Epoch: 136830 | Loss: 0.39426589012145996 | Test loss: 0.39798882603645325\n",
      "Epoch: 136840 | Loss: 0.3942541778087616 | Test loss: 0.39797496795654297\n",
      "Epoch: 136850 | Loss: 0.39424237608909607 | Test loss: 0.3979611396789551\n",
      "Epoch: 136860 | Loss: 0.3942306339740753 | Test loss: 0.3979472815990448\n",
      "Epoch: 136870 | Loss: 0.3942188620567322 | Test loss: 0.3979334533214569\n",
      "Epoch: 136880 | Loss: 0.3942071199417114 | Test loss: 0.39791959524154663\n",
      "Epoch: 136890 | Loss: 0.3941953182220459 | Test loss: 0.39790573716163635\n",
      "Epoch: 136900 | Loss: 0.39418357610702515 | Test loss: 0.39789190888404846\n",
      "Epoch: 136910 | Loss: 0.394171804189682 | Test loss: 0.3978780508041382\n",
      "Epoch: 136920 | Loss: 0.39416006207466125 | Test loss: 0.3978641927242279\n",
      "Epoch: 136930 | Loss: 0.3941483199596405 | Test loss: 0.39785036444664\n",
      "Epoch: 136940 | Loss: 0.394136518239975 | Test loss: 0.3978365361690521\n",
      "Epoch: 136950 | Loss: 0.3941247761249542 | Test loss: 0.39782267808914185\n",
      "Epoch: 136960 | Loss: 0.39411303400993347 | Test loss: 0.39780884981155396\n",
      "Epoch: 136970 | Loss: 0.39410126209259033 | Test loss: 0.3977949917316437\n",
      "Epoch: 136980 | Loss: 0.3940895199775696 | Test loss: 0.3977811336517334\n",
      "Epoch: 136990 | Loss: 0.39407774806022644 | Test loss: 0.3977673053741455\n",
      "Epoch: 137000 | Loss: 0.3940659761428833 | Test loss: 0.39775344729423523\n",
      "Epoch: 137010 | Loss: 0.39405423402786255 | Test loss: 0.39773961901664734\n",
      "Epoch: 137020 | Loss: 0.3940424621105194 | Test loss: 0.3977257311344147\n",
      "Epoch: 137030 | Loss: 0.39403071999549866 | Test loss: 0.3977119028568268\n",
      "Epoch: 137040 | Loss: 0.3940189480781555 | Test loss: 0.3976980447769165\n",
      "Epoch: 137050 | Loss: 0.3940071761608124 | Test loss: 0.3976841866970062\n",
      "Epoch: 137060 | Loss: 0.39399540424346924 | Test loss: 0.39767035841941833\n",
      "Epoch: 137070 | Loss: 0.3939836621284485 | Test loss: 0.39765653014183044\n",
      "Epoch: 137080 | Loss: 0.39397189021110535 | Test loss: 0.39764267206192017\n",
      "Epoch: 137090 | Loss: 0.3939601480960846 | Test loss: 0.3976288437843323\n",
      "Epoch: 137100 | Loss: 0.39394840598106384 | Test loss: 0.3976149559020996\n",
      "Epoch: 137110 | Loss: 0.3939366042613983 | Test loss: 0.3976011276245117\n",
      "Epoch: 137120 | Loss: 0.39392486214637756 | Test loss: 0.39758729934692383\n",
      "Epoch: 137130 | Loss: 0.3939130902290344 | Test loss: 0.39757344126701355\n",
      "Epoch: 137140 | Loss: 0.39390134811401367 | Test loss: 0.39755961298942566\n",
      "Epoch: 137150 | Loss: 0.3938896059989929 | Test loss: 0.39754578471183777\n",
      "Epoch: 137160 | Loss: 0.3938778340816498 | Test loss: 0.3975318968296051\n",
      "Epoch: 137170 | Loss: 0.39386606216430664 | Test loss: 0.3975180685520172\n",
      "Epoch: 137180 | Loss: 0.3938543200492859 | Test loss: 0.39750421047210693\n",
      "Epoch: 137190 | Loss: 0.39384251832962036 | Test loss: 0.39749035239219666\n",
      "Epoch: 137200 | Loss: 0.3938307762145996 | Test loss: 0.39747652411460876\n",
      "Epoch: 137210 | Loss: 0.39381906390190125 | Test loss: 0.3974626958370209\n",
      "Epoch: 137220 | Loss: 0.3938072621822357 | Test loss: 0.3974488377571106\n",
      "Epoch: 137230 | Loss: 0.3937954902648926 | Test loss: 0.3974349796772003\n",
      "Epoch: 137240 | Loss: 0.3937837779521942 | Test loss: 0.39742112159729004\n",
      "Epoch: 137250 | Loss: 0.3937719762325287 | Test loss: 0.39740729331970215\n",
      "Epoch: 137260 | Loss: 0.39376023411750793 | Test loss: 0.39739343523979187\n",
      "Epoch: 137270 | Loss: 0.3937484622001648 | Test loss: 0.397379606962204\n",
      "Epoch: 137280 | Loss: 0.39373672008514404 | Test loss: 0.3973657488822937\n",
      "Epoch: 137290 | Loss: 0.3937249183654785 | Test loss: 0.3973518908023834\n",
      "Epoch: 137300 | Loss: 0.39371317625045776 | Test loss: 0.39733806252479553\n",
      "Epoch: 137310 | Loss: 0.3937014043331146 | Test loss: 0.39732420444488525\n",
      "Epoch: 137320 | Loss: 0.39368966221809387 | Test loss: 0.397310346364975\n",
      "Epoch: 137330 | Loss: 0.3936779201030731 | Test loss: 0.3972965180873871\n",
      "Epoch: 137340 | Loss: 0.3936661183834076 | Test loss: 0.3972826898097992\n",
      "Epoch: 137350 | Loss: 0.39365437626838684 | Test loss: 0.3972688317298889\n",
      "Epoch: 137360 | Loss: 0.3936426341533661 | Test loss: 0.397255003452301\n",
      "Epoch: 137370 | Loss: 0.39363086223602295 | Test loss: 0.39724114537239075\n",
      "Epoch: 137380 | Loss: 0.3936191201210022 | Test loss: 0.39722728729248047\n",
      "Epoch: 137390 | Loss: 0.39360734820365906 | Test loss: 0.3972134590148926\n",
      "Epoch: 137400 | Loss: 0.3935955762863159 | Test loss: 0.3971996009349823\n",
      "Epoch: 137410 | Loss: 0.39358383417129517 | Test loss: 0.3971857726573944\n",
      "Epoch: 137420 | Loss: 0.393572062253952 | Test loss: 0.39717188477516174\n",
      "Epoch: 137430 | Loss: 0.3935603201389313 | Test loss: 0.39715805649757385\n",
      "Epoch: 137440 | Loss: 0.39354854822158813 | Test loss: 0.3971441984176636\n",
      "Epoch: 137450 | Loss: 0.393536776304245 | Test loss: 0.3971303403377533\n",
      "Epoch: 137460 | Loss: 0.39352500438690186 | Test loss: 0.3971165120601654\n",
      "Epoch: 137470 | Loss: 0.3935132622718811 | Test loss: 0.3971026837825775\n",
      "Epoch: 137480 | Loss: 0.39350149035453796 | Test loss: 0.39708882570266724\n",
      "Epoch: 137490 | Loss: 0.3934897482395172 | Test loss: 0.39707499742507935\n",
      "Epoch: 137500 | Loss: 0.39347800612449646 | Test loss: 0.3970611095428467\n",
      "Epoch: 137510 | Loss: 0.39346620440483093 | Test loss: 0.3970472812652588\n",
      "Epoch: 137520 | Loss: 0.3934544622898102 | Test loss: 0.3970334529876709\n",
      "Epoch: 137530 | Loss: 0.39344269037246704 | Test loss: 0.3970195949077606\n",
      "Epoch: 137540 | Loss: 0.3934309482574463 | Test loss: 0.39700576663017273\n",
      "Epoch: 137550 | Loss: 0.39341920614242554 | Test loss: 0.39699193835258484\n",
      "Epoch: 137560 | Loss: 0.3934074342250824 | Test loss: 0.3969780504703522\n",
      "Epoch: 137570 | Loss: 0.39339566230773926 | Test loss: 0.3969642221927643\n",
      "Epoch: 137580 | Loss: 0.3933839201927185 | Test loss: 0.396950364112854\n",
      "Epoch: 137590 | Loss: 0.393372118473053 | Test loss: 0.3969365060329437\n",
      "Epoch: 137600 | Loss: 0.3933603763580322 | Test loss: 0.39692267775535583\n",
      "Epoch: 137610 | Loss: 0.39334866404533386 | Test loss: 0.39690884947776794\n",
      "Epoch: 137620 | Loss: 0.39333686232566833 | Test loss: 0.39689499139785767\n",
      "Epoch: 137630 | Loss: 0.3933250904083252 | Test loss: 0.3968811333179474\n",
      "Epoch: 137640 | Loss: 0.39331337809562683 | Test loss: 0.3968672752380371\n",
      "Epoch: 137650 | Loss: 0.3933015763759613 | Test loss: 0.3968534469604492\n",
      "Epoch: 137660 | Loss: 0.39328983426094055 | Test loss: 0.39683958888053894\n",
      "Epoch: 137670 | Loss: 0.3932780623435974 | Test loss: 0.39682576060295105\n",
      "Epoch: 137680 | Loss: 0.39326632022857666 | Test loss: 0.39681190252304077\n",
      "Epoch: 137690 | Loss: 0.39325451850891113 | Test loss: 0.3967980444431305\n",
      "Epoch: 137700 | Loss: 0.3932427763938904 | Test loss: 0.3967842161655426\n",
      "Epoch: 137710 | Loss: 0.39323100447654724 | Test loss: 0.3967703580856323\n",
      "Epoch: 137720 | Loss: 0.3932192623615265 | Test loss: 0.39675650000572205\n",
      "Epoch: 137730 | Loss: 0.39320752024650574 | Test loss: 0.39674267172813416\n",
      "Epoch: 137740 | Loss: 0.3931957185268402 | Test loss: 0.39672884345054626\n",
      "Epoch: 137750 | Loss: 0.39318397641181946 | Test loss: 0.396714985370636\n",
      "Epoch: 137760 | Loss: 0.3931722342967987 | Test loss: 0.3967011570930481\n",
      "Epoch: 137770 | Loss: 0.39316046237945557 | Test loss: 0.3966872990131378\n",
      "Epoch: 137780 | Loss: 0.3931487202644348 | Test loss: 0.39667344093322754\n",
      "Epoch: 137790 | Loss: 0.3931369483470917 | Test loss: 0.39665961265563965\n",
      "Epoch: 137800 | Loss: 0.39312517642974854 | Test loss: 0.39664575457572937\n",
      "Epoch: 137810 | Loss: 0.3931134343147278 | Test loss: 0.3966319262981415\n",
      "Epoch: 137820 | Loss: 0.39310166239738464 | Test loss: 0.3966180384159088\n",
      "Epoch: 137830 | Loss: 0.3930899202823639 | Test loss: 0.3966042101383209\n",
      "Epoch: 137840 | Loss: 0.39307814836502075 | Test loss: 0.39659035205841064\n",
      "Epoch: 137850 | Loss: 0.3930663764476776 | Test loss: 0.39657649397850037\n",
      "Epoch: 137860 | Loss: 0.3930546045303345 | Test loss: 0.3965626657009125\n",
      "Epoch: 137870 | Loss: 0.3930428624153137 | Test loss: 0.3965488374233246\n",
      "Epoch: 137880 | Loss: 0.3930310904979706 | Test loss: 0.3965349793434143\n",
      "Epoch: 137890 | Loss: 0.39301934838294983 | Test loss: 0.3965211510658264\n",
      "Epoch: 137900 | Loss: 0.3930076062679291 | Test loss: 0.39650726318359375\n",
      "Epoch: 137910 | Loss: 0.39299580454826355 | Test loss: 0.39649343490600586\n",
      "Epoch: 137920 | Loss: 0.3929840624332428 | Test loss: 0.39647960662841797\n",
      "Epoch: 137930 | Loss: 0.39297229051589966 | Test loss: 0.3964657485485077\n",
      "Epoch: 137940 | Loss: 0.3929605484008789 | Test loss: 0.3964519202709198\n",
      "Epoch: 137950 | Loss: 0.39294880628585815 | Test loss: 0.3964380919933319\n",
      "Epoch: 137960 | Loss: 0.392937034368515 | Test loss: 0.39642420411109924\n",
      "Epoch: 137970 | Loss: 0.3929252624511719 | Test loss: 0.39641037583351135\n",
      "Epoch: 137980 | Loss: 0.3929135203361511 | Test loss: 0.3963965177536011\n",
      "Epoch: 137990 | Loss: 0.3929017186164856 | Test loss: 0.3963826596736908\n",
      "Epoch: 138000 | Loss: 0.39288997650146484 | Test loss: 0.3963688313961029\n",
      "Epoch: 138010 | Loss: 0.3928782641887665 | Test loss: 0.396355003118515\n",
      "Epoch: 138020 | Loss: 0.39286646246910095 | Test loss: 0.39634114503860474\n",
      "Epoch: 138030 | Loss: 0.3928546905517578 | Test loss: 0.39632728695869446\n",
      "Epoch: 138040 | Loss: 0.39284297823905945 | Test loss: 0.3963134288787842\n",
      "Epoch: 138050 | Loss: 0.3928311765193939 | Test loss: 0.3962996006011963\n",
      "Epoch: 138060 | Loss: 0.39281943440437317 | Test loss: 0.396285742521286\n",
      "Epoch: 138070 | Loss: 0.39280766248703003 | Test loss: 0.3962719142436981\n",
      "Epoch: 138080 | Loss: 0.3927959203720093 | Test loss: 0.39625805616378784\n",
      "Epoch: 138090 | Loss: 0.39278411865234375 | Test loss: 0.39624419808387756\n",
      "Epoch: 138100 | Loss: 0.392772376537323 | Test loss: 0.3962303698062897\n",
      "Epoch: 138110 | Loss: 0.39276060461997986 | Test loss: 0.3962165117263794\n",
      "Epoch: 138120 | Loss: 0.3927488625049591 | Test loss: 0.3962026536464691\n",
      "Epoch: 138130 | Loss: 0.39273712038993835 | Test loss: 0.3961888253688812\n",
      "Epoch: 138140 | Loss: 0.3927253186702728 | Test loss: 0.39617499709129333\n",
      "Epoch: 138150 | Loss: 0.3927135765552521 | Test loss: 0.39616113901138306\n",
      "Epoch: 138160 | Loss: 0.3927018344402313 | Test loss: 0.39614731073379517\n",
      "Epoch: 138170 | Loss: 0.3926900625228882 | Test loss: 0.3961334526538849\n",
      "Epoch: 138180 | Loss: 0.39267832040786743 | Test loss: 0.3961195945739746\n",
      "Epoch: 138190 | Loss: 0.3926665484905243 | Test loss: 0.3961057662963867\n",
      "Epoch: 138200 | Loss: 0.39265477657318115 | Test loss: 0.39609190821647644\n",
      "Epoch: 138210 | Loss: 0.3926430344581604 | Test loss: 0.39607807993888855\n",
      "Epoch: 138220 | Loss: 0.39263126254081726 | Test loss: 0.3960641920566559\n",
      "Epoch: 138230 | Loss: 0.3926195204257965 | Test loss: 0.396050363779068\n",
      "Epoch: 138240 | Loss: 0.39260774850845337 | Test loss: 0.3960365056991577\n",
      "Epoch: 138250 | Loss: 0.39259597659111023 | Test loss: 0.39602264761924744\n",
      "Epoch: 138260 | Loss: 0.3925842046737671 | Test loss: 0.39600881934165955\n",
      "Epoch: 138270 | Loss: 0.39257246255874634 | Test loss: 0.39599499106407166\n",
      "Epoch: 138280 | Loss: 0.3925606906414032 | Test loss: 0.3959811329841614\n",
      "Epoch: 138290 | Loss: 0.39254894852638245 | Test loss: 0.3959673047065735\n",
      "Epoch: 138300 | Loss: 0.3925372064113617 | Test loss: 0.3959534168243408\n",
      "Epoch: 138310 | Loss: 0.39252540469169617 | Test loss: 0.39593958854675293\n",
      "Epoch: 138320 | Loss: 0.3925136625766754 | Test loss: 0.39592576026916504\n",
      "Epoch: 138330 | Loss: 0.3925018906593323 | Test loss: 0.39591190218925476\n",
      "Epoch: 138340 | Loss: 0.3924901485443115 | Test loss: 0.39589807391166687\n",
      "Epoch: 138350 | Loss: 0.39247840642929077 | Test loss: 0.395884245634079\n",
      "Epoch: 138360 | Loss: 0.39246663451194763 | Test loss: 0.3958703577518463\n",
      "Epoch: 138370 | Loss: 0.3924548625946045 | Test loss: 0.3958565294742584\n",
      "Epoch: 138380 | Loss: 0.39244312047958374 | Test loss: 0.39584267139434814\n",
      "Epoch: 138390 | Loss: 0.3924313187599182 | Test loss: 0.39582881331443787\n",
      "Epoch: 138400 | Loss: 0.39241957664489746 | Test loss: 0.39581498503685\n",
      "Epoch: 138410 | Loss: 0.3924078643321991 | Test loss: 0.3958011567592621\n",
      "Epoch: 138420 | Loss: 0.39239606261253357 | Test loss: 0.3957872986793518\n",
      "Epoch: 138430 | Loss: 0.39238429069519043 | Test loss: 0.39577344059944153\n",
      "Epoch: 138440 | Loss: 0.39237257838249207 | Test loss: 0.39575958251953125\n",
      "Epoch: 138450 | Loss: 0.39236077666282654 | Test loss: 0.39574575424194336\n",
      "Epoch: 138460 | Loss: 0.3923490345478058 | Test loss: 0.3957318961620331\n",
      "Epoch: 138470 | Loss: 0.39233726263046265 | Test loss: 0.3957180678844452\n",
      "Epoch: 138480 | Loss: 0.3923255205154419 | Test loss: 0.3957042098045349\n",
      "Epoch: 138490 | Loss: 0.39231371879577637 | Test loss: 0.39569035172462463\n",
      "Epoch: 138500 | Loss: 0.3923019766807556 | Test loss: 0.39567652344703674\n",
      "Epoch: 138510 | Loss: 0.3922902047634125 | Test loss: 0.39566266536712646\n",
      "Epoch: 138520 | Loss: 0.3922784626483917 | Test loss: 0.3956488072872162\n",
      "Epoch: 138530 | Loss: 0.39226672053337097 | Test loss: 0.3956349790096283\n",
      "Epoch: 138540 | Loss: 0.39225491881370544 | Test loss: 0.3956211507320404\n",
      "Epoch: 138550 | Loss: 0.3922431766986847 | Test loss: 0.3956072926521301\n",
      "Epoch: 138560 | Loss: 0.39223143458366394 | Test loss: 0.39559346437454224\n",
      "Epoch: 138570 | Loss: 0.3922196626663208 | Test loss: 0.39557960629463196\n",
      "Epoch: 138580 | Loss: 0.39220792055130005 | Test loss: 0.3955657482147217\n",
      "Epoch: 138590 | Loss: 0.3921961486339569 | Test loss: 0.3955519199371338\n",
      "Epoch: 138600 | Loss: 0.39218437671661377 | Test loss: 0.3955380618572235\n",
      "Epoch: 138610 | Loss: 0.392172634601593 | Test loss: 0.3955242335796356\n",
      "Epoch: 138620 | Loss: 0.3921608626842499 | Test loss: 0.39551034569740295\n",
      "Epoch: 138630 | Loss: 0.3921491205692291 | Test loss: 0.39549651741981506\n",
      "Epoch: 138640 | Loss: 0.392137348651886 | Test loss: 0.3954826593399048\n",
      "Epoch: 138650 | Loss: 0.39212557673454285 | Test loss: 0.3954688012599945\n",
      "Epoch: 138660 | Loss: 0.3921138048171997 | Test loss: 0.3954549729824066\n",
      "Epoch: 138670 | Loss: 0.39210206270217896 | Test loss: 0.3954411447048187\n",
      "Epoch: 138680 | Loss: 0.3920902907848358 | Test loss: 0.39542728662490845\n",
      "Epoch: 138690 | Loss: 0.39207854866981506 | Test loss: 0.39541345834732056\n",
      "Epoch: 138700 | Loss: 0.3920668065547943 | Test loss: 0.3953995704650879\n",
      "Epoch: 138710 | Loss: 0.3920550048351288 | Test loss: 0.3953857421875\n",
      "Epoch: 138720 | Loss: 0.39204326272010803 | Test loss: 0.3953719139099121\n",
      "Epoch: 138730 | Loss: 0.3920314908027649 | Test loss: 0.39535805583000183\n",
      "Epoch: 138740 | Loss: 0.39201974868774414 | Test loss: 0.39534422755241394\n",
      "Epoch: 138750 | Loss: 0.3920080065727234 | Test loss: 0.39533039927482605\n",
      "Epoch: 138760 | Loss: 0.39199623465538025 | Test loss: 0.3953165113925934\n",
      "Epoch: 138770 | Loss: 0.3919844627380371 | Test loss: 0.3953026831150055\n",
      "Epoch: 138780 | Loss: 0.39197272062301636 | Test loss: 0.3952888250350952\n",
      "Epoch: 138790 | Loss: 0.39196091890335083 | Test loss: 0.39527496695518494\n",
      "Epoch: 138800 | Loss: 0.3919491767883301 | Test loss: 0.39526113867759705\n",
      "Epoch: 138810 | Loss: 0.3919374644756317 | Test loss: 0.39524731040000916\n",
      "Epoch: 138820 | Loss: 0.3919256627559662 | Test loss: 0.3952334523200989\n",
      "Epoch: 138830 | Loss: 0.39191389083862305 | Test loss: 0.3952195942401886\n",
      "Epoch: 138840 | Loss: 0.3919021785259247 | Test loss: 0.3952057361602783\n",
      "Epoch: 138850 | Loss: 0.39189037680625916 | Test loss: 0.39519190788269043\n",
      "Epoch: 138860 | Loss: 0.3918786346912384 | Test loss: 0.39517804980278015\n",
      "Epoch: 138870 | Loss: 0.39186686277389526 | Test loss: 0.39516422152519226\n",
      "Epoch: 138880 | Loss: 0.3918551206588745 | Test loss: 0.395150363445282\n",
      "Epoch: 138890 | Loss: 0.391843318939209 | Test loss: 0.3951365053653717\n",
      "Epoch: 138900 | Loss: 0.39183157682418823 | Test loss: 0.3951226770877838\n",
      "Epoch: 138910 | Loss: 0.3918198049068451 | Test loss: 0.39510881900787354\n",
      "Epoch: 138920 | Loss: 0.39180806279182434 | Test loss: 0.39509496092796326\n",
      "Epoch: 138930 | Loss: 0.3917963206768036 | Test loss: 0.39508113265037537\n",
      "Epoch: 138940 | Loss: 0.39178451895713806 | Test loss: 0.3950673043727875\n",
      "Epoch: 138950 | Loss: 0.3917727768421173 | Test loss: 0.3950534462928772\n",
      "Epoch: 138960 | Loss: 0.39176103472709656 | Test loss: 0.3950396180152893\n",
      "Epoch: 138970 | Loss: 0.3917492628097534 | Test loss: 0.39502575993537903\n",
      "Epoch: 138980 | Loss: 0.39173752069473267 | Test loss: 0.39501190185546875\n",
      "Epoch: 138990 | Loss: 0.3917257487773895 | Test loss: 0.39499807357788086\n",
      "Epoch: 139000 | Loss: 0.3917139768600464 | Test loss: 0.3949842154979706\n",
      "Epoch: 139010 | Loss: 0.39170223474502563 | Test loss: 0.3949703872203827\n",
      "Epoch: 139020 | Loss: 0.3916904628276825 | Test loss: 0.39495649933815\n",
      "Epoch: 139030 | Loss: 0.39167872071266174 | Test loss: 0.39494267106056213\n",
      "Epoch: 139040 | Loss: 0.3916669487953186 | Test loss: 0.39492881298065186\n",
      "Epoch: 139050 | Loss: 0.39165517687797546 | Test loss: 0.3949149549007416\n",
      "Epoch: 139060 | Loss: 0.3916434049606323 | Test loss: 0.3949011266231537\n",
      "Epoch: 139070 | Loss: 0.3916316628456116 | Test loss: 0.3948872983455658\n",
      "Epoch: 139080 | Loss: 0.39161989092826843 | Test loss: 0.3948734402656555\n",
      "Epoch: 139090 | Loss: 0.3916081488132477 | Test loss: 0.3948596119880676\n",
      "Epoch: 139100 | Loss: 0.39159640669822693 | Test loss: 0.39484572410583496\n",
      "Epoch: 139110 | Loss: 0.3915846049785614 | Test loss: 0.39483189582824707\n",
      "Epoch: 139120 | Loss: 0.39157286286354065 | Test loss: 0.3948180675506592\n",
      "Epoch: 139130 | Loss: 0.3915610909461975 | Test loss: 0.3948042094707489\n",
      "Epoch: 139140 | Loss: 0.39154934883117676 | Test loss: 0.394790381193161\n",
      "Epoch: 139150 | Loss: 0.391537606716156 | Test loss: 0.3947765529155731\n",
      "Epoch: 139160 | Loss: 0.39152583479881287 | Test loss: 0.39476266503334045\n",
      "Epoch: 139170 | Loss: 0.3915140628814697 | Test loss: 0.39474883675575256\n",
      "Epoch: 139180 | Loss: 0.391502320766449 | Test loss: 0.3947349786758423\n",
      "Epoch: 139190 | Loss: 0.39149051904678345 | Test loss: 0.394721120595932\n",
      "Epoch: 139200 | Loss: 0.3914787769317627 | Test loss: 0.3947072923183441\n",
      "Epoch: 139210 | Loss: 0.39146706461906433 | Test loss: 0.3946934640407562\n",
      "Epoch: 139220 | Loss: 0.3914552628993988 | Test loss: 0.39467960596084595\n",
      "Epoch: 139230 | Loss: 0.39144349098205566 | Test loss: 0.39466574788093567\n",
      "Epoch: 139240 | Loss: 0.3914317786693573 | Test loss: 0.3946518898010254\n",
      "Epoch: 139250 | Loss: 0.3914199769496918 | Test loss: 0.3946380615234375\n",
      "Epoch: 139260 | Loss: 0.391408234834671 | Test loss: 0.3946242034435272\n",
      "Epoch: 139270 | Loss: 0.3913964629173279 | Test loss: 0.39461037516593933\n",
      "Epoch: 139280 | Loss: 0.39138472080230713 | Test loss: 0.39459651708602905\n",
      "Epoch: 139290 | Loss: 0.3913729190826416 | Test loss: 0.3945826590061188\n",
      "Epoch: 139300 | Loss: 0.39136117696762085 | Test loss: 0.3945688307285309\n",
      "Epoch: 139310 | Loss: 0.3913494050502777 | Test loss: 0.3945549726486206\n",
      "Epoch: 139320 | Loss: 0.39133769273757935 | Test loss: 0.3945411145687103\n",
      "Epoch: 139330 | Loss: 0.3913259208202362 | Test loss: 0.39452728629112244\n",
      "Epoch: 139340 | Loss: 0.3913141191005707 | Test loss: 0.39451345801353455\n",
      "Epoch: 139350 | Loss: 0.3913023769855499 | Test loss: 0.39449959993362427\n",
      "Epoch: 139360 | Loss: 0.3912906348705292 | Test loss: 0.3944857716560364\n",
      "Epoch: 139370 | Loss: 0.39127886295318604 | Test loss: 0.3944719135761261\n",
      "Epoch: 139380 | Loss: 0.3912671208381653 | Test loss: 0.3944580554962158\n",
      "Epoch: 139390 | Loss: 0.39125534892082214 | Test loss: 0.39444422721862793\n",
      "Epoch: 139400 | Loss: 0.391243577003479 | Test loss: 0.39443036913871765\n",
      "Epoch: 139410 | Loss: 0.39123183488845825 | Test loss: 0.39441654086112976\n",
      "Epoch: 139420 | Loss: 0.3912200629711151 | Test loss: 0.3944026529788971\n",
      "Epoch: 139430 | Loss: 0.39120832085609436 | Test loss: 0.3943888247013092\n",
      "Epoch: 139440 | Loss: 0.3911965489387512 | Test loss: 0.3943749666213989\n",
      "Epoch: 139450 | Loss: 0.3911847770214081 | Test loss: 0.39436110854148865\n",
      "Epoch: 139460 | Loss: 0.39117300510406494 | Test loss: 0.39434728026390076\n",
      "Epoch: 139470 | Loss: 0.3911612629890442 | Test loss: 0.39433345198631287\n",
      "Epoch: 139480 | Loss: 0.39114949107170105 | Test loss: 0.3943195939064026\n",
      "Epoch: 139490 | Loss: 0.3911377489566803 | Test loss: 0.3943057656288147\n",
      "Epoch: 139500 | Loss: 0.39112600684165955 | Test loss: 0.39429187774658203\n",
      "Epoch: 139510 | Loss: 0.391114205121994 | Test loss: 0.39427804946899414\n",
      "Epoch: 139520 | Loss: 0.39110246300697327 | Test loss: 0.39426422119140625\n",
      "Epoch: 139530 | Loss: 0.3910906910896301 | Test loss: 0.39425036311149597\n",
      "Epoch: 139540 | Loss: 0.3910789489746094 | Test loss: 0.3942365348339081\n",
      "Epoch: 139550 | Loss: 0.3910672068595886 | Test loss: 0.3942227065563202\n",
      "Epoch: 139560 | Loss: 0.3910554349422455 | Test loss: 0.3942088186740875\n",
      "Epoch: 139570 | Loss: 0.39104366302490234 | Test loss: 0.39419499039649963\n",
      "Epoch: 139580 | Loss: 0.3910319209098816 | Test loss: 0.39418113231658936\n",
      "Epoch: 139590 | Loss: 0.39102011919021606 | Test loss: 0.3941672742366791\n",
      "Epoch: 139600 | Loss: 0.3910083770751953 | Test loss: 0.3941534459590912\n",
      "Epoch: 139610 | Loss: 0.39099666476249695 | Test loss: 0.3941396176815033\n",
      "Epoch: 139620 | Loss: 0.3909848630428314 | Test loss: 0.394125759601593\n",
      "Epoch: 139630 | Loss: 0.3909730911254883 | Test loss: 0.39411190152168274\n",
      "Epoch: 139640 | Loss: 0.3909613788127899 | Test loss: 0.39409804344177246\n",
      "Epoch: 139650 | Loss: 0.3909495770931244 | Test loss: 0.39408421516418457\n",
      "Epoch: 139660 | Loss: 0.39093783497810364 | Test loss: 0.3940703570842743\n",
      "Epoch: 139670 | Loss: 0.3909260630607605 | Test loss: 0.3940565288066864\n",
      "Epoch: 139680 | Loss: 0.39091432094573975 | Test loss: 0.3940426707267761\n",
      "Epoch: 139690 | Loss: 0.3909025192260742 | Test loss: 0.39402881264686584\n",
      "Epoch: 139700 | Loss: 0.39089077711105347 | Test loss: 0.39401498436927795\n",
      "Epoch: 139710 | Loss: 0.3908790051937103 | Test loss: 0.3940011262893677\n",
      "Epoch: 139720 | Loss: 0.39086729288101196 | Test loss: 0.3939872682094574\n",
      "Epoch: 139730 | Loss: 0.3908555209636688 | Test loss: 0.3939734399318695\n",
      "Epoch: 139740 | Loss: 0.3908437192440033 | Test loss: 0.3939596116542816\n",
      "Epoch: 139750 | Loss: 0.39083197712898254 | Test loss: 0.39394575357437134\n",
      "Epoch: 139760 | Loss: 0.3908202350139618 | Test loss: 0.39393192529678345\n",
      "Epoch: 139770 | Loss: 0.39080846309661865 | Test loss: 0.39391806721687317\n",
      "Epoch: 139780 | Loss: 0.3907967209815979 | Test loss: 0.3939042091369629\n",
      "Epoch: 139790 | Loss: 0.39078494906425476 | Test loss: 0.393890380859375\n",
      "Epoch: 139800 | Loss: 0.3907731771469116 | Test loss: 0.3938765227794647\n",
      "Epoch: 139810 | Loss: 0.39076143503189087 | Test loss: 0.39386269450187683\n",
      "Epoch: 139820 | Loss: 0.39074966311454773 | Test loss: 0.39384880661964417\n",
      "Epoch: 139830 | Loss: 0.390737920999527 | Test loss: 0.3938349783420563\n",
      "Epoch: 139840 | Loss: 0.39072614908218384 | Test loss: 0.393821120262146\n",
      "Epoch: 139850 | Loss: 0.3907143771648407 | Test loss: 0.3938072621822357\n",
      "Epoch: 139860 | Loss: 0.39070260524749756 | Test loss: 0.3937934339046478\n",
      "Epoch: 139870 | Loss: 0.3906908631324768 | Test loss: 0.39377960562705994\n",
      "Epoch: 139880 | Loss: 0.39067909121513367 | Test loss: 0.39376574754714966\n",
      "Epoch: 139890 | Loss: 0.3906673491001129 | Test loss: 0.39375191926956177\n",
      "Epoch: 139900 | Loss: 0.39065560698509216 | Test loss: 0.3937380313873291\n",
      "Epoch: 139910 | Loss: 0.39064380526542664 | Test loss: 0.3937242031097412\n",
      "Epoch: 139920 | Loss: 0.3906320631504059 | Test loss: 0.3937103748321533\n",
      "Epoch: 139930 | Loss: 0.39062029123306274 | Test loss: 0.39369651675224304\n",
      "Epoch: 139940 | Loss: 0.390608549118042 | Test loss: 0.39368268847465515\n",
      "Epoch: 139950 | Loss: 0.39059680700302124 | Test loss: 0.39366886019706726\n",
      "Epoch: 139960 | Loss: 0.3905850350856781 | Test loss: 0.3936549723148346\n",
      "Epoch: 139970 | Loss: 0.39057326316833496 | Test loss: 0.3936411440372467\n",
      "Epoch: 139980 | Loss: 0.3905615210533142 | Test loss: 0.3936272859573364\n",
      "Epoch: 139990 | Loss: 0.3905497193336487 | Test loss: 0.39361342787742615\n",
      "Epoch: 140000 | Loss: 0.39053797721862793 | Test loss: 0.39359959959983826\n",
      "Epoch: 140010 | Loss: 0.39052626490592957 | Test loss: 0.39358577132225037\n",
      "Epoch: 140020 | Loss: 0.39051446318626404 | Test loss: 0.3935719132423401\n",
      "Epoch: 140030 | Loss: 0.3905026912689209 | Test loss: 0.3935580551624298\n",
      "Epoch: 140040 | Loss: 0.39049097895622253 | Test loss: 0.39354419708251953\n",
      "Epoch: 140050 | Loss: 0.390479177236557 | Test loss: 0.39353036880493164\n",
      "Epoch: 140060 | Loss: 0.39046743512153625 | Test loss: 0.39351651072502136\n",
      "Epoch: 140070 | Loss: 0.3904556632041931 | Test loss: 0.39350268244743347\n",
      "Epoch: 140080 | Loss: 0.39044392108917236 | Test loss: 0.3934888243675232\n",
      "Epoch: 140090 | Loss: 0.39043211936950684 | Test loss: 0.3934749662876129\n",
      "Epoch: 140100 | Loss: 0.3904203772544861 | Test loss: 0.393461138010025\n",
      "Epoch: 140110 | Loss: 0.39040860533714294 | Test loss: 0.39344727993011475\n",
      "Epoch: 140120 | Loss: 0.3903968930244446 | Test loss: 0.39343342185020447\n",
      "Epoch: 140130 | Loss: 0.39038512110710144 | Test loss: 0.3934195935726166\n",
      "Epoch: 140140 | Loss: 0.3903733193874359 | Test loss: 0.3934057652950287\n",
      "Epoch: 140150 | Loss: 0.39036157727241516 | Test loss: 0.3933919072151184\n",
      "Epoch: 140160 | Loss: 0.3903498351573944 | Test loss: 0.3933780789375305\n",
      "Epoch: 140170 | Loss: 0.39033806324005127 | Test loss: 0.39336422085762024\n",
      "Epoch: 140180 | Loss: 0.3903263211250305 | Test loss: 0.39335036277770996\n",
      "Epoch: 140190 | Loss: 0.3903145492076874 | Test loss: 0.39333653450012207\n",
      "Epoch: 140200 | Loss: 0.39030277729034424 | Test loss: 0.3933226764202118\n",
      "Epoch: 140210 | Loss: 0.3902910351753235 | Test loss: 0.3933088481426239\n",
      "Epoch: 140220 | Loss: 0.39027926325798035 | Test loss: 0.39329496026039124\n",
      "Epoch: 140230 | Loss: 0.3902675211429596 | Test loss: 0.39328113198280334\n",
      "Epoch: 140240 | Loss: 0.39025574922561646 | Test loss: 0.39326727390289307\n",
      "Epoch: 140250 | Loss: 0.3902439773082733 | Test loss: 0.3932534158229828\n",
      "Epoch: 140260 | Loss: 0.3902322053909302 | Test loss: 0.3932395875453949\n",
      "Epoch: 140270 | Loss: 0.3902204632759094 | Test loss: 0.393225759267807\n",
      "Epoch: 140280 | Loss: 0.3902086913585663 | Test loss: 0.39321190118789673\n",
      "Epoch: 140290 | Loss: 0.39019694924354553 | Test loss: 0.39319807291030884\n",
      "Epoch: 140300 | Loss: 0.3901852071285248 | Test loss: 0.39318418502807617\n",
      "Epoch: 140310 | Loss: 0.39017340540885925 | Test loss: 0.3931703567504883\n",
      "Epoch: 140320 | Loss: 0.3901616632938385 | Test loss: 0.3931565284729004\n",
      "Epoch: 140330 | Loss: 0.39014989137649536 | Test loss: 0.3931426703929901\n",
      "Epoch: 140340 | Loss: 0.3901381492614746 | Test loss: 0.3931288421154022\n",
      "Epoch: 140350 | Loss: 0.39012640714645386 | Test loss: 0.39311501383781433\n",
      "Epoch: 140360 | Loss: 0.3901146352291107 | Test loss: 0.39310112595558167\n",
      "Epoch: 140370 | Loss: 0.3901028633117676 | Test loss: 0.3930872976779938\n",
      "Epoch: 140380 | Loss: 0.3900911211967468 | Test loss: 0.3930734395980835\n",
      "Epoch: 140390 | Loss: 0.3900793194770813 | Test loss: 0.3930595815181732\n",
      "Epoch: 140400 | Loss: 0.39006757736206055 | Test loss: 0.3930457532405853\n",
      "Epoch: 140410 | Loss: 0.3900558650493622 | Test loss: 0.39303192496299744\n",
      "Epoch: 140420 | Loss: 0.39004406332969666 | Test loss: 0.39301806688308716\n",
      "Epoch: 140430 | Loss: 0.3900322914123535 | Test loss: 0.3930042088031769\n",
      "Epoch: 140440 | Loss: 0.39002057909965515 | Test loss: 0.3929903507232666\n",
      "Epoch: 140450 | Loss: 0.3900087773799896 | Test loss: 0.3929765224456787\n",
      "Epoch: 140460 | Loss: 0.38999703526496887 | Test loss: 0.39296266436576843\n",
      "Epoch: 140470 | Loss: 0.38998526334762573 | Test loss: 0.39294883608818054\n",
      "Epoch: 140480 | Loss: 0.389973521232605 | Test loss: 0.39293497800827026\n",
      "Epoch: 140490 | Loss: 0.38996171951293945 | Test loss: 0.39292111992836\n",
      "Epoch: 140500 | Loss: 0.3899499773979187 | Test loss: 0.3929072916507721\n",
      "Epoch: 140510 | Loss: 0.38993820548057556 | Test loss: 0.3928934335708618\n",
      "Epoch: 140520 | Loss: 0.3899264931678772 | Test loss: 0.39287957549095154\n",
      "Epoch: 140530 | Loss: 0.38991472125053406 | Test loss: 0.39286574721336365\n",
      "Epoch: 140540 | Loss: 0.38990291953086853 | Test loss: 0.39285191893577576\n",
      "Epoch: 140550 | Loss: 0.3898911774158478 | Test loss: 0.3928380608558655\n",
      "Epoch: 140560 | Loss: 0.389879435300827 | Test loss: 0.3928242325782776\n",
      "Epoch: 140570 | Loss: 0.3898676633834839 | Test loss: 0.3928103744983673\n",
      "Epoch: 140580 | Loss: 0.38985592126846313 | Test loss: 0.39279651641845703\n",
      "Epoch: 140590 | Loss: 0.38984414935112 | Test loss: 0.39278268814086914\n",
      "Epoch: 140600 | Loss: 0.38983237743377686 | Test loss: 0.39276883006095886\n",
      "Epoch: 140610 | Loss: 0.3898206353187561 | Test loss: 0.39275500178337097\n",
      "Epoch: 140620 | Loss: 0.38980886340141296 | Test loss: 0.3927411139011383\n",
      "Epoch: 140630 | Loss: 0.3897971212863922 | Test loss: 0.3927272856235504\n",
      "Epoch: 140640 | Loss: 0.3897853493690491 | Test loss: 0.39271342754364014\n",
      "Epoch: 140650 | Loss: 0.38977357745170593 | Test loss: 0.39269956946372986\n",
      "Epoch: 140660 | Loss: 0.3897618055343628 | Test loss: 0.39268574118614197\n",
      "Epoch: 140670 | Loss: 0.38975006341934204 | Test loss: 0.3926719129085541\n",
      "Epoch: 140680 | Loss: 0.3897382915019989 | Test loss: 0.3926580548286438\n",
      "Epoch: 140690 | Loss: 0.38972654938697815 | Test loss: 0.3926442265510559\n",
      "Epoch: 140700 | Loss: 0.3897148072719574 | Test loss: 0.39263033866882324\n",
      "Epoch: 140710 | Loss: 0.38970300555229187 | Test loss: 0.39261651039123535\n",
      "Epoch: 140720 | Loss: 0.3896912634372711 | Test loss: 0.39260268211364746\n",
      "Epoch: 140730 | Loss: 0.389679491519928 | Test loss: 0.3925888240337372\n",
      "Epoch: 140740 | Loss: 0.3896677494049072 | Test loss: 0.3925749957561493\n",
      "Epoch: 140750 | Loss: 0.3896560072898865 | Test loss: 0.3925611674785614\n",
      "Epoch: 140760 | Loss: 0.38964423537254333 | Test loss: 0.39254727959632874\n",
      "Epoch: 140770 | Loss: 0.3896324634552002 | Test loss: 0.39253345131874084\n",
      "Epoch: 140780 | Loss: 0.38962072134017944 | Test loss: 0.39251959323883057\n",
      "Epoch: 140790 | Loss: 0.3896089196205139 | Test loss: 0.3925057351589203\n",
      "Epoch: 140800 | Loss: 0.38959717750549316 | Test loss: 0.3924919068813324\n",
      "Epoch: 140810 | Loss: 0.3895854651927948 | Test loss: 0.3924780786037445\n",
      "Epoch: 140820 | Loss: 0.3895736634731293 | Test loss: 0.39246422052383423\n",
      "Epoch: 140830 | Loss: 0.38956189155578613 | Test loss: 0.39245036244392395\n",
      "Epoch: 140840 | Loss: 0.38955017924308777 | Test loss: 0.39243650436401367\n",
      "Epoch: 140850 | Loss: 0.38953837752342224 | Test loss: 0.3924226760864258\n",
      "Epoch: 140860 | Loss: 0.3895266354084015 | Test loss: 0.3924088180065155\n",
      "Epoch: 140870 | Loss: 0.38951486349105835 | Test loss: 0.3923949897289276\n",
      "Epoch: 140880 | Loss: 0.3895031213760376 | Test loss: 0.39238113164901733\n",
      "Epoch: 140890 | Loss: 0.38949131965637207 | Test loss: 0.39236727356910706\n",
      "Epoch: 140900 | Loss: 0.3894795775413513 | Test loss: 0.39235344529151917\n",
      "Epoch: 140910 | Loss: 0.3894678056240082 | Test loss: 0.3923395872116089\n",
      "Epoch: 140920 | Loss: 0.3894560933113098 | Test loss: 0.3923257291316986\n",
      "Epoch: 140930 | Loss: 0.3894443213939667 | Test loss: 0.3923119008541107\n",
      "Epoch: 140940 | Loss: 0.38943251967430115 | Test loss: 0.3922980725765228\n",
      "Epoch: 140950 | Loss: 0.3894207775592804 | Test loss: 0.39228421449661255\n",
      "Epoch: 140960 | Loss: 0.38940903544425964 | Test loss: 0.39227038621902466\n",
      "Epoch: 140970 | Loss: 0.3893972635269165 | Test loss: 0.3922565281391144\n",
      "Epoch: 140980 | Loss: 0.38938552141189575 | Test loss: 0.3922426700592041\n",
      "Epoch: 140990 | Loss: 0.3893737494945526 | Test loss: 0.3922288417816162\n",
      "Epoch: 141000 | Loss: 0.3893619775772095 | Test loss: 0.39221498370170593\n",
      "Epoch: 141010 | Loss: 0.3893502354621887 | Test loss: 0.39220115542411804\n",
      "Epoch: 141020 | Loss: 0.3893384635448456 | Test loss: 0.3921872675418854\n",
      "Epoch: 141030 | Loss: 0.38932672142982483 | Test loss: 0.3921734392642975\n",
      "Epoch: 141040 | Loss: 0.3893149495124817 | Test loss: 0.3921595811843872\n",
      "Epoch: 141050 | Loss: 0.38930317759513855 | Test loss: 0.39214572310447693\n",
      "Epoch: 141060 | Loss: 0.3892914056777954 | Test loss: 0.39213189482688904\n",
      "Epoch: 141070 | Loss: 0.38927966356277466 | Test loss: 0.39211806654930115\n",
      "Epoch: 141080 | Loss: 0.3892678916454315 | Test loss: 0.39210420846939087\n",
      "Epoch: 141090 | Loss: 0.38925614953041077 | Test loss: 0.392090380191803\n",
      "Epoch: 141100 | Loss: 0.38924440741539 | Test loss: 0.3920764923095703\n",
      "Epoch: 141110 | Loss: 0.3892326056957245 | Test loss: 0.3920626640319824\n",
      "Epoch: 141120 | Loss: 0.38922086358070374 | Test loss: 0.39204883575439453\n",
      "Epoch: 141130 | Loss: 0.3892090916633606 | Test loss: 0.39203497767448425\n",
      "Epoch: 141140 | Loss: 0.38919734954833984 | Test loss: 0.39202114939689636\n",
      "Epoch: 141150 | Loss: 0.3891856074333191 | Test loss: 0.39200732111930847\n",
      "Epoch: 141160 | Loss: 0.38917383551597595 | Test loss: 0.3919934332370758\n",
      "Epoch: 141170 | Loss: 0.3891620635986328 | Test loss: 0.3919796049594879\n",
      "Epoch: 141180 | Loss: 0.38915032148361206 | Test loss: 0.39196574687957764\n",
      "Epoch: 141190 | Loss: 0.38913851976394653 | Test loss: 0.39195188879966736\n",
      "Epoch: 141200 | Loss: 0.3891267776489258 | Test loss: 0.39193806052207947\n",
      "Epoch: 141210 | Loss: 0.3891150653362274 | Test loss: 0.3919242322444916\n",
      "Epoch: 141220 | Loss: 0.3891032636165619 | Test loss: 0.3919103741645813\n",
      "Epoch: 141230 | Loss: 0.38909149169921875 | Test loss: 0.391896516084671\n",
      "Epoch: 141240 | Loss: 0.3890797793865204 | Test loss: 0.39188265800476074\n",
      "Epoch: 141250 | Loss: 0.38906797766685486 | Test loss: 0.39186882972717285\n",
      "Epoch: 141260 | Loss: 0.3890562355518341 | Test loss: 0.3918549716472626\n",
      "Epoch: 141270 | Loss: 0.38904446363449097 | Test loss: 0.3918411433696747\n",
      "Epoch: 141280 | Loss: 0.3890327215194702 | Test loss: 0.3918272852897644\n",
      "Epoch: 141290 | Loss: 0.3890209197998047 | Test loss: 0.3918134272098541\n",
      "Epoch: 141300 | Loss: 0.38900917768478394 | Test loss: 0.39179959893226624\n",
      "Epoch: 141310 | Loss: 0.3889974057674408 | Test loss: 0.39178574085235596\n",
      "Epoch: 141320 | Loss: 0.38898569345474243 | Test loss: 0.3917718827724457\n",
      "Epoch: 141330 | Loss: 0.3889739215373993 | Test loss: 0.3917580544948578\n",
      "Epoch: 141340 | Loss: 0.38896211981773376 | Test loss: 0.3917442262172699\n",
      "Epoch: 141350 | Loss: 0.388950377702713 | Test loss: 0.3917303681373596\n",
      "Epoch: 141360 | Loss: 0.38893863558769226 | Test loss: 0.39171653985977173\n",
      "Epoch: 141370 | Loss: 0.3889268636703491 | Test loss: 0.39170268177986145\n",
      "Epoch: 141380 | Loss: 0.38891512155532837 | Test loss: 0.39168882369995117\n",
      "Epoch: 141390 | Loss: 0.38890334963798523 | Test loss: 0.3916749954223633\n",
      "Epoch: 141400 | Loss: 0.3888915777206421 | Test loss: 0.391661137342453\n",
      "Epoch: 141410 | Loss: 0.38887983560562134 | Test loss: 0.3916473090648651\n",
      "Epoch: 141420 | Loss: 0.3888680636882782 | Test loss: 0.39163342118263245\n",
      "Epoch: 141430 | Loss: 0.38885632157325745 | Test loss: 0.39161959290504456\n",
      "Epoch: 141440 | Loss: 0.3888445496559143 | Test loss: 0.3916057348251343\n",
      "Epoch: 141450 | Loss: 0.38883277773857117 | Test loss: 0.391591876745224\n",
      "Epoch: 141460 | Loss: 0.388821005821228 | Test loss: 0.3915780484676361\n",
      "Epoch: 141470 | Loss: 0.3888092637062073 | Test loss: 0.3915642201900482\n",
      "Epoch: 141480 | Loss: 0.38879749178886414 | Test loss: 0.39155036211013794\n",
      "Epoch: 141490 | Loss: 0.3887857496738434 | Test loss: 0.39153653383255005\n",
      "Epoch: 141500 | Loss: 0.38877400755882263 | Test loss: 0.3915226459503174\n",
      "Epoch: 141510 | Loss: 0.3887622058391571 | Test loss: 0.3915088176727295\n",
      "Epoch: 141520 | Loss: 0.38875046372413635 | Test loss: 0.3914949893951416\n",
      "Epoch: 141530 | Loss: 0.3887386918067932 | Test loss: 0.3914811313152313\n",
      "Epoch: 141540 | Loss: 0.38872694969177246 | Test loss: 0.39146730303764343\n",
      "Epoch: 141550 | Loss: 0.3887152075767517 | Test loss: 0.39145347476005554\n",
      "Epoch: 141560 | Loss: 0.38870343565940857 | Test loss: 0.3914395868778229\n",
      "Epoch: 141570 | Loss: 0.38869166374206543 | Test loss: 0.391425758600235\n",
      "Epoch: 141580 | Loss: 0.3886799216270447 | Test loss: 0.3914119005203247\n",
      "Epoch: 141590 | Loss: 0.38866811990737915 | Test loss: 0.39139804244041443\n",
      "Epoch: 141600 | Loss: 0.3886563777923584 | Test loss: 0.39138421416282654\n",
      "Epoch: 141610 | Loss: 0.38864466547966003 | Test loss: 0.39137038588523865\n",
      "Epoch: 141620 | Loss: 0.3886328637599945 | Test loss: 0.39135652780532837\n",
      "Epoch: 141630 | Loss: 0.38862109184265137 | Test loss: 0.3913426697254181\n",
      "Epoch: 141640 | Loss: 0.388609379529953 | Test loss: 0.3913288116455078\n",
      "Epoch: 141650 | Loss: 0.3885975778102875 | Test loss: 0.3913149833679199\n",
      "Epoch: 141660 | Loss: 0.3885858356952667 | Test loss: 0.39130112528800964\n",
      "Epoch: 141670 | Loss: 0.3885740637779236 | Test loss: 0.39128729701042175\n",
      "Epoch: 141680 | Loss: 0.38856232166290283 | Test loss: 0.3912734389305115\n",
      "Epoch: 141690 | Loss: 0.3885505199432373 | Test loss: 0.3912595808506012\n",
      "Epoch: 141700 | Loss: 0.38853877782821655 | Test loss: 0.3912457525730133\n",
      "Epoch: 141710 | Loss: 0.3885270059108734 | Test loss: 0.391231894493103\n",
      "Epoch: 141720 | Loss: 0.38851529359817505 | Test loss: 0.39121803641319275\n",
      "Epoch: 141730 | Loss: 0.3885035216808319 | Test loss: 0.39120420813560486\n",
      "Epoch: 141740 | Loss: 0.3884917199611664 | Test loss: 0.39119037985801697\n",
      "Epoch: 141750 | Loss: 0.38847997784614563 | Test loss: 0.3911765217781067\n",
      "Epoch: 141760 | Loss: 0.3884682357311249 | Test loss: 0.3911626935005188\n",
      "Epoch: 141770 | Loss: 0.38845646381378174 | Test loss: 0.3911488354206085\n",
      "Epoch: 141780 | Loss: 0.388444721698761 | Test loss: 0.39113497734069824\n",
      "Epoch: 141790 | Loss: 0.38843294978141785 | Test loss: 0.39112114906311035\n",
      "Epoch: 141800 | Loss: 0.3884211778640747 | Test loss: 0.3911072909832001\n",
      "Epoch: 141810 | Loss: 0.38840943574905396 | Test loss: 0.3910934627056122\n",
      "Epoch: 141820 | Loss: 0.3883976638317108 | Test loss: 0.3910795748233795\n",
      "Epoch: 141830 | Loss: 0.38838592171669006 | Test loss: 0.3910657465457916\n",
      "Epoch: 141840 | Loss: 0.3883741497993469 | Test loss: 0.39105188846588135\n",
      "Epoch: 141850 | Loss: 0.3883623778820038 | Test loss: 0.39103803038597107\n",
      "Epoch: 141860 | Loss: 0.38835060596466064 | Test loss: 0.3910242021083832\n",
      "Epoch: 141870 | Loss: 0.3883388638496399 | Test loss: 0.3910103738307953\n",
      "Epoch: 141880 | Loss: 0.38832709193229675 | Test loss: 0.390996515750885\n",
      "Epoch: 141890 | Loss: 0.388315349817276 | Test loss: 0.3909826874732971\n",
      "Epoch: 141900 | Loss: 0.38830360770225525 | Test loss: 0.39096879959106445\n",
      "Epoch: 141910 | Loss: 0.3882918059825897 | Test loss: 0.39095497131347656\n",
      "Epoch: 141920 | Loss: 0.38828006386756897 | Test loss: 0.39094114303588867\n",
      "Epoch: 141930 | Loss: 0.38826829195022583 | Test loss: 0.3909272849559784\n",
      "Epoch: 141940 | Loss: 0.3882565498352051 | Test loss: 0.3909134566783905\n",
      "Epoch: 141950 | Loss: 0.3882448077201843 | Test loss: 0.3908996284008026\n",
      "Epoch: 141960 | Loss: 0.3882330358028412 | Test loss: 0.39088574051856995\n",
      "Epoch: 141970 | Loss: 0.38822126388549805 | Test loss: 0.39087191224098206\n",
      "Epoch: 141980 | Loss: 0.3882095217704773 | Test loss: 0.3908580541610718\n",
      "Epoch: 141990 | Loss: 0.38819772005081177 | Test loss: 0.3908441960811615\n",
      "Epoch: 142000 | Loss: 0.388185977935791 | Test loss: 0.3908303678035736\n",
      "Epoch: 142010 | Loss: 0.38817426562309265 | Test loss: 0.3908165395259857\n",
      "Epoch: 142020 | Loss: 0.3881624639034271 | Test loss: 0.39080268144607544\n",
      "Epoch: 142030 | Loss: 0.388150691986084 | Test loss: 0.39078882336616516\n",
      "Epoch: 142040 | Loss: 0.3881389796733856 | Test loss: 0.3907749652862549\n",
      "Epoch: 142050 | Loss: 0.3881271779537201 | Test loss: 0.390761137008667\n",
      "Epoch: 142060 | Loss: 0.38811543583869934 | Test loss: 0.3907472789287567\n",
      "Epoch: 142070 | Loss: 0.3881036639213562 | Test loss: 0.3907334506511688\n",
      "Epoch: 142080 | Loss: 0.38809192180633545 | Test loss: 0.39071959257125854\n",
      "Epoch: 142090 | Loss: 0.3880801200866699 | Test loss: 0.39070573449134827\n",
      "Epoch: 142100 | Loss: 0.38806837797164917 | Test loss: 0.3906919062137604\n",
      "Epoch: 142110 | Loss: 0.38805660605430603 | Test loss: 0.3906780481338501\n",
      "Epoch: 142120 | Loss: 0.38804489374160767 | Test loss: 0.3906641900539398\n",
      "Epoch: 142130 | Loss: 0.3880331218242645 | Test loss: 0.39065036177635193\n",
      "Epoch: 142140 | Loss: 0.388021320104599 | Test loss: 0.39063653349876404\n",
      "Epoch: 142150 | Loss: 0.38800957798957825 | Test loss: 0.39062267541885376\n",
      "Epoch: 142160 | Loss: 0.3879978358745575 | Test loss: 0.39060884714126587\n",
      "Epoch: 142170 | Loss: 0.38798606395721436 | Test loss: 0.3905949890613556\n",
      "Epoch: 142180 | Loss: 0.3879743218421936 | Test loss: 0.3905811309814453\n",
      "Epoch: 142190 | Loss: 0.38796254992485046 | Test loss: 0.3905673027038574\n",
      "Epoch: 142200 | Loss: 0.3879507780075073 | Test loss: 0.39055344462394714\n",
      "Epoch: 142210 | Loss: 0.3879390358924866 | Test loss: 0.39053961634635925\n",
      "Epoch: 142220 | Loss: 0.38792726397514343 | Test loss: 0.3905257284641266\n",
      "Epoch: 142230 | Loss: 0.3879155218601227 | Test loss: 0.3905119001865387\n",
      "Epoch: 142240 | Loss: 0.38790374994277954 | Test loss: 0.3904980421066284\n",
      "Epoch: 142250 | Loss: 0.3878919780254364 | Test loss: 0.39048418402671814\n",
      "Epoch: 142260 | Loss: 0.38788020610809326 | Test loss: 0.39047035574913025\n",
      "Epoch: 142270 | Loss: 0.3878684639930725 | Test loss: 0.39045652747154236\n",
      "Epoch: 142280 | Loss: 0.38785669207572937 | Test loss: 0.3904426693916321\n",
      "Epoch: 142290 | Loss: 0.3878449499607086 | Test loss: 0.3904288411140442\n",
      "Epoch: 142300 | Loss: 0.38783320784568787 | Test loss: 0.3904149532318115\n",
      "Epoch: 142310 | Loss: 0.38782140612602234 | Test loss: 0.39040112495422363\n",
      "Epoch: 142320 | Loss: 0.3878096640110016 | Test loss: 0.39038729667663574\n",
      "Epoch: 142330 | Loss: 0.38779789209365845 | Test loss: 0.39037343859672546\n",
      "Epoch: 142340 | Loss: 0.3877861499786377 | Test loss: 0.3903596103191376\n",
      "Epoch: 142350 | Loss: 0.38777440786361694 | Test loss: 0.3903457820415497\n",
      "Epoch: 142360 | Loss: 0.3877626359462738 | Test loss: 0.390331894159317\n",
      "Epoch: 142370 | Loss: 0.38775086402893066 | Test loss: 0.3903180658817291\n",
      "Epoch: 142380 | Loss: 0.3877391219139099 | Test loss: 0.39030420780181885\n",
      "Epoch: 142390 | Loss: 0.3877273201942444 | Test loss: 0.39029034972190857\n",
      "Epoch: 142400 | Loss: 0.38771557807922363 | Test loss: 0.3902765214443207\n",
      "Epoch: 142410 | Loss: 0.38770386576652527 | Test loss: 0.3902626931667328\n",
      "Epoch: 142420 | Loss: 0.38769206404685974 | Test loss: 0.3902488350868225\n",
      "Epoch: 142430 | Loss: 0.3876802921295166 | Test loss: 0.39023497700691223\n",
      "Epoch: 142440 | Loss: 0.38766857981681824 | Test loss: 0.39022111892700195\n",
      "Epoch: 142450 | Loss: 0.3876567780971527 | Test loss: 0.39020729064941406\n",
      "Epoch: 142460 | Loss: 0.38764503598213196 | Test loss: 0.3901934325695038\n",
      "Epoch: 142470 | Loss: 0.3876332640647888 | Test loss: 0.3901796042919159\n",
      "Epoch: 142480 | Loss: 0.38762152194976807 | Test loss: 0.3901657462120056\n",
      "Epoch: 142490 | Loss: 0.38760972023010254 | Test loss: 0.39015188813209534\n",
      "Epoch: 142500 | Loss: 0.3875979781150818 | Test loss: 0.39013805985450745\n",
      "Epoch: 142510 | Loss: 0.38758620619773865 | Test loss: 0.39012420177459717\n",
      "Epoch: 142520 | Loss: 0.3875744938850403 | Test loss: 0.3901103436946869\n",
      "Epoch: 142530 | Loss: 0.38756272196769714 | Test loss: 0.390096515417099\n",
      "Epoch: 142540 | Loss: 0.3875509202480316 | Test loss: 0.3900826871395111\n",
      "Epoch: 142550 | Loss: 0.38753917813301086 | Test loss: 0.39006882905960083\n",
      "Epoch: 142560 | Loss: 0.3875274360179901 | Test loss: 0.39005500078201294\n",
      "Epoch: 142570 | Loss: 0.387515664100647 | Test loss: 0.39004114270210266\n",
      "Epoch: 142580 | Loss: 0.3875039219856262 | Test loss: 0.3900272846221924\n",
      "Epoch: 142590 | Loss: 0.3874921500682831 | Test loss: 0.3900134563446045\n",
      "Epoch: 142600 | Loss: 0.38748037815093994 | Test loss: 0.3899995982646942\n",
      "Epoch: 142610 | Loss: 0.3874686360359192 | Test loss: 0.3899857699871063\n",
      "Epoch: 142620 | Loss: 0.38745686411857605 | Test loss: 0.38997188210487366\n",
      "Epoch: 142630 | Loss: 0.3874451220035553 | Test loss: 0.38995805382728577\n",
      "Epoch: 142640 | Loss: 0.38743335008621216 | Test loss: 0.3899441957473755\n",
      "Epoch: 142650 | Loss: 0.387421578168869 | Test loss: 0.3899303376674652\n",
      "Epoch: 142660 | Loss: 0.3874098062515259 | Test loss: 0.3899165093898773\n",
      "Epoch: 142670 | Loss: 0.3873980641365051 | Test loss: 0.38990268111228943\n",
      "Epoch: 142680 | Loss: 0.387386292219162 | Test loss: 0.38988882303237915\n",
      "Epoch: 142690 | Loss: 0.38737455010414124 | Test loss: 0.38987499475479126\n",
      "Epoch: 142700 | Loss: 0.3873628079891205 | Test loss: 0.3898611068725586\n",
      "Epoch: 142710 | Loss: 0.38735100626945496 | Test loss: 0.3898472785949707\n",
      "Epoch: 142720 | Loss: 0.3873392641544342 | Test loss: 0.3898334503173828\n",
      "Epoch: 142730 | Loss: 0.38732749223709106 | Test loss: 0.38981959223747253\n",
      "Epoch: 142740 | Loss: 0.3873157501220703 | Test loss: 0.38980576395988464\n",
      "Epoch: 142750 | Loss: 0.38730400800704956 | Test loss: 0.38979193568229675\n",
      "Epoch: 142760 | Loss: 0.3872922360897064 | Test loss: 0.3897780478000641\n",
      "Epoch: 142770 | Loss: 0.3872804641723633 | Test loss: 0.3897642195224762\n",
      "Epoch: 142780 | Loss: 0.38726872205734253 | Test loss: 0.3897503614425659\n",
      "Epoch: 142790 | Loss: 0.387256920337677 | Test loss: 0.38973650336265564\n",
      "Epoch: 142800 | Loss: 0.38724517822265625 | Test loss: 0.38972267508506775\n",
      "Epoch: 142810 | Loss: 0.3872334659099579 | Test loss: 0.38970884680747986\n",
      "Epoch: 142820 | Loss: 0.38722166419029236 | Test loss: 0.3896949887275696\n",
      "Epoch: 142830 | Loss: 0.3872098922729492 | Test loss: 0.3896811306476593\n",
      "Epoch: 142840 | Loss: 0.38719817996025085 | Test loss: 0.389667272567749\n",
      "Epoch: 142850 | Loss: 0.3871863782405853 | Test loss: 0.38965344429016113\n",
      "Epoch: 142860 | Loss: 0.3871746361255646 | Test loss: 0.38963958621025085\n",
      "Epoch: 142870 | Loss: 0.38716286420822144 | Test loss: 0.38962575793266296\n",
      "Epoch: 142880 | Loss: 0.3871511220932007 | Test loss: 0.3896118998527527\n",
      "Epoch: 142890 | Loss: 0.38713932037353516 | Test loss: 0.3895980417728424\n",
      "Epoch: 142900 | Loss: 0.3871275782585144 | Test loss: 0.3895842134952545\n",
      "Epoch: 142910 | Loss: 0.38711580634117126 | Test loss: 0.38957035541534424\n",
      "Epoch: 142920 | Loss: 0.3871040940284729 | Test loss: 0.38955649733543396\n",
      "Epoch: 142930 | Loss: 0.38709232211112976 | Test loss: 0.38954266905784607\n",
      "Epoch: 142940 | Loss: 0.38708052039146423 | Test loss: 0.3895288407802582\n",
      "Epoch: 142950 | Loss: 0.3870687782764435 | Test loss: 0.3895149827003479\n",
      "Epoch: 142960 | Loss: 0.38705703616142273 | Test loss: 0.38950115442276\n",
      "Epoch: 142970 | Loss: 0.3870452642440796 | Test loss: 0.38948729634284973\n",
      "Epoch: 142980 | Loss: 0.38703352212905884 | Test loss: 0.38947343826293945\n",
      "Epoch: 142990 | Loss: 0.3870217502117157 | Test loss: 0.38945960998535156\n",
      "Epoch: 143000 | Loss: 0.38700997829437256 | Test loss: 0.3894457519054413\n",
      "Epoch: 143010 | Loss: 0.3869982361793518 | Test loss: 0.3894319236278534\n",
      "Epoch: 143020 | Loss: 0.38698646426200867 | Test loss: 0.3894180357456207\n",
      "Epoch: 143030 | Loss: 0.3869747221469879 | Test loss: 0.38940420746803284\n",
      "Epoch: 143040 | Loss: 0.3869629502296448 | Test loss: 0.38939034938812256\n",
      "Epoch: 143050 | Loss: 0.38695117831230164 | Test loss: 0.3893764913082123\n",
      "Epoch: 143060 | Loss: 0.3869394063949585 | Test loss: 0.3893626630306244\n",
      "Epoch: 143070 | Loss: 0.38692766427993774 | Test loss: 0.3893488347530365\n",
      "Epoch: 143080 | Loss: 0.3869158923625946 | Test loss: 0.3893349766731262\n",
      "Epoch: 143090 | Loss: 0.38690415024757385 | Test loss: 0.38932114839553833\n",
      "Epoch: 143100 | Loss: 0.3868924081325531 | Test loss: 0.38930726051330566\n",
      "Epoch: 143110 | Loss: 0.3868806064128876 | Test loss: 0.3892934322357178\n",
      "Epoch: 143120 | Loss: 0.3868688642978668 | Test loss: 0.3892796039581299\n",
      "Epoch: 143130 | Loss: 0.3868570923805237 | Test loss: 0.3892657458782196\n",
      "Epoch: 143140 | Loss: 0.38684535026550293 | Test loss: 0.3892519176006317\n",
      "Epoch: 143150 | Loss: 0.3868336081504822 | Test loss: 0.3892380893230438\n",
      "Epoch: 143160 | Loss: 0.38682183623313904 | Test loss: 0.38922420144081116\n",
      "Epoch: 143170 | Loss: 0.3868100643157959 | Test loss: 0.38921037316322327\n",
      "Epoch: 143180 | Loss: 0.38679832220077515 | Test loss: 0.389196515083313\n",
      "Epoch: 143190 | Loss: 0.3867865204811096 | Test loss: 0.3891826570034027\n",
      "Epoch: 143200 | Loss: 0.38677477836608887 | Test loss: 0.3891688287258148\n",
      "Epoch: 143210 | Loss: 0.3867630660533905 | Test loss: 0.38915500044822693\n",
      "Epoch: 143220 | Loss: 0.386751264333725 | Test loss: 0.38914114236831665\n",
      "Epoch: 143230 | Loss: 0.38673949241638184 | Test loss: 0.38912728428840637\n",
      "Epoch: 143240 | Loss: 0.38672778010368347 | Test loss: 0.3891134262084961\n",
      "Epoch: 143250 | Loss: 0.38671597838401794 | Test loss: 0.3890995979309082\n",
      "Epoch: 143260 | Loss: 0.3867042362689972 | Test loss: 0.3890857398509979\n",
      "Epoch: 143270 | Loss: 0.38669246435165405 | Test loss: 0.38907191157341003\n",
      "Epoch: 143280 | Loss: 0.3866807222366333 | Test loss: 0.38905805349349976\n",
      "Epoch: 143290 | Loss: 0.3866689205169678 | Test loss: 0.3890441954135895\n",
      "Epoch: 143300 | Loss: 0.386657178401947 | Test loss: 0.3890303671360016\n",
      "Epoch: 143310 | Loss: 0.3866454064846039 | Test loss: 0.3890165090560913\n",
      "Epoch: 143320 | Loss: 0.3866336941719055 | Test loss: 0.38900265097618103\n",
      "Epoch: 143330 | Loss: 0.3866219222545624 | Test loss: 0.38898882269859314\n",
      "Epoch: 143340 | Loss: 0.38661012053489685 | Test loss: 0.38897499442100525\n",
      "Epoch: 143350 | Loss: 0.3865983784198761 | Test loss: 0.38896113634109497\n",
      "Epoch: 143360 | Loss: 0.38658663630485535 | Test loss: 0.3889473080635071\n",
      "Epoch: 143370 | Loss: 0.3865748643875122 | Test loss: 0.3889334499835968\n",
      "Epoch: 143380 | Loss: 0.38656312227249146 | Test loss: 0.3889195919036865\n",
      "Epoch: 143390 | Loss: 0.3865513503551483 | Test loss: 0.38890576362609863\n",
      "Epoch: 143400 | Loss: 0.3865395784378052 | Test loss: 0.38889190554618835\n",
      "Epoch: 143410 | Loss: 0.3865278363227844 | Test loss: 0.38887807726860046\n",
      "Epoch: 143420 | Loss: 0.3865160644054413 | Test loss: 0.3888641893863678\n",
      "Epoch: 143430 | Loss: 0.38650432229042053 | Test loss: 0.3888503611087799\n",
      "Epoch: 143440 | Loss: 0.3864925503730774 | Test loss: 0.38883650302886963\n",
      "Epoch: 143450 | Loss: 0.38648077845573425 | Test loss: 0.38882264494895935\n",
      "Epoch: 143460 | Loss: 0.3864690065383911 | Test loss: 0.38880881667137146\n",
      "Epoch: 143470 | Loss: 0.38645726442337036 | Test loss: 0.38879498839378357\n",
      "Epoch: 143480 | Loss: 0.3864454925060272 | Test loss: 0.3887811303138733\n",
      "Epoch: 143490 | Loss: 0.38643375039100647 | Test loss: 0.3887673020362854\n",
      "Epoch: 143500 | Loss: 0.3864220082759857 | Test loss: 0.38875341415405273\n",
      "Epoch: 143510 | Loss: 0.3864102065563202 | Test loss: 0.38873958587646484\n",
      "Epoch: 143520 | Loss: 0.38639846444129944 | Test loss: 0.38872575759887695\n",
      "Epoch: 143530 | Loss: 0.3863866925239563 | Test loss: 0.3887118995189667\n",
      "Epoch: 143540 | Loss: 0.38637495040893555 | Test loss: 0.3886980712413788\n",
      "Epoch: 143550 | Loss: 0.3863632082939148 | Test loss: 0.3886842429637909\n",
      "Epoch: 143560 | Loss: 0.38635143637657166 | Test loss: 0.3886703550815582\n",
      "Epoch: 143570 | Loss: 0.3863396644592285 | Test loss: 0.38865652680397034\n",
      "Epoch: 143580 | Loss: 0.38632792234420776 | Test loss: 0.38864266872406006\n",
      "Epoch: 143590 | Loss: 0.38631612062454224 | Test loss: 0.3886288106441498\n",
      "Epoch: 143600 | Loss: 0.3863043785095215 | Test loss: 0.3886149823665619\n",
      "Epoch: 143610 | Loss: 0.3862926661968231 | Test loss: 0.388601154088974\n",
      "Epoch: 143620 | Loss: 0.3862808644771576 | Test loss: 0.3885872960090637\n",
      "Epoch: 143630 | Loss: 0.38626909255981445 | Test loss: 0.38857343792915344\n",
      "Epoch: 143640 | Loss: 0.3862573802471161 | Test loss: 0.38855957984924316\n",
      "Epoch: 143650 | Loss: 0.38624557852745056 | Test loss: 0.3885457515716553\n",
      "Epoch: 143660 | Loss: 0.3862338364124298 | Test loss: 0.388531893491745\n",
      "Epoch: 143670 | Loss: 0.38622206449508667 | Test loss: 0.3885180652141571\n",
      "Epoch: 143680 | Loss: 0.3862103223800659 | Test loss: 0.3885042071342468\n",
      "Epoch: 143690 | Loss: 0.3861985206604004 | Test loss: 0.38849034905433655\n",
      "Epoch: 143700 | Loss: 0.38618677854537964 | Test loss: 0.38847652077674866\n",
      "Epoch: 143710 | Loss: 0.3861750066280365 | Test loss: 0.3884626626968384\n",
      "Epoch: 143720 | Loss: 0.38616329431533813 | Test loss: 0.3884488046169281\n",
      "Epoch: 143730 | Loss: 0.386151522397995 | Test loss: 0.3884349763393402\n",
      "Epoch: 143740 | Loss: 0.38613972067832947 | Test loss: 0.3884211480617523\n",
      "Epoch: 143750 | Loss: 0.3861279785633087 | Test loss: 0.38840728998184204\n",
      "Epoch: 143760 | Loss: 0.38611623644828796 | Test loss: 0.38839346170425415\n",
      "Epoch: 143770 | Loss: 0.3861044645309448 | Test loss: 0.38837960362434387\n",
      "Epoch: 143780 | Loss: 0.3860927224159241 | Test loss: 0.3883657455444336\n",
      "Epoch: 143790 | Loss: 0.38608095049858093 | Test loss: 0.3883519172668457\n",
      "Epoch: 143800 | Loss: 0.3860691785812378 | Test loss: 0.3883380591869354\n",
      "Epoch: 143810 | Loss: 0.38605743646621704 | Test loss: 0.38832423090934753\n",
      "Epoch: 143820 | Loss: 0.3860456645488739 | Test loss: 0.38831034302711487\n",
      "Epoch: 143830 | Loss: 0.38603392243385315 | Test loss: 0.388296514749527\n",
      "Epoch: 143840 | Loss: 0.38602215051651 | Test loss: 0.3882826566696167\n",
      "Epoch: 143850 | Loss: 0.38601037859916687 | Test loss: 0.3882687985897064\n",
      "Epoch: 143860 | Loss: 0.38599860668182373 | Test loss: 0.38825497031211853\n",
      "Epoch: 143870 | Loss: 0.385986864566803 | Test loss: 0.38824114203453064\n",
      "Epoch: 143880 | Loss: 0.38597509264945984 | Test loss: 0.38822728395462036\n",
      "Epoch: 143890 | Loss: 0.3859633505344391 | Test loss: 0.38821345567703247\n",
      "Epoch: 143900 | Loss: 0.38595160841941833 | Test loss: 0.3881995677947998\n",
      "Epoch: 143910 | Loss: 0.3859398066997528 | Test loss: 0.3881857395172119\n",
      "Epoch: 143920 | Loss: 0.38592806458473206 | Test loss: 0.388171911239624\n",
      "Epoch: 143930 | Loss: 0.3859162926673889 | Test loss: 0.38815805315971375\n",
      "Epoch: 143940 | Loss: 0.38590455055236816 | Test loss: 0.38814422488212585\n",
      "Epoch: 143950 | Loss: 0.3858928084373474 | Test loss: 0.38813039660453796\n",
      "Epoch: 143960 | Loss: 0.3858810365200043 | Test loss: 0.3881165087223053\n",
      "Epoch: 143970 | Loss: 0.38586926460266113 | Test loss: 0.3881026804447174\n",
      "Epoch: 143980 | Loss: 0.3858575224876404 | Test loss: 0.38808882236480713\n",
      "Epoch: 143990 | Loss: 0.38584572076797485 | Test loss: 0.38807496428489685\n",
      "Epoch: 144000 | Loss: 0.3858339786529541 | Test loss: 0.38806113600730896\n",
      "Epoch: 144010 | Loss: 0.38582226634025574 | Test loss: 0.38804730772972107\n",
      "Epoch: 144020 | Loss: 0.3858104646205902 | Test loss: 0.3880334496498108\n",
      "Epoch: 144030 | Loss: 0.38579869270324707 | Test loss: 0.3880195915699005\n",
      "Epoch: 144040 | Loss: 0.3857869803905487 | Test loss: 0.38800573348999023\n",
      "Epoch: 144050 | Loss: 0.3857751786708832 | Test loss: 0.38799190521240234\n",
      "Epoch: 144060 | Loss: 0.3857634365558624 | Test loss: 0.38797804713249207\n",
      "Epoch: 144070 | Loss: 0.3857516646385193 | Test loss: 0.3879642188549042\n",
      "Epoch: 144080 | Loss: 0.38573992252349854 | Test loss: 0.3879503607749939\n",
      "Epoch: 144090 | Loss: 0.385728120803833 | Test loss: 0.3879365026950836\n",
      "Epoch: 144100 | Loss: 0.38571637868881226 | Test loss: 0.3879226744174957\n",
      "Epoch: 144110 | Loss: 0.3857046067714691 | Test loss: 0.38790881633758545\n",
      "Epoch: 144120 | Loss: 0.38569289445877075 | Test loss: 0.38789495825767517\n",
      "Epoch: 144130 | Loss: 0.3856811225414276 | Test loss: 0.3878811299800873\n",
      "Epoch: 144140 | Loss: 0.3856693208217621 | Test loss: 0.3878673017024994\n",
      "Epoch: 144150 | Loss: 0.38565757870674133 | Test loss: 0.3878534436225891\n",
      "Epoch: 144160 | Loss: 0.3856458365917206 | Test loss: 0.3878396153450012\n",
      "Epoch: 144170 | Loss: 0.38563406467437744 | Test loss: 0.38782575726509094\n",
      "Epoch: 144180 | Loss: 0.3856223225593567 | Test loss: 0.38781189918518066\n",
      "Epoch: 144190 | Loss: 0.38561055064201355 | Test loss: 0.3877980709075928\n",
      "Epoch: 144200 | Loss: 0.3855987787246704 | Test loss: 0.3877842128276825\n",
      "Epoch: 144210 | Loss: 0.38558703660964966 | Test loss: 0.3877703845500946\n",
      "Epoch: 144220 | Loss: 0.3855752646923065 | Test loss: 0.38775649666786194\n",
      "Epoch: 144230 | Loss: 0.38556352257728577 | Test loss: 0.38774266839027405\n",
      "Epoch: 144240 | Loss: 0.3855517506599426 | Test loss: 0.38772881031036377\n",
      "Epoch: 144250 | Loss: 0.3855399787425995 | Test loss: 0.3877149522304535\n",
      "Epoch: 144260 | Loss: 0.38552820682525635 | Test loss: 0.3877011239528656\n",
      "Epoch: 144270 | Loss: 0.3855164647102356 | Test loss: 0.3876872956752777\n",
      "Epoch: 144280 | Loss: 0.38550469279289246 | Test loss: 0.38767343759536743\n",
      "Epoch: 144290 | Loss: 0.3854929506778717 | Test loss: 0.38765960931777954\n",
      "Epoch: 144300 | Loss: 0.38548120856285095 | Test loss: 0.3876457214355469\n",
      "Epoch: 144310 | Loss: 0.3854694068431854 | Test loss: 0.387631893157959\n",
      "Epoch: 144320 | Loss: 0.3854576647281647 | Test loss: 0.3876180648803711\n",
      "Epoch: 144330 | Loss: 0.38544589281082153 | Test loss: 0.3876042068004608\n",
      "Epoch: 144340 | Loss: 0.3854341506958008 | Test loss: 0.3875903785228729\n",
      "Epoch: 144350 | Loss: 0.38542240858078003 | Test loss: 0.38757655024528503\n",
      "Epoch: 144360 | Loss: 0.3854106366634369 | Test loss: 0.38756266236305237\n",
      "Epoch: 144370 | Loss: 0.38539886474609375 | Test loss: 0.3875488340854645\n",
      "Epoch: 144380 | Loss: 0.385387122631073 | Test loss: 0.3875349760055542\n",
      "Epoch: 144390 | Loss: 0.38537532091140747 | Test loss: 0.3875211179256439\n",
      "Epoch: 144400 | Loss: 0.3853635787963867 | Test loss: 0.38750728964805603\n",
      "Epoch: 144410 | Loss: 0.38535186648368835 | Test loss: 0.38749346137046814\n",
      "Epoch: 144420 | Loss: 0.3853400647640228 | Test loss: 0.38747960329055786\n",
      "Epoch: 144430 | Loss: 0.3853282928466797 | Test loss: 0.3874657452106476\n",
      "Epoch: 144440 | Loss: 0.3853165805339813 | Test loss: 0.3874518871307373\n",
      "Epoch: 144450 | Loss: 0.3853047788143158 | Test loss: 0.3874380588531494\n",
      "Epoch: 144460 | Loss: 0.38529303669929504 | Test loss: 0.38742420077323914\n",
      "Epoch: 144470 | Loss: 0.3852812647819519 | Test loss: 0.38741037249565125\n",
      "Epoch: 144480 | Loss: 0.38526952266693115 | Test loss: 0.38739651441574097\n",
      "Epoch: 144490 | Loss: 0.3852577209472656 | Test loss: 0.3873826563358307\n",
      "Epoch: 144500 | Loss: 0.3852459788322449 | Test loss: 0.3873688280582428\n",
      "Epoch: 144510 | Loss: 0.38523420691490173 | Test loss: 0.3873549699783325\n",
      "Epoch: 144520 | Loss: 0.38522249460220337 | Test loss: 0.38734111189842224\n",
      "Epoch: 144530 | Loss: 0.38521072268486023 | Test loss: 0.38732728362083435\n",
      "Epoch: 144540 | Loss: 0.3851989209651947 | Test loss: 0.38731345534324646\n",
      "Epoch: 144550 | Loss: 0.38518717885017395 | Test loss: 0.3872995972633362\n",
      "Epoch: 144560 | Loss: 0.3851754367351532 | Test loss: 0.3872857689857483\n",
      "Epoch: 144570 | Loss: 0.38516366481781006 | Test loss: 0.387271910905838\n",
      "Epoch: 144580 | Loss: 0.3851519227027893 | Test loss: 0.38725805282592773\n",
      "Epoch: 144590 | Loss: 0.38514015078544617 | Test loss: 0.38724422454833984\n",
      "Epoch: 144600 | Loss: 0.385128378868103 | Test loss: 0.38723036646842957\n",
      "Epoch: 144610 | Loss: 0.3851166367530823 | Test loss: 0.3872165381908417\n",
      "Epoch: 144620 | Loss: 0.38510486483573914 | Test loss: 0.387202650308609\n",
      "Epoch: 144630 | Loss: 0.3850931227207184 | Test loss: 0.3871888220310211\n",
      "Epoch: 144640 | Loss: 0.38508135080337524 | Test loss: 0.38717496395111084\n",
      "Epoch: 144650 | Loss: 0.3850695788860321 | Test loss: 0.38716110587120056\n",
      "Epoch: 144660 | Loss: 0.38505780696868896 | Test loss: 0.38714727759361267\n",
      "Epoch: 144670 | Loss: 0.3850460648536682 | Test loss: 0.3871334493160248\n",
      "Epoch: 144680 | Loss: 0.3850342929363251 | Test loss: 0.3871195912361145\n",
      "Epoch: 144690 | Loss: 0.3850225508213043 | Test loss: 0.3871057629585266\n",
      "Epoch: 144700 | Loss: 0.38501080870628357 | Test loss: 0.38709187507629395\n",
      "Epoch: 144710 | Loss: 0.38499900698661804 | Test loss: 0.38707804679870605\n",
      "Epoch: 144720 | Loss: 0.3849872648715973 | Test loss: 0.38706421852111816\n",
      "Epoch: 144730 | Loss: 0.38497549295425415 | Test loss: 0.3870503604412079\n",
      "Epoch: 144740 | Loss: 0.3849637508392334 | Test loss: 0.38703653216362\n",
      "Epoch: 144750 | Loss: 0.38495200872421265 | Test loss: 0.3870227038860321\n",
      "Epoch: 144760 | Loss: 0.3849402368068695 | Test loss: 0.38700881600379944\n",
      "Epoch: 144770 | Loss: 0.38492846488952637 | Test loss: 0.38699498772621155\n",
      "Epoch: 144780 | Loss: 0.3849167227745056 | Test loss: 0.38698112964630127\n",
      "Epoch: 144790 | Loss: 0.3849049210548401 | Test loss: 0.386967271566391\n",
      "Epoch: 144800 | Loss: 0.38489317893981934 | Test loss: 0.3869534432888031\n",
      "Epoch: 144810 | Loss: 0.38488146662712097 | Test loss: 0.3869396150112152\n",
      "Epoch: 144820 | Loss: 0.38486966490745544 | Test loss: 0.38692575693130493\n",
      "Epoch: 144830 | Loss: 0.3848578929901123 | Test loss: 0.38691189885139465\n",
      "Epoch: 144840 | Loss: 0.38484618067741394 | Test loss: 0.3868980407714844\n",
      "Epoch: 144850 | Loss: 0.3848343789577484 | Test loss: 0.3868842124938965\n",
      "Epoch: 144860 | Loss: 0.38482263684272766 | Test loss: 0.3868703544139862\n",
      "Epoch: 144870 | Loss: 0.3848108649253845 | Test loss: 0.3868565261363983\n",
      "Epoch: 144880 | Loss: 0.38479912281036377 | Test loss: 0.38684266805648804\n",
      "Epoch: 144890 | Loss: 0.38478732109069824 | Test loss: 0.38682880997657776\n",
      "Epoch: 144900 | Loss: 0.3847755789756775 | Test loss: 0.38681498169898987\n",
      "Epoch: 144910 | Loss: 0.38476380705833435 | Test loss: 0.3868011236190796\n",
      "Epoch: 144920 | Loss: 0.384752094745636 | Test loss: 0.3867872655391693\n",
      "Epoch: 144930 | Loss: 0.38474032282829285 | Test loss: 0.3867734372615814\n",
      "Epoch: 144940 | Loss: 0.3847285211086273 | Test loss: 0.38675960898399353\n",
      "Epoch: 144950 | Loss: 0.38471677899360657 | Test loss: 0.38674575090408325\n",
      "Epoch: 144960 | Loss: 0.3847050368785858 | Test loss: 0.38673192262649536\n",
      "Epoch: 144970 | Loss: 0.3846932649612427 | Test loss: 0.3867180645465851\n",
      "Epoch: 144980 | Loss: 0.3846815228462219 | Test loss: 0.3867042064666748\n",
      "Epoch: 144990 | Loss: 0.3846697509288788 | Test loss: 0.3866903781890869\n",
      "Epoch: 145000 | Loss: 0.38465797901153564 | Test loss: 0.38667652010917664\n",
      "Epoch: 145010 | Loss: 0.3846462368965149 | Test loss: 0.38666269183158875\n",
      "Epoch: 145020 | Loss: 0.38463446497917175 | Test loss: 0.3866488039493561\n",
      "Epoch: 145030 | Loss: 0.384622722864151 | Test loss: 0.3866349756717682\n",
      "Epoch: 145040 | Loss: 0.38461095094680786 | Test loss: 0.3866211175918579\n",
      "Epoch: 145050 | Loss: 0.3845991790294647 | Test loss: 0.38660725951194763\n",
      "Epoch: 145060 | Loss: 0.3845874071121216 | Test loss: 0.38659343123435974\n",
      "Epoch: 145070 | Loss: 0.38457566499710083 | Test loss: 0.38657960295677185\n",
      "Epoch: 145080 | Loss: 0.3845638930797577 | Test loss: 0.3865657448768616\n",
      "Epoch: 145090 | Loss: 0.38455215096473694 | Test loss: 0.3865519165992737\n",
      "Epoch: 145100 | Loss: 0.3845404088497162 | Test loss: 0.386538028717041\n",
      "Epoch: 145110 | Loss: 0.38452860713005066 | Test loss: 0.3865242004394531\n",
      "Epoch: 145120 | Loss: 0.3845168650150299 | Test loss: 0.38651037216186523\n",
      "Epoch: 145130 | Loss: 0.38450509309768677 | Test loss: 0.38649651408195496\n",
      "Epoch: 145140 | Loss: 0.384493350982666 | Test loss: 0.38648268580436707\n",
      "Epoch: 145150 | Loss: 0.38448160886764526 | Test loss: 0.3864688575267792\n",
      "Epoch: 145160 | Loss: 0.3844698369503021 | Test loss: 0.3864549696445465\n",
      "Epoch: 145170 | Loss: 0.384458065032959 | Test loss: 0.3864411413669586\n",
      "Epoch: 145180 | Loss: 0.38444632291793823 | Test loss: 0.38642728328704834\n",
      "Epoch: 145190 | Loss: 0.3844345211982727 | Test loss: 0.38641342520713806\n",
      "Epoch: 145200 | Loss: 0.38442277908325195 | Test loss: 0.38639959692955017\n",
      "Epoch: 145210 | Loss: 0.3844110667705536 | Test loss: 0.3863857686519623\n",
      "Epoch: 145220 | Loss: 0.38439926505088806 | Test loss: 0.386371910572052\n",
      "Epoch: 145230 | Loss: 0.3843874931335449 | Test loss: 0.3863580524921417\n",
      "Epoch: 145240 | Loss: 0.38437578082084656 | Test loss: 0.38634419441223145\n",
      "Epoch: 145250 | Loss: 0.38436397910118103 | Test loss: 0.38633036613464355\n",
      "Epoch: 145260 | Loss: 0.3843522369861603 | Test loss: 0.3863165080547333\n",
      "Epoch: 145270 | Loss: 0.38434046506881714 | Test loss: 0.3863026797771454\n",
      "Epoch: 145280 | Loss: 0.3843287229537964 | Test loss: 0.3862888216972351\n",
      "Epoch: 145290 | Loss: 0.38431692123413086 | Test loss: 0.38627496361732483\n",
      "Epoch: 145300 | Loss: 0.3843051791191101 | Test loss: 0.38626113533973694\n",
      "Epoch: 145310 | Loss: 0.38429340720176697 | Test loss: 0.38624727725982666\n",
      "Epoch: 145320 | Loss: 0.3842816948890686 | Test loss: 0.3862334191799164\n",
      "Epoch: 145330 | Loss: 0.38426992297172546 | Test loss: 0.3862195909023285\n",
      "Epoch: 145340 | Loss: 0.38425812125205994 | Test loss: 0.3862057626247406\n",
      "Epoch: 145350 | Loss: 0.3842463791370392 | Test loss: 0.3861919045448303\n",
      "Epoch: 145360 | Loss: 0.38423463702201843 | Test loss: 0.38617807626724243\n",
      "Epoch: 145370 | Loss: 0.3842228651046753 | Test loss: 0.38616421818733215\n",
      "Epoch: 145380 | Loss: 0.38421112298965454 | Test loss: 0.3861503601074219\n",
      "Epoch: 145390 | Loss: 0.3841993510723114 | Test loss: 0.386136531829834\n",
      "Epoch: 145400 | Loss: 0.38418757915496826 | Test loss: 0.3861226737499237\n",
      "Epoch: 145410 | Loss: 0.3841758370399475 | Test loss: 0.3861088454723358\n",
      "Epoch: 145420 | Loss: 0.38416406512260437 | Test loss: 0.38609495759010315\n",
      "Epoch: 145430 | Loss: 0.3841523230075836 | Test loss: 0.38608112931251526\n",
      "Epoch: 145440 | Loss: 0.3841405510902405 | Test loss: 0.386067271232605\n",
      "Epoch: 145450 | Loss: 0.38412877917289734 | Test loss: 0.3860534131526947\n",
      "Epoch: 145460 | Loss: 0.3841170072555542 | Test loss: 0.3860395848751068\n",
      "Epoch: 145470 | Loss: 0.38410526514053345 | Test loss: 0.3860257565975189\n",
      "Epoch: 145480 | Loss: 0.3840934932231903 | Test loss: 0.38601189851760864\n",
      "Epoch: 145490 | Loss: 0.38408175110816956 | Test loss: 0.38599807024002075\n",
      "Epoch: 145500 | Loss: 0.3840700089931488 | Test loss: 0.3859841823577881\n",
      "Epoch: 145510 | Loss: 0.3840582072734833 | Test loss: 0.3859703540802002\n",
      "Epoch: 145520 | Loss: 0.3840464651584625 | Test loss: 0.3859565258026123\n",
      "Epoch: 145530 | Loss: 0.3840346932411194 | Test loss: 0.385942667722702\n",
      "Epoch: 145540 | Loss: 0.38402295112609863 | Test loss: 0.38592883944511414\n",
      "Epoch: 145550 | Loss: 0.3840112090110779 | Test loss: 0.38591501116752625\n",
      "Epoch: 145560 | Loss: 0.38399943709373474 | Test loss: 0.3859011232852936\n",
      "Epoch: 145570 | Loss: 0.3839876651763916 | Test loss: 0.3858872950077057\n",
      "Epoch: 145580 | Loss: 0.38397592306137085 | Test loss: 0.3858734369277954\n",
      "Epoch: 145590 | Loss: 0.3839641213417053 | Test loss: 0.38585957884788513\n",
      "Epoch: 145600 | Loss: 0.38395237922668457 | Test loss: 0.38584575057029724\n",
      "Epoch: 145610 | Loss: 0.3839406669139862 | Test loss: 0.38583192229270935\n",
      "Epoch: 145620 | Loss: 0.3839288651943207 | Test loss: 0.3858180642127991\n",
      "Epoch: 145630 | Loss: 0.38391709327697754 | Test loss: 0.3858042061328888\n",
      "Epoch: 145640 | Loss: 0.3839053809642792 | Test loss: 0.3857903480529785\n",
      "Epoch: 145650 | Loss: 0.38389357924461365 | Test loss: 0.3857765197753906\n",
      "Epoch: 145660 | Loss: 0.3838818371295929 | Test loss: 0.38576266169548035\n",
      "Epoch: 145670 | Loss: 0.38387006521224976 | Test loss: 0.38574883341789246\n",
      "Epoch: 145680 | Loss: 0.383858323097229 | Test loss: 0.3857349753379822\n",
      "Epoch: 145690 | Loss: 0.3838465213775635 | Test loss: 0.3857211172580719\n",
      "Epoch: 145700 | Loss: 0.3838347792625427 | Test loss: 0.385707288980484\n",
      "Epoch: 145710 | Loss: 0.3838230073451996 | Test loss: 0.38569343090057373\n",
      "Epoch: 145720 | Loss: 0.3838112950325012 | Test loss: 0.38567957282066345\n",
      "Epoch: 145730 | Loss: 0.3837995231151581 | Test loss: 0.38566574454307556\n",
      "Epoch: 145740 | Loss: 0.38378772139549255 | Test loss: 0.38565191626548767\n",
      "Epoch: 145750 | Loss: 0.3837759792804718 | Test loss: 0.3856380581855774\n",
      "Epoch: 145760 | Loss: 0.38376423716545105 | Test loss: 0.3856242299079895\n",
      "Epoch: 145770 | Loss: 0.3837524652481079 | Test loss: 0.3856103718280792\n",
      "Epoch: 145780 | Loss: 0.38374072313308716 | Test loss: 0.38559651374816895\n",
      "Epoch: 145790 | Loss: 0.383728951215744 | Test loss: 0.38558268547058105\n",
      "Epoch: 145800 | Loss: 0.3837171792984009 | Test loss: 0.3855688273906708\n",
      "Epoch: 145810 | Loss: 0.3837054371833801 | Test loss: 0.3855549991130829\n",
      "Epoch: 145820 | Loss: 0.383693665266037 | Test loss: 0.3855411112308502\n",
      "Epoch: 145830 | Loss: 0.38368192315101624 | Test loss: 0.38552728295326233\n",
      "Epoch: 145840 | Loss: 0.3836701512336731 | Test loss: 0.38551342487335205\n",
      "Epoch: 145850 | Loss: 0.38365837931632996 | Test loss: 0.3854995667934418\n",
      "Epoch: 145860 | Loss: 0.3836466073989868 | Test loss: 0.3854857385158539\n",
      "Epoch: 145870 | Loss: 0.38363486528396606 | Test loss: 0.385471910238266\n",
      "Epoch: 145880 | Loss: 0.3836230933666229 | Test loss: 0.3854580521583557\n",
      "Epoch: 145890 | Loss: 0.3836113512516022 | Test loss: 0.3854442238807678\n",
      "Epoch: 145900 | Loss: 0.3835996091365814 | Test loss: 0.38543033599853516\n",
      "Epoch: 145910 | Loss: 0.3835878074169159 | Test loss: 0.38541650772094727\n",
      "Epoch: 145920 | Loss: 0.38357606530189514 | Test loss: 0.3854026794433594\n",
      "Epoch: 145930 | Loss: 0.383564293384552 | Test loss: 0.3853888213634491\n",
      "Epoch: 145940 | Loss: 0.38355255126953125 | Test loss: 0.3853749930858612\n",
      "Epoch: 145950 | Loss: 0.3835408091545105 | Test loss: 0.3853611648082733\n",
      "Epoch: 145960 | Loss: 0.38352903723716736 | Test loss: 0.38534727692604065\n",
      "Epoch: 145970 | Loss: 0.3835172653198242 | Test loss: 0.38533344864845276\n",
      "Epoch: 145980 | Loss: 0.38350552320480347 | Test loss: 0.3853195905685425\n",
      "Epoch: 145990 | Loss: 0.38349372148513794 | Test loss: 0.3853057324886322\n",
      "Epoch: 146000 | Loss: 0.3834819793701172 | Test loss: 0.3852919042110443\n",
      "Epoch: 146010 | Loss: 0.3834702670574188 | Test loss: 0.3852780759334564\n",
      "Epoch: 146020 | Loss: 0.3834584653377533 | Test loss: 0.38526421785354614\n",
      "Epoch: 146030 | Loss: 0.38344669342041016 | Test loss: 0.38525035977363586\n",
      "Epoch: 146040 | Loss: 0.3834349811077118 | Test loss: 0.3852365016937256\n",
      "Epoch: 146050 | Loss: 0.38342317938804626 | Test loss: 0.3852226734161377\n",
      "Epoch: 146060 | Loss: 0.3834114372730255 | Test loss: 0.3852088153362274\n",
      "Epoch: 146070 | Loss: 0.3833996653556824 | Test loss: 0.3851949870586395\n",
      "Epoch: 146080 | Loss: 0.3833879232406616 | Test loss: 0.38518112897872925\n",
      "Epoch: 146090 | Loss: 0.3833761215209961 | Test loss: 0.38516727089881897\n",
      "Epoch: 146100 | Loss: 0.38336437940597534 | Test loss: 0.3851534426212311\n",
      "Epoch: 146110 | Loss: 0.3833526074886322 | Test loss: 0.3851395845413208\n",
      "Epoch: 146120 | Loss: 0.38334089517593384 | Test loss: 0.3851257264614105\n",
      "Epoch: 146130 | Loss: 0.3833291232585907 | Test loss: 0.38511189818382263\n",
      "Epoch: 146140 | Loss: 0.38331732153892517 | Test loss: 0.38509806990623474\n",
      "Epoch: 146150 | Loss: 0.3833055794239044 | Test loss: 0.38508421182632446\n",
      "Epoch: 146160 | Loss: 0.38329383730888367 | Test loss: 0.3850703835487366\n",
      "Epoch: 146170 | Loss: 0.3832820653915405 | Test loss: 0.3850565254688263\n",
      "Epoch: 146180 | Loss: 0.3832703232765198 | Test loss: 0.385042667388916\n",
      "Epoch: 146190 | Loss: 0.38325855135917664 | Test loss: 0.3850288391113281\n",
      "Epoch: 146200 | Loss: 0.3832467794418335 | Test loss: 0.38501498103141785\n",
      "Epoch: 146210 | Loss: 0.38323503732681274 | Test loss: 0.38500115275382996\n",
      "Epoch: 146220 | Loss: 0.3832232654094696 | Test loss: 0.3849872648715973\n",
      "Epoch: 146230 | Loss: 0.38321152329444885 | Test loss: 0.3849734365940094\n",
      "Epoch: 146240 | Loss: 0.3831997513771057 | Test loss: 0.3849595785140991\n",
      "Epoch: 146250 | Loss: 0.3831879794597626 | Test loss: 0.38494572043418884\n",
      "Epoch: 146260 | Loss: 0.38317620754241943 | Test loss: 0.38493189215660095\n",
      "Epoch: 146270 | Loss: 0.3831644654273987 | Test loss: 0.38491806387901306\n",
      "Epoch: 146280 | Loss: 0.38315269351005554 | Test loss: 0.3849042057991028\n",
      "Epoch: 146290 | Loss: 0.3831409513950348 | Test loss: 0.3848903775215149\n",
      "Epoch: 146300 | Loss: 0.38312920928001404 | Test loss: 0.3848764896392822\n",
      "Epoch: 146310 | Loss: 0.3831174075603485 | Test loss: 0.38486266136169434\n",
      "Epoch: 146320 | Loss: 0.38310566544532776 | Test loss: 0.38484883308410645\n",
      "Epoch: 146330 | Loss: 0.3830938935279846 | Test loss: 0.38483497500419617\n",
      "Epoch: 146340 | Loss: 0.38308215141296387 | Test loss: 0.3848211467266083\n",
      "Epoch: 146350 | Loss: 0.3830704092979431 | Test loss: 0.3848073184490204\n",
      "Epoch: 146360 | Loss: 0.3830586373806 | Test loss: 0.3847934305667877\n",
      "Epoch: 146370 | Loss: 0.38304686546325684 | Test loss: 0.38477960228919983\n",
      "Epoch: 146380 | Loss: 0.3830351233482361 | Test loss: 0.38476574420928955\n",
      "Epoch: 146390 | Loss: 0.38302332162857056 | Test loss: 0.3847518861293793\n",
      "Epoch: 146400 | Loss: 0.3830115795135498 | Test loss: 0.3847380578517914\n",
      "Epoch: 146410 | Loss: 0.38299986720085144 | Test loss: 0.3847242295742035\n",
      "Epoch: 146420 | Loss: 0.3829880654811859 | Test loss: 0.3847103714942932\n",
      "Epoch: 146430 | Loss: 0.3829762935638428 | Test loss: 0.38469651341438293\n",
      "Epoch: 146440 | Loss: 0.3829645812511444 | Test loss: 0.38468265533447266\n",
      "Epoch: 146450 | Loss: 0.3829527795314789 | Test loss: 0.38466882705688477\n",
      "Epoch: 146460 | Loss: 0.38294103741645813 | Test loss: 0.3846549689769745\n",
      "Epoch: 146470 | Loss: 0.382929265499115 | Test loss: 0.3846411406993866\n",
      "Epoch: 146480 | Loss: 0.38291752338409424 | Test loss: 0.3846272826194763\n",
      "Epoch: 146490 | Loss: 0.3829057216644287 | Test loss: 0.38461342453956604\n",
      "Epoch: 146500 | Loss: 0.38289397954940796 | Test loss: 0.38459959626197815\n",
      "Epoch: 146510 | Loss: 0.3828822076320648 | Test loss: 0.38458573818206787\n",
      "Epoch: 146520 | Loss: 0.38287049531936646 | Test loss: 0.3845718801021576\n",
      "Epoch: 146530 | Loss: 0.3828587234020233 | Test loss: 0.3845580518245697\n",
      "Epoch: 146540 | Loss: 0.3828469216823578 | Test loss: 0.3845442235469818\n",
      "Epoch: 146550 | Loss: 0.38283517956733704 | Test loss: 0.38453036546707153\n",
      "Epoch: 146560 | Loss: 0.3828234374523163 | Test loss: 0.38451653718948364\n",
      "Epoch: 146570 | Loss: 0.38281166553497314 | Test loss: 0.38450267910957336\n",
      "Epoch: 146580 | Loss: 0.3827999234199524 | Test loss: 0.3844888210296631\n",
      "Epoch: 146590 | Loss: 0.38278815150260925 | Test loss: 0.3844749927520752\n",
      "Epoch: 146600 | Loss: 0.3827763795852661 | Test loss: 0.3844611346721649\n",
      "Epoch: 146610 | Loss: 0.38276463747024536 | Test loss: 0.384447306394577\n",
      "Epoch: 146620 | Loss: 0.3827528655529022 | Test loss: 0.38443341851234436\n",
      "Epoch: 146630 | Loss: 0.38274112343788147 | Test loss: 0.38441959023475647\n",
      "Epoch: 146640 | Loss: 0.38272935152053833 | Test loss: 0.3844057321548462\n",
      "Epoch: 146650 | Loss: 0.3827175796031952 | Test loss: 0.3843918740749359\n",
      "Epoch: 146660 | Loss: 0.38270580768585205 | Test loss: 0.384378045797348\n",
      "Epoch: 146670 | Loss: 0.3826940655708313 | Test loss: 0.38436421751976013\n",
      "Epoch: 146680 | Loss: 0.38268229365348816 | Test loss: 0.38435035943984985\n",
      "Epoch: 146690 | Loss: 0.3826705515384674 | Test loss: 0.38433653116226196\n",
      "Epoch: 146700 | Loss: 0.38265880942344666 | Test loss: 0.3843226432800293\n",
      "Epoch: 146710 | Loss: 0.38264700770378113 | Test loss: 0.3843088150024414\n",
      "Epoch: 146720 | Loss: 0.3826352655887604 | Test loss: 0.3842949867248535\n",
      "Epoch: 146730 | Loss: 0.38262349367141724 | Test loss: 0.38428112864494324\n",
      "Epoch: 146740 | Loss: 0.3826117515563965 | Test loss: 0.38426730036735535\n",
      "Epoch: 146750 | Loss: 0.38260000944137573 | Test loss: 0.38425347208976746\n",
      "Epoch: 146760 | Loss: 0.3825882375240326 | Test loss: 0.3842395842075348\n",
      "Epoch: 146770 | Loss: 0.38257646560668945 | Test loss: 0.3842257559299469\n",
      "Epoch: 146780 | Loss: 0.3825647234916687 | Test loss: 0.3842118978500366\n",
      "Epoch: 146790 | Loss: 0.3825529217720032 | Test loss: 0.38419803977012634\n",
      "Epoch: 146800 | Loss: 0.3825411796569824 | Test loss: 0.38418421149253845\n",
      "Epoch: 146810 | Loss: 0.38252946734428406 | Test loss: 0.38417038321495056\n",
      "Epoch: 146820 | Loss: 0.38251766562461853 | Test loss: 0.3841565251350403\n",
      "Epoch: 146830 | Loss: 0.3825058937072754 | Test loss: 0.38414266705513\n",
      "Epoch: 146840 | Loss: 0.382494181394577 | Test loss: 0.3841288089752197\n",
      "Epoch: 146850 | Loss: 0.3824823796749115 | Test loss: 0.38411498069763184\n",
      "Epoch: 146860 | Loss: 0.38247063755989075 | Test loss: 0.38410112261772156\n",
      "Epoch: 146870 | Loss: 0.3824588656425476 | Test loss: 0.38408729434013367\n",
      "Epoch: 146880 | Loss: 0.38244712352752686 | Test loss: 0.3840734362602234\n",
      "Epoch: 146890 | Loss: 0.38243532180786133 | Test loss: 0.3840595781803131\n",
      "Epoch: 146900 | Loss: 0.3824235796928406 | Test loss: 0.3840457499027252\n",
      "Epoch: 146910 | Loss: 0.38241180777549744 | Test loss: 0.38403189182281494\n",
      "Epoch: 146920 | Loss: 0.3824000954627991 | Test loss: 0.38401803374290466\n",
      "Epoch: 146930 | Loss: 0.38238832354545593 | Test loss: 0.3840042054653168\n",
      "Epoch: 146940 | Loss: 0.3823765218257904 | Test loss: 0.3839903771877289\n",
      "Epoch: 146950 | Loss: 0.38236477971076965 | Test loss: 0.3839765191078186\n",
      "Epoch: 146960 | Loss: 0.3823530375957489 | Test loss: 0.3839626908302307\n",
      "Epoch: 146970 | Loss: 0.38234126567840576 | Test loss: 0.38394883275032043\n",
      "Epoch: 146980 | Loss: 0.382329523563385 | Test loss: 0.38393497467041016\n",
      "Epoch: 146990 | Loss: 0.38231775164604187 | Test loss: 0.38392114639282227\n",
      "Epoch: 147000 | Loss: 0.38230597972869873 | Test loss: 0.383907288312912\n",
      "Epoch: 147010 | Loss: 0.382294237613678 | Test loss: 0.3838934600353241\n",
      "Epoch: 147020 | Loss: 0.38228246569633484 | Test loss: 0.38387957215309143\n",
      "Epoch: 147030 | Loss: 0.3822707235813141 | Test loss: 0.38386574387550354\n",
      "Epoch: 147040 | Loss: 0.38225895166397095 | Test loss: 0.38385188579559326\n",
      "Epoch: 147050 | Loss: 0.3822471797466278 | Test loss: 0.383838027715683\n",
      "Epoch: 147060 | Loss: 0.38223540782928467 | Test loss: 0.3838241994380951\n",
      "Epoch: 147070 | Loss: 0.3822236657142639 | Test loss: 0.3838103711605072\n",
      "Epoch: 147080 | Loss: 0.3822118937969208 | Test loss: 0.3837965130805969\n",
      "Epoch: 147090 | Loss: 0.3822001516819 | Test loss: 0.38378268480300903\n",
      "Epoch: 147100 | Loss: 0.3821884095668793 | Test loss: 0.38376879692077637\n",
      "Epoch: 147110 | Loss: 0.38217660784721375 | Test loss: 0.3837549686431885\n",
      "Epoch: 147120 | Loss: 0.382164865732193 | Test loss: 0.3837411403656006\n",
      "Epoch: 147130 | Loss: 0.38215309381484985 | Test loss: 0.3837272822856903\n",
      "Epoch: 147140 | Loss: 0.3821413516998291 | Test loss: 0.3837134540081024\n",
      "Epoch: 147150 | Loss: 0.38212960958480835 | Test loss: 0.3836996257305145\n",
      "Epoch: 147160 | Loss: 0.3821178376674652 | Test loss: 0.38368573784828186\n",
      "Epoch: 147170 | Loss: 0.38210606575012207 | Test loss: 0.38367190957069397\n",
      "Epoch: 147180 | Loss: 0.3820943236351013 | Test loss: 0.3836580514907837\n",
      "Epoch: 147190 | Loss: 0.3820825219154358 | Test loss: 0.3836441934108734\n",
      "Epoch: 147200 | Loss: 0.38207077980041504 | Test loss: 0.3836303651332855\n",
      "Epoch: 147210 | Loss: 0.3820590674877167 | Test loss: 0.38361653685569763\n",
      "Epoch: 147220 | Loss: 0.38204726576805115 | Test loss: 0.38360267877578735\n",
      "Epoch: 147230 | Loss: 0.382035493850708 | Test loss: 0.3835888206958771\n",
      "Epoch: 147240 | Loss: 0.38202378153800964 | Test loss: 0.3835749626159668\n",
      "Epoch: 147250 | Loss: 0.3820119798183441 | Test loss: 0.3835611343383789\n",
      "Epoch: 147260 | Loss: 0.38200023770332336 | Test loss: 0.38354727625846863\n",
      "Epoch: 147270 | Loss: 0.3819884657859802 | Test loss: 0.38353344798088074\n",
      "Epoch: 147280 | Loss: 0.3819767236709595 | Test loss: 0.38351958990097046\n",
      "Epoch: 147290 | Loss: 0.38196492195129395 | Test loss: 0.3835057318210602\n",
      "Epoch: 147300 | Loss: 0.3819531798362732 | Test loss: 0.3834919035434723\n",
      "Epoch: 147310 | Loss: 0.38194140791893005 | Test loss: 0.383478045463562\n",
      "Epoch: 147320 | Loss: 0.3819296956062317 | Test loss: 0.38346418738365173\n",
      "Epoch: 147330 | Loss: 0.38191792368888855 | Test loss: 0.38345035910606384\n",
      "Epoch: 147340 | Loss: 0.381906121969223 | Test loss: 0.38343653082847595\n",
      "Epoch: 147350 | Loss: 0.38189437985420227 | Test loss: 0.3834226727485657\n",
      "Epoch: 147360 | Loss: 0.3818826377391815 | Test loss: 0.3834088444709778\n",
      "Epoch: 147370 | Loss: 0.3818708658218384 | Test loss: 0.3833949863910675\n",
      "Epoch: 147380 | Loss: 0.3818591237068176 | Test loss: 0.3833811283111572\n",
      "Epoch: 147390 | Loss: 0.3818473517894745 | Test loss: 0.38336730003356934\n",
      "Epoch: 147400 | Loss: 0.38183557987213135 | Test loss: 0.38335344195365906\n",
      "Epoch: 147410 | Loss: 0.3818238377571106 | Test loss: 0.38333961367607117\n",
      "Epoch: 147420 | Loss: 0.38181206583976746 | Test loss: 0.3833257257938385\n",
      "Epoch: 147430 | Loss: 0.3818003237247467 | Test loss: 0.3833118975162506\n",
      "Epoch: 147440 | Loss: 0.38178855180740356 | Test loss: 0.38329803943634033\n",
      "Epoch: 147450 | Loss: 0.3817767798900604 | Test loss: 0.38328418135643005\n",
      "Epoch: 147460 | Loss: 0.3817650079727173 | Test loss: 0.38327035307884216\n",
      "Epoch: 147470 | Loss: 0.38175326585769653 | Test loss: 0.3832565248012543\n",
      "Epoch: 147480 | Loss: 0.3817414939403534 | Test loss: 0.383242666721344\n",
      "Epoch: 147490 | Loss: 0.38172975182533264 | Test loss: 0.3832288384437561\n",
      "Epoch: 147500 | Loss: 0.3817180097103119 | Test loss: 0.38321495056152344\n",
      "Epoch: 147510 | Loss: 0.38170620799064636 | Test loss: 0.38320112228393555\n",
      "Epoch: 147520 | Loss: 0.3816944658756256 | Test loss: 0.38318729400634766\n",
      "Epoch: 147530 | Loss: 0.38168269395828247 | Test loss: 0.3831734359264374\n",
      "Epoch: 147540 | Loss: 0.3816709518432617 | Test loss: 0.3831596076488495\n",
      "Epoch: 147550 | Loss: 0.38165920972824097 | Test loss: 0.3831457793712616\n",
      "Epoch: 147560 | Loss: 0.3816474378108978 | Test loss: 0.38313189148902893\n",
      "Epoch: 147570 | Loss: 0.3816356658935547 | Test loss: 0.38311806321144104\n",
      "Epoch: 147580 | Loss: 0.38162392377853394 | Test loss: 0.38310420513153076\n",
      "Epoch: 147590 | Loss: 0.3816121220588684 | Test loss: 0.3830903470516205\n",
      "Epoch: 147600 | Loss: 0.38160037994384766 | Test loss: 0.3830765187740326\n",
      "Epoch: 147610 | Loss: 0.3815886676311493 | Test loss: 0.3830626904964447\n",
      "Epoch: 147620 | Loss: 0.38157686591148376 | Test loss: 0.3830488324165344\n",
      "Epoch: 147630 | Loss: 0.3815650939941406 | Test loss: 0.38303497433662415\n",
      "Epoch: 147640 | Loss: 0.38155338168144226 | Test loss: 0.38302111625671387\n",
      "Epoch: 147650 | Loss: 0.38154157996177673 | Test loss: 0.383007287979126\n",
      "Epoch: 147660 | Loss: 0.381529837846756 | Test loss: 0.3829934298992157\n",
      "Epoch: 147670 | Loss: 0.38151806592941284 | Test loss: 0.3829796016216278\n",
      "Epoch: 147680 | Loss: 0.3815063238143921 | Test loss: 0.38296574354171753\n",
      "Epoch: 147690 | Loss: 0.38149452209472656 | Test loss: 0.38295188546180725\n",
      "Epoch: 147700 | Loss: 0.3814827799797058 | Test loss: 0.38293805718421936\n",
      "Epoch: 147710 | Loss: 0.38147100806236267 | Test loss: 0.3829241991043091\n",
      "Epoch: 147720 | Loss: 0.3814592957496643 | Test loss: 0.3829103410243988\n",
      "Epoch: 147730 | Loss: 0.38144752383232117 | Test loss: 0.3828965127468109\n",
      "Epoch: 147740 | Loss: 0.38143572211265564 | Test loss: 0.382882684469223\n",
      "Epoch: 147750 | Loss: 0.3814239799976349 | Test loss: 0.38286882638931274\n",
      "Epoch: 147760 | Loss: 0.38141223788261414 | Test loss: 0.38285499811172485\n",
      "Epoch: 147770 | Loss: 0.381400465965271 | Test loss: 0.3828411400318146\n",
      "Epoch: 147780 | Loss: 0.38138872385025024 | Test loss: 0.3828272819519043\n",
      "Epoch: 147790 | Loss: 0.3813769519329071 | Test loss: 0.3828134536743164\n",
      "Epoch: 147800 | Loss: 0.38136518001556396 | Test loss: 0.38279959559440613\n",
      "Epoch: 147810 | Loss: 0.3813534379005432 | Test loss: 0.38278576731681824\n",
      "Epoch: 147820 | Loss: 0.3813416659832001 | Test loss: 0.38277187943458557\n",
      "Epoch: 147830 | Loss: 0.3813299238681793 | Test loss: 0.3827580511569977\n",
      "Epoch: 147840 | Loss: 0.3813181519508362 | Test loss: 0.3827441930770874\n",
      "Epoch: 147850 | Loss: 0.38130638003349304 | Test loss: 0.3827303349971771\n",
      "Epoch: 147860 | Loss: 0.3812946081161499 | Test loss: 0.38271650671958923\n",
      "Epoch: 147870 | Loss: 0.38128286600112915 | Test loss: 0.38270267844200134\n",
      "Epoch: 147880 | Loss: 0.381271094083786 | Test loss: 0.38268882036209106\n",
      "Epoch: 147890 | Loss: 0.38125935196876526 | Test loss: 0.3826749920845032\n",
      "Epoch: 147900 | Loss: 0.3812476098537445 | Test loss: 0.3826611042022705\n",
      "Epoch: 147910 | Loss: 0.381235808134079 | Test loss: 0.3826472759246826\n",
      "Epoch: 147920 | Loss: 0.3812240660190582 | Test loss: 0.3826334476470947\n",
      "Epoch: 147930 | Loss: 0.3812122941017151 | Test loss: 0.38261958956718445\n",
      "Epoch: 147940 | Loss: 0.38120055198669434 | Test loss: 0.38260576128959656\n",
      "Epoch: 147950 | Loss: 0.3811888098716736 | Test loss: 0.38259193301200867\n",
      "Epoch: 147960 | Loss: 0.38117703795433044 | Test loss: 0.382578045129776\n",
      "Epoch: 147970 | Loss: 0.3811652660369873 | Test loss: 0.3825642168521881\n",
      "Epoch: 147980 | Loss: 0.38115352392196655 | Test loss: 0.38255035877227783\n",
      "Epoch: 147990 | Loss: 0.381141722202301 | Test loss: 0.38253650069236755\n",
      "Epoch: 148000 | Loss: 0.3811299800872803 | Test loss: 0.38252267241477966\n",
      "Epoch: 148010 | Loss: 0.3811182677745819 | Test loss: 0.3825088441371918\n",
      "Epoch: 148020 | Loss: 0.3811064660549164 | Test loss: 0.3824949860572815\n",
      "Epoch: 148030 | Loss: 0.38109469413757324 | Test loss: 0.3824811279773712\n",
      "Epoch: 148040 | Loss: 0.3810829818248749 | Test loss: 0.38246726989746094\n",
      "Epoch: 148050 | Loss: 0.38107118010520935 | Test loss: 0.38245344161987305\n",
      "Epoch: 148060 | Loss: 0.3810594379901886 | Test loss: 0.38243958353996277\n",
      "Epoch: 148070 | Loss: 0.38104766607284546 | Test loss: 0.3824257552623749\n",
      "Epoch: 148080 | Loss: 0.3810359239578247 | Test loss: 0.3824118971824646\n",
      "Epoch: 148090 | Loss: 0.3810241222381592 | Test loss: 0.3823980391025543\n",
      "Epoch: 148100 | Loss: 0.3810123801231384 | Test loss: 0.38238421082496643\n",
      "Epoch: 148110 | Loss: 0.3810006082057953 | Test loss: 0.38237035274505615\n",
      "Epoch: 148120 | Loss: 0.3809888958930969 | Test loss: 0.3823564946651459\n",
      "Epoch: 148130 | Loss: 0.3809771239757538 | Test loss: 0.382342666387558\n",
      "Epoch: 148140 | Loss: 0.38096532225608826 | Test loss: 0.3823288381099701\n",
      "Epoch: 148150 | Loss: 0.3809535801410675 | Test loss: 0.3823149800300598\n",
      "Epoch: 148160 | Loss: 0.38094183802604675 | Test loss: 0.3823011517524719\n",
      "Epoch: 148170 | Loss: 0.3809300661087036 | Test loss: 0.38228729367256165\n",
      "Epoch: 148180 | Loss: 0.38091832399368286 | Test loss: 0.38227343559265137\n",
      "Epoch: 148190 | Loss: 0.3809065520763397 | Test loss: 0.3822596073150635\n",
      "Epoch: 148200 | Loss: 0.3808947801589966 | Test loss: 0.3822457492351532\n",
      "Epoch: 148210 | Loss: 0.38088303804397583 | Test loss: 0.3822319209575653\n",
      "Epoch: 148220 | Loss: 0.3808712661266327 | Test loss: 0.38221803307533264\n",
      "Epoch: 148230 | Loss: 0.38085952401161194 | Test loss: 0.38220420479774475\n",
      "Epoch: 148240 | Loss: 0.3808477520942688 | Test loss: 0.3821903467178345\n",
      "Epoch: 148250 | Loss: 0.38083598017692566 | Test loss: 0.3821764886379242\n",
      "Epoch: 148260 | Loss: 0.3808242082595825 | Test loss: 0.3821626603603363\n",
      "Epoch: 148270 | Loss: 0.38081246614456177 | Test loss: 0.3821488320827484\n",
      "Epoch: 148280 | Loss: 0.38080069422721863 | Test loss: 0.38213497400283813\n",
      "Epoch: 148290 | Loss: 0.3807889521121979 | Test loss: 0.38212114572525024\n",
      "Epoch: 148300 | Loss: 0.3807772099971771 | Test loss: 0.3821072578430176\n",
      "Epoch: 148310 | Loss: 0.3807654082775116 | Test loss: 0.3820934295654297\n",
      "Epoch: 148320 | Loss: 0.38075366616249084 | Test loss: 0.3820796012878418\n",
      "Epoch: 148330 | Loss: 0.3807418942451477 | Test loss: 0.3820657432079315\n",
      "Epoch: 148340 | Loss: 0.38073015213012695 | Test loss: 0.38205191493034363\n",
      "Epoch: 148350 | Loss: 0.3807184100151062 | Test loss: 0.38203808665275574\n",
      "Epoch: 148360 | Loss: 0.38070663809776306 | Test loss: 0.38202419877052307\n",
      "Epoch: 148370 | Loss: 0.3806948661804199 | Test loss: 0.3820103704929352\n",
      "Epoch: 148380 | Loss: 0.38068312406539917 | Test loss: 0.3819965124130249\n",
      "Epoch: 148390 | Loss: 0.38067132234573364 | Test loss: 0.3819826543331146\n",
      "Epoch: 148400 | Loss: 0.3806595802307129 | Test loss: 0.38196882605552673\n",
      "Epoch: 148410 | Loss: 0.3806478679180145 | Test loss: 0.38195499777793884\n",
      "Epoch: 148420 | Loss: 0.380636066198349 | Test loss: 0.38194113969802856\n",
      "Epoch: 148430 | Loss: 0.38062429428100586 | Test loss: 0.3819272816181183\n",
      "Epoch: 148440 | Loss: 0.3806125819683075 | Test loss: 0.381913423538208\n",
      "Epoch: 148450 | Loss: 0.38060078024864197 | Test loss: 0.3818995952606201\n",
      "Epoch: 148460 | Loss: 0.3805890381336212 | Test loss: 0.38188573718070984\n",
      "Epoch: 148470 | Loss: 0.3805772662162781 | Test loss: 0.38187190890312195\n",
      "Epoch: 148480 | Loss: 0.3805655241012573 | Test loss: 0.38185805082321167\n",
      "Epoch: 148490 | Loss: 0.3805537223815918 | Test loss: 0.3818441927433014\n",
      "Epoch: 148500 | Loss: 0.38054198026657104 | Test loss: 0.3818303644657135\n",
      "Epoch: 148510 | Loss: 0.3805302083492279 | Test loss: 0.3818165063858032\n",
      "Epoch: 148520 | Loss: 0.38051849603652954 | Test loss: 0.38180264830589294\n",
      "Epoch: 148530 | Loss: 0.3805067241191864 | Test loss: 0.38178882002830505\n",
      "Epoch: 148540 | Loss: 0.3804949223995209 | Test loss: 0.38177499175071716\n",
      "Epoch: 148550 | Loss: 0.3804831802845001 | Test loss: 0.3817611336708069\n",
      "Epoch: 148560 | Loss: 0.38047143816947937 | Test loss: 0.381747305393219\n",
      "Epoch: 148570 | Loss: 0.38045966625213623 | Test loss: 0.3817334473133087\n",
      "Epoch: 148580 | Loss: 0.3804479241371155 | Test loss: 0.38171958923339844\n",
      "Epoch: 148590 | Loss: 0.38043615221977234 | Test loss: 0.38170576095581055\n",
      "Epoch: 148600 | Loss: 0.3804243803024292 | Test loss: 0.38169190287590027\n",
      "Epoch: 148610 | Loss: 0.38041263818740845 | Test loss: 0.3816780745983124\n",
      "Epoch: 148620 | Loss: 0.3804008662700653 | Test loss: 0.3816641867160797\n",
      "Epoch: 148630 | Loss: 0.38038912415504456 | Test loss: 0.3816503584384918\n",
      "Epoch: 148640 | Loss: 0.3803773522377014 | Test loss: 0.38163650035858154\n",
      "Epoch: 148650 | Loss: 0.3803655803203583 | Test loss: 0.38162264227867126\n",
      "Epoch: 148660 | Loss: 0.38035380840301514 | Test loss: 0.3816088140010834\n",
      "Epoch: 148670 | Loss: 0.3803420662879944 | Test loss: 0.3815949857234955\n",
      "Epoch: 148680 | Loss: 0.38033029437065125 | Test loss: 0.3815811276435852\n",
      "Epoch: 148690 | Loss: 0.3803185522556305 | Test loss: 0.3815672993659973\n",
      "Epoch: 148700 | Loss: 0.38030681014060974 | Test loss: 0.38155341148376465\n",
      "Epoch: 148710 | Loss: 0.3802950084209442 | Test loss: 0.38153958320617676\n",
      "Epoch: 148720 | Loss: 0.38028326630592346 | Test loss: 0.38152575492858887\n",
      "Epoch: 148730 | Loss: 0.3802714943885803 | Test loss: 0.3815118968486786\n",
      "Epoch: 148740 | Loss: 0.38025975227355957 | Test loss: 0.3814980685710907\n",
      "Epoch: 148750 | Loss: 0.3802480101585388 | Test loss: 0.3814842402935028\n",
      "Epoch: 148760 | Loss: 0.3802362382411957 | Test loss: 0.38147035241127014\n",
      "Epoch: 148770 | Loss: 0.38022446632385254 | Test loss: 0.38145652413368225\n",
      "Epoch: 148780 | Loss: 0.3802127242088318 | Test loss: 0.381442666053772\n",
      "Epoch: 148790 | Loss: 0.38020092248916626 | Test loss: 0.3814288079738617\n",
      "Epoch: 148800 | Loss: 0.3801891803741455 | Test loss: 0.3814149796962738\n",
      "Epoch: 148810 | Loss: 0.38017746806144714 | Test loss: 0.3814011514186859\n",
      "Epoch: 148820 | Loss: 0.3801656663417816 | Test loss: 0.38138729333877563\n",
      "Epoch: 148830 | Loss: 0.3801538944244385 | Test loss: 0.38137343525886536\n",
      "Epoch: 148840 | Loss: 0.3801421821117401 | Test loss: 0.3813595771789551\n",
      "Epoch: 148850 | Loss: 0.3801303803920746 | Test loss: 0.3813457489013672\n",
      "Epoch: 148860 | Loss: 0.38011863827705383 | Test loss: 0.3813318908214569\n",
      "Epoch: 148870 | Loss: 0.3801068663597107 | Test loss: 0.381318062543869\n",
      "Epoch: 148880 | Loss: 0.38009512424468994 | Test loss: 0.38130420446395874\n",
      "Epoch: 148890 | Loss: 0.3800833225250244 | Test loss: 0.38129034638404846\n",
      "Epoch: 148900 | Loss: 0.38007158041000366 | Test loss: 0.38127651810646057\n",
      "Epoch: 148910 | Loss: 0.3800598084926605 | Test loss: 0.3812626600265503\n",
      "Epoch: 148920 | Loss: 0.38004809617996216 | Test loss: 0.38124880194664\n",
      "Epoch: 148930 | Loss: 0.380036324262619 | Test loss: 0.3812349736690521\n",
      "Epoch: 148940 | Loss: 0.3800245225429535 | Test loss: 0.38122114539146423\n",
      "Epoch: 148950 | Loss: 0.38001278042793274 | Test loss: 0.38120728731155396\n",
      "Epoch: 148960 | Loss: 0.380001038312912 | Test loss: 0.38119345903396606\n",
      "Epoch: 148970 | Loss: 0.37998926639556885 | Test loss: 0.3811796009540558\n",
      "Epoch: 148980 | Loss: 0.3799775242805481 | Test loss: 0.3811657428741455\n",
      "Epoch: 148990 | Loss: 0.37996575236320496 | Test loss: 0.3811519145965576\n",
      "Epoch: 149000 | Loss: 0.3799539804458618 | Test loss: 0.38113805651664734\n",
      "Epoch: 149010 | Loss: 0.37994223833084106 | Test loss: 0.38112422823905945\n",
      "Epoch: 149020 | Loss: 0.3799304664134979 | Test loss: 0.3811103403568268\n",
      "Epoch: 149030 | Loss: 0.3799187242984772 | Test loss: 0.3810965120792389\n",
      "Epoch: 149040 | Loss: 0.37990695238113403 | Test loss: 0.3810826539993286\n",
      "Epoch: 149050 | Loss: 0.3798951804637909 | Test loss: 0.38106879591941833\n",
      "Epoch: 149060 | Loss: 0.37988340854644775 | Test loss: 0.38105496764183044\n",
      "Epoch: 149070 | Loss: 0.379871666431427 | Test loss: 0.38104113936424255\n",
      "Epoch: 149080 | Loss: 0.37985989451408386 | Test loss: 0.3810272812843323\n",
      "Epoch: 149090 | Loss: 0.3798481523990631 | Test loss: 0.3810134530067444\n",
      "Epoch: 149100 | Loss: 0.37983641028404236 | Test loss: 0.3809995651245117\n",
      "Epoch: 149110 | Loss: 0.37982460856437683 | Test loss: 0.38098573684692383\n",
      "Epoch: 149120 | Loss: 0.3798128664493561 | Test loss: 0.38097190856933594\n",
      "Epoch: 149130 | Loss: 0.37980109453201294 | Test loss: 0.38095805048942566\n",
      "Epoch: 149140 | Loss: 0.3797893524169922 | Test loss: 0.38094422221183777\n",
      "Epoch: 149150 | Loss: 0.37977761030197144 | Test loss: 0.3809303939342499\n",
      "Epoch: 149160 | Loss: 0.3797658383846283 | Test loss: 0.3809165060520172\n",
      "Epoch: 149170 | Loss: 0.37975406646728516 | Test loss: 0.3809026777744293\n",
      "Epoch: 149180 | Loss: 0.3797423243522644 | Test loss: 0.38088881969451904\n",
      "Epoch: 149190 | Loss: 0.3797305226325989 | Test loss: 0.38087496161460876\n",
      "Epoch: 149200 | Loss: 0.3797187805175781 | Test loss: 0.3808611333370209\n",
      "Epoch: 149210 | Loss: 0.37970706820487976 | Test loss: 0.380847305059433\n",
      "Epoch: 149220 | Loss: 0.37969526648521423 | Test loss: 0.3808334469795227\n",
      "Epoch: 149230 | Loss: 0.3796834945678711 | Test loss: 0.3808195888996124\n",
      "Epoch: 149240 | Loss: 0.37967178225517273 | Test loss: 0.38080573081970215\n",
      "Epoch: 149250 | Loss: 0.3796599805355072 | Test loss: 0.38079190254211426\n",
      "Epoch: 149260 | Loss: 0.37964823842048645 | Test loss: 0.380778044462204\n",
      "Epoch: 149270 | Loss: 0.3796364665031433 | Test loss: 0.3807642161846161\n",
      "Epoch: 149280 | Loss: 0.37962472438812256 | Test loss: 0.3807503581047058\n",
      "Epoch: 149290 | Loss: 0.37961292266845703 | Test loss: 0.38073650002479553\n",
      "Epoch: 149300 | Loss: 0.3796011805534363 | Test loss: 0.38072267174720764\n",
      "Epoch: 149310 | Loss: 0.37958940863609314 | Test loss: 0.38070881366729736\n",
      "Epoch: 149320 | Loss: 0.3795776963233948 | Test loss: 0.3806949555873871\n",
      "Epoch: 149330 | Loss: 0.37956592440605164 | Test loss: 0.3806811273097992\n",
      "Epoch: 149340 | Loss: 0.3795541226863861 | Test loss: 0.3806672990322113\n",
      "Epoch: 149350 | Loss: 0.37954238057136536 | Test loss: 0.380653440952301\n",
      "Epoch: 149360 | Loss: 0.3795306384563446 | Test loss: 0.38063961267471313\n",
      "Epoch: 149370 | Loss: 0.37951886653900146 | Test loss: 0.38062575459480286\n",
      "Epoch: 149380 | Loss: 0.3795071244239807 | Test loss: 0.3806118965148926\n",
      "Epoch: 149390 | Loss: 0.3794953525066376 | Test loss: 0.3805980682373047\n",
      "Epoch: 149400 | Loss: 0.37948358058929443 | Test loss: 0.3805842101573944\n",
      "Epoch: 149410 | Loss: 0.3794718384742737 | Test loss: 0.3805703818798065\n",
      "Epoch: 149420 | Loss: 0.37946006655693054 | Test loss: 0.38055649399757385\n",
      "Epoch: 149430 | Loss: 0.3794483244419098 | Test loss: 0.38054266571998596\n",
      "Epoch: 149440 | Loss: 0.37943655252456665 | Test loss: 0.3805288076400757\n",
      "Epoch: 149450 | Loss: 0.3794247806072235 | Test loss: 0.3805149495601654\n",
      "Epoch: 149460 | Loss: 0.37941300868988037 | Test loss: 0.3805011212825775\n",
      "Epoch: 149470 | Loss: 0.3794012665748596 | Test loss: 0.3804872930049896\n",
      "Epoch: 149480 | Loss: 0.3793894946575165 | Test loss: 0.38047343492507935\n",
      "Epoch: 149490 | Loss: 0.3793777525424957 | Test loss: 0.38045960664749146\n",
      "Epoch: 149500 | Loss: 0.379366010427475 | Test loss: 0.3804457187652588\n",
      "Epoch: 149510 | Loss: 0.37935420870780945 | Test loss: 0.3804318904876709\n",
      "Epoch: 149520 | Loss: 0.3793424665927887 | Test loss: 0.380418062210083\n",
      "Epoch: 149530 | Loss: 0.37933069467544556 | Test loss: 0.38040420413017273\n",
      "Epoch: 149540 | Loss: 0.3793189525604248 | Test loss: 0.38039037585258484\n",
      "Epoch: 149550 | Loss: 0.37930721044540405 | Test loss: 0.38037654757499695\n",
      "Epoch: 149560 | Loss: 0.3792954385280609 | Test loss: 0.3803626596927643\n",
      "Epoch: 149570 | Loss: 0.3792836666107178 | Test loss: 0.3803488314151764\n",
      "Epoch: 149580 | Loss: 0.379271924495697 | Test loss: 0.3803349733352661\n",
      "Epoch: 149590 | Loss: 0.3792601227760315 | Test loss: 0.38032111525535583\n",
      "Epoch: 149600 | Loss: 0.37924838066101074 | Test loss: 0.38030728697776794\n",
      "Epoch: 149610 | Loss: 0.3792366683483124 | Test loss: 0.38029345870018005\n",
      "Epoch: 149620 | Loss: 0.37922486662864685 | Test loss: 0.3802796006202698\n",
      "Epoch: 149630 | Loss: 0.3792130947113037 | Test loss: 0.3802657425403595\n",
      "Epoch: 149640 | Loss: 0.37920138239860535 | Test loss: 0.3802518844604492\n",
      "Epoch: 149650 | Loss: 0.3791895806789398 | Test loss: 0.38023805618286133\n",
      "Epoch: 149660 | Loss: 0.37917783856391907 | Test loss: 0.38022419810295105\n",
      "Epoch: 149670 | Loss: 0.3791660666465759 | Test loss: 0.38021036982536316\n",
      "Epoch: 149680 | Loss: 0.3791543245315552 | Test loss: 0.3801965117454529\n",
      "Epoch: 149690 | Loss: 0.37914252281188965 | Test loss: 0.3801826536655426\n",
      "Epoch: 149700 | Loss: 0.3791307806968689 | Test loss: 0.3801688253879547\n",
      "Epoch: 149710 | Loss: 0.37911900877952576 | Test loss: 0.38015496730804443\n",
      "Epoch: 149720 | Loss: 0.3791072964668274 | Test loss: 0.38014110922813416\n",
      "Epoch: 149730 | Loss: 0.37909552454948425 | Test loss: 0.38012728095054626\n",
      "Epoch: 149740 | Loss: 0.3790837228298187 | Test loss: 0.3801134526729584\n",
      "Epoch: 149750 | Loss: 0.379071980714798 | Test loss: 0.3800995945930481\n",
      "Epoch: 149760 | Loss: 0.3790602385997772 | Test loss: 0.3800857663154602\n",
      "Epoch: 149770 | Loss: 0.3790484666824341 | Test loss: 0.3800719082355499\n",
      "Epoch: 149780 | Loss: 0.37903672456741333 | Test loss: 0.38005805015563965\n",
      "Epoch: 149790 | Loss: 0.3790249526500702 | Test loss: 0.38004422187805176\n",
      "Epoch: 149800 | Loss: 0.37901318073272705 | Test loss: 0.3800303637981415\n",
      "Epoch: 149810 | Loss: 0.3790014386177063 | Test loss: 0.3800165355205536\n",
      "Epoch: 149820 | Loss: 0.37898966670036316 | Test loss: 0.3800026476383209\n",
      "Epoch: 149830 | Loss: 0.3789779245853424 | Test loss: 0.37998881936073303\n",
      "Epoch: 149840 | Loss: 0.37896615266799927 | Test loss: 0.37997496128082275\n",
      "Epoch: 149850 | Loss: 0.37895438075065613 | Test loss: 0.3799611032009125\n",
      "Epoch: 149860 | Loss: 0.378942608833313 | Test loss: 0.3799472749233246\n",
      "Epoch: 149870 | Loss: 0.37893086671829224 | Test loss: 0.3799334466457367\n",
      "Epoch: 149880 | Loss: 0.3789190948009491 | Test loss: 0.3799195885658264\n",
      "Epoch: 149890 | Loss: 0.37890735268592834 | Test loss: 0.3799057602882385\n",
      "Epoch: 149900 | Loss: 0.3788956105709076 | Test loss: 0.37989187240600586\n",
      "Epoch: 149910 | Loss: 0.37888380885124207 | Test loss: 0.37987804412841797\n",
      "Epoch: 149920 | Loss: 0.3788720667362213 | Test loss: 0.3798642158508301\n",
      "Epoch: 149930 | Loss: 0.3788602948188782 | Test loss: 0.3798503577709198\n",
      "Epoch: 149940 | Loss: 0.3788485527038574 | Test loss: 0.3798365294933319\n",
      "Epoch: 149950 | Loss: 0.37883681058883667 | Test loss: 0.379822701215744\n",
      "Epoch: 149960 | Loss: 0.37882503867149353 | Test loss: 0.37980881333351135\n",
      "Epoch: 149970 | Loss: 0.3788132667541504 | Test loss: 0.37979498505592346\n",
      "Epoch: 149980 | Loss: 0.37880152463912964 | Test loss: 0.3797811269760132\n",
      "Epoch: 149990 | Loss: 0.3787897229194641 | Test loss: 0.3797672688961029\n",
      "Epoch: 150000 | Loss: 0.37877798080444336 | Test loss: 0.379753440618515\n",
      "Epoch: 150010 | Loss: 0.378766268491745 | Test loss: 0.3797396123409271\n",
      "Epoch: 150020 | Loss: 0.37875446677207947 | Test loss: 0.37972575426101685\n",
      "Epoch: 150030 | Loss: 0.37874269485473633 | Test loss: 0.37971189618110657\n",
      "Epoch: 150040 | Loss: 0.37873098254203796 | Test loss: 0.3796980381011963\n",
      "Epoch: 150050 | Loss: 0.37871918082237244 | Test loss: 0.3796842098236084\n",
      "Epoch: 150060 | Loss: 0.3787074387073517 | Test loss: 0.3796703517436981\n",
      "Epoch: 150070 | Loss: 0.37869566679000854 | Test loss: 0.37965652346611023\n",
      "Epoch: 150080 | Loss: 0.3786839246749878 | Test loss: 0.37964266538619995\n",
      "Epoch: 150090 | Loss: 0.37867212295532227 | Test loss: 0.3796288073062897\n",
      "Epoch: 150100 | Loss: 0.3786603808403015 | Test loss: 0.3796149790287018\n",
      "Epoch: 150110 | Loss: 0.3786486089229584 | Test loss: 0.3796011209487915\n",
      "Epoch: 150120 | Loss: 0.37863689661026 | Test loss: 0.3795872628688812\n",
      "Epoch: 150130 | Loss: 0.37862512469291687 | Test loss: 0.37957343459129333\n",
      "Epoch: 150140 | Loss: 0.37861332297325134 | Test loss: 0.37955960631370544\n",
      "Epoch: 150150 | Loss: 0.3786015808582306 | Test loss: 0.37954574823379517\n",
      "Epoch: 150160 | Loss: 0.37858983874320984 | Test loss: 0.3795319199562073\n",
      "Epoch: 150170 | Loss: 0.3785780668258667 | Test loss: 0.379518061876297\n",
      "Epoch: 150180 | Loss: 0.37856632471084595 | Test loss: 0.3795042037963867\n",
      "Epoch: 150190 | Loss: 0.3785545527935028 | Test loss: 0.37949037551879883\n",
      "Epoch: 150200 | Loss: 0.37854278087615967 | Test loss: 0.37947651743888855\n",
      "Epoch: 150210 | Loss: 0.3785310387611389 | Test loss: 0.37946268916130066\n",
      "Epoch: 150220 | Loss: 0.3785192668437958 | Test loss: 0.379448801279068\n",
      "Epoch: 150230 | Loss: 0.378507524728775 | Test loss: 0.3794349730014801\n",
      "Epoch: 150240 | Loss: 0.3784957528114319 | Test loss: 0.3794211149215698\n",
      "Epoch: 150250 | Loss: 0.37848398089408875 | Test loss: 0.37940725684165955\n",
      "Epoch: 150260 | Loss: 0.3784722089767456 | Test loss: 0.37939342856407166\n",
      "Epoch: 150270 | Loss: 0.37846046686172485 | Test loss: 0.37937960028648376\n",
      "Epoch: 150280 | Loss: 0.3784486949443817 | Test loss: 0.3793657422065735\n",
      "Epoch: 150290 | Loss: 0.37843695282936096 | Test loss: 0.3793519139289856\n",
      "Epoch: 150300 | Loss: 0.3784252107143402 | Test loss: 0.37933802604675293\n",
      "Epoch: 150310 | Loss: 0.3784134089946747 | Test loss: 0.37932419776916504\n",
      "Epoch: 150320 | Loss: 0.37840166687965393 | Test loss: 0.37931036949157715\n",
      "Epoch: 150330 | Loss: 0.3783898949623108 | Test loss: 0.37929651141166687\n",
      "Epoch: 150340 | Loss: 0.37837815284729004 | Test loss: 0.379282683134079\n",
      "Epoch: 150350 | Loss: 0.3783664107322693 | Test loss: 0.3792688548564911\n",
      "Epoch: 150360 | Loss: 0.37835463881492615 | Test loss: 0.3792549669742584\n",
      "Epoch: 150370 | Loss: 0.378342866897583 | Test loss: 0.37924113869667053\n",
      "Epoch: 150380 | Loss: 0.37833112478256226 | Test loss: 0.37922728061676025\n",
      "Epoch: 150390 | Loss: 0.37831932306289673 | Test loss: 0.37921342253685\n",
      "Epoch: 150400 | Loss: 0.378307580947876 | Test loss: 0.3791995942592621\n",
      "Epoch: 150410 | Loss: 0.3782958686351776 | Test loss: 0.3791857659816742\n",
      "Epoch: 150420 | Loss: 0.3782840669155121 | Test loss: 0.3791719079017639\n",
      "Epoch: 150430 | Loss: 0.37827229499816895 | Test loss: 0.37915804982185364\n",
      "Epoch: 150440 | Loss: 0.3782605826854706 | Test loss: 0.37914419174194336\n",
      "Epoch: 150450 | Loss: 0.37824878096580505 | Test loss: 0.37913036346435547\n",
      "Epoch: 150460 | Loss: 0.3782370388507843 | Test loss: 0.3791165053844452\n",
      "Epoch: 150470 | Loss: 0.37822526693344116 | Test loss: 0.3791026771068573\n",
      "Epoch: 150480 | Loss: 0.3782135248184204 | Test loss: 0.379088819026947\n",
      "Epoch: 150490 | Loss: 0.3782017230987549 | Test loss: 0.37907496094703674\n",
      "Epoch: 150500 | Loss: 0.37818998098373413 | Test loss: 0.37906113266944885\n",
      "Epoch: 150510 | Loss: 0.378178209066391 | Test loss: 0.3790472745895386\n",
      "Epoch: 150520 | Loss: 0.3781664967536926 | Test loss: 0.3790334165096283\n",
      "Epoch: 150530 | Loss: 0.3781547248363495 | Test loss: 0.3790195882320404\n",
      "Epoch: 150540 | Loss: 0.37814292311668396 | Test loss: 0.3790057599544525\n",
      "Epoch: 150550 | Loss: 0.3781311810016632 | Test loss: 0.37899190187454224\n",
      "Epoch: 150560 | Loss: 0.37811943888664246 | Test loss: 0.37897807359695435\n",
      "Epoch: 150570 | Loss: 0.3781076669692993 | Test loss: 0.37896421551704407\n",
      "Epoch: 150580 | Loss: 0.37809592485427856 | Test loss: 0.3789503574371338\n",
      "Epoch: 150590 | Loss: 0.3780841529369354 | Test loss: 0.3789365291595459\n",
      "Epoch: 150600 | Loss: 0.3780723810195923 | Test loss: 0.3789226710796356\n",
      "Epoch: 150610 | Loss: 0.37806063890457153 | Test loss: 0.37890884280204773\n",
      "Epoch: 150620 | Loss: 0.3780488669872284 | Test loss: 0.37889495491981506\n",
      "Epoch: 150630 | Loss: 0.37803712487220764 | Test loss: 0.3788811266422272\n",
      "Epoch: 150640 | Loss: 0.3780253529548645 | Test loss: 0.3788672685623169\n",
      "Epoch: 150650 | Loss: 0.37801358103752136 | Test loss: 0.3788534104824066\n",
      "Epoch: 150660 | Loss: 0.3780018091201782 | Test loss: 0.3788395822048187\n",
      "Epoch: 150670 | Loss: 0.37799006700515747 | Test loss: 0.37882575392723083\n",
      "Epoch: 150680 | Loss: 0.37797829508781433 | Test loss: 0.37881189584732056\n",
      "Epoch: 150690 | Loss: 0.3779665529727936 | Test loss: 0.37879806756973267\n",
      "Epoch: 150700 | Loss: 0.3779548108577728 | Test loss: 0.3787841796875\n",
      "Epoch: 150710 | Loss: 0.3779430091381073 | Test loss: 0.3787703514099121\n",
      "Epoch: 150720 | Loss: 0.37793126702308655 | Test loss: 0.3787565231323242\n",
      "Epoch: 150730 | Loss: 0.3779194951057434 | Test loss: 0.37874266505241394\n",
      "Epoch: 150740 | Loss: 0.37790775299072266 | Test loss: 0.37872883677482605\n",
      "Epoch: 150750 | Loss: 0.3778960108757019 | Test loss: 0.37871500849723816\n",
      "Epoch: 150760 | Loss: 0.37788423895835876 | Test loss: 0.3787011206150055\n",
      "Epoch: 150770 | Loss: 0.3778724670410156 | Test loss: 0.3786872923374176\n",
      "Epoch: 150780 | Loss: 0.3778607249259949 | Test loss: 0.3786734342575073\n",
      "Epoch: 150790 | Loss: 0.37784892320632935 | Test loss: 0.37865957617759705\n",
      "Epoch: 150800 | Loss: 0.3778371810913086 | Test loss: 0.37864574790000916\n",
      "Epoch: 150810 | Loss: 0.37782546877861023 | Test loss: 0.37863191962242126\n",
      "Epoch: 150820 | Loss: 0.3778136670589447 | Test loss: 0.378618061542511\n",
      "Epoch: 150830 | Loss: 0.37780189514160156 | Test loss: 0.3786042034626007\n",
      "Epoch: 150840 | Loss: 0.3777901828289032 | Test loss: 0.37859034538269043\n",
      "Epoch: 150850 | Loss: 0.37777838110923767 | Test loss: 0.37857651710510254\n",
      "Epoch: 150860 | Loss: 0.3777666389942169 | Test loss: 0.37856265902519226\n",
      "Epoch: 150870 | Loss: 0.3777548670768738 | Test loss: 0.37854883074760437\n",
      "Epoch: 150880 | Loss: 0.377743124961853 | Test loss: 0.3785349726676941\n",
      "Epoch: 150890 | Loss: 0.3777313232421875 | Test loss: 0.3785211145877838\n",
      "Epoch: 150900 | Loss: 0.37771958112716675 | Test loss: 0.3785072863101959\n",
      "Epoch: 150910 | Loss: 0.3777078092098236 | Test loss: 0.37849342823028564\n",
      "Epoch: 150920 | Loss: 0.37769609689712524 | Test loss: 0.37847957015037537\n",
      "Epoch: 150930 | Loss: 0.3776843249797821 | Test loss: 0.3784657418727875\n",
      "Epoch: 150940 | Loss: 0.3776725232601166 | Test loss: 0.3784519135951996\n",
      "Epoch: 150950 | Loss: 0.3776607811450958 | Test loss: 0.3784380555152893\n",
      "Epoch: 150960 | Loss: 0.3776490390300751 | Test loss: 0.3784242272377014\n",
      "Epoch: 150970 | Loss: 0.37763726711273193 | Test loss: 0.37841036915779114\n",
      "Epoch: 150980 | Loss: 0.3776255249977112 | Test loss: 0.37839651107788086\n",
      "Epoch: 150990 | Loss: 0.37761375308036804 | Test loss: 0.37838268280029297\n",
      "Epoch: 151000 | Loss: 0.3776019811630249 | Test loss: 0.3783688247203827\n",
      "Epoch: 151010 | Loss: 0.37759023904800415 | Test loss: 0.3783549964427948\n",
      "Epoch: 151020 | Loss: 0.377578467130661 | Test loss: 0.37834110856056213\n",
      "Epoch: 151030 | Loss: 0.37756672501564026 | Test loss: 0.37832728028297424\n",
      "Epoch: 151040 | Loss: 0.3775549530982971 | Test loss: 0.37831342220306396\n",
      "Epoch: 151050 | Loss: 0.377543181180954 | Test loss: 0.3782995641231537\n",
      "Epoch: 151060 | Loss: 0.37753140926361084 | Test loss: 0.3782857358455658\n",
      "Epoch: 151070 | Loss: 0.3775196671485901 | Test loss: 0.3782719075679779\n",
      "Epoch: 151080 | Loss: 0.37750789523124695 | Test loss: 0.3782580494880676\n",
      "Epoch: 151090 | Loss: 0.3774961531162262 | Test loss: 0.37824422121047974\n",
      "Epoch: 151100 | Loss: 0.37748441100120544 | Test loss: 0.37823033332824707\n",
      "Epoch: 151110 | Loss: 0.3774726092815399 | Test loss: 0.3782165050506592\n",
      "Epoch: 151120 | Loss: 0.37746086716651917 | Test loss: 0.3782026767730713\n",
      "Epoch: 151130 | Loss: 0.377449095249176 | Test loss: 0.378188818693161\n",
      "Epoch: 151140 | Loss: 0.3774373531341553 | Test loss: 0.3781749904155731\n",
      "Epoch: 151150 | Loss: 0.3774256110191345 | Test loss: 0.37816116213798523\n",
      "Epoch: 151160 | Loss: 0.3774138391017914 | Test loss: 0.37814727425575256\n",
      "Epoch: 151170 | Loss: 0.37740206718444824 | Test loss: 0.3781334459781647\n",
      "Epoch: 151180 | Loss: 0.3773903250694275 | Test loss: 0.3781195878982544\n",
      "Epoch: 151190 | Loss: 0.37737852334976196 | Test loss: 0.3781057298183441\n",
      "Epoch: 151200 | Loss: 0.3773667812347412 | Test loss: 0.3780919015407562\n",
      "Epoch: 151210 | Loss: 0.37735506892204285 | Test loss: 0.37807807326316833\n",
      "Epoch: 151220 | Loss: 0.3773432672023773 | Test loss: 0.37806421518325806\n",
      "Epoch: 151230 | Loss: 0.3773314952850342 | Test loss: 0.3780503571033478\n",
      "Epoch: 151240 | Loss: 0.3773197829723358 | Test loss: 0.3780364990234375\n",
      "Epoch: 151250 | Loss: 0.3773079812526703 | Test loss: 0.3780226707458496\n",
      "Epoch: 151260 | Loss: 0.37729623913764954 | Test loss: 0.37800881266593933\n",
      "Epoch: 151270 | Loss: 0.3772844672203064 | Test loss: 0.37799498438835144\n",
      "Epoch: 151280 | Loss: 0.37727272510528564 | Test loss: 0.37798112630844116\n",
      "Epoch: 151290 | Loss: 0.3772609233856201 | Test loss: 0.3779672682285309\n",
      "Epoch: 151300 | Loss: 0.37724918127059937 | Test loss: 0.377953439950943\n",
      "Epoch: 151310 | Loss: 0.3772374093532562 | Test loss: 0.3779395818710327\n",
      "Epoch: 151320 | Loss: 0.37722569704055786 | Test loss: 0.37792572379112244\n",
      "Epoch: 151330 | Loss: 0.3772139251232147 | Test loss: 0.37791189551353455\n",
      "Epoch: 151340 | Loss: 0.3772021234035492 | Test loss: 0.37789806723594666\n",
      "Epoch: 151350 | Loss: 0.37719038128852844 | Test loss: 0.3778842091560364\n",
      "Epoch: 151360 | Loss: 0.3771786391735077 | Test loss: 0.3778703808784485\n",
      "Epoch: 151370 | Loss: 0.37716686725616455 | Test loss: 0.3778565227985382\n",
      "Epoch: 151380 | Loss: 0.3771551251411438 | Test loss: 0.37784266471862793\n",
      "Epoch: 151390 | Loss: 0.37714335322380066 | Test loss: 0.37782883644104004\n",
      "Epoch: 151400 | Loss: 0.3771315813064575 | Test loss: 0.37781497836112976\n",
      "Epoch: 151410 | Loss: 0.37711983919143677 | Test loss: 0.37780115008354187\n",
      "Epoch: 151420 | Loss: 0.37710806727409363 | Test loss: 0.3777872622013092\n",
      "Epoch: 151430 | Loss: 0.3770963251590729 | Test loss: 0.3777734339237213\n",
      "Epoch: 151440 | Loss: 0.37708455324172974 | Test loss: 0.37775957584381104\n",
      "Epoch: 151450 | Loss: 0.3770727813243866 | Test loss: 0.37774571776390076\n",
      "Epoch: 151460 | Loss: 0.37706100940704346 | Test loss: 0.37773188948631287\n",
      "Epoch: 151470 | Loss: 0.3770492672920227 | Test loss: 0.377718061208725\n",
      "Epoch: 151480 | Loss: 0.37703749537467957 | Test loss: 0.3777042031288147\n",
      "Epoch: 151490 | Loss: 0.3770257532596588 | Test loss: 0.3776903748512268\n",
      "Epoch: 151500 | Loss: 0.37701401114463806 | Test loss: 0.37767648696899414\n",
      "Epoch: 151510 | Loss: 0.37700220942497253 | Test loss: 0.37766265869140625\n",
      "Epoch: 151520 | Loss: 0.3769904673099518 | Test loss: 0.37764883041381836\n",
      "Epoch: 151530 | Loss: 0.37697869539260864 | Test loss: 0.3776349723339081\n",
      "Epoch: 151540 | Loss: 0.3769669532775879 | Test loss: 0.3776211440563202\n",
      "Epoch: 151550 | Loss: 0.37695521116256714 | Test loss: 0.3776073157787323\n",
      "Epoch: 151560 | Loss: 0.376943439245224 | Test loss: 0.37759342789649963\n",
      "Epoch: 151570 | Loss: 0.37693166732788086 | Test loss: 0.37757959961891174\n",
      "Epoch: 151580 | Loss: 0.3769199252128601 | Test loss: 0.37756574153900146\n",
      "Epoch: 151590 | Loss: 0.3769081234931946 | Test loss: 0.3775518834590912\n",
      "Epoch: 151600 | Loss: 0.37689638137817383 | Test loss: 0.3775380551815033\n",
      "Epoch: 151610 | Loss: 0.37688466906547546 | Test loss: 0.3775242269039154\n",
      "Epoch: 151620 | Loss: 0.37687286734580994 | Test loss: 0.3775103688240051\n",
      "Epoch: 151630 | Loss: 0.3768610954284668 | Test loss: 0.37749651074409485\n",
      "Epoch: 151640 | Loss: 0.37684938311576843 | Test loss: 0.37748265266418457\n",
      "Epoch: 151650 | Loss: 0.3768375813961029 | Test loss: 0.3774688243865967\n",
      "Epoch: 151660 | Loss: 0.37682583928108215 | Test loss: 0.3774549663066864\n",
      "Epoch: 151670 | Loss: 0.376814067363739 | Test loss: 0.3774411380290985\n",
      "Epoch: 151680 | Loss: 0.37680232524871826 | Test loss: 0.37742727994918823\n",
      "Epoch: 151690 | Loss: 0.37679052352905273 | Test loss: 0.37741342186927795\n",
      "Epoch: 151700 | Loss: 0.376778781414032 | Test loss: 0.37739959359169006\n",
      "Epoch: 151710 | Loss: 0.37676700949668884 | Test loss: 0.3773857355117798\n",
      "Epoch: 151720 | Loss: 0.3767552971839905 | Test loss: 0.3773718774318695\n",
      "Epoch: 151730 | Loss: 0.37674352526664734 | Test loss: 0.3773580491542816\n",
      "Epoch: 151740 | Loss: 0.3767317235469818 | Test loss: 0.3773442208766937\n",
      "Epoch: 151750 | Loss: 0.37671998143196106 | Test loss: 0.37733036279678345\n",
      "Epoch: 151760 | Loss: 0.3767082393169403 | Test loss: 0.37731653451919556\n",
      "Epoch: 151770 | Loss: 0.37669646739959717 | Test loss: 0.3773026764392853\n",
      "Epoch: 151780 | Loss: 0.3766847252845764 | Test loss: 0.377288818359375\n",
      "Epoch: 151790 | Loss: 0.3766729533672333 | Test loss: 0.3772749900817871\n",
      "Epoch: 151800 | Loss: 0.37666118144989014 | Test loss: 0.37726113200187683\n",
      "Epoch: 151810 | Loss: 0.3766494393348694 | Test loss: 0.37724730372428894\n",
      "Epoch: 151820 | Loss: 0.37663766741752625 | Test loss: 0.3772334158420563\n",
      "Epoch: 151830 | Loss: 0.3766259253025055 | Test loss: 0.3772195875644684\n",
      "Epoch: 151840 | Loss: 0.37661415338516235 | Test loss: 0.3772057294845581\n",
      "Epoch: 151850 | Loss: 0.3766023814678192 | Test loss: 0.3771918714046478\n",
      "Epoch: 151860 | Loss: 0.3765906095504761 | Test loss: 0.37717804312705994\n",
      "Epoch: 151870 | Loss: 0.3765788674354553 | Test loss: 0.37716421484947205\n",
      "Epoch: 151880 | Loss: 0.3765670955181122 | Test loss: 0.37715035676956177\n",
      "Epoch: 151890 | Loss: 0.37655535340309143 | Test loss: 0.3771365284919739\n",
      "Epoch: 151900 | Loss: 0.3765436112880707 | Test loss: 0.3771226406097412\n",
      "Epoch: 151910 | Loss: 0.37653180956840515 | Test loss: 0.3771088123321533\n",
      "Epoch: 151920 | Loss: 0.3765200674533844 | Test loss: 0.37709498405456543\n",
      "Epoch: 151930 | Loss: 0.37650829553604126 | Test loss: 0.37708112597465515\n",
      "Epoch: 151940 | Loss: 0.3764965534210205 | Test loss: 0.37706729769706726\n",
      "Epoch: 151950 | Loss: 0.37648481130599976 | Test loss: 0.37705346941947937\n",
      "Epoch: 151960 | Loss: 0.3764730393886566 | Test loss: 0.3770395815372467\n",
      "Epoch: 151970 | Loss: 0.3764612674713135 | Test loss: 0.3770257532596588\n",
      "Epoch: 151980 | Loss: 0.3764495253562927 | Test loss: 0.37701189517974854\n",
      "Epoch: 151990 | Loss: 0.3764377236366272 | Test loss: 0.37699803709983826\n",
      "Epoch: 152000 | Loss: 0.37642598152160645 | Test loss: 0.37698420882225037\n",
      "Epoch: 152010 | Loss: 0.3764142692089081 | Test loss: 0.3769703805446625\n",
      "Epoch: 152020 | Loss: 0.37640246748924255 | Test loss: 0.3769565224647522\n",
      "Epoch: 152030 | Loss: 0.3763906955718994 | Test loss: 0.3769426643848419\n",
      "Epoch: 152040 | Loss: 0.37637898325920105 | Test loss: 0.37692880630493164\n",
      "Epoch: 152050 | Loss: 0.3763671815395355 | Test loss: 0.37691497802734375\n",
      "Epoch: 152060 | Loss: 0.37635543942451477 | Test loss: 0.37690111994743347\n",
      "Epoch: 152070 | Loss: 0.37634366750717163 | Test loss: 0.3768872916698456\n",
      "Epoch: 152080 | Loss: 0.3763319253921509 | Test loss: 0.3768734335899353\n",
      "Epoch: 152090 | Loss: 0.37632012367248535 | Test loss: 0.376859575510025\n",
      "Epoch: 152100 | Loss: 0.3763083815574646 | Test loss: 0.37684574723243713\n",
      "Epoch: 152110 | Loss: 0.37629660964012146 | Test loss: 0.37683188915252686\n",
      "Epoch: 152120 | Loss: 0.3762848973274231 | Test loss: 0.3768180310726166\n",
      "Epoch: 152130 | Loss: 0.37627312541007996 | Test loss: 0.3768042027950287\n",
      "Epoch: 152140 | Loss: 0.37626132369041443 | Test loss: 0.3767903745174408\n",
      "Epoch: 152150 | Loss: 0.3762495815753937 | Test loss: 0.3767765164375305\n",
      "Epoch: 152160 | Loss: 0.3762378394603729 | Test loss: 0.3767626881599426\n",
      "Epoch: 152170 | Loss: 0.3762260675430298 | Test loss: 0.37674883008003235\n",
      "Epoch: 152180 | Loss: 0.37621432542800903 | Test loss: 0.37673497200012207\n",
      "Epoch: 152190 | Loss: 0.3762025535106659 | Test loss: 0.3767211437225342\n",
      "Epoch: 152200 | Loss: 0.37619078159332275 | Test loss: 0.3767072856426239\n",
      "Epoch: 152210 | Loss: 0.376179039478302 | Test loss: 0.376693457365036\n",
      "Epoch: 152220 | Loss: 0.37616726756095886 | Test loss: 0.37667956948280334\n",
      "Epoch: 152230 | Loss: 0.3761555254459381 | Test loss: 0.37666574120521545\n",
      "Epoch: 152240 | Loss: 0.37614375352859497 | Test loss: 0.3766518831253052\n",
      "Epoch: 152250 | Loss: 0.37613198161125183 | Test loss: 0.3766380250453949\n",
      "Epoch: 152260 | Loss: 0.3761202096939087 | Test loss: 0.376624196767807\n",
      "Epoch: 152270 | Loss: 0.37610846757888794 | Test loss: 0.3766103684902191\n",
      "Epoch: 152280 | Loss: 0.3760966956615448 | Test loss: 0.37659651041030884\n",
      "Epoch: 152290 | Loss: 0.37608495354652405 | Test loss: 0.37658268213272095\n",
      "Epoch: 152300 | Loss: 0.3760732114315033 | Test loss: 0.3765687942504883\n",
      "Epoch: 152310 | Loss: 0.37606140971183777 | Test loss: 0.3765549659729004\n",
      "Epoch: 152320 | Loss: 0.376049667596817 | Test loss: 0.3765411376953125\n",
      "Epoch: 152330 | Loss: 0.3760378956794739 | Test loss: 0.3765272796154022\n",
      "Epoch: 152340 | Loss: 0.3760261535644531 | Test loss: 0.37651345133781433\n",
      "Epoch: 152350 | Loss: 0.3760144114494324 | Test loss: 0.37649962306022644\n",
      "Epoch: 152360 | Loss: 0.37600263953208923 | Test loss: 0.3764857351779938\n",
      "Epoch: 152370 | Loss: 0.3759908676147461 | Test loss: 0.3764719069004059\n",
      "Epoch: 152380 | Loss: 0.37597912549972534 | Test loss: 0.3764580488204956\n",
      "Epoch: 152390 | Loss: 0.3759673237800598 | Test loss: 0.3764441907405853\n",
      "Epoch: 152400 | Loss: 0.37595558166503906 | Test loss: 0.37643036246299744\n",
      "Epoch: 152410 | Loss: 0.3759438693523407 | Test loss: 0.37641653418540955\n",
      "Epoch: 152420 | Loss: 0.37593206763267517 | Test loss: 0.37640267610549927\n",
      "Epoch: 152430 | Loss: 0.37592029571533203 | Test loss: 0.376388818025589\n",
      "Epoch: 152440 | Loss: 0.37590858340263367 | Test loss: 0.3763749599456787\n",
      "Epoch: 152450 | Loss: 0.37589678168296814 | Test loss: 0.3763611316680908\n",
      "Epoch: 152460 | Loss: 0.3758850395679474 | Test loss: 0.37634727358818054\n",
      "Epoch: 152470 | Loss: 0.37587326765060425 | Test loss: 0.37633344531059265\n",
      "Epoch: 152480 | Loss: 0.3758615255355835 | Test loss: 0.3763195872306824\n",
      "Epoch: 152490 | Loss: 0.37584972381591797 | Test loss: 0.3763057291507721\n",
      "Epoch: 152500 | Loss: 0.3758379817008972 | Test loss: 0.3762919008731842\n",
      "Epoch: 152510 | Loss: 0.3758262097835541 | Test loss: 0.3762780427932739\n",
      "Epoch: 152520 | Loss: 0.3758144974708557 | Test loss: 0.37626418471336365\n",
      "Epoch: 152530 | Loss: 0.3758027255535126 | Test loss: 0.37625035643577576\n",
      "Epoch: 152540 | Loss: 0.37579092383384705 | Test loss: 0.37623652815818787\n",
      "Epoch: 152550 | Loss: 0.3757791817188263 | Test loss: 0.3762226700782776\n",
      "Epoch: 152560 | Loss: 0.37576743960380554 | Test loss: 0.3762088418006897\n",
      "Epoch: 152570 | Loss: 0.3757556676864624 | Test loss: 0.3761949837207794\n",
      "Epoch: 152580 | Loss: 0.37574392557144165 | Test loss: 0.37618112564086914\n",
      "Epoch: 152590 | Loss: 0.3757321536540985 | Test loss: 0.37616729736328125\n",
      "Epoch: 152600 | Loss: 0.37572038173675537 | Test loss: 0.37615343928337097\n",
      "Epoch: 152610 | Loss: 0.3757086396217346 | Test loss: 0.3761396110057831\n",
      "Epoch: 152620 | Loss: 0.3756968677043915 | Test loss: 0.3761257231235504\n",
      "Epoch: 152630 | Loss: 0.3756851255893707 | Test loss: 0.3761118948459625\n",
      "Epoch: 152640 | Loss: 0.3756733536720276 | Test loss: 0.37609803676605225\n",
      "Epoch: 152650 | Loss: 0.37566158175468445 | Test loss: 0.37608417868614197\n",
      "Epoch: 152660 | Loss: 0.3756498098373413 | Test loss: 0.3760703504085541\n",
      "Epoch: 152670 | Loss: 0.37563806772232056 | Test loss: 0.3760565221309662\n",
      "Epoch: 152680 | Loss: 0.3756262958049774 | Test loss: 0.3760426640510559\n",
      "Epoch: 152690 | Loss: 0.37561455368995667 | Test loss: 0.376028835773468\n",
      "Epoch: 152700 | Loss: 0.3756028115749359 | Test loss: 0.37601494789123535\n",
      "Epoch: 152710 | Loss: 0.3755910098552704 | Test loss: 0.37600111961364746\n",
      "Epoch: 152720 | Loss: 0.37557926774024963 | Test loss: 0.37598729133605957\n",
      "Epoch: 152730 | Loss: 0.3755674958229065 | Test loss: 0.3759734332561493\n",
      "Epoch: 152740 | Loss: 0.37555575370788574 | Test loss: 0.3759596049785614\n",
      "Epoch: 152750 | Loss: 0.375544011592865 | Test loss: 0.3759457767009735\n",
      "Epoch: 152760 | Loss: 0.37553223967552185 | Test loss: 0.37593188881874084\n",
      "Epoch: 152770 | Loss: 0.3755204677581787 | Test loss: 0.37591806054115295\n",
      "Epoch: 152780 | Loss: 0.37550872564315796 | Test loss: 0.3759042024612427\n",
      "Epoch: 152790 | Loss: 0.37549692392349243 | Test loss: 0.3758903443813324\n",
      "Epoch: 152800 | Loss: 0.3754851818084717 | Test loss: 0.3758765161037445\n",
      "Epoch: 152810 | Loss: 0.3754734694957733 | Test loss: 0.3758626878261566\n",
      "Epoch: 152820 | Loss: 0.3754616677761078 | Test loss: 0.37584882974624634\n",
      "Epoch: 152830 | Loss: 0.37544989585876465 | Test loss: 0.37583497166633606\n",
      "Epoch: 152840 | Loss: 0.3754381835460663 | Test loss: 0.3758211135864258\n",
      "Epoch: 152850 | Loss: 0.37542638182640076 | Test loss: 0.3758072853088379\n",
      "Epoch: 152860 | Loss: 0.37541463971138 | Test loss: 0.3757934272289276\n",
      "Epoch: 152870 | Loss: 0.37540286779403687 | Test loss: 0.3757795989513397\n",
      "Epoch: 152880 | Loss: 0.3753911256790161 | Test loss: 0.37576574087142944\n",
      "Epoch: 152890 | Loss: 0.3753793239593506 | Test loss: 0.37575188279151917\n",
      "Epoch: 152900 | Loss: 0.37536758184432983 | Test loss: 0.3757380545139313\n",
      "Epoch: 152910 | Loss: 0.3753558099269867 | Test loss: 0.375724196434021\n",
      "Epoch: 152920 | Loss: 0.37534409761428833 | Test loss: 0.3757103383541107\n",
      "Epoch: 152930 | Loss: 0.3753323256969452 | Test loss: 0.3756965100765228\n",
      "Epoch: 152940 | Loss: 0.37532052397727966 | Test loss: 0.37568268179893494\n",
      "Epoch: 152950 | Loss: 0.3753087818622589 | Test loss: 0.37566882371902466\n",
      "Epoch: 152960 | Loss: 0.37529703974723816 | Test loss: 0.37565499544143677\n",
      "Epoch: 152970 | Loss: 0.375285267829895 | Test loss: 0.3756411373615265\n",
      "Epoch: 152980 | Loss: 0.37527352571487427 | Test loss: 0.3756272792816162\n",
      "Epoch: 152990 | Loss: 0.37526175379753113 | Test loss: 0.3756134510040283\n",
      "Epoch: 153000 | Loss: 0.375249981880188 | Test loss: 0.37559959292411804\n",
      "Epoch: 153010 | Loss: 0.37523823976516724 | Test loss: 0.37558576464653015\n",
      "Epoch: 153020 | Loss: 0.3752264678478241 | Test loss: 0.3755718767642975\n",
      "Epoch: 153030 | Loss: 0.37521472573280334 | Test loss: 0.3755580484867096\n",
      "Epoch: 153040 | Loss: 0.3752029538154602 | Test loss: 0.3755441904067993\n",
      "Epoch: 153050 | Loss: 0.37519118189811707 | Test loss: 0.37553033232688904\n",
      "Epoch: 153060 | Loss: 0.3751794099807739 | Test loss: 0.37551650404930115\n",
      "Epoch: 153070 | Loss: 0.3751676678657532 | Test loss: 0.37550267577171326\n",
      "Epoch: 153080 | Loss: 0.37515589594841003 | Test loss: 0.375488817691803\n",
      "Epoch: 153090 | Loss: 0.3751441538333893 | Test loss: 0.3754749894142151\n",
      "Epoch: 153100 | Loss: 0.37513241171836853 | Test loss: 0.3754611015319824\n",
      "Epoch: 153110 | Loss: 0.375120609998703 | Test loss: 0.37544727325439453\n",
      "Epoch: 153120 | Loss: 0.37510886788368225 | Test loss: 0.37543344497680664\n",
      "Epoch: 153130 | Loss: 0.3750970959663391 | Test loss: 0.37541958689689636\n",
      "Epoch: 153140 | Loss: 0.37508535385131836 | Test loss: 0.37540575861930847\n",
      "Epoch: 153150 | Loss: 0.3750736117362976 | Test loss: 0.3753919303417206\n",
      "Epoch: 153160 | Loss: 0.37506183981895447 | Test loss: 0.3753780424594879\n",
      "Epoch: 153170 | Loss: 0.37505006790161133 | Test loss: 0.3753642141819\n",
      "Epoch: 153180 | Loss: 0.3750383257865906 | Test loss: 0.37535035610198975\n",
      "Epoch: 153190 | Loss: 0.37502652406692505 | Test loss: 0.37533649802207947\n",
      "Epoch: 153200 | Loss: 0.3750147819519043 | Test loss: 0.3753226697444916\n",
      "Epoch: 153210 | Loss: 0.37500306963920593 | Test loss: 0.3753088414669037\n",
      "Epoch: 153220 | Loss: 0.3749912679195404 | Test loss: 0.3752949833869934\n",
      "Epoch: 153230 | Loss: 0.37497949600219727 | Test loss: 0.37528112530708313\n",
      "Epoch: 153240 | Loss: 0.3749677836894989 | Test loss: 0.37526726722717285\n",
      "Epoch: 153250 | Loss: 0.3749559819698334 | Test loss: 0.37525343894958496\n",
      "Epoch: 153260 | Loss: 0.3749442398548126 | Test loss: 0.3752395808696747\n",
      "Epoch: 153270 | Loss: 0.3749324679374695 | Test loss: 0.3752257525920868\n",
      "Epoch: 153280 | Loss: 0.37492072582244873 | Test loss: 0.3752118945121765\n",
      "Epoch: 153290 | Loss: 0.3749089241027832 | Test loss: 0.37519803643226624\n",
      "Epoch: 153300 | Loss: 0.37489718198776245 | Test loss: 0.37518420815467834\n",
      "Epoch: 153310 | Loss: 0.3748854100704193 | Test loss: 0.37517035007476807\n",
      "Epoch: 153320 | Loss: 0.37487369775772095 | Test loss: 0.3751564919948578\n",
      "Epoch: 153330 | Loss: 0.3748619258403778 | Test loss: 0.3751426637172699\n",
      "Epoch: 153340 | Loss: 0.3748501241207123 | Test loss: 0.375128835439682\n",
      "Epoch: 153350 | Loss: 0.37483838200569153 | Test loss: 0.37511497735977173\n",
      "Epoch: 153360 | Loss: 0.3748266398906708 | Test loss: 0.37510114908218384\n",
      "Epoch: 153370 | Loss: 0.37481486797332764 | Test loss: 0.37508729100227356\n",
      "Epoch: 153380 | Loss: 0.3748031258583069 | Test loss: 0.3750734329223633\n",
      "Epoch: 153390 | Loss: 0.37479135394096375 | Test loss: 0.3750596046447754\n",
      "Epoch: 153400 | Loss: 0.3747795820236206 | Test loss: 0.3750457465648651\n",
      "Epoch: 153410 | Loss: 0.37476783990859985 | Test loss: 0.3750319182872772\n",
      "Epoch: 153420 | Loss: 0.3747560679912567 | Test loss: 0.37501803040504456\n",
      "Epoch: 153430 | Loss: 0.37474432587623596 | Test loss: 0.37500420212745667\n",
      "Epoch: 153440 | Loss: 0.3747325539588928 | Test loss: 0.3749903440475464\n",
      "Epoch: 153450 | Loss: 0.3747207820415497 | Test loss: 0.3749764859676361\n",
      "Epoch: 153460 | Loss: 0.37470901012420654 | Test loss: 0.3749626576900482\n",
      "Epoch: 153470 | Loss: 0.3746972680091858 | Test loss: 0.3749488294124603\n",
      "Epoch: 153480 | Loss: 0.37468549609184265 | Test loss: 0.37493497133255005\n",
      "Epoch: 153490 | Loss: 0.3746737539768219 | Test loss: 0.37492114305496216\n",
      "Epoch: 153500 | Loss: 0.37466201186180115 | Test loss: 0.3749072551727295\n",
      "Epoch: 153510 | Loss: 0.3746502101421356 | Test loss: 0.3748934268951416\n",
      "Epoch: 153520 | Loss: 0.37463846802711487 | Test loss: 0.3748795986175537\n",
      "Epoch: 153530 | Loss: 0.37462669610977173 | Test loss: 0.37486574053764343\n",
      "Epoch: 153540 | Loss: 0.374614953994751 | Test loss: 0.37485191226005554\n",
      "Epoch: 153550 | Loss: 0.3746032118797302 | Test loss: 0.37483808398246765\n",
      "Epoch: 153560 | Loss: 0.3745914399623871 | Test loss: 0.374824196100235\n",
      "Epoch: 153570 | Loss: 0.37457966804504395 | Test loss: 0.3748103678226471\n",
      "Epoch: 153580 | Loss: 0.3745679259300232 | Test loss: 0.3747965097427368\n",
      "Epoch: 153590 | Loss: 0.37455612421035767 | Test loss: 0.37478265166282654\n",
      "Epoch: 153600 | Loss: 0.3745443820953369 | Test loss: 0.37476882338523865\n",
      "Epoch: 153610 | Loss: 0.37453266978263855 | Test loss: 0.37475499510765076\n",
      "Epoch: 153620 | Loss: 0.374520868062973 | Test loss: 0.3747411370277405\n",
      "Epoch: 153630 | Loss: 0.3745090961456299 | Test loss: 0.3747272789478302\n",
      "Epoch: 153640 | Loss: 0.3744973838329315 | Test loss: 0.3747134208679199\n",
      "Epoch: 153650 | Loss: 0.374485582113266 | Test loss: 0.37469959259033203\n",
      "Epoch: 153660 | Loss: 0.37447383999824524 | Test loss: 0.37468573451042175\n",
      "Epoch: 153670 | Loss: 0.3744620680809021 | Test loss: 0.37467190623283386\n",
      "Epoch: 153680 | Loss: 0.37445032596588135 | Test loss: 0.3746580481529236\n",
      "Epoch: 153690 | Loss: 0.3744385242462158 | Test loss: 0.3746441900730133\n",
      "Epoch: 153700 | Loss: 0.37442678213119507 | Test loss: 0.3746303617954254\n",
      "Epoch: 153710 | Loss: 0.37441501021385193 | Test loss: 0.37461650371551514\n",
      "Epoch: 153720 | Loss: 0.37440329790115356 | Test loss: 0.37460264563560486\n",
      "Epoch: 153730 | Loss: 0.3743915259838104 | Test loss: 0.37458881735801697\n",
      "Epoch: 153740 | Loss: 0.3743797242641449 | Test loss: 0.3745749890804291\n",
      "Epoch: 153750 | Loss: 0.37436798214912415 | Test loss: 0.3745611310005188\n",
      "Epoch: 153760 | Loss: 0.3743562400341034 | Test loss: 0.3745473027229309\n",
      "Epoch: 153770 | Loss: 0.37434446811676025 | Test loss: 0.37453344464302063\n",
      "Epoch: 153780 | Loss: 0.3743327260017395 | Test loss: 0.37451958656311035\n",
      "Epoch: 153790 | Loss: 0.37432095408439636 | Test loss: 0.37450575828552246\n",
      "Epoch: 153800 | Loss: 0.3743091821670532 | Test loss: 0.3744919002056122\n",
      "Epoch: 153810 | Loss: 0.37429744005203247 | Test loss: 0.3744780719280243\n",
      "Epoch: 153820 | Loss: 0.37428566813468933 | Test loss: 0.3744641840457916\n",
      "Epoch: 153830 | Loss: 0.3742739260196686 | Test loss: 0.37445035576820374\n",
      "Epoch: 153840 | Loss: 0.37426215410232544 | Test loss: 0.37443649768829346\n",
      "Epoch: 153850 | Loss: 0.3742503821849823 | Test loss: 0.3744226396083832\n",
      "Epoch: 153860 | Loss: 0.37423861026763916 | Test loss: 0.3744088113307953\n",
      "Epoch: 153870 | Loss: 0.3742268681526184 | Test loss: 0.3743949830532074\n",
      "Epoch: 153880 | Loss: 0.37421509623527527 | Test loss: 0.3743811249732971\n",
      "Epoch: 153890 | Loss: 0.3742033541202545 | Test loss: 0.37436729669570923\n",
      "Epoch: 153900 | Loss: 0.37419161200523376 | Test loss: 0.37435340881347656\n",
      "Epoch: 153910 | Loss: 0.37417981028556824 | Test loss: 0.37433958053588867\n",
      "Epoch: 153920 | Loss: 0.3741680681705475 | Test loss: 0.3743257522583008\n",
      "Epoch: 153930 | Loss: 0.37415629625320435 | Test loss: 0.3743118941783905\n",
      "Epoch: 153940 | Loss: 0.3741445541381836 | Test loss: 0.3742980659008026\n",
      "Epoch: 153950 | Loss: 0.37413281202316284 | Test loss: 0.3742842376232147\n",
      "Epoch: 153960 | Loss: 0.3741210401058197 | Test loss: 0.37427034974098206\n",
      "Epoch: 153970 | Loss: 0.37410926818847656 | Test loss: 0.37425652146339417\n",
      "Epoch: 153980 | Loss: 0.3740975260734558 | Test loss: 0.3742426633834839\n",
      "Epoch: 153990 | Loss: 0.3740857243537903 | Test loss: 0.3742288053035736\n",
      "Epoch: 154000 | Loss: 0.37407398223876953 | Test loss: 0.3742149770259857\n",
      "Epoch: 154010 | Loss: 0.37406226992607117 | Test loss: 0.3742011487483978\n",
      "Epoch: 154020 | Loss: 0.37405046820640564 | Test loss: 0.37418729066848755\n",
      "Epoch: 154030 | Loss: 0.3740386962890625 | Test loss: 0.37417343258857727\n",
      "Epoch: 154040 | Loss: 0.37402698397636414 | Test loss: 0.374159574508667\n",
      "Epoch: 154050 | Loss: 0.3740151822566986 | Test loss: 0.3741457462310791\n",
      "Epoch: 154060 | Loss: 0.37400344014167786 | Test loss: 0.3741318881511688\n",
      "Epoch: 154070 | Loss: 0.3739916682243347 | Test loss: 0.37411805987358093\n",
      "Epoch: 154080 | Loss: 0.37397992610931396 | Test loss: 0.37410420179367065\n",
      "Epoch: 154090 | Loss: 0.37396812438964844 | Test loss: 0.3740903437137604\n",
      "Epoch: 154100 | Loss: 0.3739563822746277 | Test loss: 0.3740765154361725\n",
      "Epoch: 154110 | Loss: 0.37394461035728455 | Test loss: 0.3740626573562622\n",
      "Epoch: 154120 | Loss: 0.3739328980445862 | Test loss: 0.37404879927635193\n",
      "Epoch: 154130 | Loss: 0.37392112612724304 | Test loss: 0.37403497099876404\n",
      "Epoch: 154140 | Loss: 0.3739093244075775 | Test loss: 0.37402114272117615\n",
      "Epoch: 154150 | Loss: 0.37389758229255676 | Test loss: 0.37400728464126587\n",
      "Epoch: 154160 | Loss: 0.373885840177536 | Test loss: 0.373993456363678\n",
      "Epoch: 154170 | Loss: 0.37387406826019287 | Test loss: 0.3739795982837677\n",
      "Epoch: 154180 | Loss: 0.3738623261451721 | Test loss: 0.3739657402038574\n",
      "Epoch: 154190 | Loss: 0.373850554227829 | Test loss: 0.37395191192626953\n",
      "Epoch: 154200 | Loss: 0.37383878231048584 | Test loss: 0.37393805384635925\n",
      "Epoch: 154210 | Loss: 0.3738270401954651 | Test loss: 0.37392422556877136\n",
      "Epoch: 154220 | Loss: 0.37381526827812195 | Test loss: 0.3739103376865387\n",
      "Epoch: 154230 | Loss: 0.3738035261631012 | Test loss: 0.3738965094089508\n",
      "Epoch: 154240 | Loss: 0.37379175424575806 | Test loss: 0.3738826513290405\n",
      "Epoch: 154250 | Loss: 0.3737799823284149 | Test loss: 0.37386879324913025\n",
      "Epoch: 154260 | Loss: 0.3737682104110718 | Test loss: 0.37385496497154236\n",
      "Epoch: 154270 | Loss: 0.373756468296051 | Test loss: 0.37384113669395447\n",
      "Epoch: 154280 | Loss: 0.3737446963787079 | Test loss: 0.3738272786140442\n",
      "Epoch: 154290 | Loss: 0.37373295426368713 | Test loss: 0.3738134503364563\n",
      "Epoch: 154300 | Loss: 0.3737212121486664 | Test loss: 0.37379956245422363\n",
      "Epoch: 154310 | Loss: 0.37370941042900085 | Test loss: 0.37378573417663574\n",
      "Epoch: 154320 | Loss: 0.3736976683139801 | Test loss: 0.37377190589904785\n",
      "Epoch: 154330 | Loss: 0.37368589639663696 | Test loss: 0.3737580478191376\n",
      "Epoch: 154340 | Loss: 0.3736741542816162 | Test loss: 0.3737442195415497\n",
      "Epoch: 154350 | Loss: 0.37366241216659546 | Test loss: 0.3737303912639618\n",
      "Epoch: 154360 | Loss: 0.3736506402492523 | Test loss: 0.3737165033817291\n",
      "Epoch: 154370 | Loss: 0.3736388683319092 | Test loss: 0.37370267510414124\n",
      "Epoch: 154380 | Loss: 0.3736271262168884 | Test loss: 0.37368881702423096\n",
      "Epoch: 154390 | Loss: 0.3736153244972229 | Test loss: 0.3736749589443207\n",
      "Epoch: 154400 | Loss: 0.37360358238220215 | Test loss: 0.3736611306667328\n",
      "Epoch: 154410 | Loss: 0.3735918700695038 | Test loss: 0.3736473023891449\n",
      "Epoch: 154420 | Loss: 0.37358006834983826 | Test loss: 0.3736334443092346\n",
      "Epoch: 154430 | Loss: 0.3735682964324951 | Test loss: 0.37361958622932434\n",
      "Epoch: 154440 | Loss: 0.37355658411979675 | Test loss: 0.37360572814941406\n",
      "Epoch: 154450 | Loss: 0.3735447824001312 | Test loss: 0.37359189987182617\n",
      "Epoch: 154460 | Loss: 0.3735330402851105 | Test loss: 0.3735780417919159\n",
      "Epoch: 154470 | Loss: 0.37352126836776733 | Test loss: 0.373564213514328\n",
      "Epoch: 154480 | Loss: 0.3735095262527466 | Test loss: 0.3735503554344177\n",
      "Epoch: 154490 | Loss: 0.37349772453308105 | Test loss: 0.37353649735450745\n",
      "Epoch: 154500 | Loss: 0.3734859824180603 | Test loss: 0.37352266907691956\n",
      "Epoch: 154510 | Loss: 0.37347421050071716 | Test loss: 0.3735088109970093\n",
      "Epoch: 154520 | Loss: 0.3734624981880188 | Test loss: 0.373494952917099\n",
      "Epoch: 154530 | Loss: 0.37345072627067566 | Test loss: 0.3734811246395111\n",
      "Epoch: 154540 | Loss: 0.37343892455101013 | Test loss: 0.3734672963619232\n",
      "Epoch: 154550 | Loss: 0.3734271824359894 | Test loss: 0.37345343828201294\n",
      "Epoch: 154560 | Loss: 0.37341544032096863 | Test loss: 0.37343961000442505\n",
      "Epoch: 154570 | Loss: 0.3734036684036255 | Test loss: 0.37342575192451477\n",
      "Epoch: 154580 | Loss: 0.37339192628860474 | Test loss: 0.3734118938446045\n",
      "Epoch: 154590 | Loss: 0.3733801543712616 | Test loss: 0.3733980655670166\n",
      "Epoch: 154600 | Loss: 0.37336838245391846 | Test loss: 0.3733842074871063\n",
      "Epoch: 154610 | Loss: 0.3733566403388977 | Test loss: 0.37337037920951843\n",
      "Epoch: 154620 | Loss: 0.37334486842155457 | Test loss: 0.37335649132728577\n",
      "Epoch: 154630 | Loss: 0.3733331263065338 | Test loss: 0.3733426630496979\n",
      "Epoch: 154640 | Loss: 0.3733213543891907 | Test loss: 0.3733288049697876\n",
      "Epoch: 154650 | Loss: 0.37330958247184753 | Test loss: 0.3733149468898773\n",
      "Epoch: 154660 | Loss: 0.3732978105545044 | Test loss: 0.37330111861228943\n",
      "Epoch: 154670 | Loss: 0.37328606843948364 | Test loss: 0.37328729033470154\n",
      "Epoch: 154680 | Loss: 0.3732742965221405 | Test loss: 0.37327343225479126\n",
      "Epoch: 154690 | Loss: 0.37326255440711975 | Test loss: 0.37325960397720337\n",
      "Epoch: 154700 | Loss: 0.373250812292099 | Test loss: 0.3732457160949707\n",
      "Epoch: 154710 | Loss: 0.37323901057243347 | Test loss: 0.3732318878173828\n",
      "Epoch: 154720 | Loss: 0.3732272684574127 | Test loss: 0.3732180595397949\n",
      "Epoch: 154730 | Loss: 0.3732154965400696 | Test loss: 0.37320420145988464\n",
      "Epoch: 154740 | Loss: 0.37320375442504883 | Test loss: 0.37319037318229675\n",
      "Epoch: 154750 | Loss: 0.3731920123100281 | Test loss: 0.37317654490470886\n",
      "Epoch: 154760 | Loss: 0.37318024039268494 | Test loss: 0.3731626570224762\n",
      "Epoch: 154770 | Loss: 0.3731684684753418 | Test loss: 0.3731488287448883\n",
      "Epoch: 154780 | Loss: 0.37315672636032104 | Test loss: 0.373134970664978\n",
      "Epoch: 154790 | Loss: 0.3731449246406555 | Test loss: 0.37312111258506775\n",
      "Epoch: 154800 | Loss: 0.37313318252563477 | Test loss: 0.37310728430747986\n",
      "Epoch: 154810 | Loss: 0.3731214702129364 | Test loss: 0.37309345602989197\n",
      "Epoch: 154820 | Loss: 0.3731096684932709 | Test loss: 0.3730795979499817\n",
      "Epoch: 154830 | Loss: 0.37309789657592773 | Test loss: 0.3730657398700714\n",
      "Epoch: 154840 | Loss: 0.37308618426322937 | Test loss: 0.37305188179016113\n",
      "Epoch: 154850 | Loss: 0.37307438254356384 | Test loss: 0.37303805351257324\n",
      "Epoch: 154860 | Loss: 0.3730626404285431 | Test loss: 0.37302419543266296\n",
      "Epoch: 154870 | Loss: 0.37305086851119995 | Test loss: 0.3730103671550751\n",
      "Epoch: 154880 | Loss: 0.3730391263961792 | Test loss: 0.3729965090751648\n",
      "Epoch: 154890 | Loss: 0.37302732467651367 | Test loss: 0.3729826509952545\n",
      "Epoch: 154900 | Loss: 0.3730155825614929 | Test loss: 0.3729688227176666\n",
      "Epoch: 154910 | Loss: 0.3730038106441498 | Test loss: 0.37295496463775635\n",
      "Epoch: 154920 | Loss: 0.3729920983314514 | Test loss: 0.37294110655784607\n",
      "Epoch: 154930 | Loss: 0.3729803264141083 | Test loss: 0.3729272782802582\n",
      "Epoch: 154940 | Loss: 0.37296852469444275 | Test loss: 0.3729134500026703\n",
      "Epoch: 154950 | Loss: 0.372956782579422 | Test loss: 0.37289959192276\n",
      "Epoch: 154960 | Loss: 0.37294504046440125 | Test loss: 0.3728857636451721\n",
      "Epoch: 154970 | Loss: 0.3729332685470581 | Test loss: 0.37287190556526184\n",
      "Epoch: 154980 | Loss: 0.37292152643203735 | Test loss: 0.37285804748535156\n",
      "Epoch: 154990 | Loss: 0.3729097545146942 | Test loss: 0.37284421920776367\n",
      "Epoch: 155000 | Loss: 0.3728979825973511 | Test loss: 0.3728303611278534\n",
      "Epoch: 155010 | Loss: 0.3728862404823303 | Test loss: 0.3728165328502655\n",
      "Epoch: 155020 | Loss: 0.3728744685649872 | Test loss: 0.37280264496803284\n",
      "Epoch: 155030 | Loss: 0.37286272644996643 | Test loss: 0.37278881669044495\n",
      "Epoch: 155040 | Loss: 0.3728509545326233 | Test loss: 0.37277495861053467\n",
      "Epoch: 155050 | Loss: 0.37283918261528015 | Test loss: 0.3727611005306244\n",
      "Epoch: 155060 | Loss: 0.372827410697937 | Test loss: 0.3727472722530365\n",
      "Epoch: 155070 | Loss: 0.37281566858291626 | Test loss: 0.3727334439754486\n",
      "Epoch: 155080 | Loss: 0.3728038966655731 | Test loss: 0.37271958589553833\n",
      "Epoch: 155090 | Loss: 0.37279215455055237 | Test loss: 0.37270575761795044\n",
      "Epoch: 155100 | Loss: 0.3727804124355316 | Test loss: 0.3726918697357178\n",
      "Epoch: 155110 | Loss: 0.3727686107158661 | Test loss: 0.3726780414581299\n",
      "Epoch: 155120 | Loss: 0.37275686860084534 | Test loss: 0.372664213180542\n",
      "Epoch: 155130 | Loss: 0.3727450966835022 | Test loss: 0.3726503551006317\n",
      "Epoch: 155140 | Loss: 0.37273335456848145 | Test loss: 0.3726365268230438\n",
      "Epoch: 155150 | Loss: 0.3727216124534607 | Test loss: 0.37262269854545593\n",
      "Epoch: 155160 | Loss: 0.37270984053611755 | Test loss: 0.37260881066322327\n",
      "Epoch: 155170 | Loss: 0.3726980686187744 | Test loss: 0.3725949823856354\n",
      "Epoch: 155180 | Loss: 0.37268632650375366 | Test loss: 0.3725811243057251\n",
      "Epoch: 155190 | Loss: 0.37267452478408813 | Test loss: 0.3725672662258148\n",
      "Epoch: 155200 | Loss: 0.3726627826690674 | Test loss: 0.37255343794822693\n",
      "Epoch: 155210 | Loss: 0.372651070356369 | Test loss: 0.37253960967063904\n",
      "Epoch: 155220 | Loss: 0.3726392686367035 | Test loss: 0.37252575159072876\n",
      "Epoch: 155230 | Loss: 0.37262749671936035 | Test loss: 0.3725118935108185\n",
      "Epoch: 155240 | Loss: 0.372615784406662 | Test loss: 0.3724980354309082\n",
      "Epoch: 155250 | Loss: 0.37260398268699646 | Test loss: 0.3724842071533203\n",
      "Epoch: 155260 | Loss: 0.3725922405719757 | Test loss: 0.37247034907341003\n",
      "Epoch: 155270 | Loss: 0.37258046865463257 | Test loss: 0.37245652079582214\n",
      "Epoch: 155280 | Loss: 0.3725687265396118 | Test loss: 0.37244266271591187\n",
      "Epoch: 155290 | Loss: 0.3725569248199463 | Test loss: 0.3724288046360016\n",
      "Epoch: 155300 | Loss: 0.37254518270492554 | Test loss: 0.3724149763584137\n",
      "Epoch: 155310 | Loss: 0.3725334107875824 | Test loss: 0.3724011182785034\n",
      "Epoch: 155320 | Loss: 0.37252169847488403 | Test loss: 0.37238726019859314\n",
      "Epoch: 155330 | Loss: 0.3725099265575409 | Test loss: 0.37237343192100525\n",
      "Epoch: 155340 | Loss: 0.37249812483787537 | Test loss: 0.37235960364341736\n",
      "Epoch: 155350 | Loss: 0.3724863827228546 | Test loss: 0.3723457455635071\n",
      "Epoch: 155360 | Loss: 0.37247464060783386 | Test loss: 0.3723319172859192\n",
      "Epoch: 155370 | Loss: 0.3724628686904907 | Test loss: 0.3723180592060089\n",
      "Epoch: 155380 | Loss: 0.37245112657546997 | Test loss: 0.37230420112609863\n",
      "Epoch: 155390 | Loss: 0.37243935465812683 | Test loss: 0.37229037284851074\n",
      "Epoch: 155400 | Loss: 0.3724275827407837 | Test loss: 0.37227651476860046\n",
      "Epoch: 155410 | Loss: 0.37241584062576294 | Test loss: 0.3722626864910126\n",
      "Epoch: 155420 | Loss: 0.3724040687084198 | Test loss: 0.3722487986087799\n",
      "Epoch: 155430 | Loss: 0.37239232659339905 | Test loss: 0.372234970331192\n",
      "Epoch: 155440 | Loss: 0.3723805546760559 | Test loss: 0.37222111225128174\n",
      "Epoch: 155450 | Loss: 0.37236878275871277 | Test loss: 0.37220725417137146\n",
      "Epoch: 155460 | Loss: 0.37235701084136963 | Test loss: 0.37219342589378357\n",
      "Epoch: 155470 | Loss: 0.3723452687263489 | Test loss: 0.3721795976161957\n",
      "Epoch: 155480 | Loss: 0.37233349680900574 | Test loss: 0.3721657395362854\n",
      "Epoch: 155490 | Loss: 0.372321754693985 | Test loss: 0.3721519112586975\n",
      "Epoch: 155500 | Loss: 0.37231001257896423 | Test loss: 0.37213802337646484\n",
      "Epoch: 155510 | Loss: 0.3722982108592987 | Test loss: 0.37212419509887695\n",
      "Epoch: 155520 | Loss: 0.37228646874427795 | Test loss: 0.37211036682128906\n",
      "Epoch: 155530 | Loss: 0.3722746968269348 | Test loss: 0.3720965087413788\n",
      "Epoch: 155540 | Loss: 0.37226295471191406 | Test loss: 0.3720826804637909\n",
      "Epoch: 155550 | Loss: 0.3722512125968933 | Test loss: 0.372068852186203\n",
      "Epoch: 155560 | Loss: 0.37223944067955017 | Test loss: 0.37205496430397034\n",
      "Epoch: 155570 | Loss: 0.37222766876220703 | Test loss: 0.37204113602638245\n",
      "Epoch: 155580 | Loss: 0.3722159266471863 | Test loss: 0.37202727794647217\n",
      "Epoch: 155590 | Loss: 0.37220412492752075 | Test loss: 0.3720134198665619\n",
      "Epoch: 155600 | Loss: 0.3721923828125 | Test loss: 0.371999591588974\n",
      "Epoch: 155610 | Loss: 0.37218067049980164 | Test loss: 0.3719857633113861\n",
      "Epoch: 155620 | Loss: 0.3721688687801361 | Test loss: 0.37197190523147583\n",
      "Epoch: 155630 | Loss: 0.37215709686279297 | Test loss: 0.37195804715156555\n",
      "Epoch: 155640 | Loss: 0.3721453845500946 | Test loss: 0.3719441890716553\n",
      "Epoch: 155650 | Loss: 0.3721335828304291 | Test loss: 0.3719303607940674\n",
      "Epoch: 155660 | Loss: 0.3721218407154083 | Test loss: 0.3719165027141571\n",
      "Epoch: 155670 | Loss: 0.3721100687980652 | Test loss: 0.3719026744365692\n",
      "Epoch: 155680 | Loss: 0.37209832668304443 | Test loss: 0.37188881635665894\n",
      "Epoch: 155690 | Loss: 0.3720865249633789 | Test loss: 0.37187495827674866\n",
      "Epoch: 155700 | Loss: 0.37207478284835815 | Test loss: 0.37186112999916077\n",
      "Epoch: 155710 | Loss: 0.372063010931015 | Test loss: 0.3718472719192505\n",
      "Epoch: 155720 | Loss: 0.37205129861831665 | Test loss: 0.3718334138393402\n",
      "Epoch: 155730 | Loss: 0.3720395267009735 | Test loss: 0.3718195855617523\n",
      "Epoch: 155740 | Loss: 0.372027724981308 | Test loss: 0.37180575728416443\n",
      "Epoch: 155750 | Loss: 0.37201598286628723 | Test loss: 0.37179189920425415\n",
      "Epoch: 155760 | Loss: 0.3720042407512665 | Test loss: 0.37177807092666626\n",
      "Epoch: 155770 | Loss: 0.37199246883392334 | Test loss: 0.371764212846756\n",
      "Epoch: 155780 | Loss: 0.3719807267189026 | Test loss: 0.3717503547668457\n",
      "Epoch: 155790 | Loss: 0.37196895480155945 | Test loss: 0.3717365264892578\n",
      "Epoch: 155800 | Loss: 0.3719571828842163 | Test loss: 0.37172266840934753\n",
      "Epoch: 155810 | Loss: 0.37194544076919556 | Test loss: 0.37170884013175964\n",
      "Epoch: 155820 | Loss: 0.3719336688518524 | Test loss: 0.371694952249527\n",
      "Epoch: 155830 | Loss: 0.37192192673683167 | Test loss: 0.3716811239719391\n",
      "Epoch: 155840 | Loss: 0.3719101548194885 | Test loss: 0.3716672658920288\n",
      "Epoch: 155850 | Loss: 0.3718983829021454 | Test loss: 0.37165340781211853\n",
      "Epoch: 155860 | Loss: 0.37188661098480225 | Test loss: 0.37163957953453064\n",
      "Epoch: 155870 | Loss: 0.3718748688697815 | Test loss: 0.37162575125694275\n",
      "Epoch: 155880 | Loss: 0.37186309695243835 | Test loss: 0.37161189317703247\n",
      "Epoch: 155890 | Loss: 0.3718513548374176 | Test loss: 0.3715980648994446\n",
      "Epoch: 155900 | Loss: 0.37183961272239685 | Test loss: 0.3715841770172119\n",
      "Epoch: 155910 | Loss: 0.3718278110027313 | Test loss: 0.371570348739624\n",
      "Epoch: 155920 | Loss: 0.37181606888771057 | Test loss: 0.37155652046203613\n",
      "Epoch: 155930 | Loss: 0.37180429697036743 | Test loss: 0.37154266238212585\n",
      "Epoch: 155940 | Loss: 0.3717925548553467 | Test loss: 0.37152883410453796\n",
      "Epoch: 155950 | Loss: 0.3717808127403259 | Test loss: 0.3715150058269501\n",
      "Epoch: 155960 | Loss: 0.3717690408229828 | Test loss: 0.3715011179447174\n",
      "Epoch: 155970 | Loss: 0.37175726890563965 | Test loss: 0.3714872896671295\n",
      "Epoch: 155980 | Loss: 0.3717455267906189 | Test loss: 0.37147343158721924\n",
      "Epoch: 155990 | Loss: 0.37173372507095337 | Test loss: 0.37145957350730896\n",
      "Epoch: 156000 | Loss: 0.3717219829559326 | Test loss: 0.37144574522972107\n",
      "Epoch: 156010 | Loss: 0.37171027064323425 | Test loss: 0.3714319169521332\n",
      "Epoch: 156020 | Loss: 0.3716984689235687 | Test loss: 0.3714180588722229\n",
      "Epoch: 156030 | Loss: 0.3716866970062256 | Test loss: 0.3714042007923126\n",
      "Epoch: 156040 | Loss: 0.3716749846935272 | Test loss: 0.37139034271240234\n",
      "Epoch: 156050 | Loss: 0.3716631829738617 | Test loss: 0.37137651443481445\n",
      "Epoch: 156060 | Loss: 0.37165144085884094 | Test loss: 0.3713626563549042\n",
      "Epoch: 156070 | Loss: 0.3716396689414978 | Test loss: 0.3713488280773163\n",
      "Epoch: 156080 | Loss: 0.37162792682647705 | Test loss: 0.371334969997406\n",
      "Epoch: 156090 | Loss: 0.3716161251068115 | Test loss: 0.3713211119174957\n",
      "Epoch: 156100 | Loss: 0.37160438299179077 | Test loss: 0.37130728363990784\n",
      "Epoch: 156110 | Loss: 0.37159261107444763 | Test loss: 0.37129342555999756\n",
      "Epoch: 156120 | Loss: 0.37158089876174927 | Test loss: 0.3712795674800873\n",
      "Epoch: 156130 | Loss: 0.37156912684440613 | Test loss: 0.3712657392024994\n",
      "Epoch: 156140 | Loss: 0.3715573251247406 | Test loss: 0.3712519109249115\n",
      "Epoch: 156150 | Loss: 0.37154558300971985 | Test loss: 0.3712380528450012\n",
      "Epoch: 156160 | Loss: 0.3715338408946991 | Test loss: 0.37122422456741333\n",
      "Epoch: 156170 | Loss: 0.37152206897735596 | Test loss: 0.37121036648750305\n",
      "Epoch: 156180 | Loss: 0.3715103268623352 | Test loss: 0.3711965084075928\n",
      "Epoch: 156190 | Loss: 0.37149855494499207 | Test loss: 0.3711826801300049\n",
      "Epoch: 156200 | Loss: 0.3714867830276489 | Test loss: 0.3711688220500946\n",
      "Epoch: 156210 | Loss: 0.3714750409126282 | Test loss: 0.3711549937725067\n",
      "Epoch: 156220 | Loss: 0.37146326899528503 | Test loss: 0.37114110589027405\n",
      "Epoch: 156230 | Loss: 0.3714515268802643 | Test loss: 0.37112727761268616\n",
      "Epoch: 156240 | Loss: 0.37143975496292114 | Test loss: 0.3711134195327759\n",
      "Epoch: 156250 | Loss: 0.371427983045578 | Test loss: 0.3710995614528656\n",
      "Epoch: 156260 | Loss: 0.37141621112823486 | Test loss: 0.3710857331752777\n",
      "Epoch: 156270 | Loss: 0.3714044690132141 | Test loss: 0.3710719048976898\n",
      "Epoch: 156280 | Loss: 0.37139269709587097 | Test loss: 0.37105804681777954\n",
      "Epoch: 156290 | Loss: 0.3713809549808502 | Test loss: 0.37104421854019165\n",
      "Epoch: 156300 | Loss: 0.37136921286582947 | Test loss: 0.371030330657959\n",
      "Epoch: 156310 | Loss: 0.37135741114616394 | Test loss: 0.3710165023803711\n",
      "Epoch: 156320 | Loss: 0.3713456690311432 | Test loss: 0.3710026741027832\n",
      "Epoch: 156330 | Loss: 0.37133389711380005 | Test loss: 0.3709888160228729\n",
      "Epoch: 156340 | Loss: 0.3713221549987793 | Test loss: 0.37097498774528503\n",
      "Epoch: 156350 | Loss: 0.37131041288375854 | Test loss: 0.37096115946769714\n",
      "Epoch: 156360 | Loss: 0.3712986409664154 | Test loss: 0.3709472715854645\n",
      "Epoch: 156370 | Loss: 0.37128686904907227 | Test loss: 0.3709334433078766\n",
      "Epoch: 156380 | Loss: 0.3712751269340515 | Test loss: 0.3709195852279663\n",
      "Epoch: 156390 | Loss: 0.371263325214386 | Test loss: 0.37090572714805603\n",
      "Epoch: 156400 | Loss: 0.37125158309936523 | Test loss: 0.37089189887046814\n",
      "Epoch: 156410 | Loss: 0.37123987078666687 | Test loss: 0.37087807059288025\n",
      "Epoch: 156420 | Loss: 0.37122806906700134 | Test loss: 0.37086421251296997\n",
      "Epoch: 156430 | Loss: 0.3712162971496582 | Test loss: 0.3708503544330597\n",
      "Epoch: 156440 | Loss: 0.37120458483695984 | Test loss: 0.3708364963531494\n",
      "Epoch: 156450 | Loss: 0.3711927831172943 | Test loss: 0.3708226680755615\n",
      "Epoch: 156460 | Loss: 0.37118104100227356 | Test loss: 0.37080880999565125\n",
      "Epoch: 156470 | Loss: 0.3711692690849304 | Test loss: 0.37079498171806335\n",
      "Epoch: 156480 | Loss: 0.37115752696990967 | Test loss: 0.3707811236381531\n",
      "Epoch: 156490 | Loss: 0.37114572525024414 | Test loss: 0.3707672655582428\n",
      "Epoch: 156500 | Loss: 0.3711339831352234 | Test loss: 0.3707534372806549\n",
      "Epoch: 156510 | Loss: 0.37112221121788025 | Test loss: 0.37073957920074463\n",
      "Epoch: 156520 | Loss: 0.3711104989051819 | Test loss: 0.37072572112083435\n",
      "Epoch: 156530 | Loss: 0.37109872698783875 | Test loss: 0.37071189284324646\n",
      "Epoch: 156540 | Loss: 0.3710869252681732 | Test loss: 0.37069806456565857\n",
      "Epoch: 156550 | Loss: 0.37107518315315247 | Test loss: 0.3706842064857483\n",
      "Epoch: 156560 | Loss: 0.3710634410381317 | Test loss: 0.3706703782081604\n",
      "Epoch: 156570 | Loss: 0.3710516691207886 | Test loss: 0.3706565201282501\n",
      "Epoch: 156580 | Loss: 0.3710399270057678 | Test loss: 0.37064266204833984\n",
      "Epoch: 156590 | Loss: 0.3710281550884247 | Test loss: 0.37062883377075195\n",
      "Epoch: 156600 | Loss: 0.37101638317108154 | Test loss: 0.3706149756908417\n",
      "Epoch: 156610 | Loss: 0.3710046410560608 | Test loss: 0.3706011474132538\n",
      "Epoch: 156620 | Loss: 0.37099286913871765 | Test loss: 0.3705872595310211\n",
      "Epoch: 156630 | Loss: 0.3709811270236969 | Test loss: 0.3705734312534332\n",
      "Epoch: 156640 | Loss: 0.37096935510635376 | Test loss: 0.37055957317352295\n",
      "Epoch: 156650 | Loss: 0.3709575831890106 | Test loss: 0.37054571509361267\n",
      "Epoch: 156660 | Loss: 0.3709458112716675 | Test loss: 0.3705318868160248\n",
      "Epoch: 156670 | Loss: 0.37093406915664673 | Test loss: 0.3705180585384369\n",
      "Epoch: 156680 | Loss: 0.3709222972393036 | Test loss: 0.3705042004585266\n",
      "Epoch: 156690 | Loss: 0.37091055512428284 | Test loss: 0.3704903721809387\n",
      "Epoch: 156700 | Loss: 0.3708988130092621 | Test loss: 0.37047648429870605\n",
      "Epoch: 156710 | Loss: 0.37088701128959656 | Test loss: 0.37046265602111816\n",
      "Epoch: 156720 | Loss: 0.3708752691745758 | Test loss: 0.3704488277435303\n",
      "Epoch: 156730 | Loss: 0.37086349725723267 | Test loss: 0.37043496966362\n",
      "Epoch: 156740 | Loss: 0.3708517551422119 | Test loss: 0.3704211413860321\n",
      "Epoch: 156750 | Loss: 0.37084001302719116 | Test loss: 0.3704073131084442\n",
      "Epoch: 156760 | Loss: 0.370828241109848 | Test loss: 0.37039342522621155\n",
      "Epoch: 156770 | Loss: 0.3708164691925049 | Test loss: 0.37037959694862366\n",
      "Epoch: 156780 | Loss: 0.37080472707748413 | Test loss: 0.3703657388687134\n",
      "Epoch: 156790 | Loss: 0.3707929253578186 | Test loss: 0.3703518807888031\n",
      "Epoch: 156800 | Loss: 0.37078118324279785 | Test loss: 0.3703380525112152\n",
      "Epoch: 156810 | Loss: 0.3707694709300995 | Test loss: 0.3703242242336273\n",
      "Epoch: 156820 | Loss: 0.37075766921043396 | Test loss: 0.37031036615371704\n",
      "Epoch: 156830 | Loss: 0.3707458972930908 | Test loss: 0.37029650807380676\n",
      "Epoch: 156840 | Loss: 0.37073418498039246 | Test loss: 0.3702826499938965\n",
      "Epoch: 156850 | Loss: 0.37072238326072693 | Test loss: 0.3702688217163086\n",
      "Epoch: 156860 | Loss: 0.3707106411457062 | Test loss: 0.3702549636363983\n",
      "Epoch: 156870 | Loss: 0.37069886922836304 | Test loss: 0.3702411353588104\n",
      "Epoch: 156880 | Loss: 0.3706871271133423 | Test loss: 0.37022727727890015\n",
      "Epoch: 156890 | Loss: 0.37067532539367676 | Test loss: 0.37021341919898987\n",
      "Epoch: 156900 | Loss: 0.370663583278656 | Test loss: 0.370199590921402\n",
      "Epoch: 156910 | Loss: 0.37065181136131287 | Test loss: 0.3701857328414917\n",
      "Epoch: 156920 | Loss: 0.3706400990486145 | Test loss: 0.3701718747615814\n",
      "Epoch: 156930 | Loss: 0.37062832713127136 | Test loss: 0.37015804648399353\n",
      "Epoch: 156940 | Loss: 0.37061652541160583 | Test loss: 0.37014421820640564\n",
      "Epoch: 156950 | Loss: 0.3706047832965851 | Test loss: 0.37013036012649536\n",
      "Epoch: 156960 | Loss: 0.37059304118156433 | Test loss: 0.37011653184890747\n",
      "Epoch: 156970 | Loss: 0.3705812692642212 | Test loss: 0.3701026737689972\n",
      "Epoch: 156980 | Loss: 0.37056952714920044 | Test loss: 0.3700888156890869\n",
      "Epoch: 156990 | Loss: 0.3705577552318573 | Test loss: 0.370074987411499\n",
      "Epoch: 157000 | Loss: 0.37054598331451416 | Test loss: 0.37006112933158875\n",
      "Epoch: 157010 | Loss: 0.3705342411994934 | Test loss: 0.37004730105400085\n",
      "Epoch: 157020 | Loss: 0.37052246928215027 | Test loss: 0.3700334131717682\n",
      "Epoch: 157030 | Loss: 0.3705107271671295 | Test loss: 0.3700195848941803\n",
      "Epoch: 157040 | Loss: 0.3704989552497864 | Test loss: 0.37000572681427\n",
      "Epoch: 157050 | Loss: 0.37048718333244324 | Test loss: 0.36999186873435974\n",
      "Epoch: 157060 | Loss: 0.3704754114151001 | Test loss: 0.36997804045677185\n",
      "Epoch: 157070 | Loss: 0.37046366930007935 | Test loss: 0.36996421217918396\n",
      "Epoch: 157080 | Loss: 0.3704518973827362 | Test loss: 0.3699503540992737\n",
      "Epoch: 157090 | Loss: 0.37044015526771545 | Test loss: 0.3699365258216858\n",
      "Epoch: 157100 | Loss: 0.3704284131526947 | Test loss: 0.3699226379394531\n",
      "Epoch: 157110 | Loss: 0.3704166114330292 | Test loss: 0.36990880966186523\n",
      "Epoch: 157120 | Loss: 0.3704048693180084 | Test loss: 0.36989498138427734\n",
      "Epoch: 157130 | Loss: 0.3703930974006653 | Test loss: 0.36988112330436707\n",
      "Epoch: 157140 | Loss: 0.37038135528564453 | Test loss: 0.3698672950267792\n",
      "Epoch: 157150 | Loss: 0.3703696131706238 | Test loss: 0.3698534667491913\n",
      "Epoch: 157160 | Loss: 0.37035784125328064 | Test loss: 0.3698395788669586\n",
      "Epoch: 157170 | Loss: 0.3703460693359375 | Test loss: 0.3698257505893707\n",
      "Epoch: 157180 | Loss: 0.37033432722091675 | Test loss: 0.36981189250946045\n",
      "Epoch: 157190 | Loss: 0.3703225255012512 | Test loss: 0.36979803442955017\n",
      "Epoch: 157200 | Loss: 0.37031078338623047 | Test loss: 0.3697842061519623\n",
      "Epoch: 157210 | Loss: 0.3702990710735321 | Test loss: 0.3697703778743744\n",
      "Epoch: 157220 | Loss: 0.3702872693538666 | Test loss: 0.3697565197944641\n",
      "Epoch: 157230 | Loss: 0.37027549743652344 | Test loss: 0.36974266171455383\n",
      "Epoch: 157240 | Loss: 0.3702637851238251 | Test loss: 0.36972880363464355\n",
      "Epoch: 157250 | Loss: 0.37025198340415955 | Test loss: 0.36971497535705566\n",
      "Epoch: 157260 | Loss: 0.3702402412891388 | Test loss: 0.3697011172771454\n",
      "Epoch: 157270 | Loss: 0.37022846937179565 | Test loss: 0.3696872889995575\n",
      "Epoch: 157280 | Loss: 0.3702167272567749 | Test loss: 0.3696734309196472\n",
      "Epoch: 157290 | Loss: 0.3702049255371094 | Test loss: 0.36965957283973694\n",
      "Epoch: 157300 | Loss: 0.3701931834220886 | Test loss: 0.36964574456214905\n",
      "Epoch: 157310 | Loss: 0.3701814115047455 | Test loss: 0.36963188648223877\n",
      "Epoch: 157320 | Loss: 0.3701696991920471 | Test loss: 0.3696180284023285\n",
      "Epoch: 157330 | Loss: 0.370157927274704 | Test loss: 0.3696042001247406\n",
      "Epoch: 157340 | Loss: 0.37014612555503845 | Test loss: 0.3695903718471527\n",
      "Epoch: 157350 | Loss: 0.3701343834400177 | Test loss: 0.36957651376724243\n",
      "Epoch: 157360 | Loss: 0.37012264132499695 | Test loss: 0.36956268548965454\n",
      "Epoch: 157370 | Loss: 0.3701108694076538 | Test loss: 0.36954882740974426\n",
      "Epoch: 157380 | Loss: 0.37009912729263306 | Test loss: 0.369534969329834\n",
      "Epoch: 157390 | Loss: 0.3700873553752899 | Test loss: 0.3695211410522461\n",
      "Epoch: 157400 | Loss: 0.3700755834579468 | Test loss: 0.3695072829723358\n",
      "Epoch: 157410 | Loss: 0.370063841342926 | Test loss: 0.3694934546947479\n",
      "Epoch: 157420 | Loss: 0.3700520694255829 | Test loss: 0.36947956681251526\n",
      "Epoch: 157430 | Loss: 0.37004032731056213 | Test loss: 0.36946573853492737\n",
      "Epoch: 157440 | Loss: 0.370028555393219 | Test loss: 0.3694518804550171\n",
      "Epoch: 157450 | Loss: 0.37001678347587585 | Test loss: 0.3694380223751068\n",
      "Epoch: 157460 | Loss: 0.3700050115585327 | Test loss: 0.3694241940975189\n",
      "Epoch: 157470 | Loss: 0.36999326944351196 | Test loss: 0.36941036581993103\n",
      "Epoch: 157480 | Loss: 0.3699814975261688 | Test loss: 0.36939650774002075\n",
      "Epoch: 157490 | Loss: 0.36996975541114807 | Test loss: 0.36938267946243286\n",
      "Epoch: 157500 | Loss: 0.3699580132961273 | Test loss: 0.3693687915802002\n",
      "Epoch: 157510 | Loss: 0.3699462115764618 | Test loss: 0.3693549633026123\n",
      "Epoch: 157520 | Loss: 0.36993446946144104 | Test loss: 0.3693411350250244\n",
      "Epoch: 157530 | Loss: 0.3699226975440979 | Test loss: 0.36932727694511414\n",
      "Epoch: 157540 | Loss: 0.36991095542907715 | Test loss: 0.36931344866752625\n",
      "Epoch: 157550 | Loss: 0.3698992133140564 | Test loss: 0.36929962038993835\n",
      "Epoch: 157560 | Loss: 0.36988744139671326 | Test loss: 0.3692857325077057\n",
      "Epoch: 157570 | Loss: 0.3698756694793701 | Test loss: 0.3692719042301178\n",
      "Epoch: 157580 | Loss: 0.36986392736434937 | Test loss: 0.3692580461502075\n",
      "Epoch: 157590 | Loss: 0.36985212564468384 | Test loss: 0.36924418807029724\n",
      "Epoch: 157600 | Loss: 0.3698403835296631 | Test loss: 0.36923035979270935\n",
      "Epoch: 157610 | Loss: 0.3698286712169647 | Test loss: 0.36921653151512146\n",
      "Epoch: 157620 | Loss: 0.3698168694972992 | Test loss: 0.3692026734352112\n",
      "Epoch: 157630 | Loss: 0.36980509757995605 | Test loss: 0.3691888153553009\n",
      "Epoch: 157640 | Loss: 0.3697933852672577 | Test loss: 0.3691749572753906\n",
      "Epoch: 157650 | Loss: 0.36978158354759216 | Test loss: 0.36916112899780273\n",
      "Epoch: 157660 | Loss: 0.3697698414325714 | Test loss: 0.36914727091789246\n",
      "Epoch: 157670 | Loss: 0.36975806951522827 | Test loss: 0.36913344264030457\n",
      "Epoch: 157680 | Loss: 0.3697463274002075 | Test loss: 0.3691195845603943\n",
      "Epoch: 157690 | Loss: 0.369734525680542 | Test loss: 0.369105726480484\n",
      "Epoch: 157700 | Loss: 0.36972278356552124 | Test loss: 0.3690918982028961\n",
      "Epoch: 157710 | Loss: 0.3697110116481781 | Test loss: 0.36907804012298584\n",
      "Epoch: 157720 | Loss: 0.36969929933547974 | Test loss: 0.36906418204307556\n",
      "Epoch: 157730 | Loss: 0.3696875274181366 | Test loss: 0.36905035376548767\n",
      "Epoch: 157740 | Loss: 0.36967572569847107 | Test loss: 0.3690365254878998\n",
      "Epoch: 157750 | Loss: 0.3696639835834503 | Test loss: 0.3690226674079895\n",
      "Epoch: 157760 | Loss: 0.36965224146842957 | Test loss: 0.3690088391304016\n",
      "Epoch: 157770 | Loss: 0.3696404695510864 | Test loss: 0.36899498105049133\n",
      "Epoch: 157780 | Loss: 0.3696287274360657 | Test loss: 0.36898112297058105\n",
      "Epoch: 157790 | Loss: 0.36961695551872253 | Test loss: 0.36896729469299316\n",
      "Epoch: 157800 | Loss: 0.3696051836013794 | Test loss: 0.3689534366130829\n",
      "Epoch: 157810 | Loss: 0.36959344148635864 | Test loss: 0.368939608335495\n",
      "Epoch: 157820 | Loss: 0.3695816695690155 | Test loss: 0.36892572045326233\n",
      "Epoch: 157830 | Loss: 0.36956992745399475 | Test loss: 0.36891189217567444\n",
      "Epoch: 157840 | Loss: 0.3695581555366516 | Test loss: 0.36889803409576416\n",
      "Epoch: 157850 | Loss: 0.36954638361930847 | Test loss: 0.3688841760158539\n",
      "Epoch: 157860 | Loss: 0.36953461170196533 | Test loss: 0.368870347738266\n",
      "Epoch: 157870 | Loss: 0.3695228695869446 | Test loss: 0.3688565194606781\n",
      "Epoch: 157880 | Loss: 0.36951109766960144 | Test loss: 0.3688426613807678\n",
      "Epoch: 157890 | Loss: 0.3694993555545807 | Test loss: 0.36882883310317993\n",
      "Epoch: 157900 | Loss: 0.36948761343955994 | Test loss: 0.36881494522094727\n",
      "Epoch: 157910 | Loss: 0.3694758117198944 | Test loss: 0.3688011169433594\n",
      "Epoch: 157920 | Loss: 0.36946406960487366 | Test loss: 0.3687872886657715\n",
      "Epoch: 157930 | Loss: 0.3694522976875305 | Test loss: 0.3687734305858612\n",
      "Epoch: 157940 | Loss: 0.36944055557250977 | Test loss: 0.3687596023082733\n",
      "Epoch: 157950 | Loss: 0.369428813457489 | Test loss: 0.3687457740306854\n",
      "Epoch: 157960 | Loss: 0.3694170415401459 | Test loss: 0.36873188614845276\n",
      "Epoch: 157970 | Loss: 0.36940526962280273 | Test loss: 0.36871805787086487\n",
      "Epoch: 157980 | Loss: 0.369393527507782 | Test loss: 0.3687041997909546\n",
      "Epoch: 157990 | Loss: 0.36938172578811646 | Test loss: 0.3686903417110443\n",
      "Epoch: 158000 | Loss: 0.3693699836730957 | Test loss: 0.3686765134334564\n",
      "Epoch: 158010 | Loss: 0.36935827136039734 | Test loss: 0.36866268515586853\n",
      "Epoch: 158020 | Loss: 0.3693464696407318 | Test loss: 0.36864882707595825\n",
      "Epoch: 158030 | Loss: 0.36933469772338867 | Test loss: 0.368634968996048\n",
      "Epoch: 158040 | Loss: 0.3693229854106903 | Test loss: 0.3686211109161377\n",
      "Epoch: 158050 | Loss: 0.3693111836910248 | Test loss: 0.3686072826385498\n",
      "Epoch: 158060 | Loss: 0.36929944157600403 | Test loss: 0.3685934245586395\n",
      "Epoch: 158070 | Loss: 0.3692876696586609 | Test loss: 0.36857959628105164\n",
      "Epoch: 158080 | Loss: 0.36927592754364014 | Test loss: 0.36856573820114136\n",
      "Epoch: 158090 | Loss: 0.3692641258239746 | Test loss: 0.3685518801212311\n",
      "Epoch: 158100 | Loss: 0.36925238370895386 | Test loss: 0.3685380518436432\n",
      "Epoch: 158110 | Loss: 0.3692406117916107 | Test loss: 0.3685241937637329\n",
      "Epoch: 158120 | Loss: 0.36922889947891235 | Test loss: 0.36851033568382263\n",
      "Epoch: 158130 | Loss: 0.3692171275615692 | Test loss: 0.36849650740623474\n",
      "Epoch: 158140 | Loss: 0.3692053258419037 | Test loss: 0.36848267912864685\n",
      "Epoch: 158150 | Loss: 0.36919358372688293 | Test loss: 0.3684688210487366\n",
      "Epoch: 158160 | Loss: 0.3691818416118622 | Test loss: 0.3684549927711487\n",
      "Epoch: 158170 | Loss: 0.36917006969451904 | Test loss: 0.3684411346912384\n",
      "Epoch: 158180 | Loss: 0.3691583275794983 | Test loss: 0.3684272766113281\n",
      "Epoch: 158190 | Loss: 0.36914655566215515 | Test loss: 0.36841344833374023\n",
      "Epoch: 158200 | Loss: 0.369134783744812 | Test loss: 0.36839959025382996\n",
      "Epoch: 158210 | Loss: 0.36912304162979126 | Test loss: 0.36838576197624207\n",
      "Epoch: 158220 | Loss: 0.3691112697124481 | Test loss: 0.3683718740940094\n",
      "Epoch: 158230 | Loss: 0.36909952759742737 | Test loss: 0.3683580458164215\n",
      "Epoch: 158240 | Loss: 0.36908775568008423 | Test loss: 0.36834418773651123\n",
      "Epoch: 158250 | Loss: 0.3690759837627411 | Test loss: 0.36833032965660095\n",
      "Epoch: 158260 | Loss: 0.36906421184539795 | Test loss: 0.36831650137901306\n",
      "Epoch: 158270 | Loss: 0.3690524697303772 | Test loss: 0.36830267310142517\n",
      "Epoch: 158280 | Loss: 0.36904069781303406 | Test loss: 0.3682888150215149\n",
      "Epoch: 158290 | Loss: 0.3690289556980133 | Test loss: 0.368274986743927\n",
      "Epoch: 158300 | Loss: 0.36901721358299255 | Test loss: 0.36826109886169434\n",
      "Epoch: 158310 | Loss: 0.369005411863327 | Test loss: 0.36824727058410645\n",
      "Epoch: 158320 | Loss: 0.3689936697483063 | Test loss: 0.36823344230651855\n",
      "Epoch: 158330 | Loss: 0.36898189783096313 | Test loss: 0.3682195842266083\n",
      "Epoch: 158340 | Loss: 0.3689701557159424 | Test loss: 0.3682057559490204\n",
      "Epoch: 158350 | Loss: 0.36895841360092163 | Test loss: 0.3681919276714325\n",
      "Epoch: 158360 | Loss: 0.3689466416835785 | Test loss: 0.36817803978919983\n",
      "Epoch: 158370 | Loss: 0.36893486976623535 | Test loss: 0.36816421151161194\n",
      "Epoch: 158380 | Loss: 0.3689231276512146 | Test loss: 0.36815035343170166\n",
      "Epoch: 158390 | Loss: 0.3689113259315491 | Test loss: 0.3681364953517914\n",
      "Epoch: 158400 | Loss: 0.3688995838165283 | Test loss: 0.3681226670742035\n",
      "Epoch: 158410 | Loss: 0.36888787150382996 | Test loss: 0.3681088387966156\n",
      "Epoch: 158420 | Loss: 0.36887606978416443 | Test loss: 0.3680949807167053\n",
      "Epoch: 158430 | Loss: 0.3688642978668213 | Test loss: 0.36808112263679504\n",
      "Epoch: 158440 | Loss: 0.3688525855541229 | Test loss: 0.36806726455688477\n",
      "Epoch: 158450 | Loss: 0.3688407838344574 | Test loss: 0.3680534362792969\n",
      "Epoch: 158460 | Loss: 0.36882904171943665 | Test loss: 0.3680395781993866\n",
      "Epoch: 158470 | Loss: 0.3688172698020935 | Test loss: 0.3680257499217987\n",
      "Epoch: 158480 | Loss: 0.36880552768707275 | Test loss: 0.3680118918418884\n",
      "Epoch: 158490 | Loss: 0.3687937259674072 | Test loss: 0.36799803376197815\n",
      "Epoch: 158500 | Loss: 0.3687819838523865 | Test loss: 0.36798420548439026\n",
      "Epoch: 158510 | Loss: 0.36877021193504333 | Test loss: 0.36797034740448\n",
      "Epoch: 158520 | Loss: 0.36875849962234497 | Test loss: 0.3679564893245697\n",
      "Epoch: 158530 | Loss: 0.36874672770500183 | Test loss: 0.3679426610469818\n",
      "Epoch: 158540 | Loss: 0.3687349259853363 | Test loss: 0.3679288327693939\n",
      "Epoch: 158550 | Loss: 0.36872318387031555 | Test loss: 0.36791497468948364\n",
      "Epoch: 158560 | Loss: 0.3687114417552948 | Test loss: 0.36790114641189575\n",
      "Epoch: 158570 | Loss: 0.36869966983795166 | Test loss: 0.3678872883319855\n",
      "Epoch: 158580 | Loss: 0.3686879277229309 | Test loss: 0.3678734302520752\n",
      "Epoch: 158590 | Loss: 0.36867615580558777 | Test loss: 0.3678596019744873\n",
      "Epoch: 158600 | Loss: 0.36866438388824463 | Test loss: 0.367845743894577\n",
      "Epoch: 158610 | Loss: 0.3686526417732239 | Test loss: 0.36783191561698914\n",
      "Epoch: 158620 | Loss: 0.36864086985588074 | Test loss: 0.36781802773475647\n",
      "Epoch: 158630 | Loss: 0.36862912774086 | Test loss: 0.3678041994571686\n",
      "Epoch: 158640 | Loss: 0.36861735582351685 | Test loss: 0.3677903413772583\n",
      "Epoch: 158650 | Loss: 0.3686055839061737 | Test loss: 0.367776483297348\n",
      "Epoch: 158660 | Loss: 0.36859381198883057 | Test loss: 0.36776265501976013\n",
      "Epoch: 158670 | Loss: 0.3685820698738098 | Test loss: 0.36774882674217224\n",
      "Epoch: 158680 | Loss: 0.3685702979564667 | Test loss: 0.36773496866226196\n",
      "Epoch: 158690 | Loss: 0.3685585558414459 | Test loss: 0.3677211403846741\n",
      "Epoch: 158700 | Loss: 0.36854681372642517 | Test loss: 0.3677072525024414\n",
      "Epoch: 158710 | Loss: 0.36853501200675964 | Test loss: 0.3676934242248535\n",
      "Epoch: 158720 | Loss: 0.3685232698917389 | Test loss: 0.3676795959472656\n",
      "Epoch: 158730 | Loss: 0.36851149797439575 | Test loss: 0.36766573786735535\n",
      "Epoch: 158740 | Loss: 0.368499755859375 | Test loss: 0.36765190958976746\n",
      "Epoch: 158750 | Loss: 0.36848801374435425 | Test loss: 0.36763808131217957\n",
      "Epoch: 158760 | Loss: 0.3684762418270111 | Test loss: 0.3676241934299469\n",
      "Epoch: 158770 | Loss: 0.36846446990966797 | Test loss: 0.367610365152359\n",
      "Epoch: 158780 | Loss: 0.3684527277946472 | Test loss: 0.36759650707244873\n",
      "Epoch: 158790 | Loss: 0.3684409260749817 | Test loss: 0.36758264899253845\n",
      "Epoch: 158800 | Loss: 0.36842918395996094 | Test loss: 0.36756882071495056\n",
      "Epoch: 158810 | Loss: 0.3684174716472626 | Test loss: 0.36755499243736267\n",
      "Epoch: 158820 | Loss: 0.36840566992759705 | Test loss: 0.3675411343574524\n",
      "Epoch: 158830 | Loss: 0.3683938980102539 | Test loss: 0.3675272762775421\n",
      "Epoch: 158840 | Loss: 0.36838218569755554 | Test loss: 0.36751341819763184\n",
      "Epoch: 158850 | Loss: 0.36837038397789 | Test loss: 0.36749958992004395\n",
      "Epoch: 158860 | Loss: 0.36835864186286926 | Test loss: 0.36748573184013367\n",
      "Epoch: 158870 | Loss: 0.3683468699455261 | Test loss: 0.3674719035625458\n",
      "Epoch: 158880 | Loss: 0.36833512783050537 | Test loss: 0.3674580454826355\n",
      "Epoch: 158890 | Loss: 0.36832332611083984 | Test loss: 0.3674441874027252\n",
      "Epoch: 158900 | Loss: 0.3683115839958191 | Test loss: 0.36743035912513733\n",
      "Epoch: 158910 | Loss: 0.36829981207847595 | Test loss: 0.36741650104522705\n",
      "Epoch: 158920 | Loss: 0.3682880997657776 | Test loss: 0.3674026429653168\n",
      "Epoch: 158930 | Loss: 0.36827632784843445 | Test loss: 0.3673888146877289\n",
      "Epoch: 158940 | Loss: 0.3682645261287689 | Test loss: 0.367374986410141\n",
      "Epoch: 158950 | Loss: 0.36825278401374817 | Test loss: 0.3673611283302307\n",
      "Epoch: 158960 | Loss: 0.3682410418987274 | Test loss: 0.3673473000526428\n",
      "Epoch: 158970 | Loss: 0.3682292699813843 | Test loss: 0.36733344197273254\n",
      "Epoch: 158980 | Loss: 0.3682175278663635 | Test loss: 0.36731958389282227\n",
      "Epoch: 158990 | Loss: 0.3682057559490204 | Test loss: 0.3673057556152344\n",
      "Epoch: 159000 | Loss: 0.36819398403167725 | Test loss: 0.3672918975353241\n",
      "Epoch: 159010 | Loss: 0.3681822419166565 | Test loss: 0.3672780692577362\n",
      "Epoch: 159020 | Loss: 0.36817046999931335 | Test loss: 0.36726418137550354\n",
      "Epoch: 159030 | Loss: 0.3681587278842926 | Test loss: 0.36725035309791565\n",
      "Epoch: 159040 | Loss: 0.36814695596694946 | Test loss: 0.36723649501800537\n",
      "Epoch: 159050 | Loss: 0.3681351840496063 | Test loss: 0.3672226369380951\n",
      "Epoch: 159060 | Loss: 0.3681234121322632 | Test loss: 0.3672088086605072\n",
      "Epoch: 159070 | Loss: 0.36811167001724243 | Test loss: 0.3671949803829193\n",
      "Epoch: 159080 | Loss: 0.3680998980998993 | Test loss: 0.36718112230300903\n",
      "Epoch: 159090 | Loss: 0.36808815598487854 | Test loss: 0.36716729402542114\n",
      "Epoch: 159100 | Loss: 0.3680764138698578 | Test loss: 0.3671534061431885\n",
      "Epoch: 159110 | Loss: 0.36806461215019226 | Test loss: 0.3671395778656006\n",
      "Epoch: 159120 | Loss: 0.3680528700351715 | Test loss: 0.3671257495880127\n",
      "Epoch: 159130 | Loss: 0.36804109811782837 | Test loss: 0.3671118915081024\n",
      "Epoch: 159140 | Loss: 0.3680293560028076 | Test loss: 0.3670980632305145\n",
      "Epoch: 159150 | Loss: 0.36801761388778687 | Test loss: 0.36708423495292664\n",
      "Epoch: 159160 | Loss: 0.3680058419704437 | Test loss: 0.36707034707069397\n",
      "Epoch: 159170 | Loss: 0.3679940700531006 | Test loss: 0.3670565187931061\n",
      "Epoch: 159180 | Loss: 0.36798232793807983 | Test loss: 0.3670426607131958\n",
      "Epoch: 159190 | Loss: 0.3679705262184143 | Test loss: 0.3670288026332855\n",
      "Epoch: 159200 | Loss: 0.36795878410339355 | Test loss: 0.36701497435569763\n",
      "Epoch: 159210 | Loss: 0.3679470717906952 | Test loss: 0.36700114607810974\n",
      "Epoch: 159220 | Loss: 0.36793527007102966 | Test loss: 0.36698728799819946\n",
      "Epoch: 159230 | Loss: 0.3679234981536865 | Test loss: 0.3669734299182892\n",
      "Epoch: 159240 | Loss: 0.36791178584098816 | Test loss: 0.3669595718383789\n",
      "Epoch: 159250 | Loss: 0.36789998412132263 | Test loss: 0.366945743560791\n",
      "Epoch: 159260 | Loss: 0.3678882420063019 | Test loss: 0.36693188548088074\n",
      "Epoch: 159270 | Loss: 0.36787647008895874 | Test loss: 0.36691805720329285\n",
      "Epoch: 159280 | Loss: 0.367864727973938 | Test loss: 0.36690419912338257\n",
      "Epoch: 159290 | Loss: 0.36785292625427246 | Test loss: 0.3668903410434723\n",
      "Epoch: 159300 | Loss: 0.3678411841392517 | Test loss: 0.3668765127658844\n",
      "Epoch: 159310 | Loss: 0.36782941222190857 | Test loss: 0.3668626546859741\n",
      "Epoch: 159320 | Loss: 0.3678176999092102 | Test loss: 0.36684879660606384\n",
      "Epoch: 159330 | Loss: 0.36780592799186707 | Test loss: 0.36683496832847595\n",
      "Epoch: 159340 | Loss: 0.36779412627220154 | Test loss: 0.36682114005088806\n",
      "Epoch: 159350 | Loss: 0.3677823841571808 | Test loss: 0.3668072819709778\n",
      "Epoch: 159360 | Loss: 0.36777064204216003 | Test loss: 0.3667934536933899\n",
      "Epoch: 159370 | Loss: 0.3677588701248169 | Test loss: 0.3667795956134796\n",
      "Epoch: 159380 | Loss: 0.36774712800979614 | Test loss: 0.36676573753356934\n",
      "Epoch: 159390 | Loss: 0.367735356092453 | Test loss: 0.36675190925598145\n",
      "Epoch: 159400 | Loss: 0.36772358417510986 | Test loss: 0.36673805117607117\n",
      "Epoch: 159410 | Loss: 0.3677118420600891 | Test loss: 0.3667242228984833\n",
      "Epoch: 159420 | Loss: 0.36770007014274597 | Test loss: 0.3667103350162506\n",
      "Epoch: 159430 | Loss: 0.3676883280277252 | Test loss: 0.3666965067386627\n",
      "Epoch: 159440 | Loss: 0.3676765561103821 | Test loss: 0.36668264865875244\n",
      "Epoch: 159450 | Loss: 0.36766478419303894 | Test loss: 0.36666879057884216\n",
      "Epoch: 159460 | Loss: 0.3676530122756958 | Test loss: 0.3666549623012543\n",
      "Epoch: 159470 | Loss: 0.36764127016067505 | Test loss: 0.3666411340236664\n",
      "Epoch: 159480 | Loss: 0.3676294982433319 | Test loss: 0.3666272759437561\n",
      "Epoch: 159490 | Loss: 0.36761775612831116 | Test loss: 0.3666134476661682\n",
      "Epoch: 159500 | Loss: 0.3676060140132904 | Test loss: 0.36659955978393555\n",
      "Epoch: 159510 | Loss: 0.3675942122936249 | Test loss: 0.36658573150634766\n",
      "Epoch: 159520 | Loss: 0.3675824701786041 | Test loss: 0.36657190322875977\n",
      "Epoch: 159530 | Loss: 0.367570698261261 | Test loss: 0.3665580451488495\n",
      "Epoch: 159540 | Loss: 0.36755895614624023 | Test loss: 0.3665442168712616\n",
      "Epoch: 159550 | Loss: 0.3675472140312195 | Test loss: 0.3665303885936737\n",
      "Epoch: 159560 | Loss: 0.36753544211387634 | Test loss: 0.36651650071144104\n",
      "Epoch: 159570 | Loss: 0.3675236701965332 | Test loss: 0.36650267243385315\n",
      "Epoch: 159580 | Loss: 0.36751192808151245 | Test loss: 0.36648881435394287\n",
      "Epoch: 159590 | Loss: 0.3675001263618469 | Test loss: 0.3664749562740326\n",
      "Epoch: 159600 | Loss: 0.36748838424682617 | Test loss: 0.3664611279964447\n",
      "Epoch: 159610 | Loss: 0.3674766719341278 | Test loss: 0.3664472997188568\n",
      "Epoch: 159620 | Loss: 0.3674648702144623 | Test loss: 0.36643344163894653\n",
      "Epoch: 159630 | Loss: 0.36745309829711914 | Test loss: 0.36641958355903625\n",
      "Epoch: 159640 | Loss: 0.3674413859844208 | Test loss: 0.366405725479126\n",
      "Epoch: 159650 | Loss: 0.36742958426475525 | Test loss: 0.3663918972015381\n",
      "Epoch: 159660 | Loss: 0.3674178421497345 | Test loss: 0.3663780391216278\n",
      "Epoch: 159670 | Loss: 0.36740607023239136 | Test loss: 0.3663642108440399\n",
      "Epoch: 159680 | Loss: 0.3673943281173706 | Test loss: 0.36635035276412964\n",
      "Epoch: 159690 | Loss: 0.3673825263977051 | Test loss: 0.36633649468421936\n",
      "Epoch: 159700 | Loss: 0.3673707842826843 | Test loss: 0.36632266640663147\n",
      "Epoch: 159710 | Loss: 0.3673590123653412 | Test loss: 0.3663088083267212\n",
      "Epoch: 159720 | Loss: 0.3673473000526428 | Test loss: 0.3662949502468109\n",
      "Epoch: 159730 | Loss: 0.3673355281352997 | Test loss: 0.366281121969223\n",
      "Epoch: 159740 | Loss: 0.36732372641563416 | Test loss: 0.36626729369163513\n",
      "Epoch: 159750 | Loss: 0.3673119843006134 | Test loss: 0.36625343561172485\n",
      "Epoch: 159760 | Loss: 0.36730024218559265 | Test loss: 0.36623960733413696\n",
      "Epoch: 159770 | Loss: 0.3672884702682495 | Test loss: 0.3662257492542267\n",
      "Epoch: 159780 | Loss: 0.36727672815322876 | Test loss: 0.3662118911743164\n",
      "Epoch: 159790 | Loss: 0.3672649562358856 | Test loss: 0.3661980628967285\n",
      "Epoch: 159800 | Loss: 0.3672531843185425 | Test loss: 0.36618420481681824\n",
      "Epoch: 159810 | Loss: 0.36724144220352173 | Test loss: 0.36617037653923035\n",
      "Epoch: 159820 | Loss: 0.3672296702861786 | Test loss: 0.3661564886569977\n",
      "Epoch: 159830 | Loss: 0.36721792817115784 | Test loss: 0.3661426603794098\n",
      "Epoch: 159840 | Loss: 0.3672061562538147 | Test loss: 0.3661288022994995\n",
      "Epoch: 159850 | Loss: 0.36719438433647156 | Test loss: 0.36611494421958923\n",
      "Epoch: 159860 | Loss: 0.3671826124191284 | Test loss: 0.36610111594200134\n",
      "Epoch: 159870 | Loss: 0.36717087030410767 | Test loss: 0.36608728766441345\n",
      "Epoch: 159880 | Loss: 0.3671590983867645 | Test loss: 0.3660734295845032\n",
      "Epoch: 159890 | Loss: 0.3671473562717438 | Test loss: 0.3660596013069153\n",
      "Epoch: 159900 | Loss: 0.367135614156723 | Test loss: 0.3660457134246826\n",
      "Epoch: 159910 | Loss: 0.3671238124370575 | Test loss: 0.3660318851470947\n",
      "Epoch: 159920 | Loss: 0.36711207032203674 | Test loss: 0.36601805686950684\n",
      "Epoch: 159930 | Loss: 0.3671002984046936 | Test loss: 0.36600419878959656\n",
      "Epoch: 159940 | Loss: 0.36708855628967285 | Test loss: 0.36599037051200867\n",
      "Epoch: 159950 | Loss: 0.3670768141746521 | Test loss: 0.3659765422344208\n",
      "Epoch: 159960 | Loss: 0.36706504225730896 | Test loss: 0.3659626543521881\n",
      "Epoch: 159970 | Loss: 0.3670532703399658 | Test loss: 0.3659488260746002\n",
      "Epoch: 159980 | Loss: 0.36704152822494507 | Test loss: 0.36593496799468994\n",
      "Epoch: 159990 | Loss: 0.36702972650527954 | Test loss: 0.36592110991477966\n",
      "Epoch: 160000 | Loss: 0.3670179843902588 | Test loss: 0.3659072816371918\n",
      "Epoch: 160010 | Loss: 0.3670062720775604 | Test loss: 0.3658934533596039\n",
      "Epoch: 160020 | Loss: 0.3669944703578949 | Test loss: 0.3658795952796936\n",
      "Epoch: 160030 | Loss: 0.36698269844055176 | Test loss: 0.3658657371997833\n",
      "Epoch: 160040 | Loss: 0.3669709861278534 | Test loss: 0.36585187911987305\n",
      "Epoch: 160050 | Loss: 0.36695918440818787 | Test loss: 0.36583805084228516\n",
      "Epoch: 160060 | Loss: 0.3669474422931671 | Test loss: 0.3658241927623749\n",
      "Epoch: 160070 | Loss: 0.366935670375824 | Test loss: 0.365810364484787\n",
      "Epoch: 160080 | Loss: 0.3669239282608032 | Test loss: 0.3657965064048767\n",
      "Epoch: 160090 | Loss: 0.3669121265411377 | Test loss: 0.36578264832496643\n",
      "Epoch: 160100 | Loss: 0.36690038442611694 | Test loss: 0.36576882004737854\n",
      "Epoch: 160110 | Loss: 0.3668886125087738 | Test loss: 0.36575496196746826\n",
      "Epoch: 160120 | Loss: 0.36687690019607544 | Test loss: 0.365741103887558\n",
      "Epoch: 160130 | Loss: 0.3668651282787323 | Test loss: 0.3657272756099701\n",
      "Epoch: 160140 | Loss: 0.3668533265590668 | Test loss: 0.3657134473323822\n",
      "Epoch: 160150 | Loss: 0.366841584444046 | Test loss: 0.3656995892524719\n",
      "Epoch: 160160 | Loss: 0.36682984232902527 | Test loss: 0.36568576097488403\n",
      "Epoch: 160170 | Loss: 0.36681807041168213 | Test loss: 0.36567190289497375\n",
      "Epoch: 160180 | Loss: 0.3668063282966614 | Test loss: 0.3656580448150635\n",
      "Epoch: 160190 | Loss: 0.36679455637931824 | Test loss: 0.3656442165374756\n",
      "Epoch: 160200 | Loss: 0.3667827844619751 | Test loss: 0.3656303584575653\n",
      "Epoch: 160210 | Loss: 0.36677104234695435 | Test loss: 0.3656165301799774\n",
      "Epoch: 160220 | Loss: 0.3667592704296112 | Test loss: 0.36560264229774475\n",
      "Epoch: 160230 | Loss: 0.36674752831459045 | Test loss: 0.36558881402015686\n",
      "Epoch: 160240 | Loss: 0.3667357563972473 | Test loss: 0.3655749559402466\n",
      "Epoch: 160250 | Loss: 0.3667239844799042 | Test loss: 0.3655610978603363\n",
      "Epoch: 160260 | Loss: 0.36671221256256104 | Test loss: 0.3655472695827484\n",
      "Epoch: 160270 | Loss: 0.3667004704475403 | Test loss: 0.3655334413051605\n",
      "Epoch: 160280 | Loss: 0.36668869853019714 | Test loss: 0.36551958322525024\n",
      "Epoch: 160290 | Loss: 0.3666769564151764 | Test loss: 0.36550575494766235\n",
      "Epoch: 160300 | Loss: 0.36666521430015564 | Test loss: 0.3654918670654297\n",
      "Epoch: 160310 | Loss: 0.3666534125804901 | Test loss: 0.3654780387878418\n",
      "Epoch: 160320 | Loss: 0.36664167046546936 | Test loss: 0.3654642105102539\n",
      "Epoch: 160330 | Loss: 0.3666298985481262 | Test loss: 0.36545035243034363\n",
      "Epoch: 160340 | Loss: 0.36661815643310547 | Test loss: 0.36543652415275574\n",
      "Epoch: 160350 | Loss: 0.3666064143180847 | Test loss: 0.36542269587516785\n",
      "Epoch: 160360 | Loss: 0.3665946424007416 | Test loss: 0.3654088079929352\n",
      "Epoch: 160370 | Loss: 0.36658287048339844 | Test loss: 0.3653949797153473\n",
      "Epoch: 160380 | Loss: 0.3665711283683777 | Test loss: 0.365381121635437\n",
      "Epoch: 160390 | Loss: 0.36655932664871216 | Test loss: 0.36536726355552673\n",
      "Epoch: 160400 | Loss: 0.3665475845336914 | Test loss: 0.36535343527793884\n",
      "Epoch: 160410 | Loss: 0.36653587222099304 | Test loss: 0.36533960700035095\n",
      "Epoch: 160420 | Loss: 0.3665240705013275 | Test loss: 0.3653257489204407\n",
      "Epoch: 160430 | Loss: 0.3665122985839844 | Test loss: 0.3653118908405304\n",
      "Epoch: 160440 | Loss: 0.366500586271286 | Test loss: 0.3652980327606201\n",
      "Epoch: 160450 | Loss: 0.3664887845516205 | Test loss: 0.3652842044830322\n",
      "Epoch: 160460 | Loss: 0.36647704243659973 | Test loss: 0.36527034640312195\n",
      "Epoch: 160470 | Loss: 0.3664652705192566 | Test loss: 0.36525651812553406\n",
      "Epoch: 160480 | Loss: 0.36645352840423584 | Test loss: 0.3652426600456238\n",
      "Epoch: 160490 | Loss: 0.3664417266845703 | Test loss: 0.3652288019657135\n",
      "Epoch: 160500 | Loss: 0.36642998456954956 | Test loss: 0.3652149736881256\n",
      "Epoch: 160510 | Loss: 0.3664182126522064 | Test loss: 0.36520111560821533\n",
      "Epoch: 160520 | Loss: 0.36640650033950806 | Test loss: 0.36518725752830505\n",
      "Epoch: 160530 | Loss: 0.3663947284221649 | Test loss: 0.36517342925071716\n",
      "Epoch: 160540 | Loss: 0.3663829267024994 | Test loss: 0.3651596009731293\n",
      "Epoch: 160550 | Loss: 0.36637118458747864 | Test loss: 0.365145742893219\n",
      "Epoch: 160560 | Loss: 0.3663594424724579 | Test loss: 0.3651319146156311\n",
      "Epoch: 160570 | Loss: 0.36634767055511475 | Test loss: 0.3651180565357208\n",
      "Epoch: 160580 | Loss: 0.366335928440094 | Test loss: 0.36510419845581055\n",
      "Epoch: 160590 | Loss: 0.36632415652275085 | Test loss: 0.36509037017822266\n",
      "Epoch: 160600 | Loss: 0.3663123846054077 | Test loss: 0.3650765120983124\n",
      "Epoch: 160610 | Loss: 0.36630064249038696 | Test loss: 0.3650626838207245\n",
      "Epoch: 160620 | Loss: 0.3662888705730438 | Test loss: 0.3650487959384918\n",
      "Epoch: 160630 | Loss: 0.36627712845802307 | Test loss: 0.36503496766090393\n",
      "Epoch: 160640 | Loss: 0.36626535654067993 | Test loss: 0.36502110958099365\n",
      "Epoch: 160650 | Loss: 0.3662535846233368 | Test loss: 0.3650072515010834\n",
      "Epoch: 160660 | Loss: 0.36624181270599365 | Test loss: 0.3649934232234955\n",
      "Epoch: 160670 | Loss: 0.3662300705909729 | Test loss: 0.3649795949459076\n",
      "Epoch: 160680 | Loss: 0.36621829867362976 | Test loss: 0.3649657368659973\n",
      "Epoch: 160690 | Loss: 0.366206556558609 | Test loss: 0.3649519085884094\n",
      "Epoch: 160700 | Loss: 0.36619481444358826 | Test loss: 0.36493802070617676\n",
      "Epoch: 160710 | Loss: 0.36618301272392273 | Test loss: 0.36492419242858887\n",
      "Epoch: 160720 | Loss: 0.366171270608902 | Test loss: 0.364910364151001\n",
      "Epoch: 160730 | Loss: 0.36615949869155884 | Test loss: 0.3648965060710907\n",
      "Epoch: 160740 | Loss: 0.3661477565765381 | Test loss: 0.3648826777935028\n",
      "Epoch: 160750 | Loss: 0.36613601446151733 | Test loss: 0.3648688495159149\n",
      "Epoch: 160760 | Loss: 0.3661242425441742 | Test loss: 0.36485496163368225\n",
      "Epoch: 160770 | Loss: 0.36611247062683105 | Test loss: 0.36484113335609436\n",
      "Epoch: 160780 | Loss: 0.3661007285118103 | Test loss: 0.3648272752761841\n",
      "Epoch: 160790 | Loss: 0.3660889267921448 | Test loss: 0.3648134171962738\n",
      "Epoch: 160800 | Loss: 0.366077184677124 | Test loss: 0.3647995889186859\n",
      "Epoch: 160810 | Loss: 0.36606547236442566 | Test loss: 0.364785760641098\n",
      "Epoch: 160820 | Loss: 0.36605367064476013 | Test loss: 0.36477190256118774\n",
      "Epoch: 160830 | Loss: 0.366041898727417 | Test loss: 0.36475804448127747\n",
      "Epoch: 160840 | Loss: 0.36603018641471863 | Test loss: 0.3647441864013672\n",
      "Epoch: 160850 | Loss: 0.3660183846950531 | Test loss: 0.3647303581237793\n",
      "Epoch: 160860 | Loss: 0.36600664258003235 | Test loss: 0.364716500043869\n",
      "Epoch: 160870 | Loss: 0.3659948706626892 | Test loss: 0.36470267176628113\n",
      "Epoch: 160880 | Loss: 0.36598312854766846 | Test loss: 0.36468881368637085\n",
      "Epoch: 160890 | Loss: 0.36597132682800293 | Test loss: 0.36467495560646057\n",
      "Epoch: 160900 | Loss: 0.3659595847129822 | Test loss: 0.3646611273288727\n",
      "Epoch: 160910 | Loss: 0.36594781279563904 | Test loss: 0.3646472692489624\n",
      "Epoch: 160920 | Loss: 0.3659361004829407 | Test loss: 0.3646334111690521\n",
      "Epoch: 160930 | Loss: 0.36592432856559753 | Test loss: 0.36461958289146423\n",
      "Epoch: 160940 | Loss: 0.365912526845932 | Test loss: 0.36460575461387634\n",
      "Epoch: 160950 | Loss: 0.36590078473091125 | Test loss: 0.36459189653396606\n",
      "Epoch: 160960 | Loss: 0.3658890426158905 | Test loss: 0.3645780682563782\n",
      "Epoch: 160970 | Loss: 0.36587727069854736 | Test loss: 0.3645642101764679\n",
      "Epoch: 160980 | Loss: 0.3658655285835266 | Test loss: 0.3645503520965576\n",
      "Epoch: 160990 | Loss: 0.36585375666618347 | Test loss: 0.3645365238189697\n",
      "Epoch: 161000 | Loss: 0.36584198474884033 | Test loss: 0.36452266573905945\n",
      "Epoch: 161010 | Loss: 0.3658302426338196 | Test loss: 0.36450883746147156\n",
      "Epoch: 161020 | Loss: 0.36581847071647644 | Test loss: 0.3644949495792389\n",
      "Epoch: 161030 | Loss: 0.3658067286014557 | Test loss: 0.364481121301651\n",
      "Epoch: 161040 | Loss: 0.36579495668411255 | Test loss: 0.3644672632217407\n",
      "Epoch: 161050 | Loss: 0.3657831847667694 | Test loss: 0.36445340514183044\n",
      "Epoch: 161060 | Loss: 0.36577141284942627 | Test loss: 0.36443957686424255\n",
      "Epoch: 161070 | Loss: 0.3657596707344055 | Test loss: 0.36442574858665466\n",
      "Epoch: 161080 | Loss: 0.3657478988170624 | Test loss: 0.3644118905067444\n",
      "Epoch: 161090 | Loss: 0.3657361567020416 | Test loss: 0.3643980622291565\n",
      "Epoch: 161100 | Loss: 0.3657244145870209 | Test loss: 0.36438417434692383\n",
      "Epoch: 161110 | Loss: 0.36571261286735535 | Test loss: 0.36437034606933594\n",
      "Epoch: 161120 | Loss: 0.3657008707523346 | Test loss: 0.36435651779174805\n",
      "Epoch: 161130 | Loss: 0.36568909883499146 | Test loss: 0.36434265971183777\n",
      "Epoch: 161140 | Loss: 0.3656773567199707 | Test loss: 0.3643288314342499\n",
      "Epoch: 161150 | Loss: 0.36566561460494995 | Test loss: 0.364315003156662\n",
      "Epoch: 161160 | Loss: 0.3656538426876068 | Test loss: 0.3643011152744293\n",
      "Epoch: 161170 | Loss: 0.36564207077026367 | Test loss: 0.36428728699684143\n",
      "Epoch: 161180 | Loss: 0.3656303286552429 | Test loss: 0.36427342891693115\n",
      "Epoch: 161190 | Loss: 0.3656185269355774 | Test loss: 0.3642595708370209\n",
      "Epoch: 161200 | Loss: 0.36560678482055664 | Test loss: 0.364245742559433\n",
      "Epoch: 161210 | Loss: 0.3655950725078583 | Test loss: 0.3642319142818451\n",
      "Epoch: 161220 | Loss: 0.36558327078819275 | Test loss: 0.3642180562019348\n",
      "Epoch: 161230 | Loss: 0.3655714988708496 | Test loss: 0.36420419812202454\n",
      "Epoch: 161240 | Loss: 0.36555978655815125 | Test loss: 0.36419034004211426\n",
      "Epoch: 161250 | Loss: 0.3655479848384857 | Test loss: 0.36417651176452637\n",
      "Epoch: 161260 | Loss: 0.36553624272346497 | Test loss: 0.3641626536846161\n",
      "Epoch: 161270 | Loss: 0.3655244708061218 | Test loss: 0.3641488254070282\n",
      "Epoch: 161280 | Loss: 0.3655127286911011 | Test loss: 0.3641349673271179\n",
      "Epoch: 161290 | Loss: 0.36550092697143555 | Test loss: 0.36412110924720764\n",
      "Epoch: 161300 | Loss: 0.3654891848564148 | Test loss: 0.36410728096961975\n",
      "Epoch: 161310 | Loss: 0.36547741293907166 | Test loss: 0.3640934228897095\n",
      "Epoch: 161320 | Loss: 0.3654657006263733 | Test loss: 0.3640795648097992\n",
      "Epoch: 161330 | Loss: 0.36545392870903015 | Test loss: 0.3640657365322113\n",
      "Epoch: 161340 | Loss: 0.3654421269893646 | Test loss: 0.3640519082546234\n",
      "Epoch: 161350 | Loss: 0.36543038487434387 | Test loss: 0.36403805017471313\n",
      "Epoch: 161360 | Loss: 0.3654186427593231 | Test loss: 0.36402422189712524\n",
      "Epoch: 161370 | Loss: 0.36540687084198 | Test loss: 0.36401036381721497\n",
      "Epoch: 161380 | Loss: 0.36539512872695923 | Test loss: 0.3639965057373047\n",
      "Epoch: 161390 | Loss: 0.3653833568096161 | Test loss: 0.3639826774597168\n",
      "Epoch: 161400 | Loss: 0.36537158489227295 | Test loss: 0.3639688193798065\n",
      "Epoch: 161410 | Loss: 0.3653598427772522 | Test loss: 0.36395499110221863\n",
      "Epoch: 161420 | Loss: 0.36534807085990906 | Test loss: 0.36394110321998596\n",
      "Epoch: 161430 | Loss: 0.3653363287448883 | Test loss: 0.36392727494239807\n",
      "Epoch: 161440 | Loss: 0.36532455682754517 | Test loss: 0.3639134168624878\n",
      "Epoch: 161450 | Loss: 0.365312784910202 | Test loss: 0.3638995587825775\n",
      "Epoch: 161460 | Loss: 0.3653010129928589 | Test loss: 0.3638857305049896\n",
      "Epoch: 161470 | Loss: 0.36528927087783813 | Test loss: 0.36387190222740173\n",
      "Epoch: 161480 | Loss: 0.365277498960495 | Test loss: 0.36385804414749146\n",
      "Epoch: 161490 | Loss: 0.36526575684547424 | Test loss: 0.36384421586990356\n",
      "Epoch: 161500 | Loss: 0.3652540147304535 | Test loss: 0.3638303279876709\n",
      "Epoch: 161510 | Loss: 0.36524221301078796 | Test loss: 0.363816499710083\n",
      "Epoch: 161520 | Loss: 0.3652304708957672 | Test loss: 0.3638026714324951\n",
      "Epoch: 161530 | Loss: 0.3652186989784241 | Test loss: 0.36378881335258484\n",
      "Epoch: 161540 | Loss: 0.3652069568634033 | Test loss: 0.36377498507499695\n",
      "Epoch: 161550 | Loss: 0.36519521474838257 | Test loss: 0.36376115679740906\n",
      "Epoch: 161560 | Loss: 0.36518344283103943 | Test loss: 0.3637472689151764\n",
      "Epoch: 161570 | Loss: 0.3651716709136963 | Test loss: 0.3637334406375885\n",
      "Epoch: 161580 | Loss: 0.36515992879867554 | Test loss: 0.3637195825576782\n",
      "Epoch: 161590 | Loss: 0.36514812707901 | Test loss: 0.36370572447776794\n",
      "Epoch: 161600 | Loss: 0.36513638496398926 | Test loss: 0.36369189620018005\n",
      "Epoch: 161610 | Loss: 0.3651246726512909 | Test loss: 0.36367806792259216\n",
      "Epoch: 161620 | Loss: 0.36511287093162537 | Test loss: 0.3636642098426819\n",
      "Epoch: 161630 | Loss: 0.3651010990142822 | Test loss: 0.3636503517627716\n",
      "Epoch: 161640 | Loss: 0.36508938670158386 | Test loss: 0.36363649368286133\n",
      "Epoch: 161650 | Loss: 0.36507758498191833 | Test loss: 0.36362266540527344\n",
      "Epoch: 161660 | Loss: 0.3650658428668976 | Test loss: 0.36360880732536316\n",
      "Epoch: 161670 | Loss: 0.36505407094955444 | Test loss: 0.36359497904777527\n",
      "Epoch: 161680 | Loss: 0.3650423288345337 | Test loss: 0.363581120967865\n",
      "Epoch: 161690 | Loss: 0.36503052711486816 | Test loss: 0.3635672628879547\n",
      "Epoch: 161700 | Loss: 0.3650187849998474 | Test loss: 0.3635534346103668\n",
      "Epoch: 161710 | Loss: 0.3650070130825043 | Test loss: 0.36353957653045654\n",
      "Epoch: 161720 | Loss: 0.3649953007698059 | Test loss: 0.36352571845054626\n",
      "Epoch: 161730 | Loss: 0.36498352885246277 | Test loss: 0.3635118901729584\n",
      "Epoch: 161740 | Loss: 0.36497172713279724 | Test loss: 0.3634980618953705\n",
      "Epoch: 161750 | Loss: 0.3649599850177765 | Test loss: 0.3634842038154602\n",
      "Epoch: 161760 | Loss: 0.36494824290275574 | Test loss: 0.3634703755378723\n",
      "Epoch: 161770 | Loss: 0.3649364709854126 | Test loss: 0.36345651745796204\n",
      "Epoch: 161780 | Loss: 0.36492472887039185 | Test loss: 0.36344265937805176\n",
      "Epoch: 161790 | Loss: 0.3649129569530487 | Test loss: 0.36342883110046387\n",
      "Epoch: 161800 | Loss: 0.36490118503570557 | Test loss: 0.3634149730205536\n",
      "Epoch: 161810 | Loss: 0.3648894429206848 | Test loss: 0.3634011447429657\n",
      "Epoch: 161820 | Loss: 0.3648776710033417 | Test loss: 0.36338725686073303\n",
      "Epoch: 161830 | Loss: 0.3648659288883209 | Test loss: 0.36337342858314514\n",
      "Epoch: 161840 | Loss: 0.3648541569709778 | Test loss: 0.36335957050323486\n",
      "Epoch: 161850 | Loss: 0.36484238505363464 | Test loss: 0.3633457124233246\n",
      "Epoch: 161860 | Loss: 0.3648306131362915 | Test loss: 0.3633318841457367\n",
      "Epoch: 161870 | Loss: 0.36481887102127075 | Test loss: 0.3633180558681488\n",
      "Epoch: 161880 | Loss: 0.3648070991039276 | Test loss: 0.3633041977882385\n",
      "Epoch: 161890 | Loss: 0.36479535698890686 | Test loss: 0.36329036951065063\n",
      "Epoch: 161900 | Loss: 0.3647836148738861 | Test loss: 0.36327648162841797\n",
      "Epoch: 161910 | Loss: 0.3647718131542206 | Test loss: 0.3632626533508301\n",
      "Epoch: 161920 | Loss: 0.36476007103919983 | Test loss: 0.3632488250732422\n",
      "Epoch: 161930 | Loss: 0.3647482991218567 | Test loss: 0.3632349669933319\n",
      "Epoch: 161940 | Loss: 0.36473655700683594 | Test loss: 0.363221138715744\n",
      "Epoch: 161950 | Loss: 0.3647248148918152 | Test loss: 0.36320731043815613\n",
      "Epoch: 161960 | Loss: 0.36471304297447205 | Test loss: 0.36319342255592346\n",
      "Epoch: 161970 | Loss: 0.3647012710571289 | Test loss: 0.36317959427833557\n",
      "Epoch: 161980 | Loss: 0.36468952894210815 | Test loss: 0.3631657361984253\n",
      "Epoch: 161990 | Loss: 0.3646777272224426 | Test loss: 0.363151878118515\n",
      "Epoch: 162000 | Loss: 0.3646659851074219 | Test loss: 0.3631380498409271\n",
      "Epoch: 162010 | Loss: 0.3646542727947235 | Test loss: 0.36312422156333923\n",
      "Epoch: 162020 | Loss: 0.364642471075058 | Test loss: 0.36311036348342896\n",
      "Epoch: 162030 | Loss: 0.36463069915771484 | Test loss: 0.3630965054035187\n",
      "Epoch: 162040 | Loss: 0.3646189868450165 | Test loss: 0.3630826473236084\n",
      "Epoch: 162050 | Loss: 0.36460718512535095 | Test loss: 0.3630688190460205\n",
      "Epoch: 162060 | Loss: 0.3645954430103302 | Test loss: 0.36305496096611023\n",
      "Epoch: 162070 | Loss: 0.36458367109298706 | Test loss: 0.36304113268852234\n",
      "Epoch: 162080 | Loss: 0.3645719289779663 | Test loss: 0.36302727460861206\n",
      "Epoch: 162090 | Loss: 0.3645601272583008 | Test loss: 0.3630134165287018\n",
      "Epoch: 162100 | Loss: 0.36454838514328003 | Test loss: 0.3629995882511139\n",
      "Epoch: 162110 | Loss: 0.3645366132259369 | Test loss: 0.3629857301712036\n",
      "Epoch: 162120 | Loss: 0.3645249009132385 | Test loss: 0.36297187209129333\n",
      "Epoch: 162130 | Loss: 0.3645131289958954 | Test loss: 0.36295804381370544\n",
      "Epoch: 162140 | Loss: 0.36450132727622986 | Test loss: 0.36294421553611755\n",
      "Epoch: 162150 | Loss: 0.3644895851612091 | Test loss: 0.3629303574562073\n",
      "Epoch: 162160 | Loss: 0.36447784304618835 | Test loss: 0.3629165291786194\n",
      "Epoch: 162170 | Loss: 0.3644660711288452 | Test loss: 0.3629026710987091\n",
      "Epoch: 162180 | Loss: 0.36445432901382446 | Test loss: 0.36288881301879883\n",
      "Epoch: 162190 | Loss: 0.3644425570964813 | Test loss: 0.36287498474121094\n",
      "Epoch: 162200 | Loss: 0.3644307851791382 | Test loss: 0.36286112666130066\n",
      "Epoch: 162210 | Loss: 0.36441904306411743 | Test loss: 0.36284729838371277\n",
      "Epoch: 162220 | Loss: 0.3644072711467743 | Test loss: 0.3628334105014801\n",
      "Epoch: 162230 | Loss: 0.36439552903175354 | Test loss: 0.3628195822238922\n",
      "Epoch: 162240 | Loss: 0.3643837571144104 | Test loss: 0.36280572414398193\n",
      "Epoch: 162250 | Loss: 0.36437198519706726 | Test loss: 0.36279186606407166\n",
      "Epoch: 162260 | Loss: 0.3643602132797241 | Test loss: 0.36277803778648376\n",
      "Epoch: 162270 | Loss: 0.36434847116470337 | Test loss: 0.3627642095088959\n",
      "Epoch: 162280 | Loss: 0.36433669924736023 | Test loss: 0.3627503514289856\n",
      "Epoch: 162290 | Loss: 0.3643249571323395 | Test loss: 0.3627365231513977\n",
      "Epoch: 162300 | Loss: 0.3643132150173187 | Test loss: 0.36272263526916504\n",
      "Epoch: 162310 | Loss: 0.3643014132976532 | Test loss: 0.36270880699157715\n",
      "Epoch: 162320 | Loss: 0.36428967118263245 | Test loss: 0.36269497871398926\n",
      "Epoch: 162330 | Loss: 0.3642779290676117 | Test loss: 0.362681120634079\n",
      "Epoch: 162340 | Loss: 0.36426615715026855 | Test loss: 0.3626672923564911\n",
      "Epoch: 162350 | Loss: 0.3642544150352478 | Test loss: 0.3626534640789032\n",
      "Epoch: 162360 | Loss: 0.36424264311790466 | Test loss: 0.36263957619667053\n",
      "Epoch: 162370 | Loss: 0.3642308712005615 | Test loss: 0.36262574791908264\n",
      "Epoch: 162380 | Loss: 0.36421912908554077 | Test loss: 0.36261188983917236\n",
      "Epoch: 162390 | Loss: 0.36420732736587524 | Test loss: 0.3625980317592621\n",
      "Epoch: 162400 | Loss: 0.3641955852508545 | Test loss: 0.3625842034816742\n",
      "Epoch: 162410 | Loss: 0.36418387293815613 | Test loss: 0.3625703752040863\n",
      "Epoch: 162420 | Loss: 0.3641720712184906 | Test loss: 0.362556517124176\n",
      "Epoch: 162430 | Loss: 0.36416029930114746 | Test loss: 0.36254265904426575\n",
      "Epoch: 162440 | Loss: 0.3641485869884491 | Test loss: 0.36252880096435547\n",
      "Epoch: 162450 | Loss: 0.36413678526878357 | Test loss: 0.3625149726867676\n",
      "Epoch: 162460 | Loss: 0.3641250431537628 | Test loss: 0.3625011146068573\n",
      "Epoch: 162470 | Loss: 0.3641132712364197 | Test loss: 0.3624872863292694\n",
      "Epoch: 162480 | Loss: 0.3641015291213989 | Test loss: 0.36247342824935913\n",
      "Epoch: 162490 | Loss: 0.3640897274017334 | Test loss: 0.36245957016944885\n",
      "Epoch: 162500 | Loss: 0.36407798528671265 | Test loss: 0.36244574189186096\n",
      "Epoch: 162510 | Loss: 0.3640662133693695 | Test loss: 0.3624318838119507\n",
      "Epoch: 162520 | Loss: 0.36405450105667114 | Test loss: 0.3624180257320404\n",
      "Epoch: 162530 | Loss: 0.364042729139328 | Test loss: 0.3624041974544525\n",
      "Epoch: 162540 | Loss: 0.3640309274196625 | Test loss: 0.3623903691768646\n",
      "Epoch: 162550 | Loss: 0.3640191853046417 | Test loss: 0.36237651109695435\n",
      "Epoch: 162560 | Loss: 0.36400744318962097 | Test loss: 0.36236268281936646\n",
      "Epoch: 162570 | Loss: 0.36399567127227783 | Test loss: 0.3623488247394562\n",
      "Epoch: 162580 | Loss: 0.3639839291572571 | Test loss: 0.3623349666595459\n",
      "Epoch: 162590 | Loss: 0.36397215723991394 | Test loss: 0.362321138381958\n",
      "Epoch: 162600 | Loss: 0.3639603853225708 | Test loss: 0.36230728030204773\n",
      "Epoch: 162610 | Loss: 0.36394864320755005 | Test loss: 0.36229345202445984\n",
      "Epoch: 162620 | Loss: 0.3639368712902069 | Test loss: 0.3622795641422272\n",
      "Epoch: 162630 | Loss: 0.36392512917518616 | Test loss: 0.3622657358646393\n",
      "Epoch: 162640 | Loss: 0.363913357257843 | Test loss: 0.362251877784729\n",
      "Epoch: 162650 | Loss: 0.3639015853404999 | Test loss: 0.3622380197048187\n",
      "Epoch: 162660 | Loss: 0.36388981342315674 | Test loss: 0.36222419142723083\n",
      "Epoch: 162670 | Loss: 0.363878071308136 | Test loss: 0.36221036314964294\n",
      "Epoch: 162680 | Loss: 0.36386629939079285 | Test loss: 0.36219650506973267\n",
      "Epoch: 162690 | Loss: 0.3638545572757721 | Test loss: 0.3621826767921448\n",
      "Epoch: 162700 | Loss: 0.36384281516075134 | Test loss: 0.3621687889099121\n",
      "Epoch: 162710 | Loss: 0.3638310134410858 | Test loss: 0.3621549606323242\n",
      "Epoch: 162720 | Loss: 0.36381927132606506 | Test loss: 0.36214113235473633\n",
      "Epoch: 162730 | Loss: 0.3638075292110443 | Test loss: 0.36212727427482605\n",
      "Epoch: 162740 | Loss: 0.36379575729370117 | Test loss: 0.36211344599723816\n",
      "Epoch: 162750 | Loss: 0.3637840151786804 | Test loss: 0.36209961771965027\n",
      "Epoch: 162760 | Loss: 0.3637722432613373 | Test loss: 0.3620857298374176\n",
      "Epoch: 162770 | Loss: 0.36376047134399414 | Test loss: 0.3620719015598297\n",
      "Epoch: 162780 | Loss: 0.3637487292289734 | Test loss: 0.36205804347991943\n",
      "Epoch: 162790 | Loss: 0.36373692750930786 | Test loss: 0.36204418540000916\n",
      "Epoch: 162800 | Loss: 0.3637251853942871 | Test loss: 0.36203035712242126\n",
      "Epoch: 162810 | Loss: 0.36371347308158875 | Test loss: 0.3620165288448334\n",
      "Epoch: 162820 | Loss: 0.3637016713619232 | Test loss: 0.3620026707649231\n",
      "Epoch: 162830 | Loss: 0.3636898994445801 | Test loss: 0.3619888126850128\n",
      "Epoch: 162840 | Loss: 0.3636781871318817 | Test loss: 0.36197495460510254\n",
      "Epoch: 162850 | Loss: 0.3636663854122162 | Test loss: 0.36196112632751465\n",
      "Epoch: 162860 | Loss: 0.36365464329719543 | Test loss: 0.36194726824760437\n",
      "Epoch: 162870 | Loss: 0.3636428713798523 | Test loss: 0.3619334399700165\n",
      "Epoch: 162880 | Loss: 0.36363112926483154 | Test loss: 0.3619195818901062\n",
      "Epoch: 162890 | Loss: 0.363619327545166 | Test loss: 0.3619057238101959\n",
      "Epoch: 162900 | Loss: 0.36360758543014526 | Test loss: 0.36189189553260803\n",
      "Epoch: 162910 | Loss: 0.3635958135128021 | Test loss: 0.36187803745269775\n",
      "Epoch: 162920 | Loss: 0.36358410120010376 | Test loss: 0.3618641793727875\n",
      "Epoch: 162930 | Loss: 0.3635723292827606 | Test loss: 0.3618503510951996\n",
      "Epoch: 162940 | Loss: 0.3635605275630951 | Test loss: 0.3618365228176117\n",
      "Epoch: 162950 | Loss: 0.36354878544807434 | Test loss: 0.3618226647377014\n",
      "Epoch: 162960 | Loss: 0.3635370433330536 | Test loss: 0.3618088364601135\n",
      "Epoch: 162970 | Loss: 0.36352527141571045 | Test loss: 0.36179497838020325\n",
      "Epoch: 162980 | Loss: 0.3635135293006897 | Test loss: 0.36178112030029297\n",
      "Epoch: 162990 | Loss: 0.36350175738334656 | Test loss: 0.3617672920227051\n",
      "Epoch: 163000 | Loss: 0.3634899854660034 | Test loss: 0.3617534339427948\n",
      "Epoch: 163010 | Loss: 0.36347824335098267 | Test loss: 0.3617396056652069\n",
      "Epoch: 163020 | Loss: 0.3634664714336395 | Test loss: 0.36172571778297424\n",
      "Epoch: 163030 | Loss: 0.3634547293186188 | Test loss: 0.36171188950538635\n",
      "Epoch: 163040 | Loss: 0.36344295740127563 | Test loss: 0.3616980314254761\n",
      "Epoch: 163050 | Loss: 0.3634311854839325 | Test loss: 0.3616841733455658\n",
      "Epoch: 163060 | Loss: 0.36341941356658936 | Test loss: 0.3616703450679779\n",
      "Epoch: 163070 | Loss: 0.3634076714515686 | Test loss: 0.36165651679039\n",
      "Epoch: 163080 | Loss: 0.36339589953422546 | Test loss: 0.36164265871047974\n",
      "Epoch: 163090 | Loss: 0.3633841574192047 | Test loss: 0.36162883043289185\n",
      "Epoch: 163100 | Loss: 0.36337241530418396 | Test loss: 0.3616149425506592\n",
      "Epoch: 163110 | Loss: 0.36336061358451843 | Test loss: 0.3616011142730713\n",
      "Epoch: 163120 | Loss: 0.3633488714694977 | Test loss: 0.3615872859954834\n",
      "Epoch: 163130 | Loss: 0.36333712935447693 | Test loss: 0.3615734279155731\n",
      "Epoch: 163140 | Loss: 0.3633253574371338 | Test loss: 0.36155959963798523\n",
      "Epoch: 163150 | Loss: 0.36331361532211304 | Test loss: 0.36154577136039734\n",
      "Epoch: 163160 | Loss: 0.3633018434047699 | Test loss: 0.3615318834781647\n",
      "Epoch: 163170 | Loss: 0.36329007148742676 | Test loss: 0.3615180552005768\n",
      "Epoch: 163180 | Loss: 0.363278329372406 | Test loss: 0.3615041971206665\n",
      "Epoch: 163190 | Loss: 0.3632665276527405 | Test loss: 0.3614903390407562\n",
      "Epoch: 163200 | Loss: 0.3632547855377197 | Test loss: 0.36147651076316833\n",
      "Epoch: 163210 | Loss: 0.36324307322502136 | Test loss: 0.36146268248558044\n",
      "Epoch: 163220 | Loss: 0.36323127150535583 | Test loss: 0.36144882440567017\n",
      "Epoch: 163230 | Loss: 0.3632194995880127 | Test loss: 0.3614349663257599\n",
      "Epoch: 163240 | Loss: 0.36320778727531433 | Test loss: 0.3614211082458496\n",
      "Epoch: 163250 | Loss: 0.3631959855556488 | Test loss: 0.3614072799682617\n",
      "Epoch: 163260 | Loss: 0.36318424344062805 | Test loss: 0.36139342188835144\n",
      "Epoch: 163270 | Loss: 0.3631724715232849 | Test loss: 0.36137959361076355\n",
      "Epoch: 163280 | Loss: 0.36316072940826416 | Test loss: 0.36136573553085327\n",
      "Epoch: 163290 | Loss: 0.36314892768859863 | Test loss: 0.361351877450943\n",
      "Epoch: 163300 | Loss: 0.3631371855735779 | Test loss: 0.3613380491733551\n",
      "Epoch: 163310 | Loss: 0.36312541365623474 | Test loss: 0.3613241910934448\n",
      "Epoch: 163320 | Loss: 0.3631137013435364 | Test loss: 0.36131033301353455\n",
      "Epoch: 163330 | Loss: 0.36310192942619324 | Test loss: 0.36129650473594666\n",
      "Epoch: 163340 | Loss: 0.3630901277065277 | Test loss: 0.36128267645835876\n",
      "Epoch: 163350 | Loss: 0.36307838559150696 | Test loss: 0.3612688183784485\n",
      "Epoch: 163360 | Loss: 0.3630666434764862 | Test loss: 0.3612549901008606\n",
      "Epoch: 163370 | Loss: 0.36305487155914307 | Test loss: 0.3612411320209503\n",
      "Epoch: 163380 | Loss: 0.3630431294441223 | Test loss: 0.36122727394104004\n",
      "Epoch: 163390 | Loss: 0.3630313575267792 | Test loss: 0.36121344566345215\n",
      "Epoch: 163400 | Loss: 0.36301958560943604 | Test loss: 0.36119958758354187\n",
      "Epoch: 163410 | Loss: 0.3630078434944153 | Test loss: 0.361185759305954\n",
      "Epoch: 163420 | Loss: 0.36299607157707214 | Test loss: 0.3611718714237213\n",
      "Epoch: 163430 | Loss: 0.3629843294620514 | Test loss: 0.3611580431461334\n",
      "Epoch: 163440 | Loss: 0.36297255754470825 | Test loss: 0.36114418506622314\n",
      "Epoch: 163450 | Loss: 0.3629607856273651 | Test loss: 0.36113032698631287\n",
      "Epoch: 163460 | Loss: 0.362949013710022 | Test loss: 0.361116498708725\n",
      "Epoch: 163470 | Loss: 0.3629372715950012 | Test loss: 0.3611026704311371\n",
      "Epoch: 163480 | Loss: 0.3629254996776581 | Test loss: 0.3610888123512268\n",
      "Epoch: 163490 | Loss: 0.36291375756263733 | Test loss: 0.3610749840736389\n",
      "Epoch: 163500 | Loss: 0.3629020154476166 | Test loss: 0.36106109619140625\n",
      "Epoch: 163510 | Loss: 0.36289021372795105 | Test loss: 0.36104726791381836\n",
      "Epoch: 163520 | Loss: 0.3628784716129303 | Test loss: 0.36103343963623047\n",
      "Epoch: 163530 | Loss: 0.36286672949790955 | Test loss: 0.3610195815563202\n",
      "Epoch: 163540 | Loss: 0.3628549575805664 | Test loss: 0.3610057532787323\n",
      "Epoch: 163550 | Loss: 0.36284321546554565 | Test loss: 0.3609919250011444\n",
      "Epoch: 163560 | Loss: 0.3628314435482025 | Test loss: 0.36097803711891174\n",
      "Epoch: 163570 | Loss: 0.3628196716308594 | Test loss: 0.36096420884132385\n",
      "Epoch: 163580 | Loss: 0.3628079295158386 | Test loss: 0.3609503507614136\n",
      "Epoch: 163590 | Loss: 0.3627961277961731 | Test loss: 0.3609364926815033\n",
      "Epoch: 163600 | Loss: 0.36278438568115234 | Test loss: 0.3609226644039154\n",
      "Epoch: 163610 | Loss: 0.362772673368454 | Test loss: 0.3609088361263275\n",
      "Epoch: 163620 | Loss: 0.36276087164878845 | Test loss: 0.36089497804641724\n",
      "Epoch: 163630 | Loss: 0.3627490997314453 | Test loss: 0.36088111996650696\n",
      "Epoch: 163640 | Loss: 0.36273738741874695 | Test loss: 0.3608672618865967\n",
      "Epoch: 163650 | Loss: 0.3627255856990814 | Test loss: 0.3608534336090088\n",
      "Epoch: 163660 | Loss: 0.36271384358406067 | Test loss: 0.3608395755290985\n",
      "Epoch: 163670 | Loss: 0.36270207166671753 | Test loss: 0.3608257472515106\n",
      "Epoch: 163680 | Loss: 0.3626903295516968 | Test loss: 0.36081188917160034\n",
      "Epoch: 163690 | Loss: 0.36267852783203125 | Test loss: 0.36079803109169006\n",
      "Epoch: 163700 | Loss: 0.3626667857170105 | Test loss: 0.3607842028141022\n",
      "Epoch: 163710 | Loss: 0.36265501379966736 | Test loss: 0.3607703447341919\n",
      "Epoch: 163720 | Loss: 0.362643301486969 | Test loss: 0.3607564866542816\n",
      "Epoch: 163730 | Loss: 0.36263152956962585 | Test loss: 0.3607426583766937\n",
      "Epoch: 163740 | Loss: 0.3626197278499603 | Test loss: 0.36072883009910583\n",
      "Epoch: 163750 | Loss: 0.3626079857349396 | Test loss: 0.36071497201919556\n",
      "Epoch: 163760 | Loss: 0.3625962436199188 | Test loss: 0.36070114374160767\n",
      "Epoch: 163770 | Loss: 0.3625844717025757 | Test loss: 0.3606872856616974\n",
      "Epoch: 163780 | Loss: 0.36257272958755493 | Test loss: 0.3606734275817871\n",
      "Epoch: 163790 | Loss: 0.3625609576702118 | Test loss: 0.3606595993041992\n",
      "Epoch: 163800 | Loss: 0.36254918575286865 | Test loss: 0.36064574122428894\n",
      "Epoch: 163810 | Loss: 0.3625374436378479 | Test loss: 0.36063191294670105\n",
      "Epoch: 163820 | Loss: 0.36252567172050476 | Test loss: 0.3606180250644684\n",
      "Epoch: 163830 | Loss: 0.362513929605484 | Test loss: 0.3606041967868805\n",
      "Epoch: 163840 | Loss: 0.36250215768814087 | Test loss: 0.3605903387069702\n",
      "Epoch: 163850 | Loss: 0.36249038577079773 | Test loss: 0.36057648062705994\n",
      "Epoch: 163860 | Loss: 0.3624786138534546 | Test loss: 0.36056265234947205\n",
      "Epoch: 163870 | Loss: 0.36246687173843384 | Test loss: 0.36054882407188416\n",
      "Epoch: 163880 | Loss: 0.3624550998210907 | Test loss: 0.3605349659919739\n",
      "Epoch: 163890 | Loss: 0.36244335770606995 | Test loss: 0.360521137714386\n",
      "Epoch: 163900 | Loss: 0.3624316155910492 | Test loss: 0.3605072498321533\n",
      "Epoch: 163910 | Loss: 0.36241981387138367 | Test loss: 0.36049342155456543\n",
      "Epoch: 163920 | Loss: 0.3624080717563629 | Test loss: 0.36047959327697754\n",
      "Epoch: 163930 | Loss: 0.36239632964134216 | Test loss: 0.36046573519706726\n",
      "Epoch: 163940 | Loss: 0.362384557723999 | Test loss: 0.36045190691947937\n",
      "Epoch: 163950 | Loss: 0.36237281560897827 | Test loss: 0.3604380786418915\n",
      "Epoch: 163960 | Loss: 0.36236104369163513 | Test loss: 0.3604241907596588\n",
      "Epoch: 163970 | Loss: 0.362349271774292 | Test loss: 0.3604103624820709\n",
      "Epoch: 163980 | Loss: 0.36233752965927124 | Test loss: 0.36039650440216064\n",
      "Epoch: 163990 | Loss: 0.3623257279396057 | Test loss: 0.36038264632225037\n",
      "Epoch: 164000 | Loss: 0.36231398582458496 | Test loss: 0.3603688180446625\n",
      "Epoch: 164010 | Loss: 0.3623022735118866 | Test loss: 0.3603549897670746\n",
      "Epoch: 164020 | Loss: 0.36229047179222107 | Test loss: 0.3603411316871643\n",
      "Epoch: 164030 | Loss: 0.36227869987487793 | Test loss: 0.36032727360725403\n",
      "Epoch: 164040 | Loss: 0.36226698756217957 | Test loss: 0.36031341552734375\n",
      "Epoch: 164050 | Loss: 0.36225518584251404 | Test loss: 0.36029958724975586\n",
      "Epoch: 164060 | Loss: 0.3622434437274933 | Test loss: 0.3602857291698456\n",
      "Epoch: 164070 | Loss: 0.36223167181015015 | Test loss: 0.3602719008922577\n",
      "Epoch: 164080 | Loss: 0.3622199296951294 | Test loss: 0.3602580428123474\n",
      "Epoch: 164090 | Loss: 0.36220812797546387 | Test loss: 0.36024418473243713\n",
      "Epoch: 164100 | Loss: 0.3621963858604431 | Test loss: 0.36023035645484924\n",
      "Epoch: 164110 | Loss: 0.3621846139431 | Test loss: 0.36021649837493896\n",
      "Epoch: 164120 | Loss: 0.3621729016304016 | Test loss: 0.3602026402950287\n",
      "Epoch: 164130 | Loss: 0.36216112971305847 | Test loss: 0.3601888120174408\n",
      "Epoch: 164140 | Loss: 0.36214932799339294 | Test loss: 0.3601749837398529\n",
      "Epoch: 164150 | Loss: 0.3621375858783722 | Test loss: 0.3601611256599426\n",
      "Epoch: 164160 | Loss: 0.36212584376335144 | Test loss: 0.36014729738235474\n",
      "Epoch: 164170 | Loss: 0.3621140718460083 | Test loss: 0.36013343930244446\n",
      "Epoch: 164180 | Loss: 0.36210232973098755 | Test loss: 0.3601195812225342\n",
      "Epoch: 164190 | Loss: 0.3620905578136444 | Test loss: 0.3601057529449463\n",
      "Epoch: 164200 | Loss: 0.36207878589630127 | Test loss: 0.360091894865036\n",
      "Epoch: 164210 | Loss: 0.3620670437812805 | Test loss: 0.3600780665874481\n",
      "Epoch: 164220 | Loss: 0.3620552718639374 | Test loss: 0.36006417870521545\n",
      "Epoch: 164230 | Loss: 0.3620435297489166 | Test loss: 0.36005035042762756\n",
      "Epoch: 164240 | Loss: 0.3620317578315735 | Test loss: 0.3600364923477173\n",
      "Epoch: 164250 | Loss: 0.36201998591423035 | Test loss: 0.360022634267807\n",
      "Epoch: 164260 | Loss: 0.3620082139968872 | Test loss: 0.3600088059902191\n",
      "Epoch: 164270 | Loss: 0.36199647188186646 | Test loss: 0.3599949777126312\n",
      "Epoch: 164280 | Loss: 0.3619846999645233 | Test loss: 0.35998111963272095\n",
      "Epoch: 164290 | Loss: 0.36197295784950256 | Test loss: 0.35996729135513306\n",
      "Epoch: 164300 | Loss: 0.3619612157344818 | Test loss: 0.3599534034729004\n",
      "Epoch: 164310 | Loss: 0.3619494140148163 | Test loss: 0.3599395751953125\n",
      "Epoch: 164320 | Loss: 0.36193767189979553 | Test loss: 0.3599257469177246\n",
      "Epoch: 164330 | Loss: 0.3619259297847748 | Test loss: 0.35991188883781433\n",
      "Epoch: 164340 | Loss: 0.36191415786743164 | Test loss: 0.35989806056022644\n",
      "Epoch: 164350 | Loss: 0.3619024157524109 | Test loss: 0.35988423228263855\n",
      "Epoch: 164360 | Loss: 0.36189064383506775 | Test loss: 0.3598703444004059\n",
      "Epoch: 164370 | Loss: 0.3618788719177246 | Test loss: 0.359856516122818\n",
      "Epoch: 164380 | Loss: 0.36186712980270386 | Test loss: 0.3598426580429077\n",
      "Epoch: 164390 | Loss: 0.36185532808303833 | Test loss: 0.35982879996299744\n",
      "Epoch: 164400 | Loss: 0.3618435859680176 | Test loss: 0.35981497168540955\n",
      "Epoch: 164410 | Loss: 0.3618318736553192 | Test loss: 0.35980114340782166\n",
      "Epoch: 164420 | Loss: 0.3618200719356537 | Test loss: 0.3597872853279114\n",
      "Epoch: 164430 | Loss: 0.36180830001831055 | Test loss: 0.3597734272480011\n",
      "Epoch: 164440 | Loss: 0.3617965877056122 | Test loss: 0.3597595691680908\n",
      "Epoch: 164450 | Loss: 0.36178478598594666 | Test loss: 0.35974574089050293\n",
      "Epoch: 164460 | Loss: 0.3617730438709259 | Test loss: 0.35973188281059265\n",
      "Epoch: 164470 | Loss: 0.36176127195358276 | Test loss: 0.35971805453300476\n",
      "Epoch: 164480 | Loss: 0.361749529838562 | Test loss: 0.3597041964530945\n",
      "Epoch: 164490 | Loss: 0.3617377281188965 | Test loss: 0.3596903383731842\n",
      "Epoch: 164500 | Loss: 0.36172598600387573 | Test loss: 0.3596765100955963\n",
      "Epoch: 164510 | Loss: 0.3617142140865326 | Test loss: 0.35966265201568604\n",
      "Epoch: 164520 | Loss: 0.36170250177383423 | Test loss: 0.35964879393577576\n",
      "Epoch: 164530 | Loss: 0.3616907298564911 | Test loss: 0.35963496565818787\n",
      "Epoch: 164540 | Loss: 0.36167892813682556 | Test loss: 0.3596211373806\n",
      "Epoch: 164550 | Loss: 0.3616671860218048 | Test loss: 0.3596072793006897\n",
      "Epoch: 164560 | Loss: 0.36165544390678406 | Test loss: 0.3595934510231018\n",
      "Epoch: 164570 | Loss: 0.3616436719894409 | Test loss: 0.35957959294319153\n",
      "Epoch: 164580 | Loss: 0.36163192987442017 | Test loss: 0.35956573486328125\n",
      "Epoch: 164590 | Loss: 0.361620157957077 | Test loss: 0.35955190658569336\n",
      "Epoch: 164600 | Loss: 0.3616083860397339 | Test loss: 0.3595380485057831\n",
      "Epoch: 164610 | Loss: 0.36159664392471313 | Test loss: 0.3595242202281952\n",
      "Epoch: 164620 | Loss: 0.36158487200737 | Test loss: 0.3595103323459625\n",
      "Epoch: 164630 | Loss: 0.36157312989234924 | Test loss: 0.35949650406837463\n",
      "Epoch: 164640 | Loss: 0.3615613579750061 | Test loss: 0.35948264598846436\n",
      "Epoch: 164650 | Loss: 0.36154958605766296 | Test loss: 0.3594687879085541\n",
      "Epoch: 164660 | Loss: 0.3615378141403198 | Test loss: 0.3594549596309662\n",
      "Epoch: 164670 | Loss: 0.3615260720252991 | Test loss: 0.3594411313533783\n",
      "Epoch: 164680 | Loss: 0.36151430010795593 | Test loss: 0.359427273273468\n",
      "Epoch: 164690 | Loss: 0.3615025579929352 | Test loss: 0.3594134449958801\n",
      "Epoch: 164700 | Loss: 0.36149081587791443 | Test loss: 0.35939955711364746\n",
      "Epoch: 164710 | Loss: 0.3614790141582489 | Test loss: 0.35938572883605957\n",
      "Epoch: 164720 | Loss: 0.36146727204322815 | Test loss: 0.3593719005584717\n",
      "Epoch: 164730 | Loss: 0.3614555299282074 | Test loss: 0.3593580424785614\n",
      "Epoch: 164740 | Loss: 0.36144375801086426 | Test loss: 0.3593442142009735\n",
      "Epoch: 164750 | Loss: 0.3614320158958435 | Test loss: 0.3593303859233856\n",
      "Epoch: 164760 | Loss: 0.36142024397850037 | Test loss: 0.35931649804115295\n",
      "Epoch: 164770 | Loss: 0.3614084720611572 | Test loss: 0.35930266976356506\n",
      "Epoch: 164780 | Loss: 0.3613967299461365 | Test loss: 0.3592888116836548\n",
      "Epoch: 164790 | Loss: 0.36138492822647095 | Test loss: 0.3592749536037445\n",
      "Epoch: 164800 | Loss: 0.3613731861114502 | Test loss: 0.3592611253261566\n",
      "Epoch: 164810 | Loss: 0.36136147379875183 | Test loss: 0.3592472970485687\n",
      "Epoch: 164820 | Loss: 0.3613496720790863 | Test loss: 0.35923343896865845\n",
      "Epoch: 164830 | Loss: 0.36133790016174316 | Test loss: 0.35921958088874817\n",
      "Epoch: 164840 | Loss: 0.3613261878490448 | Test loss: 0.3592057228088379\n",
      "Epoch: 164850 | Loss: 0.3613143861293793 | Test loss: 0.35919189453125\n",
      "Epoch: 164860 | Loss: 0.3613026440143585 | Test loss: 0.3591780364513397\n",
      "Epoch: 164870 | Loss: 0.3612908720970154 | Test loss: 0.35916420817375183\n",
      "Epoch: 164880 | Loss: 0.36127912998199463 | Test loss: 0.35915035009384155\n",
      "Epoch: 164890 | Loss: 0.3612673282623291 | Test loss: 0.3591364920139313\n",
      "Epoch: 164900 | Loss: 0.36125558614730835 | Test loss: 0.3591226637363434\n",
      "Epoch: 164910 | Loss: 0.3612438142299652 | Test loss: 0.3591088056564331\n",
      "Epoch: 164920 | Loss: 0.36123210191726685 | Test loss: 0.3590949475765228\n",
      "Epoch: 164930 | Loss: 0.3612203299999237 | Test loss: 0.35908111929893494\n",
      "Epoch: 164940 | Loss: 0.3612085282802582 | Test loss: 0.35906729102134705\n",
      "Epoch: 164950 | Loss: 0.3611967861652374 | Test loss: 0.35905343294143677\n",
      "Epoch: 164960 | Loss: 0.3611850440502167 | Test loss: 0.3590396046638489\n",
      "Epoch: 164970 | Loss: 0.36117327213287354 | Test loss: 0.3590257465839386\n",
      "Epoch: 164980 | Loss: 0.3611615300178528 | Test loss: 0.3590118885040283\n",
      "Epoch: 164990 | Loss: 0.36114975810050964 | Test loss: 0.35899806022644043\n",
      "Epoch: 165000 | Loss: 0.3611379861831665 | Test loss: 0.35898420214653015\n",
      "Epoch: 165010 | Loss: 0.36112624406814575 | Test loss: 0.35897037386894226\n",
      "Epoch: 165020 | Loss: 0.3611144721508026 | Test loss: 0.3589564859867096\n",
      "Epoch: 165030 | Loss: 0.36110273003578186 | Test loss: 0.3589426577091217\n",
      "Epoch: 165040 | Loss: 0.3610909581184387 | Test loss: 0.3589287996292114\n",
      "Epoch: 165050 | Loss: 0.3610791862010956 | Test loss: 0.35891494154930115\n",
      "Epoch: 165060 | Loss: 0.36106741428375244 | Test loss: 0.35890111327171326\n",
      "Epoch: 165070 | Loss: 0.3610556721687317 | Test loss: 0.35888728499412537\n",
      "Epoch: 165080 | Loss: 0.36104390025138855 | Test loss: 0.3588734269142151\n",
      "Epoch: 165090 | Loss: 0.3610321581363678 | Test loss: 0.3588595986366272\n",
      "Epoch: 165100 | Loss: 0.36102041602134705 | Test loss: 0.35884571075439453\n",
      "Epoch: 165110 | Loss: 0.3610086143016815 | Test loss: 0.35883188247680664\n",
      "Epoch: 165120 | Loss: 0.36099687218666077 | Test loss: 0.35881805419921875\n",
      "Epoch: 165130 | Loss: 0.36098513007164 | Test loss: 0.35880419611930847\n",
      "Epoch: 165140 | Loss: 0.3609733581542969 | Test loss: 0.3587903678417206\n",
      "Epoch: 165150 | Loss: 0.3609616160392761 | Test loss: 0.3587765395641327\n",
      "Epoch: 165160 | Loss: 0.360949844121933 | Test loss: 0.3587626516819\n",
      "Epoch: 165170 | Loss: 0.36093807220458984 | Test loss: 0.35874882340431213\n",
      "Epoch: 165180 | Loss: 0.3609263300895691 | Test loss: 0.35873496532440186\n",
      "Epoch: 165190 | Loss: 0.36091452836990356 | Test loss: 0.3587211072444916\n",
      "Epoch: 165200 | Loss: 0.3609027862548828 | Test loss: 0.3587072789669037\n",
      "Epoch: 165210 | Loss: 0.36089107394218445 | Test loss: 0.3586934506893158\n",
      "Epoch: 165220 | Loss: 0.3608792722225189 | Test loss: 0.3586795926094055\n",
      "Epoch: 165230 | Loss: 0.3608675003051758 | Test loss: 0.35866573452949524\n",
      "Epoch: 165240 | Loss: 0.3608557879924774 | Test loss: 0.35865187644958496\n",
      "Epoch: 165250 | Loss: 0.3608439862728119 | Test loss: 0.35863804817199707\n",
      "Epoch: 165260 | Loss: 0.36083224415779114 | Test loss: 0.3586241900920868\n",
      "Epoch: 165270 | Loss: 0.360820472240448 | Test loss: 0.3586103618144989\n",
      "Epoch: 165280 | Loss: 0.36080873012542725 | Test loss: 0.3585965037345886\n",
      "Epoch: 165290 | Loss: 0.3607969284057617 | Test loss: 0.35858264565467834\n",
      "Epoch: 165300 | Loss: 0.36078518629074097 | Test loss: 0.35856881737709045\n",
      "Epoch: 165310 | Loss: 0.3607734143733978 | Test loss: 0.3585549592971802\n",
      "Epoch: 165320 | Loss: 0.36076170206069946 | Test loss: 0.3585411012172699\n",
      "Epoch: 165330 | Loss: 0.3607499301433563 | Test loss: 0.358527272939682\n",
      "Epoch: 165340 | Loss: 0.3607381284236908 | Test loss: 0.3585134446620941\n",
      "Epoch: 165350 | Loss: 0.36072638630867004 | Test loss: 0.35849958658218384\n",
      "Epoch: 165360 | Loss: 0.3607146441936493 | Test loss: 0.35848575830459595\n",
      "Epoch: 165370 | Loss: 0.36070287227630615 | Test loss: 0.35847190022468567\n",
      "Epoch: 165380 | Loss: 0.3606911301612854 | Test loss: 0.3584580421447754\n",
      "Epoch: 165390 | Loss: 0.36067935824394226 | Test loss: 0.3584442138671875\n",
      "Epoch: 165400 | Loss: 0.3606675863265991 | Test loss: 0.3584303557872772\n",
      "Epoch: 165410 | Loss: 0.36065584421157837 | Test loss: 0.35841652750968933\n",
      "Epoch: 165420 | Loss: 0.36064407229423523 | Test loss: 0.35840263962745667\n",
      "Epoch: 165430 | Loss: 0.3606323301792145 | Test loss: 0.3583888113498688\n",
      "Epoch: 165440 | Loss: 0.36062055826187134 | Test loss: 0.3583749532699585\n",
      "Epoch: 165450 | Loss: 0.3606087863445282 | Test loss: 0.3583610951900482\n",
      "Epoch: 165460 | Loss: 0.36059701442718506 | Test loss: 0.3583472669124603\n",
      "Epoch: 165470 | Loss: 0.3605852723121643 | Test loss: 0.35833343863487244\n",
      "Epoch: 165480 | Loss: 0.36057350039482117 | Test loss: 0.35831958055496216\n",
      "Epoch: 165490 | Loss: 0.3605617582798004 | Test loss: 0.35830575227737427\n",
      "Epoch: 165500 | Loss: 0.36055001616477966 | Test loss: 0.3582918643951416\n",
      "Epoch: 165510 | Loss: 0.36053821444511414 | Test loss: 0.3582780361175537\n",
      "Epoch: 165520 | Loss: 0.3605264723300934 | Test loss: 0.3582642078399658\n",
      "Epoch: 165530 | Loss: 0.36051473021507263 | Test loss: 0.35825034976005554\n",
      "Epoch: 165540 | Loss: 0.3605029582977295 | Test loss: 0.35823652148246765\n",
      "Epoch: 165550 | Loss: 0.36049121618270874 | Test loss: 0.35822269320487976\n",
      "Epoch: 165560 | Loss: 0.3604794442653656 | Test loss: 0.3582088053226471\n",
      "Epoch: 165570 | Loss: 0.36046767234802246 | Test loss: 0.3581949770450592\n",
      "Epoch: 165580 | Loss: 0.3604559302330017 | Test loss: 0.3581811189651489\n",
      "Epoch: 165590 | Loss: 0.3604441285133362 | Test loss: 0.35816726088523865\n",
      "Epoch: 165600 | Loss: 0.36043238639831543 | Test loss: 0.35815343260765076\n",
      "Epoch: 165610 | Loss: 0.36042067408561707 | Test loss: 0.35813960433006287\n",
      "Epoch: 165620 | Loss: 0.36040887236595154 | Test loss: 0.3581257462501526\n",
      "Epoch: 165630 | Loss: 0.3603971004486084 | Test loss: 0.3581118881702423\n",
      "Epoch: 165640 | Loss: 0.36038538813591003 | Test loss: 0.35809803009033203\n",
      "Epoch: 165650 | Loss: 0.3603735864162445 | Test loss: 0.35808420181274414\n",
      "Epoch: 165660 | Loss: 0.36036184430122375 | Test loss: 0.35807034373283386\n",
      "Epoch: 165670 | Loss: 0.3603500723838806 | Test loss: 0.35805651545524597\n",
      "Epoch: 165680 | Loss: 0.36033833026885986 | Test loss: 0.3580426573753357\n",
      "Epoch: 165690 | Loss: 0.36032652854919434 | Test loss: 0.3580287992954254\n",
      "Epoch: 165700 | Loss: 0.3603147864341736 | Test loss: 0.3580149710178375\n",
      "Epoch: 165710 | Loss: 0.36030301451683044 | Test loss: 0.35800111293792725\n",
      "Epoch: 165720 | Loss: 0.3602913022041321 | Test loss: 0.35798725485801697\n",
      "Epoch: 165730 | Loss: 0.36027953028678894 | Test loss: 0.3579734265804291\n",
      "Epoch: 165740 | Loss: 0.3602677285671234 | Test loss: 0.3579595983028412\n",
      "Epoch: 165750 | Loss: 0.36025598645210266 | Test loss: 0.3579457402229309\n",
      "Epoch: 165760 | Loss: 0.3602442443370819 | Test loss: 0.357931911945343\n",
      "Epoch: 165770 | Loss: 0.36023247241973877 | Test loss: 0.35791805386543274\n",
      "Epoch: 165780 | Loss: 0.360220730304718 | Test loss: 0.35790419578552246\n",
      "Epoch: 165790 | Loss: 0.3602089583873749 | Test loss: 0.35789036750793457\n",
      "Epoch: 165800 | Loss: 0.36019718647003174 | Test loss: 0.3578765094280243\n",
      "Epoch: 165810 | Loss: 0.360185444355011 | Test loss: 0.3578626811504364\n",
      "Epoch: 165820 | Loss: 0.36017367243766785 | Test loss: 0.35784879326820374\n",
      "Epoch: 165830 | Loss: 0.3601619303226471 | Test loss: 0.35783496499061584\n",
      "Epoch: 165840 | Loss: 0.36015015840530396 | Test loss: 0.35782110691070557\n",
      "Epoch: 165850 | Loss: 0.3601383864879608 | Test loss: 0.3578072488307953\n",
      "Epoch: 165860 | Loss: 0.3601266145706177 | Test loss: 0.3577934205532074\n",
      "Epoch: 165870 | Loss: 0.3601148724555969 | Test loss: 0.3577795922756195\n",
      "Epoch: 165880 | Loss: 0.3601031005382538 | Test loss: 0.35776573419570923\n",
      "Epoch: 165890 | Loss: 0.36009135842323303 | Test loss: 0.35775190591812134\n",
      "Epoch: 165900 | Loss: 0.3600796163082123 | Test loss: 0.35773801803588867\n",
      "Epoch: 165910 | Loss: 0.36006781458854675 | Test loss: 0.3577241897583008\n",
      "Epoch: 165920 | Loss: 0.360056072473526 | Test loss: 0.3577103614807129\n",
      "Epoch: 165930 | Loss: 0.36004433035850525 | Test loss: 0.3576965034008026\n",
      "Epoch: 165940 | Loss: 0.3600325584411621 | Test loss: 0.3576826751232147\n",
      "Epoch: 165950 | Loss: 0.36002081632614136 | Test loss: 0.35766884684562683\n",
      "Epoch: 165960 | Loss: 0.3600090444087982 | Test loss: 0.35765495896339417\n",
      "Epoch: 165970 | Loss: 0.3599972724914551 | Test loss: 0.3576411306858063\n",
      "Epoch: 165980 | Loss: 0.3599855303764343 | Test loss: 0.357627272605896\n",
      "Epoch: 165990 | Loss: 0.3599737286567688 | Test loss: 0.3576134145259857\n",
      "Epoch: 166000 | Loss: 0.35996198654174805 | Test loss: 0.3575995862483978\n",
      "Epoch: 166010 | Loss: 0.3599502742290497 | Test loss: 0.35758575797080994\n",
      "Epoch: 166020 | Loss: 0.35993847250938416 | Test loss: 0.35757189989089966\n",
      "Epoch: 166030 | Loss: 0.359926700592041 | Test loss: 0.3575580418109894\n",
      "Epoch: 166040 | Loss: 0.35991498827934265 | Test loss: 0.3575441837310791\n",
      "Epoch: 166050 | Loss: 0.3599031865596771 | Test loss: 0.3575303554534912\n",
      "Epoch: 166060 | Loss: 0.35989144444465637 | Test loss: 0.35751649737358093\n",
      "Epoch: 166070 | Loss: 0.35987967252731323 | Test loss: 0.35750266909599304\n",
      "Epoch: 166080 | Loss: 0.3598679304122925 | Test loss: 0.35748881101608276\n",
      "Epoch: 166090 | Loss: 0.35985612869262695 | Test loss: 0.3574749529361725\n",
      "Epoch: 166100 | Loss: 0.3598443865776062 | Test loss: 0.3574611246585846\n",
      "Epoch: 166110 | Loss: 0.35983261466026306 | Test loss: 0.3574472665786743\n",
      "Epoch: 166120 | Loss: 0.3598209023475647 | Test loss: 0.35743340849876404\n",
      "Epoch: 166130 | Loss: 0.35980913043022156 | Test loss: 0.35741958022117615\n",
      "Epoch: 166140 | Loss: 0.35979732871055603 | Test loss: 0.35740575194358826\n",
      "Epoch: 166150 | Loss: 0.3597855865955353 | Test loss: 0.357391893863678\n",
      "Epoch: 166160 | Loss: 0.3597738444805145 | Test loss: 0.3573780655860901\n",
      "Epoch: 166170 | Loss: 0.3597620725631714 | Test loss: 0.3573642075061798\n",
      "Epoch: 166180 | Loss: 0.35975033044815063 | Test loss: 0.35735034942626953\n",
      "Epoch: 166190 | Loss: 0.3597385585308075 | Test loss: 0.35733652114868164\n",
      "Epoch: 166200 | Loss: 0.35972678661346436 | Test loss: 0.35732266306877136\n",
      "Epoch: 166210 | Loss: 0.3597150444984436 | Test loss: 0.35730883479118347\n",
      "Epoch: 166220 | Loss: 0.35970327258110046 | Test loss: 0.3572949469089508\n",
      "Epoch: 166230 | Loss: 0.3596915304660797 | Test loss: 0.3572811186313629\n",
      "Epoch: 166240 | Loss: 0.3596797585487366 | Test loss: 0.35726726055145264\n",
      "Epoch: 166250 | Loss: 0.35966798663139343 | Test loss: 0.35725340247154236\n",
      "Epoch: 166260 | Loss: 0.3596562147140503 | Test loss: 0.35723957419395447\n",
      "Epoch: 166270 | Loss: 0.35964447259902954 | Test loss: 0.3572257459163666\n",
      "Epoch: 166280 | Loss: 0.3596327006816864 | Test loss: 0.3572118878364563\n",
      "Epoch: 166290 | Loss: 0.35962095856666565 | Test loss: 0.3571980595588684\n",
      "Epoch: 166300 | Loss: 0.3596092164516449 | Test loss: 0.35718417167663574\n",
      "Epoch: 166310 | Loss: 0.35959741473197937 | Test loss: 0.35717034339904785\n",
      "Epoch: 166320 | Loss: 0.3595856726169586 | Test loss: 0.35715651512145996\n",
      "Epoch: 166330 | Loss: 0.35957393050193787 | Test loss: 0.3571426570415497\n",
      "Epoch: 166340 | Loss: 0.3595621585845947 | Test loss: 0.3571288287639618\n",
      "Epoch: 166350 | Loss: 0.359550416469574 | Test loss: 0.3571150004863739\n",
      "Epoch: 166360 | Loss: 0.35953864455223083 | Test loss: 0.35710111260414124\n",
      "Epoch: 166370 | Loss: 0.3595268726348877 | Test loss: 0.35708728432655334\n",
      "Epoch: 166380 | Loss: 0.35951513051986694 | Test loss: 0.35707342624664307\n",
      "Epoch: 166390 | Loss: 0.3595033288002014 | Test loss: 0.3570595681667328\n",
      "Epoch: 166400 | Loss: 0.35949158668518066 | Test loss: 0.3570457398891449\n",
      "Epoch: 166410 | Loss: 0.3594798743724823 | Test loss: 0.357031911611557\n",
      "Epoch: 166420 | Loss: 0.3594680726528168 | Test loss: 0.35701805353164673\n",
      "Epoch: 166430 | Loss: 0.35945630073547363 | Test loss: 0.35700419545173645\n",
      "Epoch: 166440 | Loss: 0.35944458842277527 | Test loss: 0.35699033737182617\n",
      "Epoch: 166450 | Loss: 0.35943278670310974 | Test loss: 0.3569765090942383\n",
      "Epoch: 166460 | Loss: 0.359421044588089 | Test loss: 0.356962651014328\n",
      "Epoch: 166470 | Loss: 0.35940927267074585 | Test loss: 0.3569488227367401\n",
      "Epoch: 166480 | Loss: 0.3593975305557251 | Test loss: 0.35693496465682983\n",
      "Epoch: 166490 | Loss: 0.35938572883605957 | Test loss: 0.35692110657691956\n",
      "Epoch: 166500 | Loss: 0.3593739867210388 | Test loss: 0.35690727829933167\n",
      "Epoch: 166510 | Loss: 0.3593622148036957 | Test loss: 0.3568934202194214\n",
      "Epoch: 166520 | Loss: 0.3593505024909973 | Test loss: 0.3568795621395111\n",
      "Epoch: 166530 | Loss: 0.3593387305736542 | Test loss: 0.3568657338619232\n",
      "Epoch: 166540 | Loss: 0.35932692885398865 | Test loss: 0.3568519055843353\n",
      "Epoch: 166550 | Loss: 0.3593151867389679 | Test loss: 0.35683804750442505\n",
      "Epoch: 166560 | Loss: 0.35930344462394714 | Test loss: 0.35682421922683716\n",
      "Epoch: 166570 | Loss: 0.359291672706604 | Test loss: 0.3568103611469269\n",
      "Epoch: 166580 | Loss: 0.35927993059158325 | Test loss: 0.3567965030670166\n",
      "Epoch: 166590 | Loss: 0.3592681586742401 | Test loss: 0.3567826747894287\n",
      "Epoch: 166600 | Loss: 0.359256386756897 | Test loss: 0.35676881670951843\n",
      "Epoch: 166610 | Loss: 0.3592446446418762 | Test loss: 0.35675498843193054\n",
      "Epoch: 166620 | Loss: 0.3592328727245331 | Test loss: 0.3567411005496979\n",
      "Epoch: 166630 | Loss: 0.35922113060951233 | Test loss: 0.35672727227211\n",
      "Epoch: 166640 | Loss: 0.3592093586921692 | Test loss: 0.3567134141921997\n",
      "Epoch: 166650 | Loss: 0.35919758677482605 | Test loss: 0.35669955611228943\n",
      "Epoch: 166660 | Loss: 0.3591858148574829 | Test loss: 0.35668572783470154\n",
      "Epoch: 166670 | Loss: 0.35917407274246216 | Test loss: 0.35667189955711365\n",
      "Epoch: 166680 | Loss: 0.359162300825119 | Test loss: 0.35665804147720337\n",
      "Epoch: 166690 | Loss: 0.35915055871009827 | Test loss: 0.3566442131996155\n",
      "Epoch: 166700 | Loss: 0.3591388165950775 | Test loss: 0.3566303253173828\n",
      "Epoch: 166710 | Loss: 0.359127014875412 | Test loss: 0.3566164970397949\n",
      "Epoch: 166720 | Loss: 0.35911527276039124 | Test loss: 0.35660266876220703\n",
      "Epoch: 166730 | Loss: 0.3591035306453705 | Test loss: 0.35658881068229675\n",
      "Epoch: 166740 | Loss: 0.35909175872802734 | Test loss: 0.35657498240470886\n",
      "Epoch: 166750 | Loss: 0.3590800166130066 | Test loss: 0.35656115412712097\n",
      "Epoch: 166760 | Loss: 0.35906824469566345 | Test loss: 0.3565472662448883\n",
      "Epoch: 166770 | Loss: 0.3590564727783203 | Test loss: 0.3565334379673004\n",
      "Epoch: 166780 | Loss: 0.35904473066329956 | Test loss: 0.35651957988739014\n",
      "Epoch: 166790 | Loss: 0.35903292894363403 | Test loss: 0.35650572180747986\n",
      "Epoch: 166800 | Loss: 0.3590211868286133 | Test loss: 0.35649189352989197\n",
      "Epoch: 166810 | Loss: 0.3590094745159149 | Test loss: 0.3564780652523041\n",
      "Epoch: 166820 | Loss: 0.3589976727962494 | Test loss: 0.3564642071723938\n",
      "Epoch: 166830 | Loss: 0.35898590087890625 | Test loss: 0.3564503490924835\n",
      "Epoch: 166840 | Loss: 0.3589741885662079 | Test loss: 0.35643649101257324\n",
      "Epoch: 166850 | Loss: 0.35896238684654236 | Test loss: 0.35642266273498535\n",
      "Epoch: 166860 | Loss: 0.3589506447315216 | Test loss: 0.3564088046550751\n",
      "Epoch: 166870 | Loss: 0.35893887281417847 | Test loss: 0.3563949763774872\n",
      "Epoch: 166880 | Loss: 0.3589271306991577 | Test loss: 0.3563811182975769\n",
      "Epoch: 166890 | Loss: 0.3589153289794922 | Test loss: 0.3563672602176666\n",
      "Epoch: 166900 | Loss: 0.35890358686447144 | Test loss: 0.35635343194007874\n",
      "Epoch: 166910 | Loss: 0.3588918149471283 | Test loss: 0.35633957386016846\n",
      "Epoch: 166920 | Loss: 0.35888010263442993 | Test loss: 0.3563257157802582\n",
      "Epoch: 166930 | Loss: 0.3588683307170868 | Test loss: 0.3563118875026703\n",
      "Epoch: 166940 | Loss: 0.35885652899742126 | Test loss: 0.3562980592250824\n",
      "Epoch: 166950 | Loss: 0.3588447868824005 | Test loss: 0.3562842011451721\n",
      "Epoch: 166960 | Loss: 0.35883304476737976 | Test loss: 0.35627037286758423\n",
      "Epoch: 166970 | Loss: 0.3588212728500366 | Test loss: 0.35625651478767395\n",
      "Epoch: 166980 | Loss: 0.35880953073501587 | Test loss: 0.35624265670776367\n",
      "Epoch: 166990 | Loss: 0.35879775881767273 | Test loss: 0.3562288284301758\n",
      "Epoch: 167000 | Loss: 0.3587859869003296 | Test loss: 0.3562149703502655\n",
      "Epoch: 167010 | Loss: 0.35877424478530884 | Test loss: 0.3562011420726776\n",
      "Epoch: 167020 | Loss: 0.3587624728679657 | Test loss: 0.35618725419044495\n",
      "Epoch: 167030 | Loss: 0.35875073075294495 | Test loss: 0.35617342591285706\n",
      "Epoch: 167040 | Loss: 0.3587389588356018 | Test loss: 0.3561595678329468\n",
      "Epoch: 167050 | Loss: 0.35872718691825867 | Test loss: 0.3561457097530365\n",
      "Epoch: 167060 | Loss: 0.3587154150009155 | Test loss: 0.3561318814754486\n",
      "Epoch: 167070 | Loss: 0.3587036728858948 | Test loss: 0.3561180531978607\n",
      "Epoch: 167080 | Loss: 0.35869190096855164 | Test loss: 0.35610419511795044\n",
      "Epoch: 167090 | Loss: 0.3586801588535309 | Test loss: 0.35609036684036255\n",
      "Epoch: 167100 | Loss: 0.35866841673851013 | Test loss: 0.3560764789581299\n",
      "Epoch: 167110 | Loss: 0.3586566150188446 | Test loss: 0.356062650680542\n",
      "Epoch: 167120 | Loss: 0.35864487290382385 | Test loss: 0.3560488224029541\n",
      "Epoch: 167130 | Loss: 0.3586331307888031 | Test loss: 0.3560349643230438\n",
      "Epoch: 167140 | Loss: 0.35862135887145996 | Test loss: 0.35602113604545593\n",
      "Epoch: 167150 | Loss: 0.3586096167564392 | Test loss: 0.35600730776786804\n",
      "Epoch: 167160 | Loss: 0.35859784483909607 | Test loss: 0.3559934198856354\n",
      "Epoch: 167170 | Loss: 0.35858607292175293 | Test loss: 0.3559795916080475\n",
      "Epoch: 167180 | Loss: 0.3585743308067322 | Test loss: 0.3559657335281372\n",
      "Epoch: 167190 | Loss: 0.35856252908706665 | Test loss: 0.35595187544822693\n",
      "Epoch: 167200 | Loss: 0.3585507869720459 | Test loss: 0.35593804717063904\n",
      "Epoch: 167210 | Loss: 0.35853907465934753 | Test loss: 0.35592421889305115\n",
      "Epoch: 167220 | Loss: 0.358527272939682 | Test loss: 0.35591036081314087\n",
      "Epoch: 167230 | Loss: 0.35851550102233887 | Test loss: 0.3558965027332306\n",
      "Epoch: 167240 | Loss: 0.3585037887096405 | Test loss: 0.3558826446533203\n",
      "Epoch: 167250 | Loss: 0.358491986989975 | Test loss: 0.3558688163757324\n",
      "Epoch: 167260 | Loss: 0.3584802448749542 | Test loss: 0.35585495829582214\n",
      "Epoch: 167270 | Loss: 0.3584684729576111 | Test loss: 0.35584113001823425\n",
      "Epoch: 167280 | Loss: 0.35845673084259033 | Test loss: 0.355827271938324\n",
      "Epoch: 167290 | Loss: 0.3584449291229248 | Test loss: 0.3558134138584137\n",
      "Epoch: 167300 | Loss: 0.35843318700790405 | Test loss: 0.3557995855808258\n",
      "Epoch: 167310 | Loss: 0.3584214150905609 | Test loss: 0.3557857275009155\n",
      "Epoch: 167320 | Loss: 0.35840970277786255 | Test loss: 0.35577186942100525\n",
      "Epoch: 167330 | Loss: 0.3583979308605194 | Test loss: 0.35575804114341736\n",
      "Epoch: 167340 | Loss: 0.3583861291408539 | Test loss: 0.35574421286582947\n",
      "Epoch: 167350 | Loss: 0.35837438702583313 | Test loss: 0.3557303547859192\n",
      "Epoch: 167360 | Loss: 0.3583626449108124 | Test loss: 0.3557165265083313\n",
      "Epoch: 167370 | Loss: 0.35835087299346924 | Test loss: 0.355702668428421\n",
      "Epoch: 167380 | Loss: 0.3583391308784485 | Test loss: 0.35568881034851074\n",
      "Epoch: 167390 | Loss: 0.35832735896110535 | Test loss: 0.35567498207092285\n",
      "Epoch: 167400 | Loss: 0.3583155870437622 | Test loss: 0.3556611239910126\n",
      "Epoch: 167410 | Loss: 0.35830384492874146 | Test loss: 0.3556472957134247\n",
      "Epoch: 167420 | Loss: 0.3582920730113983 | Test loss: 0.355633407831192\n",
      "Epoch: 167430 | Loss: 0.35828033089637756 | Test loss: 0.3556195795536041\n",
      "Epoch: 167440 | Loss: 0.3582685589790344 | Test loss: 0.35560572147369385\n",
      "Epoch: 167450 | Loss: 0.3582567870616913 | Test loss: 0.35559186339378357\n",
      "Epoch: 167460 | Loss: 0.35824501514434814 | Test loss: 0.3555780351161957\n",
      "Epoch: 167470 | Loss: 0.3582332730293274 | Test loss: 0.3555642068386078\n",
      "Epoch: 167480 | Loss: 0.35822150111198425 | Test loss: 0.3555503487586975\n",
      "Epoch: 167490 | Loss: 0.3582097589969635 | Test loss: 0.3555365204811096\n",
      "Epoch: 167500 | Loss: 0.35819801688194275 | Test loss: 0.35552263259887695\n",
      "Epoch: 167510 | Loss: 0.3581862151622772 | Test loss: 0.35550880432128906\n",
      "Epoch: 167520 | Loss: 0.35817447304725647 | Test loss: 0.35549497604370117\n",
      "Epoch: 167530 | Loss: 0.3581627309322357 | Test loss: 0.3554811179637909\n",
      "Epoch: 167540 | Loss: 0.3581509590148926 | Test loss: 0.355467289686203\n",
      "Epoch: 167550 | Loss: 0.3581392168998718 | Test loss: 0.3554534614086151\n",
      "Epoch: 167560 | Loss: 0.3581274449825287 | Test loss: 0.35543957352638245\n",
      "Epoch: 167570 | Loss: 0.35811567306518555 | Test loss: 0.35542574524879456\n",
      "Epoch: 167580 | Loss: 0.3581039309501648 | Test loss: 0.3554118871688843\n",
      "Epoch: 167590 | Loss: 0.35809212923049927 | Test loss: 0.355398029088974\n",
      "Epoch: 167600 | Loss: 0.3580803871154785 | Test loss: 0.3553842008113861\n",
      "Epoch: 167610 | Loss: 0.35806867480278015 | Test loss: 0.3553703725337982\n",
      "Epoch: 167620 | Loss: 0.3580568730831146 | Test loss: 0.35535651445388794\n",
      "Epoch: 167630 | Loss: 0.3580451011657715 | Test loss: 0.35534265637397766\n",
      "Epoch: 167640 | Loss: 0.3580333888530731 | Test loss: 0.3553287982940674\n",
      "Epoch: 167650 | Loss: 0.3580215871334076 | Test loss: 0.3553149700164795\n",
      "Epoch: 167660 | Loss: 0.35800984501838684 | Test loss: 0.3553011119365692\n",
      "Epoch: 167670 | Loss: 0.3579980731010437 | Test loss: 0.3552872836589813\n",
      "Epoch: 167680 | Loss: 0.35798633098602295 | Test loss: 0.35527342557907104\n",
      "Epoch: 167690 | Loss: 0.3579745292663574 | Test loss: 0.35525956749916077\n",
      "Epoch: 167700 | Loss: 0.35796278715133667 | Test loss: 0.3552457392215729\n",
      "Epoch: 167710 | Loss: 0.35795101523399353 | Test loss: 0.3552318811416626\n",
      "Epoch: 167720 | Loss: 0.35793930292129517 | Test loss: 0.3552180230617523\n",
      "Epoch: 167730 | Loss: 0.357927531003952 | Test loss: 0.35520419478416443\n",
      "Epoch: 167740 | Loss: 0.3579157292842865 | Test loss: 0.35519036650657654\n",
      "Epoch: 167750 | Loss: 0.35790398716926575 | Test loss: 0.35517650842666626\n",
      "Epoch: 167760 | Loss: 0.357892245054245 | Test loss: 0.35516268014907837\n",
      "Epoch: 167770 | Loss: 0.35788047313690186 | Test loss: 0.3551488220691681\n",
      "Epoch: 167780 | Loss: 0.3578687310218811 | Test loss: 0.3551349639892578\n",
      "Epoch: 167790 | Loss: 0.35785695910453796 | Test loss: 0.3551211357116699\n",
      "Epoch: 167800 | Loss: 0.3578451871871948 | Test loss: 0.35510727763175964\n",
      "Epoch: 167810 | Loss: 0.3578334450721741 | Test loss: 0.35509344935417175\n",
      "Epoch: 167820 | Loss: 0.35782167315483093 | Test loss: 0.3550795614719391\n",
      "Epoch: 167830 | Loss: 0.3578099310398102 | Test loss: 0.3550657331943512\n",
      "Epoch: 167840 | Loss: 0.35779815912246704 | Test loss: 0.3550518751144409\n",
      "Epoch: 167850 | Loss: 0.3577863872051239 | Test loss: 0.35503801703453064\n",
      "Epoch: 167860 | Loss: 0.35777461528778076 | Test loss: 0.35502418875694275\n",
      "Epoch: 167870 | Loss: 0.35776287317276 | Test loss: 0.35501036047935486\n",
      "Epoch: 167880 | Loss: 0.35775110125541687 | Test loss: 0.3549965023994446\n",
      "Epoch: 167890 | Loss: 0.3577393591403961 | Test loss: 0.3549826741218567\n",
      "Epoch: 167900 | Loss: 0.35772761702537537 | Test loss: 0.354968786239624\n",
      "Epoch: 167910 | Loss: 0.35771581530570984 | Test loss: 0.35495495796203613\n",
      "Epoch: 167920 | Loss: 0.3577040731906891 | Test loss: 0.35494112968444824\n",
      "Epoch: 167930 | Loss: 0.35769233107566833 | Test loss: 0.35492727160453796\n",
      "Epoch: 167940 | Loss: 0.3576805591583252 | Test loss: 0.3549134433269501\n",
      "Epoch: 167950 | Loss: 0.35766881704330444 | Test loss: 0.3548996150493622\n",
      "Epoch: 167960 | Loss: 0.3576570451259613 | Test loss: 0.3548857271671295\n",
      "Epoch: 167970 | Loss: 0.35764527320861816 | Test loss: 0.3548718988895416\n",
      "Epoch: 167980 | Loss: 0.3576335310935974 | Test loss: 0.35485804080963135\n",
      "Epoch: 167990 | Loss: 0.3576217293739319 | Test loss: 0.35484418272972107\n",
      "Epoch: 168000 | Loss: 0.35760998725891113 | Test loss: 0.3548303544521332\n",
      "Epoch: 168010 | Loss: 0.35759827494621277 | Test loss: 0.3548165261745453\n",
      "Epoch: 168020 | Loss: 0.35758647322654724 | Test loss: 0.354802668094635\n",
      "Epoch: 168030 | Loss: 0.3575747013092041 | Test loss: 0.35478881001472473\n",
      "Epoch: 168040 | Loss: 0.35756298899650574 | Test loss: 0.35477495193481445\n",
      "Epoch: 168050 | Loss: 0.3575511872768402 | Test loss: 0.35476112365722656\n",
      "Epoch: 168060 | Loss: 0.35753944516181946 | Test loss: 0.3547472655773163\n",
      "Epoch: 168070 | Loss: 0.3575276732444763 | Test loss: 0.3547334372997284\n",
      "Epoch: 168080 | Loss: 0.35751593112945557 | Test loss: 0.3547195792198181\n",
      "Epoch: 168090 | Loss: 0.35750412940979004 | Test loss: 0.35470572113990784\n",
      "Epoch: 168100 | Loss: 0.3574923872947693 | Test loss: 0.35469189286231995\n",
      "Epoch: 168110 | Loss: 0.35748061537742615 | Test loss: 0.35467803478240967\n",
      "Epoch: 168120 | Loss: 0.3574689030647278 | Test loss: 0.3546641767024994\n",
      "Epoch: 168130 | Loss: 0.35745713114738464 | Test loss: 0.3546503484249115\n",
      "Epoch: 168140 | Loss: 0.3574453294277191 | Test loss: 0.3546365201473236\n",
      "Epoch: 168150 | Loss: 0.35743358731269836 | Test loss: 0.35462266206741333\n",
      "Epoch: 168160 | Loss: 0.3574218451976776 | Test loss: 0.35460883378982544\n",
      "Epoch: 168170 | Loss: 0.3574100732803345 | Test loss: 0.35459497570991516\n",
      "Epoch: 168180 | Loss: 0.3573983311653137 | Test loss: 0.3545811176300049\n",
      "Epoch: 168190 | Loss: 0.3573865592479706 | Test loss: 0.354567289352417\n",
      "Epoch: 168200 | Loss: 0.35737478733062744 | Test loss: 0.3545534312725067\n",
      "Epoch: 168210 | Loss: 0.3573630452156067 | Test loss: 0.3545396029949188\n",
      "Epoch: 168220 | Loss: 0.35735127329826355 | Test loss: 0.35452571511268616\n",
      "Epoch: 168230 | Loss: 0.3573395311832428 | Test loss: 0.35451188683509827\n",
      "Epoch: 168240 | Loss: 0.35732775926589966 | Test loss: 0.354498028755188\n",
      "Epoch: 168250 | Loss: 0.3573159873485565 | Test loss: 0.3544841706752777\n",
      "Epoch: 168260 | Loss: 0.3573042154312134 | Test loss: 0.3544703423976898\n",
      "Epoch: 168270 | Loss: 0.3572924733161926 | Test loss: 0.35445651412010193\n",
      "Epoch: 168280 | Loss: 0.3572807013988495 | Test loss: 0.35444265604019165\n",
      "Epoch: 168290 | Loss: 0.35726895928382874 | Test loss: 0.35442882776260376\n",
      "Epoch: 168300 | Loss: 0.357257217168808 | Test loss: 0.3544149398803711\n",
      "Epoch: 168310 | Loss: 0.35724541544914246 | Test loss: 0.3544011116027832\n",
      "Epoch: 168320 | Loss: 0.3572336733341217 | Test loss: 0.3543872833251953\n",
      "Epoch: 168330 | Loss: 0.35722193121910095 | Test loss: 0.35437342524528503\n",
      "Epoch: 168340 | Loss: 0.3572101593017578 | Test loss: 0.35435959696769714\n",
      "Epoch: 168350 | Loss: 0.35719841718673706 | Test loss: 0.35434576869010925\n",
      "Epoch: 168360 | Loss: 0.3571866452693939 | Test loss: 0.3543318808078766\n",
      "Epoch: 168370 | Loss: 0.3571748733520508 | Test loss: 0.3543180525302887\n",
      "Epoch: 168380 | Loss: 0.35716313123703003 | Test loss: 0.3543041944503784\n",
      "Epoch: 168390 | Loss: 0.3571513295173645 | Test loss: 0.35429033637046814\n",
      "Epoch: 168400 | Loss: 0.35713958740234375 | Test loss: 0.35427650809288025\n",
      "Epoch: 168410 | Loss: 0.3571278750896454 | Test loss: 0.35426267981529236\n",
      "Epoch: 168420 | Loss: 0.35711607336997986 | Test loss: 0.3542488217353821\n",
      "Epoch: 168430 | Loss: 0.3571043014526367 | Test loss: 0.3542349636554718\n",
      "Epoch: 168440 | Loss: 0.35709258913993835 | Test loss: 0.3542211055755615\n",
      "Epoch: 168450 | Loss: 0.3570807874202728 | Test loss: 0.35420727729797363\n",
      "Epoch: 168460 | Loss: 0.3570690453052521 | Test loss: 0.35419341921806335\n",
      "Epoch: 168470 | Loss: 0.35705727338790894 | Test loss: 0.35417959094047546\n",
      "Epoch: 168480 | Loss: 0.3570455312728882 | Test loss: 0.3541657328605652\n",
      "Epoch: 168490 | Loss: 0.35703372955322266 | Test loss: 0.3541518747806549\n",
      "Epoch: 168500 | Loss: 0.3570219874382019 | Test loss: 0.354138046503067\n",
      "Epoch: 168510 | Loss: 0.35701021552085876 | Test loss: 0.35412418842315674\n",
      "Epoch: 168520 | Loss: 0.3569985032081604 | Test loss: 0.35411033034324646\n",
      "Epoch: 168530 | Loss: 0.35698673129081726 | Test loss: 0.35409650206565857\n",
      "Epoch: 168540 | Loss: 0.35697492957115173 | Test loss: 0.3540826737880707\n",
      "Epoch: 168550 | Loss: 0.356963187456131 | Test loss: 0.3540688157081604\n",
      "Epoch: 168560 | Loss: 0.35695144534111023 | Test loss: 0.3540549874305725\n",
      "Epoch: 168570 | Loss: 0.3569396734237671 | Test loss: 0.35404112935066223\n",
      "Epoch: 168580 | Loss: 0.35692793130874634 | Test loss: 0.35402727127075195\n",
      "Epoch: 168590 | Loss: 0.3569161593914032 | Test loss: 0.35401344299316406\n",
      "Epoch: 168600 | Loss: 0.35690438747406006 | Test loss: 0.3539995849132538\n",
      "Epoch: 168610 | Loss: 0.3568926453590393 | Test loss: 0.3539857566356659\n",
      "Epoch: 168620 | Loss: 0.35688087344169617 | Test loss: 0.3539718687534332\n",
      "Epoch: 168630 | Loss: 0.3568691313266754 | Test loss: 0.35395804047584534\n",
      "Epoch: 168640 | Loss: 0.3568573594093323 | Test loss: 0.35394418239593506\n",
      "Epoch: 168650 | Loss: 0.35684558749198914 | Test loss: 0.3539303243160248\n",
      "Epoch: 168660 | Loss: 0.356833815574646 | Test loss: 0.3539164960384369\n",
      "Epoch: 168670 | Loss: 0.35682207345962524 | Test loss: 0.353902667760849\n",
      "Epoch: 168680 | Loss: 0.3568103015422821 | Test loss: 0.3538888096809387\n",
      "Epoch: 168690 | Loss: 0.35679855942726135 | Test loss: 0.35387498140335083\n",
      "Epoch: 168700 | Loss: 0.3567868173122406 | Test loss: 0.35386109352111816\n",
      "Epoch: 168710 | Loss: 0.3567750155925751 | Test loss: 0.3538472652435303\n",
      "Epoch: 168720 | Loss: 0.3567632734775543 | Test loss: 0.3538334369659424\n",
      "Epoch: 168730 | Loss: 0.35675153136253357 | Test loss: 0.3538195788860321\n",
      "Epoch: 168740 | Loss: 0.35673975944519043 | Test loss: 0.3538057506084442\n",
      "Epoch: 168750 | Loss: 0.3567280173301697 | Test loss: 0.3537919223308563\n",
      "Epoch: 168760 | Loss: 0.35671624541282654 | Test loss: 0.35377803444862366\n",
      "Epoch: 168770 | Loss: 0.3567044734954834 | Test loss: 0.35376420617103577\n",
      "Epoch: 168780 | Loss: 0.35669273138046265 | Test loss: 0.3537503480911255\n",
      "Epoch: 168790 | Loss: 0.3566809296607971 | Test loss: 0.3537364900112152\n",
      "Epoch: 168800 | Loss: 0.35666918754577637 | Test loss: 0.3537226617336273\n",
      "Epoch: 168810 | Loss: 0.356657475233078 | Test loss: 0.35370883345603943\n",
      "Epoch: 168820 | Loss: 0.3566456735134125 | Test loss: 0.35369497537612915\n",
      "Epoch: 168830 | Loss: 0.35663390159606934 | Test loss: 0.35368111729621887\n",
      "Epoch: 168840 | Loss: 0.35662218928337097 | Test loss: 0.3536672592163086\n",
      "Epoch: 168850 | Loss: 0.35661038756370544 | Test loss: 0.3536534309387207\n",
      "Epoch: 168860 | Loss: 0.3565986454486847 | Test loss: 0.3536395728588104\n",
      "Epoch: 168870 | Loss: 0.35658687353134155 | Test loss: 0.35362574458122253\n",
      "Epoch: 168880 | Loss: 0.3565751314163208 | Test loss: 0.35361188650131226\n",
      "Epoch: 168890 | Loss: 0.3565633296966553 | Test loss: 0.353598028421402\n",
      "Epoch: 168900 | Loss: 0.3565515875816345 | Test loss: 0.3535842001438141\n",
      "Epoch: 168910 | Loss: 0.3565398156642914 | Test loss: 0.3535703420639038\n",
      "Epoch: 168920 | Loss: 0.356528103351593 | Test loss: 0.35355648398399353\n",
      "Epoch: 168930 | Loss: 0.3565163314342499 | Test loss: 0.35354265570640564\n",
      "Epoch: 168940 | Loss: 0.35650452971458435 | Test loss: 0.35352882742881775\n",
      "Epoch: 168950 | Loss: 0.3564927875995636 | Test loss: 0.35351496934890747\n",
      "Epoch: 168960 | Loss: 0.35648104548454285 | Test loss: 0.3535011410713196\n",
      "Epoch: 168970 | Loss: 0.3564692735671997 | Test loss: 0.3534872829914093\n",
      "Epoch: 168980 | Loss: 0.35645753145217896 | Test loss: 0.353473424911499\n",
      "Epoch: 168990 | Loss: 0.3564457595348358 | Test loss: 0.35345959663391113\n",
      "Epoch: 169000 | Loss: 0.3564339876174927 | Test loss: 0.35344573855400085\n",
      "Epoch: 169010 | Loss: 0.3564222455024719 | Test loss: 0.35343191027641296\n",
      "Epoch: 169020 | Loss: 0.3564104735851288 | Test loss: 0.3534180223941803\n",
      "Epoch: 169030 | Loss: 0.35639873147010803 | Test loss: 0.3534041941165924\n",
      "Epoch: 169040 | Loss: 0.3563869595527649 | Test loss: 0.35339033603668213\n",
      "Epoch: 169050 | Loss: 0.35637518763542175 | Test loss: 0.35337647795677185\n",
      "Epoch: 169060 | Loss: 0.3563634157180786 | Test loss: 0.35336264967918396\n",
      "Epoch: 169070 | Loss: 0.35635167360305786 | Test loss: 0.35334882140159607\n",
      "Epoch: 169080 | Loss: 0.3563399016857147 | Test loss: 0.3533349633216858\n",
      "Epoch: 169090 | Loss: 0.35632815957069397 | Test loss: 0.3533211350440979\n",
      "Epoch: 169100 | Loss: 0.3563164174556732 | Test loss: 0.35330724716186523\n",
      "Epoch: 169110 | Loss: 0.3563046157360077 | Test loss: 0.35329341888427734\n",
      "Epoch: 169120 | Loss: 0.35629287362098694 | Test loss: 0.35327959060668945\n",
      "Epoch: 169130 | Loss: 0.3562811315059662 | Test loss: 0.3532657325267792\n",
      "Epoch: 169140 | Loss: 0.35626935958862305 | Test loss: 0.3532519042491913\n",
      "Epoch: 169150 | Loss: 0.3562576174736023 | Test loss: 0.3532380759716034\n",
      "Epoch: 169160 | Loss: 0.35624584555625916 | Test loss: 0.3532241880893707\n",
      "Epoch: 169170 | Loss: 0.356234073638916 | Test loss: 0.35321035981178284\n",
      "Epoch: 169180 | Loss: 0.35622233152389526 | Test loss: 0.35319650173187256\n",
      "Epoch: 169190 | Loss: 0.35621052980422974 | Test loss: 0.3531826436519623\n",
      "Epoch: 169200 | Loss: 0.356198787689209 | Test loss: 0.3531688153743744\n",
      "Epoch: 169210 | Loss: 0.3561870753765106 | Test loss: 0.3531549870967865\n",
      "Epoch: 169220 | Loss: 0.3561752736568451 | Test loss: 0.3531411290168762\n",
      "Epoch: 169230 | Loss: 0.35616350173950195 | Test loss: 0.35312727093696594\n",
      "Epoch: 169240 | Loss: 0.3561517894268036 | Test loss: 0.35311341285705566\n",
      "Epoch: 169250 | Loss: 0.35613998770713806 | Test loss: 0.3530995845794678\n",
      "Epoch: 169260 | Loss: 0.3561282455921173 | Test loss: 0.3530857264995575\n",
      "Epoch: 169270 | Loss: 0.35611647367477417 | Test loss: 0.3530718982219696\n",
      "Epoch: 169280 | Loss: 0.3561047315597534 | Test loss: 0.3530580401420593\n",
      "Epoch: 169290 | Loss: 0.3560929298400879 | Test loss: 0.35304418206214905\n",
      "Epoch: 169300 | Loss: 0.35608118772506714 | Test loss: 0.35303035378456116\n",
      "Epoch: 169310 | Loss: 0.356069415807724 | Test loss: 0.3530164957046509\n",
      "Epoch: 169320 | Loss: 0.35605770349502563 | Test loss: 0.3530026376247406\n",
      "Epoch: 169330 | Loss: 0.3560459315776825 | Test loss: 0.3529888093471527\n",
      "Epoch: 169340 | Loss: 0.35603412985801697 | Test loss: 0.3529749810695648\n",
      "Epoch: 169350 | Loss: 0.3560223877429962 | Test loss: 0.35296112298965454\n",
      "Epoch: 169360 | Loss: 0.35601064562797546 | Test loss: 0.35294729471206665\n",
      "Epoch: 169370 | Loss: 0.3559988737106323 | Test loss: 0.35293343663215637\n",
      "Epoch: 169380 | Loss: 0.3559871315956116 | Test loss: 0.3529195785522461\n",
      "Epoch: 169390 | Loss: 0.35597535967826843 | Test loss: 0.3529057502746582\n",
      "Epoch: 169400 | Loss: 0.3559635877609253 | Test loss: 0.3528918921947479\n",
      "Epoch: 169410 | Loss: 0.35595184564590454 | Test loss: 0.35287806391716003\n",
      "Epoch: 169420 | Loss: 0.3559400737285614 | Test loss: 0.35286417603492737\n",
      "Epoch: 169430 | Loss: 0.35592833161354065 | Test loss: 0.3528503477573395\n",
      "Epoch: 169440 | Loss: 0.3559165596961975 | Test loss: 0.3528364896774292\n",
      "Epoch: 169450 | Loss: 0.35590478777885437 | Test loss: 0.3528226315975189\n",
      "Epoch: 169460 | Loss: 0.35589301586151123 | Test loss: 0.35280880331993103\n",
      "Epoch: 169470 | Loss: 0.3558812737464905 | Test loss: 0.35279497504234314\n",
      "Epoch: 169480 | Loss: 0.35586950182914734 | Test loss: 0.35278111696243286\n",
      "Epoch: 169490 | Loss: 0.3558577597141266 | Test loss: 0.35276728868484497\n",
      "Epoch: 169500 | Loss: 0.35584601759910583 | Test loss: 0.3527534008026123\n",
      "Epoch: 169510 | Loss: 0.3558342158794403 | Test loss: 0.3527395725250244\n",
      "Epoch: 169520 | Loss: 0.35582247376441956 | Test loss: 0.3527257442474365\n",
      "Epoch: 169530 | Loss: 0.3558107316493988 | Test loss: 0.35271188616752625\n",
      "Epoch: 169540 | Loss: 0.35579895973205566 | Test loss: 0.35269805788993835\n",
      "Epoch: 169550 | Loss: 0.3557872176170349 | Test loss: 0.35268422961235046\n",
      "Epoch: 169560 | Loss: 0.3557754456996918 | Test loss: 0.3526703417301178\n",
      "Epoch: 169570 | Loss: 0.35576367378234863 | Test loss: 0.3526565134525299\n",
      "Epoch: 169580 | Loss: 0.3557519316673279 | Test loss: 0.35264265537261963\n",
      "Epoch: 169590 | Loss: 0.35574012994766235 | Test loss: 0.35262879729270935\n",
      "Epoch: 169600 | Loss: 0.3557283878326416 | Test loss: 0.35261496901512146\n",
      "Epoch: 169610 | Loss: 0.35571667551994324 | Test loss: 0.35260114073753357\n",
      "Epoch: 169620 | Loss: 0.3557048738002777 | Test loss: 0.3525872826576233\n",
      "Epoch: 169630 | Loss: 0.35569310188293457 | Test loss: 0.352573424577713\n",
      "Epoch: 169640 | Loss: 0.3556813895702362 | Test loss: 0.35255956649780273\n",
      "Epoch: 169650 | Loss: 0.3556695878505707 | Test loss: 0.35254573822021484\n",
      "Epoch: 169660 | Loss: 0.3556578457355499 | Test loss: 0.35253188014030457\n",
      "Epoch: 169670 | Loss: 0.3556460738182068 | Test loss: 0.3525180518627167\n",
      "Epoch: 169680 | Loss: 0.35563433170318604 | Test loss: 0.3525041937828064\n",
      "Epoch: 169690 | Loss: 0.3556225299835205 | Test loss: 0.3524903357028961\n",
      "Epoch: 169700 | Loss: 0.35561078786849976 | Test loss: 0.3524765074253082\n",
      "Epoch: 169710 | Loss: 0.3555990159511566 | Test loss: 0.35246264934539795\n",
      "Epoch: 169720 | Loss: 0.35558730363845825 | Test loss: 0.35244879126548767\n",
      "Epoch: 169730 | Loss: 0.3555755317211151 | Test loss: 0.3524349629878998\n",
      "Epoch: 169740 | Loss: 0.3555637300014496 | Test loss: 0.3524211347103119\n",
      "Epoch: 169750 | Loss: 0.35555198788642883 | Test loss: 0.3524072766304016\n",
      "Epoch: 169760 | Loss: 0.3555402457714081 | Test loss: 0.3523934483528137\n",
      "Epoch: 169770 | Loss: 0.35552847385406494 | Test loss: 0.35237959027290344\n",
      "Epoch: 169780 | Loss: 0.3555167317390442 | Test loss: 0.35236573219299316\n",
      "Epoch: 169790 | Loss: 0.35550495982170105 | Test loss: 0.3523519039154053\n",
      "Epoch: 169800 | Loss: 0.3554931879043579 | Test loss: 0.352338045835495\n",
      "Epoch: 169810 | Loss: 0.35548144578933716 | Test loss: 0.3523242175579071\n",
      "Epoch: 169820 | Loss: 0.355469673871994 | Test loss: 0.35231032967567444\n",
      "Epoch: 169830 | Loss: 0.35545793175697327 | Test loss: 0.35229650139808655\n",
      "Epoch: 169840 | Loss: 0.3554461598396301 | Test loss: 0.35228264331817627\n",
      "Epoch: 169850 | Loss: 0.355434387922287 | Test loss: 0.352268785238266\n",
      "Epoch: 169860 | Loss: 0.35542261600494385 | Test loss: 0.3522549569606781\n",
      "Epoch: 169870 | Loss: 0.3554108738899231 | Test loss: 0.3522411286830902\n",
      "Epoch: 169880 | Loss: 0.35539910197257996 | Test loss: 0.35222727060317993\n",
      "Epoch: 169890 | Loss: 0.3553873598575592 | Test loss: 0.35221344232559204\n",
      "Epoch: 169900 | Loss: 0.35537561774253845 | Test loss: 0.3521995544433594\n",
      "Epoch: 169910 | Loss: 0.3553638160228729 | Test loss: 0.3521857261657715\n",
      "Epoch: 169920 | Loss: 0.3553520739078522 | Test loss: 0.3521718978881836\n",
      "Epoch: 169930 | Loss: 0.3553403317928314 | Test loss: 0.3521580398082733\n",
      "Epoch: 169940 | Loss: 0.3553285598754883 | Test loss: 0.3521442115306854\n",
      "Epoch: 169950 | Loss: 0.35531681776046753 | Test loss: 0.35213038325309753\n",
      "Epoch: 169960 | Loss: 0.3553050458431244 | Test loss: 0.35211649537086487\n",
      "Epoch: 169970 | Loss: 0.35529327392578125 | Test loss: 0.352102667093277\n",
      "Epoch: 169980 | Loss: 0.3552815318107605 | Test loss: 0.3520888090133667\n",
      "Epoch: 169990 | Loss: 0.35526973009109497 | Test loss: 0.3520749509334564\n",
      "Epoch: 170000 | Loss: 0.3552579879760742 | Test loss: 0.35206112265586853\n",
      "Epoch: 170010 | Loss: 0.35524627566337585 | Test loss: 0.35204729437828064\n",
      "Epoch: 170020 | Loss: 0.3552344739437103 | Test loss: 0.35203343629837036\n",
      "Epoch: 170030 | Loss: 0.3552227020263672 | Test loss: 0.3520195782184601\n",
      "Epoch: 170040 | Loss: 0.3552109897136688 | Test loss: 0.3520057201385498\n",
      "Epoch: 170050 | Loss: 0.3551991879940033 | Test loss: 0.3519918918609619\n",
      "Epoch: 170060 | Loss: 0.35518744587898254 | Test loss: 0.35197803378105164\n",
      "Epoch: 170070 | Loss: 0.3551756739616394 | Test loss: 0.35196420550346375\n",
      "Epoch: 170080 | Loss: 0.35516393184661865 | Test loss: 0.35195034742355347\n",
      "Epoch: 170090 | Loss: 0.3551521301269531 | Test loss: 0.3519364893436432\n",
      "Epoch: 170100 | Loss: 0.3551403880119324 | Test loss: 0.3519226610660553\n",
      "Epoch: 170110 | Loss: 0.35512861609458923 | Test loss: 0.351908802986145\n",
      "Epoch: 170120 | Loss: 0.35511690378189087 | Test loss: 0.35189494490623474\n",
      "Epoch: 170130 | Loss: 0.35510513186454773 | Test loss: 0.35188111662864685\n",
      "Epoch: 170140 | Loss: 0.3550933301448822 | Test loss: 0.35186728835105896\n",
      "Epoch: 170150 | Loss: 0.35508158802986145 | Test loss: 0.3518534302711487\n",
      "Epoch: 170160 | Loss: 0.3550698459148407 | Test loss: 0.3518396019935608\n",
      "Epoch: 170170 | Loss: 0.35505807399749756 | Test loss: 0.3518257439136505\n",
      "Epoch: 170180 | Loss: 0.3550463318824768 | Test loss: 0.35181188583374023\n",
      "Epoch: 170190 | Loss: 0.35503455996513367 | Test loss: 0.35179805755615234\n",
      "Epoch: 170200 | Loss: 0.3550227880477905 | Test loss: 0.35178419947624207\n",
      "Epoch: 170210 | Loss: 0.3550110459327698 | Test loss: 0.3517703711986542\n",
      "Epoch: 170220 | Loss: 0.35499927401542664 | Test loss: 0.3517564833164215\n",
      "Epoch: 170230 | Loss: 0.3549875319004059 | Test loss: 0.3517426550388336\n",
      "Epoch: 170240 | Loss: 0.35497575998306274 | Test loss: 0.35172879695892334\n",
      "Epoch: 170250 | Loss: 0.3549639880657196 | Test loss: 0.35171493887901306\n",
      "Epoch: 170260 | Loss: 0.35495221614837646 | Test loss: 0.35170111060142517\n",
      "Epoch: 170270 | Loss: 0.3549404740333557 | Test loss: 0.3516872823238373\n",
      "Epoch: 170280 | Loss: 0.3549287021160126 | Test loss: 0.351673424243927\n",
      "Epoch: 170290 | Loss: 0.3549169600009918 | Test loss: 0.3516595959663391\n",
      "Epoch: 170300 | Loss: 0.35490521788597107 | Test loss: 0.35164570808410645\n",
      "Epoch: 170310 | Loss: 0.35489341616630554 | Test loss: 0.35163187980651855\n",
      "Epoch: 170320 | Loss: 0.3548816740512848 | Test loss: 0.35161805152893066\n",
      "Epoch: 170330 | Loss: 0.35486993193626404 | Test loss: 0.3516041934490204\n",
      "Epoch: 170340 | Loss: 0.3548581600189209 | Test loss: 0.3515903651714325\n",
      "Epoch: 170350 | Loss: 0.35484641790390015 | Test loss: 0.3515765368938446\n",
      "Epoch: 170360 | Loss: 0.354834645986557 | Test loss: 0.35156264901161194\n",
      "Epoch: 170370 | Loss: 0.35482287406921387 | Test loss: 0.35154882073402405\n",
      "Epoch: 170380 | Loss: 0.3548111319541931 | Test loss: 0.35153496265411377\n",
      "Epoch: 170390 | Loss: 0.3547993302345276 | Test loss: 0.3515211045742035\n",
      "Epoch: 170400 | Loss: 0.35478758811950684 | Test loss: 0.3515072762966156\n",
      "Epoch: 170410 | Loss: 0.35477587580680847 | Test loss: 0.3514934480190277\n",
      "Epoch: 170420 | Loss: 0.35476407408714294 | Test loss: 0.35147958993911743\n",
      "Epoch: 170430 | Loss: 0.3547523021697998 | Test loss: 0.35146573185920715\n",
      "Epoch: 170440 | Loss: 0.35474058985710144 | Test loss: 0.3514518737792969\n",
      "Epoch: 170450 | Loss: 0.3547287881374359 | Test loss: 0.351438045501709\n",
      "Epoch: 170460 | Loss: 0.35471704602241516 | Test loss: 0.3514241874217987\n",
      "Epoch: 170470 | Loss: 0.354705274105072 | Test loss: 0.3514103591442108\n",
      "Epoch: 170480 | Loss: 0.35469353199005127 | Test loss: 0.35139650106430054\n",
      "Epoch: 170490 | Loss: 0.35468173027038574 | Test loss: 0.35138264298439026\n",
      "Epoch: 170500 | Loss: 0.354669988155365 | Test loss: 0.35136881470680237\n",
      "Epoch: 170510 | Loss: 0.35465821623802185 | Test loss: 0.3513549566268921\n",
      "Epoch: 170520 | Loss: 0.3546465039253235 | Test loss: 0.3513410985469818\n",
      "Epoch: 170530 | Loss: 0.35463473200798035 | Test loss: 0.3513272702693939\n",
      "Epoch: 170540 | Loss: 0.3546229302883148 | Test loss: 0.35131344199180603\n",
      "Epoch: 170550 | Loss: 0.35461118817329407 | Test loss: 0.35129958391189575\n",
      "Epoch: 170560 | Loss: 0.3545994460582733 | Test loss: 0.35128575563430786\n",
      "Epoch: 170570 | Loss: 0.3545876741409302 | Test loss: 0.3512718975543976\n",
      "Epoch: 170580 | Loss: 0.3545759320259094 | Test loss: 0.3512580394744873\n",
      "Epoch: 170590 | Loss: 0.3545641601085663 | Test loss: 0.3512442111968994\n",
      "Epoch: 170600 | Loss: 0.35455238819122314 | Test loss: 0.35123035311698914\n",
      "Epoch: 170610 | Loss: 0.3545406460762024 | Test loss: 0.35121652483940125\n",
      "Epoch: 170620 | Loss: 0.35452887415885925 | Test loss: 0.3512026369571686\n",
      "Epoch: 170630 | Loss: 0.3545171320438385 | Test loss: 0.3511888086795807\n",
      "Epoch: 170640 | Loss: 0.35450536012649536 | Test loss: 0.3511749505996704\n",
      "Epoch: 170650 | Loss: 0.3544935882091522 | Test loss: 0.35116109251976013\n",
      "Epoch: 170660 | Loss: 0.3544818162918091 | Test loss: 0.35114726424217224\n",
      "Epoch: 170670 | Loss: 0.35447007417678833 | Test loss: 0.35113343596458435\n",
      "Epoch: 170680 | Loss: 0.3544583022594452 | Test loss: 0.3511195778846741\n",
      "Epoch: 170690 | Loss: 0.35444656014442444 | Test loss: 0.3511057496070862\n",
      "Epoch: 170700 | Loss: 0.3544348180294037 | Test loss: 0.3510918617248535\n",
      "Epoch: 170710 | Loss: 0.35442301630973816 | Test loss: 0.3510780334472656\n",
      "Epoch: 170720 | Loss: 0.3544112741947174 | Test loss: 0.35106420516967773\n",
      "Epoch: 170730 | Loss: 0.35439953207969666 | Test loss: 0.35105034708976746\n",
      "Epoch: 170740 | Loss: 0.3543877601623535 | Test loss: 0.35103651881217957\n",
      "Epoch: 170750 | Loss: 0.35437601804733276 | Test loss: 0.3510226905345917\n",
      "Epoch: 170760 | Loss: 0.3543642461299896 | Test loss: 0.351008802652359\n",
      "Epoch: 170770 | Loss: 0.3543524742126465 | Test loss: 0.3509949743747711\n",
      "Epoch: 170780 | Loss: 0.35434073209762573 | Test loss: 0.35098111629486084\n",
      "Epoch: 170790 | Loss: 0.3543289303779602 | Test loss: 0.35096725821495056\n",
      "Epoch: 170800 | Loss: 0.35431718826293945 | Test loss: 0.35095342993736267\n",
      "Epoch: 170810 | Loss: 0.3543054759502411 | Test loss: 0.3509396016597748\n",
      "Epoch: 170820 | Loss: 0.35429367423057556 | Test loss: 0.3509257435798645\n",
      "Epoch: 170830 | Loss: 0.3542819023132324 | Test loss: 0.3509118854999542\n",
      "Epoch: 170840 | Loss: 0.35427019000053406 | Test loss: 0.35089802742004395\n",
      "Epoch: 170850 | Loss: 0.35425838828086853 | Test loss: 0.35088419914245605\n",
      "Epoch: 170860 | Loss: 0.3542466461658478 | Test loss: 0.3508703410625458\n",
      "Epoch: 170870 | Loss: 0.35423487424850464 | Test loss: 0.3508565127849579\n",
      "Epoch: 170880 | Loss: 0.3542231321334839 | Test loss: 0.3508426547050476\n",
      "Epoch: 170890 | Loss: 0.35421133041381836 | Test loss: 0.35082879662513733\n",
      "Epoch: 170900 | Loss: 0.3541995882987976 | Test loss: 0.35081496834754944\n",
      "Epoch: 170910 | Loss: 0.35418781638145447 | Test loss: 0.35080111026763916\n",
      "Epoch: 170920 | Loss: 0.3541761040687561 | Test loss: 0.3507872521877289\n",
      "Epoch: 170930 | Loss: 0.35416433215141296 | Test loss: 0.350773423910141\n",
      "Epoch: 170940 | Loss: 0.35415253043174744 | Test loss: 0.3507595956325531\n",
      "Epoch: 170950 | Loss: 0.3541407883167267 | Test loss: 0.3507457375526428\n",
      "Epoch: 170960 | Loss: 0.35412904620170593 | Test loss: 0.35073190927505493\n",
      "Epoch: 170970 | Loss: 0.3541172742843628 | Test loss: 0.35071805119514465\n",
      "Epoch: 170980 | Loss: 0.35410553216934204 | Test loss: 0.3507041931152344\n",
      "Epoch: 170990 | Loss: 0.3540937602519989 | Test loss: 0.3506903648376465\n",
      "Epoch: 171000 | Loss: 0.35408198833465576 | Test loss: 0.3506765067577362\n",
      "Epoch: 171010 | Loss: 0.354070246219635 | Test loss: 0.3506626784801483\n",
      "Epoch: 171020 | Loss: 0.35405847430229187 | Test loss: 0.35064879059791565\n",
      "Epoch: 171030 | Loss: 0.3540467321872711 | Test loss: 0.35063496232032776\n",
      "Epoch: 171040 | Loss: 0.354034960269928 | Test loss: 0.3506211042404175\n",
      "Epoch: 171050 | Loss: 0.35402318835258484 | Test loss: 0.3506072461605072\n",
      "Epoch: 171060 | Loss: 0.3540114164352417 | Test loss: 0.3505934178829193\n",
      "Epoch: 171070 | Loss: 0.35399967432022095 | Test loss: 0.3505795896053314\n",
      "Epoch: 171080 | Loss: 0.3539879024028778 | Test loss: 0.35056573152542114\n",
      "Epoch: 171090 | Loss: 0.35397616028785706 | Test loss: 0.35055190324783325\n",
      "Epoch: 171100 | Loss: 0.3539644181728363 | Test loss: 0.3505380153656006\n",
      "Epoch: 171110 | Loss: 0.3539526164531708 | Test loss: 0.3505241870880127\n",
      "Epoch: 171120 | Loss: 0.35394087433815 | Test loss: 0.3505103588104248\n",
      "Epoch: 171130 | Loss: 0.3539291322231293 | Test loss: 0.3504965007305145\n",
      "Epoch: 171140 | Loss: 0.35391736030578613 | Test loss: 0.35048267245292664\n",
      "Epoch: 171150 | Loss: 0.3539056181907654 | Test loss: 0.35046884417533875\n",
      "Epoch: 171160 | Loss: 0.35389384627342224 | Test loss: 0.3504549562931061\n",
      "Epoch: 171170 | Loss: 0.3538820743560791 | Test loss: 0.3504411280155182\n",
      "Epoch: 171180 | Loss: 0.35387033224105835 | Test loss: 0.3504272699356079\n",
      "Epoch: 171190 | Loss: 0.3538585305213928 | Test loss: 0.35041341185569763\n",
      "Epoch: 171200 | Loss: 0.35384678840637207 | Test loss: 0.35039958357810974\n",
      "Epoch: 171210 | Loss: 0.3538350760936737 | Test loss: 0.35038575530052185\n",
      "Epoch: 171220 | Loss: 0.3538232743740082 | Test loss: 0.3503718972206116\n",
      "Epoch: 171230 | Loss: 0.35381150245666504 | Test loss: 0.3503580391407013\n",
      "Epoch: 171240 | Loss: 0.3537997901439667 | Test loss: 0.350344181060791\n",
      "Epoch: 171250 | Loss: 0.35378798842430115 | Test loss: 0.3503303527832031\n",
      "Epoch: 171260 | Loss: 0.3537762463092804 | Test loss: 0.35031649470329285\n",
      "Epoch: 171270 | Loss: 0.35376447439193726 | Test loss: 0.35030266642570496\n",
      "Epoch: 171280 | Loss: 0.3537527322769165 | Test loss: 0.3502888083457947\n",
      "Epoch: 171290 | Loss: 0.353740930557251 | Test loss: 0.3502749502658844\n",
      "Epoch: 171300 | Loss: 0.3537291884422302 | Test loss: 0.3502611219882965\n",
      "Epoch: 171310 | Loss: 0.3537174165248871 | Test loss: 0.35024726390838623\n",
      "Epoch: 171320 | Loss: 0.3537057042121887 | Test loss: 0.35023340582847595\n",
      "Epoch: 171330 | Loss: 0.3536939322948456 | Test loss: 0.35021957755088806\n",
      "Epoch: 171340 | Loss: 0.35368213057518005 | Test loss: 0.35020574927330017\n",
      "Epoch: 171350 | Loss: 0.3536703884601593 | Test loss: 0.3501918911933899\n",
      "Epoch: 171360 | Loss: 0.35365864634513855 | Test loss: 0.350178062915802\n",
      "Epoch: 171370 | Loss: 0.3536468744277954 | Test loss: 0.3501642048358917\n",
      "Epoch: 171380 | Loss: 0.35363513231277466 | Test loss: 0.35015034675598145\n",
      "Epoch: 171390 | Loss: 0.3536233603954315 | Test loss: 0.35013651847839355\n",
      "Epoch: 171400 | Loss: 0.3536115884780884 | Test loss: 0.3501226603984833\n",
      "Epoch: 171410 | Loss: 0.3535998463630676 | Test loss: 0.3501088321208954\n",
      "Epoch: 171420 | Loss: 0.3535880744457245 | Test loss: 0.3500949442386627\n",
      "Epoch: 171430 | Loss: 0.35357633233070374 | Test loss: 0.35008111596107483\n",
      "Epoch: 171440 | Loss: 0.3535645604133606 | Test loss: 0.35006725788116455\n",
      "Epoch: 171450 | Loss: 0.35355278849601746 | Test loss: 0.3500533998012543\n",
      "Epoch: 171460 | Loss: 0.3535410165786743 | Test loss: 0.3500395715236664\n",
      "Epoch: 171470 | Loss: 0.35352927446365356 | Test loss: 0.3500257432460785\n",
      "Epoch: 171480 | Loss: 0.3535175025463104 | Test loss: 0.3500118851661682\n",
      "Epoch: 171490 | Loss: 0.3535057604312897 | Test loss: 0.3499980568885803\n",
      "Epoch: 171500 | Loss: 0.3534940183162689 | Test loss: 0.34998416900634766\n",
      "Epoch: 171510 | Loss: 0.3534822165966034 | Test loss: 0.34997034072875977\n",
      "Epoch: 171520 | Loss: 0.35347047448158264 | Test loss: 0.3499565124511719\n",
      "Epoch: 171530 | Loss: 0.3534587323665619 | Test loss: 0.3499426543712616\n",
      "Epoch: 171540 | Loss: 0.35344696044921875 | Test loss: 0.3499288260936737\n",
      "Epoch: 171550 | Loss: 0.353435218334198 | Test loss: 0.3499149978160858\n",
      "Epoch: 171560 | Loss: 0.35342344641685486 | Test loss: 0.34990110993385315\n",
      "Epoch: 171570 | Loss: 0.3534116744995117 | Test loss: 0.34988728165626526\n",
      "Epoch: 171580 | Loss: 0.35339993238449097 | Test loss: 0.349873423576355\n",
      "Epoch: 171590 | Loss: 0.35338813066482544 | Test loss: 0.3498595654964447\n",
      "Epoch: 171600 | Loss: 0.3533763885498047 | Test loss: 0.3498457372188568\n",
      "Epoch: 171610 | Loss: 0.3533646762371063 | Test loss: 0.3498319089412689\n",
      "Epoch: 171620 | Loss: 0.3533528745174408 | Test loss: 0.34981805086135864\n",
      "Epoch: 171630 | Loss: 0.35334110260009766 | Test loss: 0.34980419278144836\n",
      "Epoch: 171640 | Loss: 0.3533293902873993 | Test loss: 0.3497903347015381\n",
      "Epoch: 171650 | Loss: 0.35331758856773376 | Test loss: 0.3497765064239502\n",
      "Epoch: 171660 | Loss: 0.353305846452713 | Test loss: 0.3497626483440399\n",
      "Epoch: 171670 | Loss: 0.3532940745353699 | Test loss: 0.349748820066452\n",
      "Epoch: 171680 | Loss: 0.3532823324203491 | Test loss: 0.34973496198654175\n",
      "Epoch: 171690 | Loss: 0.3532705307006836 | Test loss: 0.34972110390663147\n",
      "Epoch: 171700 | Loss: 0.35325878858566284 | Test loss: 0.3497072756290436\n",
      "Epoch: 171710 | Loss: 0.3532470166683197 | Test loss: 0.3496934175491333\n",
      "Epoch: 171720 | Loss: 0.35323530435562134 | Test loss: 0.349679559469223\n",
      "Epoch: 171730 | Loss: 0.3532235324382782 | Test loss: 0.34966573119163513\n",
      "Epoch: 171740 | Loss: 0.35321173071861267 | Test loss: 0.34965190291404724\n",
      "Epoch: 171750 | Loss: 0.3531999886035919 | Test loss: 0.34963804483413696\n",
      "Epoch: 171760 | Loss: 0.35318824648857117 | Test loss: 0.3496242165565491\n",
      "Epoch: 171770 | Loss: 0.353176474571228 | Test loss: 0.3496103584766388\n",
      "Epoch: 171780 | Loss: 0.3531647324562073 | Test loss: 0.3495965003967285\n",
      "Epoch: 171790 | Loss: 0.35315296053886414 | Test loss: 0.3495826721191406\n",
      "Epoch: 171800 | Loss: 0.353141188621521 | Test loss: 0.34956881403923035\n",
      "Epoch: 171810 | Loss: 0.35312944650650024 | Test loss: 0.34955498576164246\n",
      "Epoch: 171820 | Loss: 0.3531176745891571 | Test loss: 0.3495410978794098\n",
      "Epoch: 171830 | Loss: 0.35310593247413635 | Test loss: 0.3495272696018219\n",
      "Epoch: 171840 | Loss: 0.3530941605567932 | Test loss: 0.3495134115219116\n",
      "Epoch: 171850 | Loss: 0.3530823886394501 | Test loss: 0.34949955344200134\n",
      "Epoch: 171860 | Loss: 0.35307061672210693 | Test loss: 0.34948572516441345\n",
      "Epoch: 171870 | Loss: 0.3530588746070862 | Test loss: 0.34947189688682556\n",
      "Epoch: 171880 | Loss: 0.35304710268974304 | Test loss: 0.3494580388069153\n",
      "Epoch: 171890 | Loss: 0.3530353605747223 | Test loss: 0.3494442105293274\n",
      "Epoch: 171900 | Loss: 0.35302361845970154 | Test loss: 0.3494303226470947\n",
      "Epoch: 171910 | Loss: 0.353011816740036 | Test loss: 0.34941649436950684\n",
      "Epoch: 171920 | Loss: 0.35300007462501526 | Test loss: 0.34940266609191895\n",
      "Epoch: 171930 | Loss: 0.3529883325099945 | Test loss: 0.34938880801200867\n",
      "Epoch: 171940 | Loss: 0.35297656059265137 | Test loss: 0.3493749797344208\n",
      "Epoch: 171950 | Loss: 0.3529648184776306 | Test loss: 0.3493611514568329\n",
      "Epoch: 171960 | Loss: 0.3529530465602875 | Test loss: 0.3493472635746002\n",
      "Epoch: 171970 | Loss: 0.35294127464294434 | Test loss: 0.34933343529701233\n",
      "Epoch: 171980 | Loss: 0.3529295325279236 | Test loss: 0.34931957721710205\n",
      "Epoch: 171990 | Loss: 0.35291773080825806 | Test loss: 0.3493057191371918\n",
      "Epoch: 172000 | Loss: 0.3529059886932373 | Test loss: 0.3492918908596039\n",
      "Epoch: 172010 | Loss: 0.35289427638053894 | Test loss: 0.349278062582016\n",
      "Epoch: 172020 | Loss: 0.3528824746608734 | Test loss: 0.3492642045021057\n",
      "Epoch: 172030 | Loss: 0.3528707027435303 | Test loss: 0.34925034642219543\n",
      "Epoch: 172040 | Loss: 0.3528589904308319 | Test loss: 0.34923648834228516\n",
      "Epoch: 172050 | Loss: 0.3528471887111664 | Test loss: 0.34922266006469727\n",
      "Epoch: 172060 | Loss: 0.35283544659614563 | Test loss: 0.349208801984787\n",
      "Epoch: 172070 | Loss: 0.3528236746788025 | Test loss: 0.3491949737071991\n",
      "Epoch: 172080 | Loss: 0.35281193256378174 | Test loss: 0.3491811156272888\n",
      "Epoch: 172090 | Loss: 0.3528001308441162 | Test loss: 0.34916725754737854\n",
      "Epoch: 172100 | Loss: 0.35278838872909546 | Test loss: 0.34915342926979065\n",
      "Epoch: 172110 | Loss: 0.3527766168117523 | Test loss: 0.34913957118988037\n",
      "Epoch: 172120 | Loss: 0.35276490449905396 | Test loss: 0.3491257131099701\n",
      "Epoch: 172130 | Loss: 0.3527531325817108 | Test loss: 0.3491118848323822\n",
      "Epoch: 172140 | Loss: 0.3527413308620453 | Test loss: 0.3490980565547943\n",
      "Epoch: 172150 | Loss: 0.35272958874702454 | Test loss: 0.34908419847488403\n",
      "Epoch: 172160 | Loss: 0.3527178466320038 | Test loss: 0.34907037019729614\n",
      "Epoch: 172170 | Loss: 0.35270607471466064 | Test loss: 0.34905651211738586\n",
      "Epoch: 172180 | Loss: 0.3526943325996399 | Test loss: 0.3490426540374756\n",
      "Epoch: 172190 | Loss: 0.35268256068229675 | Test loss: 0.3490288257598877\n",
      "Epoch: 172200 | Loss: 0.3526707887649536 | Test loss: 0.3490149676799774\n",
      "Epoch: 172210 | Loss: 0.35265904664993286 | Test loss: 0.3490011394023895\n",
      "Epoch: 172220 | Loss: 0.3526472747325897 | Test loss: 0.34898725152015686\n",
      "Epoch: 172230 | Loss: 0.35263553261756897 | Test loss: 0.34897342324256897\n",
      "Epoch: 172240 | Loss: 0.35262376070022583 | Test loss: 0.3489595651626587\n",
      "Epoch: 172250 | Loss: 0.3526119887828827 | Test loss: 0.3489457070827484\n",
      "Epoch: 172260 | Loss: 0.35260021686553955 | Test loss: 0.3489318788051605\n",
      "Epoch: 172270 | Loss: 0.3525884747505188 | Test loss: 0.34891805052757263\n",
      "Epoch: 172280 | Loss: 0.35257670283317566 | Test loss: 0.34890419244766235\n",
      "Epoch: 172290 | Loss: 0.3525649607181549 | Test loss: 0.34889036417007446\n",
      "Epoch: 172300 | Loss: 0.35255321860313416 | Test loss: 0.3488764762878418\n",
      "Epoch: 172310 | Loss: 0.35254141688346863 | Test loss: 0.3488626480102539\n",
      "Epoch: 172320 | Loss: 0.3525296747684479 | Test loss: 0.348848819732666\n",
      "Epoch: 172330 | Loss: 0.3525179326534271 | Test loss: 0.34883496165275574\n",
      "Epoch: 172340 | Loss: 0.352506160736084 | Test loss: 0.34882113337516785\n",
      "Epoch: 172350 | Loss: 0.35249441862106323 | Test loss: 0.34880730509757996\n",
      "Epoch: 172360 | Loss: 0.3524826467037201 | Test loss: 0.3487934172153473\n",
      "Epoch: 172370 | Loss: 0.35247087478637695 | Test loss: 0.3487795889377594\n",
      "Epoch: 172380 | Loss: 0.3524591326713562 | Test loss: 0.3487657308578491\n",
      "Epoch: 172390 | Loss: 0.3524473309516907 | Test loss: 0.34875187277793884\n",
      "Epoch: 172400 | Loss: 0.3524355888366699 | Test loss: 0.34873804450035095\n",
      "Epoch: 172410 | Loss: 0.35242387652397156 | Test loss: 0.34872421622276306\n",
      "Epoch: 172420 | Loss: 0.35241207480430603 | Test loss: 0.3487103581428528\n",
      "Epoch: 172430 | Loss: 0.3524003028869629 | Test loss: 0.3486965000629425\n",
      "Epoch: 172440 | Loss: 0.3523885905742645 | Test loss: 0.3486826419830322\n",
      "Epoch: 172450 | Loss: 0.352376788854599 | Test loss: 0.34866881370544434\n",
      "Epoch: 172460 | Loss: 0.35236504673957825 | Test loss: 0.34865495562553406\n",
      "Epoch: 172470 | Loss: 0.3523532748222351 | Test loss: 0.34864112734794617\n",
      "Epoch: 172480 | Loss: 0.35234153270721436 | Test loss: 0.3486272692680359\n",
      "Epoch: 172490 | Loss: 0.35232973098754883 | Test loss: 0.3486134111881256\n",
      "Epoch: 172500 | Loss: 0.3523179888725281 | Test loss: 0.3485995829105377\n",
      "Epoch: 172510 | Loss: 0.35230621695518494 | Test loss: 0.34858572483062744\n",
      "Epoch: 172520 | Loss: 0.3522945046424866 | Test loss: 0.34857186675071716\n",
      "Epoch: 172530 | Loss: 0.35228273272514343 | Test loss: 0.3485580384731293\n",
      "Epoch: 172540 | Loss: 0.3522709310054779 | Test loss: 0.3485442101955414\n",
      "Epoch: 172550 | Loss: 0.35225918889045715 | Test loss: 0.3485303521156311\n",
      "Epoch: 172560 | Loss: 0.3522474467754364 | Test loss: 0.3485165238380432\n",
      "Epoch: 172570 | Loss: 0.35223567485809326 | Test loss: 0.34850266575813293\n",
      "Epoch: 172580 | Loss: 0.3522239327430725 | Test loss: 0.34848880767822266\n",
      "Epoch: 172590 | Loss: 0.35221216082572937 | Test loss: 0.34847497940063477\n",
      "Epoch: 172600 | Loss: 0.35220038890838623 | Test loss: 0.3484611213207245\n",
      "Epoch: 172610 | Loss: 0.3521886467933655 | Test loss: 0.3484472930431366\n",
      "Epoch: 172620 | Loss: 0.35217687487602234 | Test loss: 0.34843340516090393\n",
      "Epoch: 172630 | Loss: 0.3521651327610016 | Test loss: 0.34841957688331604\n",
      "Epoch: 172640 | Loss: 0.35215336084365845 | Test loss: 0.34840571880340576\n",
      "Epoch: 172650 | Loss: 0.3521415889263153 | Test loss: 0.3483918607234955\n",
      "Epoch: 172660 | Loss: 0.35212981700897217 | Test loss: 0.3483780324459076\n",
      "Epoch: 172670 | Loss: 0.3521180748939514 | Test loss: 0.3483642041683197\n",
      "Epoch: 172680 | Loss: 0.3521063029766083 | Test loss: 0.3483503460884094\n",
      "Epoch: 172690 | Loss: 0.3520945608615875 | Test loss: 0.34833651781082153\n",
      "Epoch: 172700 | Loss: 0.3520828187465668 | Test loss: 0.34832262992858887\n",
      "Epoch: 172710 | Loss: 0.35207101702690125 | Test loss: 0.348308801651001\n",
      "Epoch: 172720 | Loss: 0.3520592749118805 | Test loss: 0.3482949733734131\n",
      "Epoch: 172730 | Loss: 0.35204753279685974 | Test loss: 0.3482811152935028\n",
      "Epoch: 172740 | Loss: 0.3520357608795166 | Test loss: 0.3482672870159149\n",
      "Epoch: 172750 | Loss: 0.35202401876449585 | Test loss: 0.348253458738327\n",
      "Epoch: 172760 | Loss: 0.3520122468471527 | Test loss: 0.34823957085609436\n",
      "Epoch: 172770 | Loss: 0.35200047492980957 | Test loss: 0.34822574257850647\n",
      "Epoch: 172780 | Loss: 0.3519887328147888 | Test loss: 0.3482118844985962\n",
      "Epoch: 172790 | Loss: 0.3519769310951233 | Test loss: 0.3481980264186859\n",
      "Epoch: 172800 | Loss: 0.35196518898010254 | Test loss: 0.348184198141098\n",
      "Epoch: 172810 | Loss: 0.3519534766674042 | Test loss: 0.34817036986351013\n",
      "Epoch: 172820 | Loss: 0.35194167494773865 | Test loss: 0.34815651178359985\n",
      "Epoch: 172830 | Loss: 0.3519299030303955 | Test loss: 0.3481426537036896\n",
      "Epoch: 172840 | Loss: 0.35191819071769714 | Test loss: 0.3481287956237793\n",
      "Epoch: 172850 | Loss: 0.3519063889980316 | Test loss: 0.3481149673461914\n",
      "Epoch: 172860 | Loss: 0.35189464688301086 | Test loss: 0.34810110926628113\n",
      "Epoch: 172870 | Loss: 0.3518828749656677 | Test loss: 0.34808728098869324\n",
      "Epoch: 172880 | Loss: 0.351871132850647 | Test loss: 0.34807342290878296\n",
      "Epoch: 172890 | Loss: 0.35185933113098145 | Test loss: 0.3480595648288727\n",
      "Epoch: 172900 | Loss: 0.3518475890159607 | Test loss: 0.3480457365512848\n",
      "Epoch: 172910 | Loss: 0.35183581709861755 | Test loss: 0.3480318784713745\n",
      "Epoch: 172920 | Loss: 0.3518241047859192 | Test loss: 0.34801802039146423\n",
      "Epoch: 172930 | Loss: 0.35181233286857605 | Test loss: 0.34800419211387634\n",
      "Epoch: 172940 | Loss: 0.3518005311489105 | Test loss: 0.34799036383628845\n",
      "Epoch: 172950 | Loss: 0.35178878903388977 | Test loss: 0.3479765057563782\n",
      "Epoch: 172960 | Loss: 0.351777046918869 | Test loss: 0.3479626774787903\n",
      "Epoch: 172970 | Loss: 0.3517652750015259 | Test loss: 0.34794881939888\n",
      "Epoch: 172980 | Loss: 0.3517535328865051 | Test loss: 0.3479349613189697\n",
      "Epoch: 172990 | Loss: 0.351741760969162 | Test loss: 0.34792113304138184\n",
      "Epoch: 173000 | Loss: 0.35172998905181885 | Test loss: 0.34790727496147156\n",
      "Epoch: 173010 | Loss: 0.3517182469367981 | Test loss: 0.34789344668388367\n",
      "Epoch: 173020 | Loss: 0.35170647501945496 | Test loss: 0.347879558801651\n",
      "Epoch: 173030 | Loss: 0.3516947329044342 | Test loss: 0.3478657305240631\n",
      "Epoch: 173040 | Loss: 0.35168296098709106 | Test loss: 0.34785187244415283\n",
      "Epoch: 173050 | Loss: 0.3516711890697479 | Test loss: 0.34783801436424255\n",
      "Epoch: 173060 | Loss: 0.3516594171524048 | Test loss: 0.34782418608665466\n",
      "Epoch: 173070 | Loss: 0.35164767503738403 | Test loss: 0.3478103578090668\n",
      "Epoch: 173080 | Loss: 0.3516359031200409 | Test loss: 0.3477964997291565\n",
      "Epoch: 173090 | Loss: 0.35162416100502014 | Test loss: 0.3477826714515686\n",
      "Epoch: 173100 | Loss: 0.3516124188899994 | Test loss: 0.34776878356933594\n",
      "Epoch: 173110 | Loss: 0.35160061717033386 | Test loss: 0.34775495529174805\n",
      "Epoch: 173120 | Loss: 0.3515888750553131 | Test loss: 0.34774112701416016\n",
      "Epoch: 173130 | Loss: 0.35157713294029236 | Test loss: 0.3477272689342499\n",
      "Epoch: 173140 | Loss: 0.3515653610229492 | Test loss: 0.347713440656662\n",
      "Epoch: 173150 | Loss: 0.35155361890792847 | Test loss: 0.3476996123790741\n",
      "Epoch: 173160 | Loss: 0.3515418469905853 | Test loss: 0.34768572449684143\n",
      "Epoch: 173170 | Loss: 0.3515300750732422 | Test loss: 0.34767189621925354\n",
      "Epoch: 173180 | Loss: 0.35151833295822144 | Test loss: 0.34765803813934326\n",
      "Epoch: 173190 | Loss: 0.3515065312385559 | Test loss: 0.347644180059433\n",
      "Epoch: 173200 | Loss: 0.35149478912353516 | Test loss: 0.3476303517818451\n",
      "Epoch: 173210 | Loss: 0.3514830768108368 | Test loss: 0.3476165235042572\n",
      "Epoch: 173220 | Loss: 0.35147127509117126 | Test loss: 0.3476026654243469\n",
      "Epoch: 173230 | Loss: 0.3514595031738281 | Test loss: 0.34758880734443665\n",
      "Epoch: 173240 | Loss: 0.35144779086112976 | Test loss: 0.34757494926452637\n",
      "Epoch: 173250 | Loss: 0.35143598914146423 | Test loss: 0.3475611209869385\n",
      "Epoch: 173260 | Loss: 0.3514242470264435 | Test loss: 0.3475472629070282\n",
      "Epoch: 173270 | Loss: 0.35141250491142273 | Test loss: 0.3475334346294403\n",
      "Epoch: 173280 | Loss: 0.3514007329940796 | Test loss: 0.34751957654953003\n",
      "Epoch: 173290 | Loss: 0.35138893127441406 | Test loss: 0.34750571846961975\n",
      "Epoch: 173300 | Loss: 0.3513771891593933 | Test loss: 0.34749189019203186\n",
      "Epoch: 173310 | Loss: 0.35136541724205017 | Test loss: 0.3474780321121216\n",
      "Epoch: 173320 | Loss: 0.3513537049293518 | Test loss: 0.3474641740322113\n",
      "Epoch: 173330 | Loss: 0.35134193301200867 | Test loss: 0.3474503457546234\n",
      "Epoch: 173340 | Loss: 0.35133013129234314 | Test loss: 0.3474365174770355\n",
      "Epoch: 173350 | Loss: 0.3513183891773224 | Test loss: 0.34742265939712524\n",
      "Epoch: 173360 | Loss: 0.35130664706230164 | Test loss: 0.34740883111953735\n",
      "Epoch: 173370 | Loss: 0.3512948751449585 | Test loss: 0.3473949730396271\n",
      "Epoch: 173380 | Loss: 0.35128313302993774 | Test loss: 0.3473811149597168\n",
      "Epoch: 173390 | Loss: 0.3512713611125946 | Test loss: 0.3473672866821289\n",
      "Epoch: 173400 | Loss: 0.35125958919525146 | Test loss: 0.34735342860221863\n",
      "Epoch: 173410 | Loss: 0.3512478470802307 | Test loss: 0.34733960032463074\n",
      "Epoch: 173420 | Loss: 0.3512360751628876 | Test loss: 0.34732571244239807\n",
      "Epoch: 173430 | Loss: 0.3512243330478668 | Test loss: 0.3473118841648102\n",
      "Epoch: 173440 | Loss: 0.3512125611305237 | Test loss: 0.3472980260848999\n",
      "Epoch: 173450 | Loss: 0.35120078921318054 | Test loss: 0.3472841680049896\n",
      "Epoch: 173460 | Loss: 0.3511890172958374 | Test loss: 0.34727033972740173\n",
      "Epoch: 173470 | Loss: 0.35117730498313904 | Test loss: 0.34725651144981384\n",
      "Epoch: 173480 | Loss: 0.3511655032634735 | Test loss: 0.34724265336990356\n",
      "Epoch: 173490 | Loss: 0.35115376114845276 | Test loss: 0.3472288250923157\n",
      "Epoch: 173500 | Loss: 0.351142019033432 | Test loss: 0.347214937210083\n",
      "Epoch: 173510 | Loss: 0.3511302173137665 | Test loss: 0.3472011089324951\n",
      "Epoch: 173520 | Loss: 0.3511184751987457 | Test loss: 0.3471872806549072\n",
      "Epoch: 173530 | Loss: 0.351106733083725 | Test loss: 0.34717342257499695\n",
      "Epoch: 173540 | Loss: 0.35109496116638184 | Test loss: 0.34715959429740906\n",
      "Epoch: 173550 | Loss: 0.3510832190513611 | Test loss: 0.34714576601982117\n",
      "Epoch: 173560 | Loss: 0.35107144713401794 | Test loss: 0.3471318781375885\n",
      "Epoch: 173570 | Loss: 0.3510596752166748 | Test loss: 0.3471180498600006\n",
      "Epoch: 173580 | Loss: 0.35104793310165405 | Test loss: 0.34710419178009033\n",
      "Epoch: 173590 | Loss: 0.3510361313819885 | Test loss: 0.34709033370018005\n",
      "Epoch: 173600 | Loss: 0.3510243892669678 | Test loss: 0.34707650542259216\n",
      "Epoch: 173610 | Loss: 0.3510126769542694 | Test loss: 0.3470626771450043\n",
      "Epoch: 173620 | Loss: 0.3510008752346039 | Test loss: 0.347048819065094\n",
      "Epoch: 173630 | Loss: 0.35098910331726074 | Test loss: 0.3470349609851837\n",
      "Epoch: 173640 | Loss: 0.3509773910045624 | Test loss: 0.34702110290527344\n",
      "Epoch: 173650 | Loss: 0.35096558928489685 | Test loss: 0.34700727462768555\n",
      "Epoch: 173660 | Loss: 0.3509538471698761 | Test loss: 0.34699341654777527\n",
      "Epoch: 173670 | Loss: 0.35094210505485535 | Test loss: 0.3469795882701874\n",
      "Epoch: 173680 | Loss: 0.3509303331375122 | Test loss: 0.3469657301902771\n",
      "Epoch: 173690 | Loss: 0.3509185314178467 | Test loss: 0.3469518721103668\n",
      "Epoch: 173700 | Loss: 0.3509067893028259 | Test loss: 0.34693804383277893\n",
      "Epoch: 173710 | Loss: 0.3508950173854828 | Test loss: 0.34692418575286865\n",
      "Epoch: 173720 | Loss: 0.3508833050727844 | Test loss: 0.3469103276729584\n",
      "Epoch: 173730 | Loss: 0.3508715331554413 | Test loss: 0.3468964993953705\n",
      "Epoch: 173740 | Loss: 0.35085973143577576 | Test loss: 0.3468826711177826\n",
      "Epoch: 173750 | Loss: 0.350847989320755 | Test loss: 0.3468688130378723\n",
      "Epoch: 173760 | Loss: 0.35083624720573425 | Test loss: 0.3468549847602844\n",
      "Epoch: 173770 | Loss: 0.3508244752883911 | Test loss: 0.34684112668037415\n",
      "Epoch: 173780 | Loss: 0.35081273317337036 | Test loss: 0.34682726860046387\n",
      "Epoch: 173790 | Loss: 0.3508009612560272 | Test loss: 0.346813440322876\n",
      "Epoch: 173800 | Loss: 0.3507891893386841 | Test loss: 0.3467995822429657\n",
      "Epoch: 173810 | Loss: 0.35077741742134094 | Test loss: 0.3467857539653778\n",
      "Epoch: 173820 | Loss: 0.3507656753063202 | Test loss: 0.34677186608314514\n",
      "Epoch: 173830 | Loss: 0.35075393319129944 | Test loss: 0.34675803780555725\n",
      "Epoch: 173840 | Loss: 0.3507421612739563 | Test loss: 0.346744179725647\n",
      "Epoch: 173850 | Loss: 0.35073038935661316 | Test loss: 0.3467303216457367\n",
      "Epoch: 173860 | Loss: 0.35071861743927 | Test loss: 0.3467164933681488\n",
      "Epoch: 173870 | Loss: 0.35070690512657166 | Test loss: 0.3467026650905609\n",
      "Epoch: 173880 | Loss: 0.35069510340690613 | Test loss: 0.34668880701065063\n",
      "Epoch: 173890 | Loss: 0.3506833612918854 | Test loss: 0.34667497873306274\n",
      "Epoch: 173900 | Loss: 0.3506716191768646 | Test loss: 0.3466610908508301\n",
      "Epoch: 173910 | Loss: 0.3506598174571991 | Test loss: 0.3466472625732422\n",
      "Epoch: 173920 | Loss: 0.35064807534217834 | Test loss: 0.3466334342956543\n",
      "Epoch: 173930 | Loss: 0.3506363034248352 | Test loss: 0.346619576215744\n",
      "Epoch: 173940 | Loss: 0.35062456130981445 | Test loss: 0.34660574793815613\n",
      "Epoch: 173950 | Loss: 0.3506128191947937 | Test loss: 0.34659191966056824\n",
      "Epoch: 173960 | Loss: 0.35060104727745056 | Test loss: 0.34657803177833557\n",
      "Epoch: 173970 | Loss: 0.3505892753601074 | Test loss: 0.3465642035007477\n",
      "Epoch: 173980 | Loss: 0.35057753324508667 | Test loss: 0.3465503454208374\n",
      "Epoch: 173990 | Loss: 0.35056573152542114 | Test loss: 0.3465364873409271\n",
      "Epoch: 174000 | Loss: 0.3505539894104004 | Test loss: 0.34652265906333923\n",
      "Epoch: 174010 | Loss: 0.35054224729537964 | Test loss: 0.34650883078575134\n",
      "Epoch: 174020 | Loss: 0.3505304753780365 | Test loss: 0.34649497270584106\n",
      "Epoch: 174030 | Loss: 0.35051870346069336 | Test loss: 0.3464811146259308\n",
      "Epoch: 174040 | Loss: 0.350506991147995 | Test loss: 0.3464672565460205\n",
      "Epoch: 174050 | Loss: 0.35049518942832947 | Test loss: 0.3464534282684326\n",
      "Epoch: 174060 | Loss: 0.3504834473133087 | Test loss: 0.34643957018852234\n",
      "Epoch: 174070 | Loss: 0.35047170519828796 | Test loss: 0.34642574191093445\n",
      "Epoch: 174080 | Loss: 0.3504599332809448 | Test loss: 0.34641188383102417\n",
      "Epoch: 174090 | Loss: 0.3504481315612793 | Test loss: 0.3463980257511139\n",
      "Epoch: 174100 | Loss: 0.35043638944625854 | Test loss: 0.346384197473526\n",
      "Epoch: 174110 | Loss: 0.3504246175289154 | Test loss: 0.3463703393936157\n",
      "Epoch: 174120 | Loss: 0.35041290521621704 | Test loss: 0.34635648131370544\n",
      "Epoch: 174130 | Loss: 0.3504011332988739 | Test loss: 0.34634265303611755\n",
      "Epoch: 174140 | Loss: 0.3503893315792084 | Test loss: 0.34632882475852966\n",
      "Epoch: 174150 | Loss: 0.3503775894641876 | Test loss: 0.3463149666786194\n",
      "Epoch: 174160 | Loss: 0.35036584734916687 | Test loss: 0.3463011384010315\n",
      "Epoch: 174170 | Loss: 0.35035407543182373 | Test loss: 0.3462872803211212\n",
      "Epoch: 174180 | Loss: 0.350342333316803 | Test loss: 0.34627342224121094\n",
      "Epoch: 174190 | Loss: 0.35033056139945984 | Test loss: 0.34625959396362305\n",
      "Epoch: 174200 | Loss: 0.3503187894821167 | Test loss: 0.34624573588371277\n",
      "Epoch: 174210 | Loss: 0.35030701756477356 | Test loss: 0.3462319076061249\n",
      "Epoch: 174220 | Loss: 0.3502952754497528 | Test loss: 0.3462180197238922\n",
      "Epoch: 174230 | Loss: 0.35028353333473206 | Test loss: 0.3462041914463043\n",
      "Epoch: 174240 | Loss: 0.3502717614173889 | Test loss: 0.34619033336639404\n",
      "Epoch: 174250 | Loss: 0.3502599895000458 | Test loss: 0.34617647528648376\n",
      "Epoch: 174260 | Loss: 0.35024821758270264 | Test loss: 0.3461626470088959\n",
      "Epoch: 174270 | Loss: 0.3502365052700043 | Test loss: 0.346148818731308\n",
      "Epoch: 174280 | Loss: 0.35022470355033875 | Test loss: 0.3461349606513977\n",
      "Epoch: 174290 | Loss: 0.350212961435318 | Test loss: 0.3461211323738098\n",
      "Epoch: 174300 | Loss: 0.35020121932029724 | Test loss: 0.34610724449157715\n",
      "Epoch: 174310 | Loss: 0.3501894176006317 | Test loss: 0.34609341621398926\n",
      "Epoch: 174320 | Loss: 0.35017767548561096 | Test loss: 0.34607958793640137\n",
      "Epoch: 174330 | Loss: 0.3501659035682678 | Test loss: 0.3460657298564911\n",
      "Epoch: 174340 | Loss: 0.35015416145324707 | Test loss: 0.3460519015789032\n",
      "Epoch: 174350 | Loss: 0.3501424193382263 | Test loss: 0.3460380733013153\n",
      "Epoch: 174360 | Loss: 0.3501306474208832 | Test loss: 0.34602418541908264\n",
      "Epoch: 174370 | Loss: 0.35011887550354004 | Test loss: 0.34601035714149475\n",
      "Epoch: 174380 | Loss: 0.3501071333885193 | Test loss: 0.3459964990615845\n",
      "Epoch: 174390 | Loss: 0.35009533166885376 | Test loss: 0.3459826409816742\n",
      "Epoch: 174400 | Loss: 0.350083589553833 | Test loss: 0.3459688127040863\n",
      "Epoch: 174410 | Loss: 0.35007184743881226 | Test loss: 0.3459549844264984\n",
      "Epoch: 174420 | Loss: 0.3500600755214691 | Test loss: 0.34594112634658813\n",
      "Epoch: 174430 | Loss: 0.350048303604126 | Test loss: 0.34592726826667786\n",
      "Epoch: 174440 | Loss: 0.3500365912914276 | Test loss: 0.3459134101867676\n",
      "Epoch: 174450 | Loss: 0.3500247895717621 | Test loss: 0.3458995819091797\n",
      "Epoch: 174460 | Loss: 0.35001304745674133 | Test loss: 0.3458857238292694\n",
      "Epoch: 174470 | Loss: 0.3500013053417206 | Test loss: 0.3458718955516815\n",
      "Epoch: 174480 | Loss: 0.34998953342437744 | Test loss: 0.34585803747177124\n",
      "Epoch: 174490 | Loss: 0.3499777317047119 | Test loss: 0.34584417939186096\n",
      "Epoch: 174500 | Loss: 0.34996598958969116 | Test loss: 0.34583035111427307\n",
      "Epoch: 174510 | Loss: 0.349954217672348 | Test loss: 0.3458164930343628\n",
      "Epoch: 174520 | Loss: 0.34994250535964966 | Test loss: 0.3458026349544525\n",
      "Epoch: 174530 | Loss: 0.3499307334423065 | Test loss: 0.3457888066768646\n",
      "Epoch: 174540 | Loss: 0.349918931722641 | Test loss: 0.34577497839927673\n",
      "Epoch: 174550 | Loss: 0.34990718960762024 | Test loss: 0.34576112031936646\n",
      "Epoch: 174560 | Loss: 0.3498954474925995 | Test loss: 0.34574729204177856\n",
      "Epoch: 174570 | Loss: 0.34988367557525635 | Test loss: 0.3457334339618683\n",
      "Epoch: 174580 | Loss: 0.3498719334602356 | Test loss: 0.345719575881958\n",
      "Epoch: 174590 | Loss: 0.34986016154289246 | Test loss: 0.3457057476043701\n",
      "Epoch: 174600 | Loss: 0.3498483896255493 | Test loss: 0.34569188952445984\n",
      "Epoch: 174610 | Loss: 0.3498366177082062 | Test loss: 0.34567806124687195\n",
      "Epoch: 174620 | Loss: 0.3498248755931854 | Test loss: 0.3456641733646393\n",
      "Epoch: 174630 | Loss: 0.3498131334781647 | Test loss: 0.3456503450870514\n",
      "Epoch: 174640 | Loss: 0.34980136156082153 | Test loss: 0.3456364870071411\n",
      "Epoch: 174650 | Loss: 0.3497895896434784 | Test loss: 0.34562262892723083\n",
      "Epoch: 174660 | Loss: 0.34977781772613525 | Test loss: 0.34560880064964294\n",
      "Epoch: 174670 | Loss: 0.3497661054134369 | Test loss: 0.34559497237205505\n",
      "Epoch: 174680 | Loss: 0.34975430369377136 | Test loss: 0.3455811142921448\n",
      "Epoch: 174690 | Loss: 0.3497425615787506 | Test loss: 0.3455672860145569\n",
      "Epoch: 174700 | Loss: 0.34973081946372986 | Test loss: 0.3455533981323242\n",
      "Epoch: 174710 | Loss: 0.34971901774406433 | Test loss: 0.34553956985473633\n",
      "Epoch: 174720 | Loss: 0.3497072756290436 | Test loss: 0.34552574157714844\n",
      "Epoch: 174730 | Loss: 0.34969550371170044 | Test loss: 0.34551188349723816\n",
      "Epoch: 174740 | Loss: 0.3496837615966797 | Test loss: 0.34549805521965027\n",
      "Epoch: 174750 | Loss: 0.34967201948165894 | Test loss: 0.3454842269420624\n",
      "Epoch: 174760 | Loss: 0.3496602475643158 | Test loss: 0.3454703390598297\n",
      "Epoch: 174770 | Loss: 0.34964847564697266 | Test loss: 0.3454565107822418\n",
      "Epoch: 174780 | Loss: 0.3496367335319519 | Test loss: 0.34544265270233154\n",
      "Epoch: 174790 | Loss: 0.3496249318122864 | Test loss: 0.34542879462242126\n",
      "Epoch: 174800 | Loss: 0.3496131896972656 | Test loss: 0.3454149663448334\n",
      "Epoch: 174810 | Loss: 0.3496014475822449 | Test loss: 0.3454011380672455\n",
      "Epoch: 174820 | Loss: 0.34958967566490173 | Test loss: 0.3453872799873352\n",
      "Epoch: 174830 | Loss: 0.3495779037475586 | Test loss: 0.3453734219074249\n",
      "Epoch: 174840 | Loss: 0.34956619143486023 | Test loss: 0.34535956382751465\n",
      "Epoch: 174850 | Loss: 0.3495543897151947 | Test loss: 0.34534573554992676\n",
      "Epoch: 174860 | Loss: 0.34954264760017395 | Test loss: 0.3453318774700165\n",
      "Epoch: 174870 | Loss: 0.3495309054851532 | Test loss: 0.3453180491924286\n",
      "Epoch: 174880 | Loss: 0.34951913356781006 | Test loss: 0.3453041911125183\n",
      "Epoch: 174890 | Loss: 0.34950733184814453 | Test loss: 0.34529033303260803\n",
      "Epoch: 174900 | Loss: 0.3494955897331238 | Test loss: 0.34527650475502014\n",
      "Epoch: 174910 | Loss: 0.34948381781578064 | Test loss: 0.34526264667510986\n",
      "Epoch: 174920 | Loss: 0.3494721055030823 | Test loss: 0.3452487885951996\n",
      "Epoch: 174930 | Loss: 0.34946033358573914 | Test loss: 0.3452349603176117\n",
      "Epoch: 174940 | Loss: 0.3494485318660736 | Test loss: 0.3452211320400238\n",
      "Epoch: 174950 | Loss: 0.34943678975105286 | Test loss: 0.3452072739601135\n",
      "Epoch: 174960 | Loss: 0.3494250476360321 | Test loss: 0.34519344568252563\n",
      "Epoch: 174970 | Loss: 0.34941327571868896 | Test loss: 0.34517958760261536\n",
      "Epoch: 174980 | Loss: 0.3494015336036682 | Test loss: 0.3451657295227051\n",
      "Epoch: 174990 | Loss: 0.3493897616863251 | Test loss: 0.3451519012451172\n",
      "Epoch: 175000 | Loss: 0.34937798976898193 | Test loss: 0.3451380431652069\n",
      "Epoch: 175010 | Loss: 0.3493662178516388 | Test loss: 0.345124214887619\n",
      "Epoch: 175020 | Loss: 0.34935447573661804 | Test loss: 0.34511032700538635\n",
      "Epoch: 175030 | Loss: 0.3493427336215973 | Test loss: 0.34509649872779846\n",
      "Epoch: 175040 | Loss: 0.34933096170425415 | Test loss: 0.3450826406478882\n",
      "Epoch: 175050 | Loss: 0.349319189786911 | Test loss: 0.3450687825679779\n",
      "Epoch: 175060 | Loss: 0.34930741786956787 | Test loss: 0.34505495429039\n",
      "Epoch: 175070 | Loss: 0.3492957055568695 | Test loss: 0.3450411260128021\n",
      "Epoch: 175080 | Loss: 0.349283903837204 | Test loss: 0.34502726793289185\n",
      "Epoch: 175090 | Loss: 0.3492721617221832 | Test loss: 0.34501343965530396\n",
      "Epoch: 175100 | Loss: 0.3492604196071625 | Test loss: 0.3449995517730713\n",
      "Epoch: 175110 | Loss: 0.34924861788749695 | Test loss: 0.3449857234954834\n",
      "Epoch: 175120 | Loss: 0.3492368757724762 | Test loss: 0.3449718952178955\n",
      "Epoch: 175130 | Loss: 0.34922510385513306 | Test loss: 0.34495803713798523\n",
      "Epoch: 175140 | Loss: 0.3492133617401123 | Test loss: 0.34494420886039734\n",
      "Epoch: 175150 | Loss: 0.34920161962509155 | Test loss: 0.34493038058280945\n",
      "Epoch: 175160 | Loss: 0.3491898477077484 | Test loss: 0.3449164927005768\n",
      "Epoch: 175170 | Loss: 0.3491780757904053 | Test loss: 0.3449026644229889\n",
      "Epoch: 175180 | Loss: 0.3491663336753845 | Test loss: 0.3448888063430786\n",
      "Epoch: 175190 | Loss: 0.349154531955719 | Test loss: 0.34487494826316833\n",
      "Epoch: 175200 | Loss: 0.34914278984069824 | Test loss: 0.34486111998558044\n",
      "Epoch: 175210 | Loss: 0.3491310477256775 | Test loss: 0.34484729170799255\n",
      "Epoch: 175220 | Loss: 0.34911927580833435 | Test loss: 0.3448334336280823\n",
      "Epoch: 175230 | Loss: 0.3491075038909912 | Test loss: 0.344819575548172\n",
      "Epoch: 175240 | Loss: 0.34909579157829285 | Test loss: 0.3448057174682617\n",
      "Epoch: 175250 | Loss: 0.3490839898586273 | Test loss: 0.34479188919067383\n",
      "Epoch: 175260 | Loss: 0.34907224774360657 | Test loss: 0.34477803111076355\n",
      "Epoch: 175270 | Loss: 0.3490605056285858 | Test loss: 0.34476420283317566\n",
      "Epoch: 175280 | Loss: 0.3490487337112427 | Test loss: 0.3447503447532654\n",
      "Epoch: 175290 | Loss: 0.34903693199157715 | Test loss: 0.3447364866733551\n",
      "Epoch: 175300 | Loss: 0.3490251898765564 | Test loss: 0.3447226583957672\n",
      "Epoch: 175310 | Loss: 0.34901341795921326 | Test loss: 0.34470880031585693\n",
      "Epoch: 175320 | Loss: 0.3490017056465149 | Test loss: 0.34469494223594666\n",
      "Epoch: 175330 | Loss: 0.34898993372917175 | Test loss: 0.34468111395835876\n",
      "Epoch: 175340 | Loss: 0.3489781320095062 | Test loss: 0.3446672856807709\n",
      "Epoch: 175350 | Loss: 0.3489663898944855 | Test loss: 0.3446534276008606\n",
      "Epoch: 175360 | Loss: 0.3489546477794647 | Test loss: 0.3446395993232727\n",
      "Epoch: 175370 | Loss: 0.3489428758621216 | Test loss: 0.3446257412433624\n",
      "Epoch: 175380 | Loss: 0.34893113374710083 | Test loss: 0.34461188316345215\n",
      "Epoch: 175390 | Loss: 0.3489193618297577 | Test loss: 0.34459805488586426\n",
      "Epoch: 175400 | Loss: 0.34890758991241455 | Test loss: 0.344584196805954\n",
      "Epoch: 175410 | Loss: 0.3488958179950714 | Test loss: 0.3445703685283661\n",
      "Epoch: 175420 | Loss: 0.34888407588005066 | Test loss: 0.3445564806461334\n",
      "Epoch: 175430 | Loss: 0.3488723337650299 | Test loss: 0.34454265236854553\n",
      "Epoch: 175440 | Loss: 0.34886056184768677 | Test loss: 0.34452879428863525\n",
      "Epoch: 175450 | Loss: 0.34884878993034363 | Test loss: 0.344514936208725\n",
      "Epoch: 175460 | Loss: 0.3488370180130005 | Test loss: 0.3445011079311371\n",
      "Epoch: 175470 | Loss: 0.3488253057003021 | Test loss: 0.3444872796535492\n",
      "Epoch: 175480 | Loss: 0.3488135039806366 | Test loss: 0.3444734215736389\n",
      "Epoch: 175490 | Loss: 0.34880176186561584 | Test loss: 0.344459593296051\n",
      "Epoch: 175500 | Loss: 0.3487900197505951 | Test loss: 0.34444570541381836\n",
      "Epoch: 175510 | Loss: 0.34877821803092957 | Test loss: 0.34443187713623047\n",
      "Epoch: 175520 | Loss: 0.3487664759159088 | Test loss: 0.3444180488586426\n",
      "Epoch: 175530 | Loss: 0.3487547039985657 | Test loss: 0.3444041907787323\n",
      "Epoch: 175540 | Loss: 0.3487429618835449 | Test loss: 0.3443903625011444\n",
      "Epoch: 175550 | Loss: 0.34873121976852417 | Test loss: 0.3443765342235565\n",
      "Epoch: 175560 | Loss: 0.34871944785118103 | Test loss: 0.34436264634132385\n",
      "Epoch: 175570 | Loss: 0.3487076759338379 | Test loss: 0.34434881806373596\n",
      "Epoch: 175580 | Loss: 0.34869593381881714 | Test loss: 0.3443349599838257\n",
      "Epoch: 175590 | Loss: 0.3486841320991516 | Test loss: 0.3443211019039154\n",
      "Epoch: 175600 | Loss: 0.34867238998413086 | Test loss: 0.3443072736263275\n",
      "Epoch: 175610 | Loss: 0.3486606478691101 | Test loss: 0.3442934453487396\n",
      "Epoch: 175620 | Loss: 0.34864887595176697 | Test loss: 0.34427958726882935\n",
      "Epoch: 175630 | Loss: 0.34863710403442383 | Test loss: 0.34426572918891907\n",
      "Epoch: 175640 | Loss: 0.34862539172172546 | Test loss: 0.3442518711090088\n",
      "Epoch: 175650 | Loss: 0.34861359000205994 | Test loss: 0.3442380428314209\n",
      "Epoch: 175660 | Loss: 0.3486018478870392 | Test loss: 0.3442241847515106\n",
      "Epoch: 175670 | Loss: 0.34859010577201843 | Test loss: 0.34421035647392273\n",
      "Epoch: 175680 | Loss: 0.3485783338546753 | Test loss: 0.34419649839401245\n",
      "Epoch: 175690 | Loss: 0.34856653213500977 | Test loss: 0.3441826403141022\n",
      "Epoch: 175700 | Loss: 0.348554790019989 | Test loss: 0.3441688120365143\n",
      "Epoch: 175710 | Loss: 0.3485430181026459 | Test loss: 0.344154953956604\n",
      "Epoch: 175720 | Loss: 0.3485313057899475 | Test loss: 0.3441410958766937\n",
      "Epoch: 175730 | Loss: 0.34851953387260437 | Test loss: 0.34412726759910583\n",
      "Epoch: 175740 | Loss: 0.34850773215293884 | Test loss: 0.34411343932151794\n",
      "Epoch: 175750 | Loss: 0.3484959900379181 | Test loss: 0.34409958124160767\n",
      "Epoch: 175760 | Loss: 0.34848424792289734 | Test loss: 0.3440857529640198\n",
      "Epoch: 175770 | Loss: 0.3484724760055542 | Test loss: 0.3440718948841095\n",
      "Epoch: 175780 | Loss: 0.34846073389053345 | Test loss: 0.3440580368041992\n",
      "Epoch: 175790 | Loss: 0.3484489619731903 | Test loss: 0.34404420852661133\n",
      "Epoch: 175800 | Loss: 0.34843719005584717 | Test loss: 0.34403035044670105\n",
      "Epoch: 175810 | Loss: 0.34842541813850403 | Test loss: 0.34401652216911316\n",
      "Epoch: 175820 | Loss: 0.3484136760234833 | Test loss: 0.3440026342868805\n",
      "Epoch: 175830 | Loss: 0.3484019339084625 | Test loss: 0.3439888060092926\n",
      "Epoch: 175840 | Loss: 0.3483901619911194 | Test loss: 0.3439749479293823\n",
      "Epoch: 175850 | Loss: 0.34837839007377625 | Test loss: 0.34396108984947205\n",
      "Epoch: 175860 | Loss: 0.3483666181564331 | Test loss: 0.34394726157188416\n",
      "Epoch: 175870 | Loss: 0.34835490584373474 | Test loss: 0.34393343329429626\n",
      "Epoch: 175880 | Loss: 0.3483431041240692 | Test loss: 0.343919575214386\n",
      "Epoch: 175890 | Loss: 0.34833136200904846 | Test loss: 0.3439057469367981\n",
      "Epoch: 175900 | Loss: 0.3483196198940277 | Test loss: 0.34389185905456543\n",
      "Epoch: 175910 | Loss: 0.3483078181743622 | Test loss: 0.34387803077697754\n",
      "Epoch: 175920 | Loss: 0.34829607605934143 | Test loss: 0.34386420249938965\n",
      "Epoch: 175930 | Loss: 0.3482843041419983 | Test loss: 0.34385034441947937\n",
      "Epoch: 175940 | Loss: 0.34827256202697754 | Test loss: 0.3438365161418915\n",
      "Epoch: 175950 | Loss: 0.3482608199119568 | Test loss: 0.3438226878643036\n",
      "Epoch: 175960 | Loss: 0.34824904799461365 | Test loss: 0.3438087999820709\n",
      "Epoch: 175970 | Loss: 0.3482372760772705 | Test loss: 0.34379497170448303\n",
      "Epoch: 175980 | Loss: 0.34822553396224976 | Test loss: 0.34378111362457275\n",
      "Epoch: 175990 | Loss: 0.34821373224258423 | Test loss: 0.3437672555446625\n",
      "Epoch: 176000 | Loss: 0.3482019901275635 | Test loss: 0.3437534272670746\n",
      "Epoch: 176010 | Loss: 0.3481902480125427 | Test loss: 0.3437395989894867\n",
      "Epoch: 176020 | Loss: 0.3481784760951996 | Test loss: 0.3437257409095764\n",
      "Epoch: 176030 | Loss: 0.34816670417785645 | Test loss: 0.34371188282966614\n",
      "Epoch: 176040 | Loss: 0.3481549918651581 | Test loss: 0.34369802474975586\n",
      "Epoch: 176050 | Loss: 0.34814319014549255 | Test loss: 0.34368419647216797\n",
      "Epoch: 176060 | Loss: 0.3481314480304718 | Test loss: 0.3436703383922577\n",
      "Epoch: 176070 | Loss: 0.34811970591545105 | Test loss: 0.3436565101146698\n",
      "Epoch: 176080 | Loss: 0.3481079339981079 | Test loss: 0.3436426520347595\n",
      "Epoch: 176090 | Loss: 0.3480961322784424 | Test loss: 0.34362879395484924\n",
      "Epoch: 176100 | Loss: 0.34808439016342163 | Test loss: 0.34361496567726135\n",
      "Epoch: 176110 | Loss: 0.3480726182460785 | Test loss: 0.3436011075973511\n",
      "Epoch: 176120 | Loss: 0.3480609059333801 | Test loss: 0.3435872495174408\n",
      "Epoch: 176130 | Loss: 0.348049134016037 | Test loss: 0.3435734212398529\n",
      "Epoch: 176140 | Loss: 0.34803733229637146 | Test loss: 0.343559592962265\n",
      "Epoch: 176150 | Loss: 0.3480255901813507 | Test loss: 0.34354573488235474\n",
      "Epoch: 176160 | Loss: 0.34801384806632996 | Test loss: 0.34353190660476685\n",
      "Epoch: 176170 | Loss: 0.3480020761489868 | Test loss: 0.34351804852485657\n",
      "Epoch: 176180 | Loss: 0.34799033403396606 | Test loss: 0.3435041904449463\n",
      "Epoch: 176190 | Loss: 0.3479785621166229 | Test loss: 0.3434903621673584\n",
      "Epoch: 176200 | Loss: 0.3479667901992798 | Test loss: 0.3434765040874481\n",
      "Epoch: 176210 | Loss: 0.34795501828193665 | Test loss: 0.34346267580986023\n",
      "Epoch: 176220 | Loss: 0.3479432761669159 | Test loss: 0.34344878792762756\n",
      "Epoch: 176230 | Loss: 0.34793153405189514 | Test loss: 0.3434349596500397\n",
      "Epoch: 176240 | Loss: 0.347919762134552 | Test loss: 0.3434211015701294\n",
      "Epoch: 176250 | Loss: 0.34790799021720886 | Test loss: 0.3434072434902191\n",
      "Epoch: 176260 | Loss: 0.3478962182998657 | Test loss: 0.3433934152126312\n",
      "Epoch: 176270 | Loss: 0.34788450598716736 | Test loss: 0.34337958693504333\n",
      "Epoch: 176280 | Loss: 0.34787270426750183 | Test loss: 0.34336572885513306\n",
      "Epoch: 176290 | Loss: 0.3478609621524811 | Test loss: 0.34335190057754517\n",
      "Epoch: 176300 | Loss: 0.3478492200374603 | Test loss: 0.3433380126953125\n",
      "Epoch: 176310 | Loss: 0.3478374183177948 | Test loss: 0.3433241844177246\n",
      "Epoch: 176320 | Loss: 0.34782567620277405 | Test loss: 0.3433103561401367\n",
      "Epoch: 176330 | Loss: 0.3478139042854309 | Test loss: 0.34329649806022644\n",
      "Epoch: 176340 | Loss: 0.34780216217041016 | Test loss: 0.34328266978263855\n",
      "Epoch: 176350 | Loss: 0.3477904200553894 | Test loss: 0.34326884150505066\n",
      "Epoch: 176360 | Loss: 0.34777864813804626 | Test loss: 0.343254953622818\n",
      "Epoch: 176370 | Loss: 0.3477668762207031 | Test loss: 0.3432411253452301\n",
      "Epoch: 176380 | Loss: 0.3477551341056824 | Test loss: 0.3432272672653198\n",
      "Epoch: 176390 | Loss: 0.34774333238601685 | Test loss: 0.34321340918540955\n",
      "Epoch: 176400 | Loss: 0.3477315902709961 | Test loss: 0.34319958090782166\n",
      "Epoch: 176410 | Loss: 0.34771984815597534 | Test loss: 0.34318575263023376\n",
      "Epoch: 176420 | Loss: 0.3477080762386322 | Test loss: 0.3431718945503235\n",
      "Epoch: 176430 | Loss: 0.34769630432128906 | Test loss: 0.3431580364704132\n",
      "Epoch: 176440 | Loss: 0.3476845920085907 | Test loss: 0.34314417839050293\n",
      "Epoch: 176450 | Loss: 0.34767279028892517 | Test loss: 0.34313035011291504\n",
      "Epoch: 176460 | Loss: 0.3476610481739044 | Test loss: 0.34311649203300476\n",
      "Epoch: 176470 | Loss: 0.34764930605888367 | Test loss: 0.34310266375541687\n",
      "Epoch: 176480 | Loss: 0.3476375341415405 | Test loss: 0.3430888056755066\n",
      "Epoch: 176490 | Loss: 0.347625732421875 | Test loss: 0.3430749475955963\n",
      "Epoch: 176500 | Loss: 0.34761399030685425 | Test loss: 0.3430611193180084\n",
      "Epoch: 176510 | Loss: 0.3476022183895111 | Test loss: 0.34304726123809814\n",
      "Epoch: 176520 | Loss: 0.34759050607681274 | Test loss: 0.34303340315818787\n",
      "Epoch: 176530 | Loss: 0.3475787341594696 | Test loss: 0.3430195748806\n",
      "Epoch: 176540 | Loss: 0.3475669324398041 | Test loss: 0.3430057466030121\n",
      "Epoch: 176550 | Loss: 0.3475551903247833 | Test loss: 0.3429918885231018\n",
      "Epoch: 176560 | Loss: 0.3475434482097626 | Test loss: 0.3429780602455139\n",
      "Epoch: 176570 | Loss: 0.34753167629241943 | Test loss: 0.34296420216560364\n",
      "Epoch: 176580 | Loss: 0.3475199341773987 | Test loss: 0.34295034408569336\n",
      "Epoch: 176590 | Loss: 0.34750816226005554 | Test loss: 0.34293651580810547\n",
      "Epoch: 176600 | Loss: 0.3474963903427124 | Test loss: 0.3429226577281952\n",
      "Epoch: 176610 | Loss: 0.34748461842536926 | Test loss: 0.3429088294506073\n",
      "Epoch: 176620 | Loss: 0.3474728763103485 | Test loss: 0.34289494156837463\n",
      "Epoch: 176630 | Loss: 0.34746113419532776 | Test loss: 0.34288111329078674\n",
      "Epoch: 176640 | Loss: 0.3474493622779846 | Test loss: 0.34286725521087646\n",
      "Epoch: 176650 | Loss: 0.3474375903606415 | Test loss: 0.3428533971309662\n",
      "Epoch: 176660 | Loss: 0.34742581844329834 | Test loss: 0.3428395688533783\n",
      "Epoch: 176670 | Loss: 0.3474141061306 | Test loss: 0.3428257405757904\n",
      "Epoch: 176680 | Loss: 0.34740230441093445 | Test loss: 0.3428118824958801\n",
      "Epoch: 176690 | Loss: 0.3473905622959137 | Test loss: 0.34279805421829224\n",
      "Epoch: 176700 | Loss: 0.34737882018089294 | Test loss: 0.34278416633605957\n",
      "Epoch: 176710 | Loss: 0.3473670184612274 | Test loss: 0.3427703380584717\n",
      "Epoch: 176720 | Loss: 0.34735527634620667 | Test loss: 0.3427565097808838\n",
      "Epoch: 176730 | Loss: 0.3473435044288635 | Test loss: 0.3427426517009735\n",
      "Epoch: 176740 | Loss: 0.3473317623138428 | Test loss: 0.3427288234233856\n",
      "Epoch: 176750 | Loss: 0.347320020198822 | Test loss: 0.34271499514579773\n",
      "Epoch: 176760 | Loss: 0.3473082482814789 | Test loss: 0.34270110726356506\n",
      "Epoch: 176770 | Loss: 0.34729647636413574 | Test loss: 0.3426872789859772\n",
      "Epoch: 176780 | Loss: 0.347284734249115 | Test loss: 0.3426734209060669\n",
      "Epoch: 176790 | Loss: 0.34727293252944946 | Test loss: 0.3426595628261566\n",
      "Epoch: 176800 | Loss: 0.3472611904144287 | Test loss: 0.3426457345485687\n",
      "Epoch: 176810 | Loss: 0.34724944829940796 | Test loss: 0.34263190627098083\n",
      "Epoch: 176820 | Loss: 0.3472376763820648 | Test loss: 0.34261804819107056\n",
      "Epoch: 176830 | Loss: 0.3472259044647217 | Test loss: 0.3426041901111603\n",
      "Epoch: 176840 | Loss: 0.3472141921520233 | Test loss: 0.34259033203125\n",
      "Epoch: 176850 | Loss: 0.3472023904323578 | Test loss: 0.3425765037536621\n",
      "Epoch: 176860 | Loss: 0.34719064831733704 | Test loss: 0.34256264567375183\n",
      "Epoch: 176870 | Loss: 0.3471789062023163 | Test loss: 0.34254881739616394\n",
      "Epoch: 176880 | Loss: 0.34716713428497314 | Test loss: 0.34253495931625366\n",
      "Epoch: 176890 | Loss: 0.3471553325653076 | Test loss: 0.3425211012363434\n",
      "Epoch: 176900 | Loss: 0.34714359045028687 | Test loss: 0.3425072729587555\n",
      "Epoch: 176910 | Loss: 0.3471318185329437 | Test loss: 0.3424934148788452\n",
      "Epoch: 176920 | Loss: 0.34712010622024536 | Test loss: 0.34247955679893494\n",
      "Epoch: 176930 | Loss: 0.3471083343029022 | Test loss: 0.34246572852134705\n",
      "Epoch: 176940 | Loss: 0.3470965325832367 | Test loss: 0.34245190024375916\n",
      "Epoch: 176950 | Loss: 0.34708479046821594 | Test loss: 0.3424380421638489\n",
      "Epoch: 176960 | Loss: 0.3470730483531952 | Test loss: 0.342424213886261\n",
      "Epoch: 176970 | Loss: 0.34706127643585205 | Test loss: 0.3424103558063507\n",
      "Epoch: 176980 | Loss: 0.3470495343208313 | Test loss: 0.34239649772644043\n",
      "Epoch: 176990 | Loss: 0.34703776240348816 | Test loss: 0.34238266944885254\n",
      "Epoch: 177000 | Loss: 0.347025990486145 | Test loss: 0.34236881136894226\n",
      "Epoch: 177010 | Loss: 0.3470142185688019 | Test loss: 0.34235498309135437\n",
      "Epoch: 177020 | Loss: 0.34700247645378113 | Test loss: 0.3423410952091217\n",
      "Epoch: 177030 | Loss: 0.3469907343387604 | Test loss: 0.3423272669315338\n",
      "Epoch: 177040 | Loss: 0.34697896242141724 | Test loss: 0.34231340885162354\n",
      "Epoch: 177050 | Loss: 0.3469671905040741 | Test loss: 0.34229955077171326\n",
      "Epoch: 177060 | Loss: 0.34695541858673096 | Test loss: 0.34228572249412537\n",
      "Epoch: 177070 | Loss: 0.3469437062740326 | Test loss: 0.3422718942165375\n",
      "Epoch: 177080 | Loss: 0.34693190455436707 | Test loss: 0.3422580361366272\n",
      "Epoch: 177090 | Loss: 0.3469201624393463 | Test loss: 0.3422442078590393\n",
      "Epoch: 177100 | Loss: 0.34690842032432556 | Test loss: 0.34223031997680664\n",
      "Epoch: 177110 | Loss: 0.34689661860466003 | Test loss: 0.34221649169921875\n",
      "Epoch: 177120 | Loss: 0.3468848764896393 | Test loss: 0.34220266342163086\n",
      "Epoch: 177130 | Loss: 0.34687310457229614 | Test loss: 0.3421888053417206\n",
      "Epoch: 177140 | Loss: 0.3468613624572754 | Test loss: 0.3421749770641327\n",
      "Epoch: 177150 | Loss: 0.34684962034225464 | Test loss: 0.3421611487865448\n",
      "Epoch: 177160 | Loss: 0.3468378484249115 | Test loss: 0.34214726090431213\n",
      "Epoch: 177170 | Loss: 0.34682607650756836 | Test loss: 0.34213343262672424\n",
      "Epoch: 177180 | Loss: 0.3468143343925476 | Test loss: 0.34211957454681396\n",
      "Epoch: 177190 | Loss: 0.3468025326728821 | Test loss: 0.3421057164669037\n",
      "Epoch: 177200 | Loss: 0.34679079055786133 | Test loss: 0.3420918881893158\n",
      "Epoch: 177210 | Loss: 0.3467790484428406 | Test loss: 0.3420780599117279\n",
      "Epoch: 177220 | Loss: 0.34676727652549744 | Test loss: 0.3420642018318176\n",
      "Epoch: 177230 | Loss: 0.3467555046081543 | Test loss: 0.34205034375190735\n",
      "Epoch: 177240 | Loss: 0.34674379229545593 | Test loss: 0.34203648567199707\n",
      "Epoch: 177250 | Loss: 0.3467319905757904 | Test loss: 0.3420226573944092\n",
      "Epoch: 177260 | Loss: 0.34672024846076965 | Test loss: 0.3420087993144989\n",
      "Epoch: 177270 | Loss: 0.3467085063457489 | Test loss: 0.341994971036911\n",
      "Epoch: 177280 | Loss: 0.34669673442840576 | Test loss: 0.34198111295700073\n",
      "Epoch: 177290 | Loss: 0.34668493270874023 | Test loss: 0.34196725487709045\n",
      "Epoch: 177300 | Loss: 0.3466731905937195 | Test loss: 0.34195342659950256\n",
      "Epoch: 177310 | Loss: 0.34666141867637634 | Test loss: 0.3419395685195923\n",
      "Epoch: 177320 | Loss: 0.346649706363678 | Test loss: 0.341925710439682\n",
      "Epoch: 177330 | Loss: 0.34663793444633484 | Test loss: 0.3419118821620941\n",
      "Epoch: 177340 | Loss: 0.3466261327266693 | Test loss: 0.3418980538845062\n",
      "Epoch: 177350 | Loss: 0.34661439061164856 | Test loss: 0.34188419580459595\n",
      "Epoch: 177360 | Loss: 0.3466026484966278 | Test loss: 0.34187036752700806\n",
      "Epoch: 177370 | Loss: 0.34659087657928467 | Test loss: 0.3418565094470978\n",
      "Epoch: 177380 | Loss: 0.3465791344642639 | Test loss: 0.3418426513671875\n",
      "Epoch: 177390 | Loss: 0.3465673625469208 | Test loss: 0.3418288230895996\n",
      "Epoch: 177400 | Loss: 0.34655559062957764 | Test loss: 0.34181496500968933\n",
      "Epoch: 177410 | Loss: 0.3465438187122345 | Test loss: 0.34180113673210144\n",
      "Epoch: 177420 | Loss: 0.34653207659721375 | Test loss: 0.3417872488498688\n",
      "Epoch: 177430 | Loss: 0.346520334482193 | Test loss: 0.3417734205722809\n",
      "Epoch: 177440 | Loss: 0.34650856256484985 | Test loss: 0.3417595624923706\n",
      "Epoch: 177450 | Loss: 0.3464967906475067 | Test loss: 0.3417457044124603\n",
      "Epoch: 177460 | Loss: 0.3464850187301636 | Test loss: 0.34173187613487244\n",
      "Epoch: 177470 | Loss: 0.3464733064174652 | Test loss: 0.34171804785728455\n",
      "Epoch: 177480 | Loss: 0.3464615046977997 | Test loss: 0.34170418977737427\n",
      "Epoch: 177490 | Loss: 0.34644976258277893 | Test loss: 0.3416903614997864\n",
      "Epoch: 177500 | Loss: 0.3464380204677582 | Test loss: 0.3416764736175537\n",
      "Epoch: 177510 | Loss: 0.34642621874809265 | Test loss: 0.3416626453399658\n",
      "Epoch: 177520 | Loss: 0.3464144766330719 | Test loss: 0.34164881706237793\n",
      "Epoch: 177530 | Loss: 0.34640270471572876 | Test loss: 0.34163495898246765\n",
      "Epoch: 177540 | Loss: 0.346390962600708 | Test loss: 0.34162113070487976\n",
      "Epoch: 177550 | Loss: 0.34637922048568726 | Test loss: 0.34160730242729187\n",
      "Epoch: 177560 | Loss: 0.3463674485683441 | Test loss: 0.3415934145450592\n",
      "Epoch: 177570 | Loss: 0.346355676651001 | Test loss: 0.3415795862674713\n",
      "Epoch: 177580 | Loss: 0.3463439345359802 | Test loss: 0.34156572818756104\n",
      "Epoch: 177590 | Loss: 0.3463321328163147 | Test loss: 0.34155187010765076\n",
      "Epoch: 177600 | Loss: 0.34632039070129395 | Test loss: 0.34153804183006287\n",
      "Epoch: 177610 | Loss: 0.3463086485862732 | Test loss: 0.341524213552475\n",
      "Epoch: 177620 | Loss: 0.34629687666893005 | Test loss: 0.3415103554725647\n",
      "Epoch: 177630 | Loss: 0.3462851047515869 | Test loss: 0.3414964973926544\n",
      "Epoch: 177640 | Loss: 0.34627339243888855 | Test loss: 0.34148263931274414\n",
      "Epoch: 177650 | Loss: 0.346261590719223 | Test loss: 0.34146881103515625\n",
      "Epoch: 177660 | Loss: 0.34624984860420227 | Test loss: 0.34145495295524597\n",
      "Epoch: 177670 | Loss: 0.3462381064891815 | Test loss: 0.3414411246776581\n",
      "Epoch: 177680 | Loss: 0.3462263345718384 | Test loss: 0.3414272665977478\n",
      "Epoch: 177690 | Loss: 0.34621453285217285 | Test loss: 0.3414134085178375\n",
      "Epoch: 177700 | Loss: 0.3462027907371521 | Test loss: 0.34139958024024963\n",
      "Epoch: 177710 | Loss: 0.34619101881980896 | Test loss: 0.34138572216033936\n",
      "Epoch: 177720 | Loss: 0.3461793065071106 | Test loss: 0.3413718640804291\n",
      "Epoch: 177730 | Loss: 0.34616753458976746 | Test loss: 0.3413580358028412\n",
      "Epoch: 177740 | Loss: 0.34615573287010193 | Test loss: 0.3413442075252533\n",
      "Epoch: 177750 | Loss: 0.3461439907550812 | Test loss: 0.341330349445343\n",
      "Epoch: 177760 | Loss: 0.3461322486400604 | Test loss: 0.3413165211677551\n",
      "Epoch: 177770 | Loss: 0.3461204767227173 | Test loss: 0.34130266308784485\n",
      "Epoch: 177780 | Loss: 0.34610873460769653 | Test loss: 0.34128880500793457\n",
      "Epoch: 177790 | Loss: 0.3460969626903534 | Test loss: 0.3412749767303467\n",
      "Epoch: 177800 | Loss: 0.34608519077301025 | Test loss: 0.3412611186504364\n",
      "Epoch: 177810 | Loss: 0.3460734188556671 | Test loss: 0.3412472903728485\n",
      "Epoch: 177820 | Loss: 0.34606167674064636 | Test loss: 0.34123340249061584\n",
      "Epoch: 177830 | Loss: 0.3460499346256256 | Test loss: 0.34121957421302795\n",
      "Epoch: 177840 | Loss: 0.34603816270828247 | Test loss: 0.3412057161331177\n",
      "Epoch: 177850 | Loss: 0.34602639079093933 | Test loss: 0.3411918580532074\n",
      "Epoch: 177860 | Loss: 0.3460146188735962 | Test loss: 0.3411780297756195\n",
      "Epoch: 177870 | Loss: 0.3460029065608978 | Test loss: 0.3411642014980316\n",
      "Epoch: 177880 | Loss: 0.3459911048412323 | Test loss: 0.34115034341812134\n",
      "Epoch: 177890 | Loss: 0.34597936272621155 | Test loss: 0.34113651514053345\n",
      "Epoch: 177900 | Loss: 0.3459676206111908 | Test loss: 0.3411226272583008\n",
      "Epoch: 177910 | Loss: 0.34595581889152527 | Test loss: 0.3411087989807129\n",
      "Epoch: 177920 | Loss: 0.3459440767765045 | Test loss: 0.341094970703125\n",
      "Epoch: 177930 | Loss: 0.3459323048591614 | Test loss: 0.3410811126232147\n",
      "Epoch: 177940 | Loss: 0.3459205627441406 | Test loss: 0.34106728434562683\n",
      "Epoch: 177950 | Loss: 0.3459088206291199 | Test loss: 0.34105345606803894\n",
      "Epoch: 177960 | Loss: 0.34589704871177673 | Test loss: 0.3410395681858063\n",
      "Epoch: 177970 | Loss: 0.3458852767944336 | Test loss: 0.3410257399082184\n",
      "Epoch: 177980 | Loss: 0.34587353467941284 | Test loss: 0.3410118818283081\n",
      "Epoch: 177990 | Loss: 0.3458617329597473 | Test loss: 0.3409980237483978\n",
      "Epoch: 178000 | Loss: 0.34584999084472656 | Test loss: 0.34098419547080994\n",
      "Epoch: 178010 | Loss: 0.3458382487297058 | Test loss: 0.34097036719322205\n",
      "Epoch: 178020 | Loss: 0.34582647681236267 | Test loss: 0.34095650911331177\n",
      "Epoch: 178030 | Loss: 0.34581470489501953 | Test loss: 0.3409426510334015\n",
      "Epoch: 178040 | Loss: 0.34580299258232117 | Test loss: 0.3409287929534912\n",
      "Epoch: 178050 | Loss: 0.34579119086265564 | Test loss: 0.3409149646759033\n",
      "Epoch: 178060 | Loss: 0.3457794487476349 | Test loss: 0.34090110659599304\n",
      "Epoch: 178070 | Loss: 0.34576770663261414 | Test loss: 0.34088727831840515\n",
      "Epoch: 178080 | Loss: 0.345755934715271 | Test loss: 0.3408734202384949\n",
      "Epoch: 178090 | Loss: 0.34574413299560547 | Test loss: 0.3408595621585846\n",
      "Epoch: 178100 | Loss: 0.3457323908805847 | Test loss: 0.3408457338809967\n",
      "Epoch: 178110 | Loss: 0.3457206189632416 | Test loss: 0.3408318758010864\n",
      "Epoch: 178120 | Loss: 0.3457089066505432 | Test loss: 0.34081801772117615\n",
      "Epoch: 178130 | Loss: 0.3456971347332001 | Test loss: 0.34080418944358826\n",
      "Epoch: 178140 | Loss: 0.34568533301353455 | Test loss: 0.34079036116600037\n",
      "Epoch: 178150 | Loss: 0.3456735908985138 | Test loss: 0.3407765030860901\n",
      "Epoch: 178160 | Loss: 0.34566184878349304 | Test loss: 0.3407626748085022\n",
      "Epoch: 178170 | Loss: 0.3456500768661499 | Test loss: 0.3407488167285919\n",
      "Epoch: 178180 | Loss: 0.34563833475112915 | Test loss: 0.34073495864868164\n",
      "Epoch: 178190 | Loss: 0.345626562833786 | Test loss: 0.34072113037109375\n",
      "Epoch: 178200 | Loss: 0.34561479091644287 | Test loss: 0.34070727229118347\n",
      "Epoch: 178210 | Loss: 0.34560301899909973 | Test loss: 0.3406934440135956\n",
      "Epoch: 178220 | Loss: 0.345591276884079 | Test loss: 0.3406795561313629\n",
      "Epoch: 178230 | Loss: 0.3455795347690582 | Test loss: 0.340665727853775\n",
      "Epoch: 178240 | Loss: 0.3455677628517151 | Test loss: 0.34065186977386475\n",
      "Epoch: 178250 | Loss: 0.34555599093437195 | Test loss: 0.34063801169395447\n",
      "Epoch: 178260 | Loss: 0.3455442190170288 | Test loss: 0.3406241834163666\n",
      "Epoch: 178270 | Loss: 0.34553250670433044 | Test loss: 0.3406103551387787\n",
      "Epoch: 178280 | Loss: 0.3455207049846649 | Test loss: 0.3405964970588684\n",
      "Epoch: 178290 | Loss: 0.34550896286964417 | Test loss: 0.3405826687812805\n",
      "Epoch: 178300 | Loss: 0.3454972207546234 | Test loss: 0.34056878089904785\n",
      "Epoch: 178310 | Loss: 0.3454854190349579 | Test loss: 0.34055495262145996\n",
      "Epoch: 178320 | Loss: 0.34547367691993713 | Test loss: 0.34054112434387207\n",
      "Epoch: 178330 | Loss: 0.345461905002594 | Test loss: 0.3405272662639618\n",
      "Epoch: 178340 | Loss: 0.34545016288757324 | Test loss: 0.3405134379863739\n",
      "Epoch: 178350 | Loss: 0.3454384207725525 | Test loss: 0.340499609708786\n",
      "Epoch: 178360 | Loss: 0.34542664885520935 | Test loss: 0.34048572182655334\n",
      "Epoch: 178370 | Loss: 0.3454148769378662 | Test loss: 0.34047189354896545\n",
      "Epoch: 178380 | Loss: 0.34540313482284546 | Test loss: 0.3404580354690552\n",
      "Epoch: 178390 | Loss: 0.34539133310317993 | Test loss: 0.3404441773891449\n",
      "Epoch: 178400 | Loss: 0.3453795909881592 | Test loss: 0.340430349111557\n",
      "Epoch: 178410 | Loss: 0.3453678488731384 | Test loss: 0.3404165208339691\n",
      "Epoch: 178420 | Loss: 0.3453560769557953 | Test loss: 0.34040266275405884\n",
      "Epoch: 178430 | Loss: 0.34534430503845215 | Test loss: 0.34038880467414856\n",
      "Epoch: 178440 | Loss: 0.3453325927257538 | Test loss: 0.3403749465942383\n",
      "Epoch: 178450 | Loss: 0.34532079100608826 | Test loss: 0.3403611183166504\n",
      "Epoch: 178460 | Loss: 0.3453090488910675 | Test loss: 0.3403472602367401\n",
      "Epoch: 178470 | Loss: 0.34529730677604675 | Test loss: 0.3403334319591522\n",
      "Epoch: 178480 | Loss: 0.3452855348587036 | Test loss: 0.34031957387924194\n",
      "Epoch: 178490 | Loss: 0.3452737331390381 | Test loss: 0.34030571579933167\n",
      "Epoch: 178500 | Loss: 0.34526199102401733 | Test loss: 0.3402918875217438\n",
      "Epoch: 178510 | Loss: 0.3452502191066742 | Test loss: 0.3402780294418335\n",
      "Epoch: 178520 | Loss: 0.34523850679397583 | Test loss: 0.3402641713619232\n",
      "Epoch: 178530 | Loss: 0.3452267348766327 | Test loss: 0.3402503430843353\n",
      "Epoch: 178540 | Loss: 0.34521493315696716 | Test loss: 0.34023651480674744\n",
      "Epoch: 178550 | Loss: 0.3452031910419464 | Test loss: 0.34022265672683716\n",
      "Epoch: 178560 | Loss: 0.34519144892692566 | Test loss: 0.34020882844924927\n",
      "Epoch: 178570 | Loss: 0.3451796770095825 | Test loss: 0.340194970369339\n",
      "Epoch: 178580 | Loss: 0.34516793489456177 | Test loss: 0.3401811122894287\n",
      "Epoch: 178590 | Loss: 0.34515616297721863 | Test loss: 0.3401672840118408\n",
      "Epoch: 178600 | Loss: 0.3451443910598755 | Test loss: 0.34015342593193054\n",
      "Epoch: 178610 | Loss: 0.34513261914253235 | Test loss: 0.34013959765434265\n",
      "Epoch: 178620 | Loss: 0.3451208770275116 | Test loss: 0.34012570977211\n",
      "Epoch: 178630 | Loss: 0.34510913491249084 | Test loss: 0.3401118814945221\n",
      "Epoch: 178640 | Loss: 0.3450973629951477 | Test loss: 0.3400980234146118\n",
      "Epoch: 178650 | Loss: 0.34508559107780457 | Test loss: 0.34008416533470154\n",
      "Epoch: 178660 | Loss: 0.3450738191604614 | Test loss: 0.34007033705711365\n",
      "Epoch: 178670 | Loss: 0.34506210684776306 | Test loss: 0.34005650877952576\n",
      "Epoch: 178680 | Loss: 0.34505030512809753 | Test loss: 0.3400426506996155\n",
      "Epoch: 178690 | Loss: 0.3450385630130768 | Test loss: 0.3400288224220276\n",
      "Epoch: 178700 | Loss: 0.34502682089805603 | Test loss: 0.3400149345397949\n",
      "Epoch: 178710 | Loss: 0.3450150191783905 | Test loss: 0.34000110626220703\n",
      "Epoch: 178720 | Loss: 0.34500327706336975 | Test loss: 0.33998727798461914\n",
      "Epoch: 178730 | Loss: 0.3449915051460266 | Test loss: 0.33997341990470886\n",
      "Epoch: 178740 | Loss: 0.34497976303100586 | Test loss: 0.33995959162712097\n",
      "Epoch: 178750 | Loss: 0.3449680209159851 | Test loss: 0.3399457633495331\n",
      "Epoch: 178760 | Loss: 0.34495624899864197 | Test loss: 0.3399318754673004\n",
      "Epoch: 178770 | Loss: 0.34494447708129883 | Test loss: 0.3399180471897125\n",
      "Epoch: 178780 | Loss: 0.3449327349662781 | Test loss: 0.33990418910980225\n",
      "Epoch: 178790 | Loss: 0.34492093324661255 | Test loss: 0.33989033102989197\n",
      "Epoch: 178800 | Loss: 0.3449091911315918 | Test loss: 0.3398765027523041\n",
      "Epoch: 178810 | Loss: 0.34489744901657104 | Test loss: 0.3398626744747162\n",
      "Epoch: 178820 | Loss: 0.3448856770992279 | Test loss: 0.3398488163948059\n",
      "Epoch: 178830 | Loss: 0.34487390518188477 | Test loss: 0.33983495831489563\n",
      "Epoch: 178840 | Loss: 0.3448621928691864 | Test loss: 0.33982110023498535\n",
      "Epoch: 178850 | Loss: 0.3448503911495209 | Test loss: 0.33980727195739746\n",
      "Epoch: 178860 | Loss: 0.3448386490345001 | Test loss: 0.3397934138774872\n",
      "Epoch: 178870 | Loss: 0.34482690691947937 | Test loss: 0.3397795855998993\n",
      "Epoch: 178880 | Loss: 0.34481513500213623 | Test loss: 0.339765727519989\n",
      "Epoch: 178890 | Loss: 0.3448033332824707 | Test loss: 0.33975186944007874\n",
      "Epoch: 178900 | Loss: 0.34479159116744995 | Test loss: 0.33973804116249084\n",
      "Epoch: 178910 | Loss: 0.3447798192501068 | Test loss: 0.33972418308258057\n",
      "Epoch: 178920 | Loss: 0.34476810693740845 | Test loss: 0.3397103250026703\n",
      "Epoch: 178930 | Loss: 0.3447563350200653 | Test loss: 0.3396964967250824\n",
      "Epoch: 178940 | Loss: 0.3447445333003998 | Test loss: 0.3396826684474945\n",
      "Epoch: 178950 | Loss: 0.34473279118537903 | Test loss: 0.33966881036758423\n",
      "Epoch: 178960 | Loss: 0.3447210490703583 | Test loss: 0.33965498208999634\n",
      "Epoch: 178970 | Loss: 0.34470927715301514 | Test loss: 0.33964112401008606\n",
      "Epoch: 178980 | Loss: 0.3446975350379944 | Test loss: 0.3396272659301758\n",
      "Epoch: 178990 | Loss: 0.34468576312065125 | Test loss: 0.3396134376525879\n",
      "Epoch: 179000 | Loss: 0.3446739912033081 | Test loss: 0.3395995795726776\n",
      "Epoch: 179010 | Loss: 0.34466221928596497 | Test loss: 0.3395857512950897\n",
      "Epoch: 179020 | Loss: 0.3446504771709442 | Test loss: 0.33957186341285706\n",
      "Epoch: 179030 | Loss: 0.34463873505592346 | Test loss: 0.33955803513526917\n",
      "Epoch: 179040 | Loss: 0.3446269631385803 | Test loss: 0.3395441770553589\n",
      "Epoch: 179050 | Loss: 0.3446151912212372 | Test loss: 0.3395303189754486\n",
      "Epoch: 179060 | Loss: 0.34460341930389404 | Test loss: 0.3395164906978607\n",
      "Epoch: 179070 | Loss: 0.3445917069911957 | Test loss: 0.3395026624202728\n",
      "Epoch: 179080 | Loss: 0.34457990527153015 | Test loss: 0.33948880434036255\n",
      "Epoch: 179090 | Loss: 0.3445681631565094 | Test loss: 0.33947497606277466\n",
      "Epoch: 179100 | Loss: 0.34455642104148865 | Test loss: 0.339461088180542\n",
      "Epoch: 179110 | Loss: 0.3445446193218231 | Test loss: 0.3394472599029541\n",
      "Epoch: 179120 | Loss: 0.34453287720680237 | Test loss: 0.3394334316253662\n",
      "Epoch: 179130 | Loss: 0.34452110528945923 | Test loss: 0.33941957354545593\n",
      "Epoch: 179140 | Loss: 0.3445093631744385 | Test loss: 0.33940574526786804\n",
      "Epoch: 179150 | Loss: 0.3444976210594177 | Test loss: 0.33939191699028015\n",
      "Epoch: 179160 | Loss: 0.3444858491420746 | Test loss: 0.3393780291080475\n",
      "Epoch: 179170 | Loss: 0.34447407722473145 | Test loss: 0.3393642008304596\n",
      "Epoch: 179180 | Loss: 0.3444623351097107 | Test loss: 0.3393503427505493\n",
      "Epoch: 179190 | Loss: 0.34445053339004517 | Test loss: 0.33933648467063904\n",
      "Epoch: 179200 | Loss: 0.3444387912750244 | Test loss: 0.33932265639305115\n",
      "Epoch: 179210 | Loss: 0.34442704916000366 | Test loss: 0.33930882811546326\n",
      "Epoch: 179220 | Loss: 0.3444152772426605 | Test loss: 0.339294970035553\n",
      "Epoch: 179230 | Loss: 0.3444035053253174 | Test loss: 0.3392811119556427\n",
      "Epoch: 179240 | Loss: 0.344391793012619 | Test loss: 0.3392672538757324\n",
      "Epoch: 179250 | Loss: 0.3443799912929535 | Test loss: 0.33925342559814453\n",
      "Epoch: 179260 | Loss: 0.34436824917793274 | Test loss: 0.33923956751823425\n",
      "Epoch: 179270 | Loss: 0.344356507062912 | Test loss: 0.33922573924064636\n",
      "Epoch: 179280 | Loss: 0.34434473514556885 | Test loss: 0.3392118811607361\n",
      "Epoch: 179290 | Loss: 0.3443329334259033 | Test loss: 0.3391980230808258\n",
      "Epoch: 179300 | Loss: 0.34432119131088257 | Test loss: 0.3391841948032379\n",
      "Epoch: 179310 | Loss: 0.34430941939353943 | Test loss: 0.33917033672332764\n",
      "Epoch: 179320 | Loss: 0.34429770708084106 | Test loss: 0.33915647864341736\n",
      "Epoch: 179330 | Loss: 0.3442859351634979 | Test loss: 0.33914265036582947\n",
      "Epoch: 179340 | Loss: 0.3442741334438324 | Test loss: 0.3391288220882416\n",
      "Epoch: 179350 | Loss: 0.34426239132881165 | Test loss: 0.3391149640083313\n",
      "Epoch: 179360 | Loss: 0.3442506492137909 | Test loss: 0.3391011357307434\n",
      "Epoch: 179370 | Loss: 0.34423887729644775 | Test loss: 0.33908727765083313\n",
      "Epoch: 179380 | Loss: 0.344227135181427 | Test loss: 0.33907341957092285\n",
      "Epoch: 179390 | Loss: 0.34421536326408386 | Test loss: 0.33905959129333496\n",
      "Epoch: 179400 | Loss: 0.3442035913467407 | Test loss: 0.3390457332134247\n",
      "Epoch: 179410 | Loss: 0.3441918194293976 | Test loss: 0.3390319049358368\n",
      "Epoch: 179420 | Loss: 0.34418007731437683 | Test loss: 0.3390180170536041\n",
      "Epoch: 179430 | Loss: 0.3441683351993561 | Test loss: 0.33900418877601624\n",
      "Epoch: 179440 | Loss: 0.34415656328201294 | Test loss: 0.33899033069610596\n",
      "Epoch: 179450 | Loss: 0.3441447913646698 | Test loss: 0.3389764726161957\n",
      "Epoch: 179460 | Loss: 0.34413301944732666 | Test loss: 0.3389626443386078\n",
      "Epoch: 179470 | Loss: 0.3441213071346283 | Test loss: 0.3389488160610199\n",
      "Epoch: 179480 | Loss: 0.34410950541496277 | Test loss: 0.3389349579811096\n",
      "Epoch: 179490 | Loss: 0.344097763299942 | Test loss: 0.33892112970352173\n",
      "Epoch: 179500 | Loss: 0.34408602118492126 | Test loss: 0.33890724182128906\n",
      "Epoch: 179510 | Loss: 0.34407421946525574 | Test loss: 0.33889341354370117\n",
      "Epoch: 179520 | Loss: 0.344062477350235 | Test loss: 0.3388795852661133\n",
      "Epoch: 179530 | Loss: 0.34405070543289185 | Test loss: 0.338865727186203\n",
      "Epoch: 179540 | Loss: 0.3440389633178711 | Test loss: 0.3388518989086151\n",
      "Epoch: 179550 | Loss: 0.34402722120285034 | Test loss: 0.3388380706310272\n",
      "Epoch: 179560 | Loss: 0.3440154492855072 | Test loss: 0.33882418274879456\n",
      "Epoch: 179570 | Loss: 0.34400367736816406 | Test loss: 0.33881035447120667\n",
      "Epoch: 179580 | Loss: 0.3439919352531433 | Test loss: 0.3387964963912964\n",
      "Epoch: 179590 | Loss: 0.3439801335334778 | Test loss: 0.3387826383113861\n",
      "Epoch: 179600 | Loss: 0.34396839141845703 | Test loss: 0.3387688100337982\n",
      "Epoch: 179610 | Loss: 0.3439566493034363 | Test loss: 0.3387549817562103\n",
      "Epoch: 179620 | Loss: 0.34394487738609314 | Test loss: 0.33874112367630005\n",
      "Epoch: 179630 | Loss: 0.34393310546875 | Test loss: 0.33872726559638977\n",
      "Epoch: 179640 | Loss: 0.34392139315605164 | Test loss: 0.3387134075164795\n",
      "Epoch: 179650 | Loss: 0.3439095914363861 | Test loss: 0.3386995792388916\n",
      "Epoch: 179660 | Loss: 0.34389784932136536 | Test loss: 0.3386857211589813\n",
      "Epoch: 179670 | Loss: 0.3438861072063446 | Test loss: 0.33867189288139343\n",
      "Epoch: 179680 | Loss: 0.34387433528900146 | Test loss: 0.33865803480148315\n",
      "Epoch: 179690 | Loss: 0.34386253356933594 | Test loss: 0.3386441767215729\n",
      "Epoch: 179700 | Loss: 0.3438507914543152 | Test loss: 0.338630348443985\n",
      "Epoch: 179710 | Loss: 0.34383901953697205 | Test loss: 0.3386164903640747\n",
      "Epoch: 179720 | Loss: 0.3438273072242737 | Test loss: 0.33860263228416443\n",
      "Epoch: 179730 | Loss: 0.34381553530693054 | Test loss: 0.33858880400657654\n",
      "Epoch: 179740 | Loss: 0.343803733587265 | Test loss: 0.33857497572898865\n",
      "Epoch: 179750 | Loss: 0.34379199147224426 | Test loss: 0.33856111764907837\n",
      "Epoch: 179760 | Loss: 0.3437802493572235 | Test loss: 0.3385472893714905\n",
      "Epoch: 179770 | Loss: 0.34376847743988037 | Test loss: 0.3385334312915802\n",
      "Epoch: 179780 | Loss: 0.3437567353248596 | Test loss: 0.3385195732116699\n",
      "Epoch: 179790 | Loss: 0.3437449634075165 | Test loss: 0.33850574493408203\n",
      "Epoch: 179800 | Loss: 0.34373319149017334 | Test loss: 0.33849188685417175\n",
      "Epoch: 179810 | Loss: 0.3437214195728302 | Test loss: 0.33847805857658386\n",
      "Epoch: 179820 | Loss: 0.34370967745780945 | Test loss: 0.3384641706943512\n",
      "Epoch: 179830 | Loss: 0.3436979353427887 | Test loss: 0.3384503424167633\n",
      "Epoch: 179840 | Loss: 0.34368616342544556 | Test loss: 0.338436484336853\n",
      "Epoch: 179850 | Loss: 0.3436743915081024 | Test loss: 0.33842262625694275\n",
      "Epoch: 179860 | Loss: 0.3436626195907593 | Test loss: 0.33840879797935486\n",
      "Epoch: 179870 | Loss: 0.3436509072780609 | Test loss: 0.33839496970176697\n",
      "Epoch: 179880 | Loss: 0.3436391055583954 | Test loss: 0.3383811116218567\n",
      "Epoch: 179890 | Loss: 0.34362736344337463 | Test loss: 0.3383672833442688\n",
      "Epoch: 179900 | Loss: 0.3436156213283539 | Test loss: 0.33835339546203613\n",
      "Epoch: 179910 | Loss: 0.34360381960868835 | Test loss: 0.33833956718444824\n",
      "Epoch: 179920 | Loss: 0.3435920774936676 | Test loss: 0.33832573890686035\n",
      "Epoch: 179930 | Loss: 0.34358030557632446 | Test loss: 0.3383118808269501\n",
      "Epoch: 179940 | Loss: 0.3435685634613037 | Test loss: 0.3382980525493622\n",
      "Epoch: 179950 | Loss: 0.34355682134628296 | Test loss: 0.3382842242717743\n",
      "Epoch: 179960 | Loss: 0.3435450494289398 | Test loss: 0.3382703363895416\n",
      "Epoch: 179970 | Loss: 0.3435332775115967 | Test loss: 0.33825650811195374\n",
      "Epoch: 179980 | Loss: 0.3435215353965759 | Test loss: 0.33824265003204346\n",
      "Epoch: 179990 | Loss: 0.3435097336769104 | Test loss: 0.3382287919521332\n",
      "Epoch: 180000 | Loss: 0.34349799156188965 | Test loss: 0.3382149636745453\n",
      "Epoch: 180010 | Loss: 0.3434862494468689 | Test loss: 0.3382011353969574\n",
      "Epoch: 180020 | Loss: 0.34347447752952576 | Test loss: 0.3381872773170471\n",
      "Epoch: 180030 | Loss: 0.3434627056121826 | Test loss: 0.33817341923713684\n",
      "Epoch: 180040 | Loss: 0.34345099329948425 | Test loss: 0.33815956115722656\n",
      "Epoch: 180050 | Loss: 0.3434391915798187 | Test loss: 0.33814573287963867\n",
      "Epoch: 180060 | Loss: 0.343427449464798 | Test loss: 0.3381318747997284\n",
      "Epoch: 180070 | Loss: 0.3434157073497772 | Test loss: 0.3381180465221405\n",
      "Epoch: 180080 | Loss: 0.3434039354324341 | Test loss: 0.3381041884422302\n",
      "Epoch: 180090 | Loss: 0.34339213371276855 | Test loss: 0.33809033036231995\n",
      "Epoch: 180100 | Loss: 0.3433803915977478 | Test loss: 0.33807650208473206\n",
      "Epoch: 180110 | Loss: 0.34336861968040466 | Test loss: 0.3380626440048218\n",
      "Epoch: 180120 | Loss: 0.3433569073677063 | Test loss: 0.3380487859249115\n",
      "Epoch: 180130 | Loss: 0.34334513545036316 | Test loss: 0.3380349576473236\n",
      "Epoch: 180140 | Loss: 0.34333333373069763 | Test loss: 0.3380211293697357\n",
      "Epoch: 180150 | Loss: 0.3433215916156769 | Test loss: 0.33800727128982544\n",
      "Epoch: 180160 | Loss: 0.34330984950065613 | Test loss: 0.33799344301223755\n",
      "Epoch: 180170 | Loss: 0.343298077583313 | Test loss: 0.33797958493232727\n",
      "Epoch: 180180 | Loss: 0.34328633546829224 | Test loss: 0.337965726852417\n",
      "Epoch: 180190 | Loss: 0.3432745635509491 | Test loss: 0.3379518985748291\n",
      "Epoch: 180200 | Loss: 0.34326279163360596 | Test loss: 0.3379380404949188\n",
      "Epoch: 180210 | Loss: 0.3432510197162628 | Test loss: 0.33792421221733093\n",
      "Epoch: 180220 | Loss: 0.34323927760124207 | Test loss: 0.33791032433509827\n",
      "Epoch: 180230 | Loss: 0.3432275354862213 | Test loss: 0.3378964960575104\n",
      "Epoch: 180240 | Loss: 0.3432157635688782 | Test loss: 0.3378826379776001\n",
      "Epoch: 180250 | Loss: 0.34320399165153503 | Test loss: 0.3378687798976898\n",
      "Epoch: 180260 | Loss: 0.3431922197341919 | Test loss: 0.33785495162010193\n",
      "Epoch: 180270 | Loss: 0.34318050742149353 | Test loss: 0.33784112334251404\n",
      "Epoch: 180280 | Loss: 0.343168705701828 | Test loss: 0.33782726526260376\n",
      "Epoch: 180290 | Loss: 0.34315696358680725 | Test loss: 0.33781343698501587\n",
      "Epoch: 180300 | Loss: 0.3431452214717865 | Test loss: 0.3377995491027832\n",
      "Epoch: 180310 | Loss: 0.34313341975212097 | Test loss: 0.3377857208251953\n",
      "Epoch: 180320 | Loss: 0.3431216776371002 | Test loss: 0.3377718925476074\n",
      "Epoch: 180330 | Loss: 0.3431099057197571 | Test loss: 0.33775803446769714\n",
      "Epoch: 180340 | Loss: 0.34309816360473633 | Test loss: 0.33774420619010925\n",
      "Epoch: 180350 | Loss: 0.3430864214897156 | Test loss: 0.33773037791252136\n",
      "Epoch: 180360 | Loss: 0.34307464957237244 | Test loss: 0.3377164900302887\n",
      "Epoch: 180370 | Loss: 0.3430628776550293 | Test loss: 0.3377026617527008\n",
      "Epoch: 180380 | Loss: 0.34305113554000854 | Test loss: 0.3376888036727905\n",
      "Epoch: 180390 | Loss: 0.343039333820343 | Test loss: 0.33767494559288025\n",
      "Epoch: 180400 | Loss: 0.34302759170532227 | Test loss: 0.33766111731529236\n",
      "Epoch: 180410 | Loss: 0.3430158495903015 | Test loss: 0.33764728903770447\n",
      "Epoch: 180420 | Loss: 0.3430040776729584 | Test loss: 0.3376334309577942\n",
      "Epoch: 180430 | Loss: 0.34299230575561523 | Test loss: 0.3376195728778839\n",
      "Epoch: 180440 | Loss: 0.34298059344291687 | Test loss: 0.33760571479797363\n",
      "Epoch: 180450 | Loss: 0.34296879172325134 | Test loss: 0.33759188652038574\n",
      "Epoch: 180460 | Loss: 0.3429570496082306 | Test loss: 0.33757802844047546\n",
      "Epoch: 180470 | Loss: 0.34294530749320984 | Test loss: 0.3375642001628876\n",
      "Epoch: 180480 | Loss: 0.3429335355758667 | Test loss: 0.3375503420829773\n",
      "Epoch: 180490 | Loss: 0.34292173385620117 | Test loss: 0.337536484003067\n",
      "Epoch: 180500 | Loss: 0.3429099917411804 | Test loss: 0.3375226557254791\n",
      "Epoch: 180510 | Loss: 0.3428982198238373 | Test loss: 0.33750879764556885\n",
      "Epoch: 180520 | Loss: 0.3428865075111389 | Test loss: 0.33749493956565857\n",
      "Epoch: 180530 | Loss: 0.3428747355937958 | Test loss: 0.3374811112880707\n",
      "Epoch: 180540 | Loss: 0.34286293387413025 | Test loss: 0.3374672830104828\n",
      "Epoch: 180550 | Loss: 0.3428511917591095 | Test loss: 0.3374534249305725\n",
      "Epoch: 180560 | Loss: 0.34283944964408875 | Test loss: 0.3374395966529846\n",
      "Epoch: 180570 | Loss: 0.3428276777267456 | Test loss: 0.33742573857307434\n",
      "Epoch: 180580 | Loss: 0.34281593561172485 | Test loss: 0.33741188049316406\n",
      "Epoch: 180590 | Loss: 0.3428041636943817 | Test loss: 0.33739805221557617\n",
      "Epoch: 180600 | Loss: 0.3427923917770386 | Test loss: 0.3373841941356659\n",
      "Epoch: 180610 | Loss: 0.34278061985969543 | Test loss: 0.337370365858078\n",
      "Epoch: 180620 | Loss: 0.3427688777446747 | Test loss: 0.33735647797584534\n",
      "Epoch: 180630 | Loss: 0.34275713562965393 | Test loss: 0.33734264969825745\n",
      "Epoch: 180640 | Loss: 0.3427453637123108 | Test loss: 0.33732879161834717\n",
      "Epoch: 180650 | Loss: 0.34273359179496765 | Test loss: 0.3373149335384369\n",
      "Epoch: 180660 | Loss: 0.3427218198776245 | Test loss: 0.337301105260849\n",
      "Epoch: 180670 | Loss: 0.34271010756492615 | Test loss: 0.3372872769832611\n",
      "Epoch: 180680 | Loss: 0.3426983058452606 | Test loss: 0.33727341890335083\n",
      "Epoch: 180690 | Loss: 0.34268656373023987 | Test loss: 0.33725959062576294\n",
      "Epoch: 180700 | Loss: 0.3426748216152191 | Test loss: 0.3372457027435303\n",
      "Epoch: 180710 | Loss: 0.3426630198955536 | Test loss: 0.3372318744659424\n",
      "Epoch: 180720 | Loss: 0.34265127778053284 | Test loss: 0.3372180461883545\n",
      "Epoch: 180730 | Loss: 0.3426395058631897 | Test loss: 0.3372041881084442\n",
      "Epoch: 180740 | Loss: 0.34262776374816895 | Test loss: 0.3371903598308563\n",
      "Epoch: 180750 | Loss: 0.3426160216331482 | Test loss: 0.33717653155326843\n",
      "Epoch: 180760 | Loss: 0.34260424971580505 | Test loss: 0.33716264367103577\n",
      "Epoch: 180770 | Loss: 0.3425924777984619 | Test loss: 0.3371488153934479\n",
      "Epoch: 180780 | Loss: 0.34258073568344116 | Test loss: 0.3371349573135376\n",
      "Epoch: 180790 | Loss: 0.34256893396377563 | Test loss: 0.3371210992336273\n",
      "Epoch: 180800 | Loss: 0.3425571918487549 | Test loss: 0.33710727095603943\n",
      "Epoch: 180810 | Loss: 0.34254544973373413 | Test loss: 0.33709344267845154\n",
      "Epoch: 180820 | Loss: 0.342533677816391 | Test loss: 0.33707958459854126\n",
      "Epoch: 180830 | Loss: 0.34252190589904785 | Test loss: 0.337065726518631\n",
      "Epoch: 180840 | Loss: 0.3425101935863495 | Test loss: 0.3370518684387207\n",
      "Epoch: 180850 | Loss: 0.34249839186668396 | Test loss: 0.3370380401611328\n",
      "Epoch: 180860 | Loss: 0.3424866497516632 | Test loss: 0.33702418208122253\n",
      "Epoch: 180870 | Loss: 0.34247490763664246 | Test loss: 0.33701035380363464\n",
      "Epoch: 180880 | Loss: 0.3424631357192993 | Test loss: 0.33699649572372437\n",
      "Epoch: 180890 | Loss: 0.3424513339996338 | Test loss: 0.3369826376438141\n",
      "Epoch: 180900 | Loss: 0.34243959188461304 | Test loss: 0.3369688093662262\n",
      "Epoch: 180910 | Loss: 0.3424278199672699 | Test loss: 0.3369549512863159\n",
      "Epoch: 180920 | Loss: 0.34241610765457153 | Test loss: 0.33694109320640564\n",
      "Epoch: 180930 | Loss: 0.3424043357372284 | Test loss: 0.33692726492881775\n",
      "Epoch: 180940 | Loss: 0.34239253401756287 | Test loss: 0.33691343665122986\n",
      "Epoch: 180950 | Loss: 0.3423807919025421 | Test loss: 0.3368995785713196\n",
      "Epoch: 180960 | Loss: 0.34236904978752136 | Test loss: 0.3368857502937317\n",
      "Epoch: 180970 | Loss: 0.3423572778701782 | Test loss: 0.3368718922138214\n",
      "Epoch: 180980 | Loss: 0.34234553575515747 | Test loss: 0.33685803413391113\n",
      "Epoch: 180990 | Loss: 0.34233376383781433 | Test loss: 0.33684420585632324\n",
      "Epoch: 181000 | Loss: 0.3423219919204712 | Test loss: 0.33683034777641296\n",
      "Epoch: 181010 | Loss: 0.34231022000312805 | Test loss: 0.3368165194988251\n",
      "Epoch: 181020 | Loss: 0.3422984778881073 | Test loss: 0.3368026316165924\n",
      "Epoch: 181030 | Loss: 0.34228673577308655 | Test loss: 0.3367888033390045\n",
      "Epoch: 181040 | Loss: 0.3422749638557434 | Test loss: 0.33677494525909424\n",
      "Epoch: 181050 | Loss: 0.34226319193840027 | Test loss: 0.33676108717918396\n",
      "Epoch: 181060 | Loss: 0.34225142002105713 | Test loss: 0.33674725890159607\n",
      "Epoch: 181070 | Loss: 0.34223970770835876 | Test loss: 0.3367334306240082\n",
      "Epoch: 181080 | Loss: 0.34222790598869324 | Test loss: 0.3367195725440979\n",
      "Epoch: 181090 | Loss: 0.3422161638736725 | Test loss: 0.33670574426651\n",
      "Epoch: 181100 | Loss: 0.34220442175865173 | Test loss: 0.33669185638427734\n",
      "Epoch: 181110 | Loss: 0.3421926200389862 | Test loss: 0.33667802810668945\n",
      "Epoch: 181120 | Loss: 0.34218087792396545 | Test loss: 0.33666419982910156\n",
      "Epoch: 181130 | Loss: 0.3421691060066223 | Test loss: 0.3366503417491913\n",
      "Epoch: 181140 | Loss: 0.34215736389160156 | Test loss: 0.3366365134716034\n",
      "Epoch: 181150 | Loss: 0.3421456217765808 | Test loss: 0.3366226851940155\n",
      "Epoch: 181160 | Loss: 0.34213384985923767 | Test loss: 0.33660879731178284\n",
      "Epoch: 181170 | Loss: 0.34212207794189453 | Test loss: 0.33659496903419495\n",
      "Epoch: 181180 | Loss: 0.3421103358268738 | Test loss: 0.33658111095428467\n",
      "Epoch: 181190 | Loss: 0.34209853410720825 | Test loss: 0.3365672528743744\n",
      "Epoch: 181200 | Loss: 0.3420867919921875 | Test loss: 0.3365534245967865\n",
      "Epoch: 181210 | Loss: 0.34207504987716675 | Test loss: 0.3365395963191986\n",
      "Epoch: 181220 | Loss: 0.3420632779598236 | Test loss: 0.33652573823928833\n",
      "Epoch: 181230 | Loss: 0.34205150604248047 | Test loss: 0.33651188015937805\n",
      "Epoch: 181240 | Loss: 0.3420397937297821 | Test loss: 0.3364980220794678\n",
      "Epoch: 181250 | Loss: 0.3420279920101166 | Test loss: 0.3364841938018799\n",
      "Epoch: 181260 | Loss: 0.3420162498950958 | Test loss: 0.3364703357219696\n",
      "Epoch: 181270 | Loss: 0.3420045077800751 | Test loss: 0.3364565074443817\n",
      "Epoch: 181280 | Loss: 0.34199273586273193 | Test loss: 0.33644264936447144\n",
      "Epoch: 181290 | Loss: 0.3419809341430664 | Test loss: 0.33642879128456116\n",
      "Epoch: 181300 | Loss: 0.34196919202804565 | Test loss: 0.33641496300697327\n",
      "Epoch: 181310 | Loss: 0.3419574201107025 | Test loss: 0.336401104927063\n",
      "Epoch: 181320 | Loss: 0.34194570779800415 | Test loss: 0.3363872468471527\n",
      "Epoch: 181330 | Loss: 0.341933935880661 | Test loss: 0.3363734185695648\n",
      "Epoch: 181340 | Loss: 0.3419221341609955 | Test loss: 0.33635959029197693\n",
      "Epoch: 181350 | Loss: 0.34191039204597473 | Test loss: 0.33634573221206665\n",
      "Epoch: 181360 | Loss: 0.341898649930954 | Test loss: 0.33633190393447876\n",
      "Epoch: 181370 | Loss: 0.34188687801361084 | Test loss: 0.3363180458545685\n",
      "Epoch: 181380 | Loss: 0.3418751358985901 | Test loss: 0.3363041877746582\n",
      "Epoch: 181390 | Loss: 0.34186336398124695 | Test loss: 0.3362903594970703\n",
      "Epoch: 181400 | Loss: 0.3418515920639038 | Test loss: 0.33627650141716003\n",
      "Epoch: 181410 | Loss: 0.34183982014656067 | Test loss: 0.33626267313957214\n",
      "Epoch: 181420 | Loss: 0.3418280780315399 | Test loss: 0.3362487852573395\n",
      "Epoch: 181430 | Loss: 0.34181633591651917 | Test loss: 0.3362349569797516\n",
      "Epoch: 181440 | Loss: 0.341804563999176 | Test loss: 0.3362210988998413\n",
      "Epoch: 181450 | Loss: 0.3417927920818329 | Test loss: 0.33620724081993103\n",
      "Epoch: 181460 | Loss: 0.34178102016448975 | Test loss: 0.33619341254234314\n",
      "Epoch: 181470 | Loss: 0.3417693078517914 | Test loss: 0.33617958426475525\n",
      "Epoch: 181480 | Loss: 0.34175750613212585 | Test loss: 0.33616572618484497\n",
      "Epoch: 181490 | Loss: 0.3417457640171051 | Test loss: 0.3361518979072571\n",
      "Epoch: 181500 | Loss: 0.34173402190208435 | Test loss: 0.3361380100250244\n",
      "Epoch: 181510 | Loss: 0.3417222201824188 | Test loss: 0.3361241817474365\n",
      "Epoch: 181520 | Loss: 0.34171047806739807 | Test loss: 0.33611035346984863\n",
      "Epoch: 181530 | Loss: 0.34169870615005493 | Test loss: 0.33609649538993835\n",
      "Epoch: 181540 | Loss: 0.3416869640350342 | Test loss: 0.33608266711235046\n",
      "Epoch: 181550 | Loss: 0.3416752219200134 | Test loss: 0.3360688388347626\n",
      "Epoch: 181560 | Loss: 0.3416634500026703 | Test loss: 0.3360549509525299\n",
      "Epoch: 181570 | Loss: 0.34165167808532715 | Test loss: 0.336041122674942\n",
      "Epoch: 181580 | Loss: 0.3416399359703064 | Test loss: 0.33602726459503174\n",
      "Epoch: 181590 | Loss: 0.34162813425064087 | Test loss: 0.33601340651512146\n",
      "Epoch: 181600 | Loss: 0.3416163921356201 | Test loss: 0.33599957823753357\n",
      "Epoch: 181610 | Loss: 0.34160465002059937 | Test loss: 0.3359857499599457\n",
      "Epoch: 181620 | Loss: 0.3415928781032562 | Test loss: 0.3359718918800354\n",
      "Epoch: 181630 | Loss: 0.3415811061859131 | Test loss: 0.3359580338001251\n",
      "Epoch: 181640 | Loss: 0.3415693938732147 | Test loss: 0.33594417572021484\n",
      "Epoch: 181650 | Loss: 0.3415575921535492 | Test loss: 0.33593034744262695\n",
      "Epoch: 181660 | Loss: 0.34154585003852844 | Test loss: 0.3359164893627167\n",
      "Epoch: 181670 | Loss: 0.3415341079235077 | Test loss: 0.3359026610851288\n",
      "Epoch: 181680 | Loss: 0.34152233600616455 | Test loss: 0.3358888030052185\n",
      "Epoch: 181690 | Loss: 0.341510534286499 | Test loss: 0.3358749449253082\n",
      "Epoch: 181700 | Loss: 0.34149879217147827 | Test loss: 0.33586111664772034\n",
      "Epoch: 181710 | Loss: 0.34148702025413513 | Test loss: 0.33584725856781006\n",
      "Epoch: 181720 | Loss: 0.34147530794143677 | Test loss: 0.3358334004878998\n",
      "Epoch: 181730 | Loss: 0.34146353602409363 | Test loss: 0.3358195722103119\n",
      "Epoch: 181740 | Loss: 0.3414517343044281 | Test loss: 0.335805743932724\n",
      "Epoch: 181750 | Loss: 0.34143999218940735 | Test loss: 0.3357918858528137\n",
      "Epoch: 181760 | Loss: 0.3414282500743866 | Test loss: 0.33577805757522583\n",
      "Epoch: 181770 | Loss: 0.34141647815704346 | Test loss: 0.33576419949531555\n",
      "Epoch: 181780 | Loss: 0.3414047360420227 | Test loss: 0.3357503414154053\n",
      "Epoch: 181790 | Loss: 0.34139296412467957 | Test loss: 0.3357365131378174\n",
      "Epoch: 181800 | Loss: 0.3413811922073364 | Test loss: 0.3357226550579071\n",
      "Epoch: 181810 | Loss: 0.3413694202899933 | Test loss: 0.3357088267803192\n",
      "Epoch: 181820 | Loss: 0.34135767817497253 | Test loss: 0.33569493889808655\n",
      "Epoch: 181830 | Loss: 0.3413459360599518 | Test loss: 0.33568111062049866\n",
      "Epoch: 181840 | Loss: 0.34133416414260864 | Test loss: 0.3356672525405884\n",
      "Epoch: 181850 | Loss: 0.3413223922252655 | Test loss: 0.3356533944606781\n",
      "Epoch: 181860 | Loss: 0.34131062030792236 | Test loss: 0.3356395661830902\n",
      "Epoch: 181870 | Loss: 0.341298907995224 | Test loss: 0.3356257379055023\n",
      "Epoch: 181880 | Loss: 0.34128710627555847 | Test loss: 0.33561187982559204\n",
      "Epoch: 181890 | Loss: 0.3412753641605377 | Test loss: 0.33559805154800415\n",
      "Epoch: 181900 | Loss: 0.34126362204551697 | Test loss: 0.3355841636657715\n",
      "Epoch: 181910 | Loss: 0.34125182032585144 | Test loss: 0.3355703353881836\n",
      "Epoch: 181920 | Loss: 0.3412400782108307 | Test loss: 0.3355565071105957\n",
      "Epoch: 181930 | Loss: 0.34122830629348755 | Test loss: 0.3355426490306854\n",
      "Epoch: 181940 | Loss: 0.3412165641784668 | Test loss: 0.33552882075309753\n",
      "Epoch: 181950 | Loss: 0.34120482206344604 | Test loss: 0.33551499247550964\n",
      "Epoch: 181960 | Loss: 0.3411930501461029 | Test loss: 0.335501104593277\n",
      "Epoch: 181970 | Loss: 0.34118127822875977 | Test loss: 0.3354872763156891\n",
      "Epoch: 181980 | Loss: 0.341169536113739 | Test loss: 0.3354734182357788\n",
      "Epoch: 181990 | Loss: 0.3411577343940735 | Test loss: 0.33545956015586853\n",
      "Epoch: 182000 | Loss: 0.34114599227905273 | Test loss: 0.33544573187828064\n",
      "Epoch: 182010 | Loss: 0.341134250164032 | Test loss: 0.33543190360069275\n",
      "Epoch: 182020 | Loss: 0.34112247824668884 | Test loss: 0.33541804552078247\n",
      "Epoch: 182030 | Loss: 0.3411107063293457 | Test loss: 0.3354041874408722\n",
      "Epoch: 182040 | Loss: 0.34109899401664734 | Test loss: 0.3353903293609619\n",
      "Epoch: 182050 | Loss: 0.3410871922969818 | Test loss: 0.335376501083374\n",
      "Epoch: 182060 | Loss: 0.34107545018196106 | Test loss: 0.33536264300346375\n",
      "Epoch: 182070 | Loss: 0.3410637080669403 | Test loss: 0.33534881472587585\n",
      "Epoch: 182080 | Loss: 0.34105193614959717 | Test loss: 0.3353349566459656\n",
      "Epoch: 182090 | Loss: 0.34104013442993164 | Test loss: 0.3353210985660553\n",
      "Epoch: 182100 | Loss: 0.3410283923149109 | Test loss: 0.3353072702884674\n",
      "Epoch: 182110 | Loss: 0.34101662039756775 | Test loss: 0.33529341220855713\n",
      "Epoch: 182120 | Loss: 0.3410049080848694 | Test loss: 0.33527955412864685\n",
      "Epoch: 182130 | Loss: 0.34099313616752625 | Test loss: 0.33526572585105896\n",
      "Epoch: 182140 | Loss: 0.3409813344478607 | Test loss: 0.33525189757347107\n",
      "Epoch: 182150 | Loss: 0.34096959233283997 | Test loss: 0.3352380394935608\n",
      "Epoch: 182160 | Loss: 0.3409578502178192 | Test loss: 0.3352242112159729\n",
      "Epoch: 182170 | Loss: 0.3409460783004761 | Test loss: 0.3352103531360626\n",
      "Epoch: 182180 | Loss: 0.3409343361854553 | Test loss: 0.33519649505615234\n",
      "Epoch: 182190 | Loss: 0.3409225642681122 | Test loss: 0.33518266677856445\n",
      "Epoch: 182200 | Loss: 0.34091079235076904 | Test loss: 0.3351688086986542\n",
      "Epoch: 182210 | Loss: 0.3408990204334259 | Test loss: 0.3351549804210663\n",
      "Epoch: 182220 | Loss: 0.34088727831840515 | Test loss: 0.3351410925388336\n",
      "Epoch: 182230 | Loss: 0.3408755362033844 | Test loss: 0.3351272642612457\n",
      "Epoch: 182240 | Loss: 0.34086376428604126 | Test loss: 0.33511340618133545\n",
      "Epoch: 182250 | Loss: 0.3408519923686981 | Test loss: 0.33509954810142517\n",
      "Epoch: 182260 | Loss: 0.340840220451355 | Test loss: 0.3350857198238373\n",
      "Epoch: 182270 | Loss: 0.3408285081386566 | Test loss: 0.3350718915462494\n",
      "Epoch: 182280 | Loss: 0.3408167064189911 | Test loss: 0.3350580334663391\n",
      "Epoch: 182290 | Loss: 0.34080496430397034 | Test loss: 0.3350442051887512\n",
      "Epoch: 182300 | Loss: 0.3407932221889496 | Test loss: 0.33503031730651855\n",
      "Epoch: 182310 | Loss: 0.34078142046928406 | Test loss: 0.33501648902893066\n",
      "Epoch: 182320 | Loss: 0.3407696783542633 | Test loss: 0.3350026607513428\n",
      "Epoch: 182330 | Loss: 0.34075790643692017 | Test loss: 0.3349888026714325\n",
      "Epoch: 182340 | Loss: 0.3407461643218994 | Test loss: 0.3349749743938446\n",
      "Epoch: 182350 | Loss: 0.34073442220687866 | Test loss: 0.3349611461162567\n",
      "Epoch: 182360 | Loss: 0.3407226502895355 | Test loss: 0.33494725823402405\n",
      "Epoch: 182370 | Loss: 0.3407108783721924 | Test loss: 0.33493342995643616\n",
      "Epoch: 182380 | Loss: 0.34069913625717163 | Test loss: 0.3349195718765259\n",
      "Epoch: 182390 | Loss: 0.3406873345375061 | Test loss: 0.3349057137966156\n",
      "Epoch: 182400 | Loss: 0.34067559242248535 | Test loss: 0.3348918855190277\n",
      "Epoch: 182410 | Loss: 0.3406638503074646 | Test loss: 0.3348780572414398\n",
      "Epoch: 182420 | Loss: 0.34065207839012146 | Test loss: 0.33486419916152954\n",
      "Epoch: 182430 | Loss: 0.3406403064727783 | Test loss: 0.33485034108161926\n",
      "Epoch: 182440 | Loss: 0.34062859416007996 | Test loss: 0.334836483001709\n",
      "Epoch: 182450 | Loss: 0.34061679244041443 | Test loss: 0.3348226547241211\n",
      "Epoch: 182460 | Loss: 0.3406050503253937 | Test loss: 0.3348087966442108\n",
      "Epoch: 182470 | Loss: 0.3405933082103729 | Test loss: 0.3347949683666229\n",
      "Epoch: 182480 | Loss: 0.3405815362930298 | Test loss: 0.33478111028671265\n",
      "Epoch: 182490 | Loss: 0.34056973457336426 | Test loss: 0.33476725220680237\n",
      "Epoch: 182500 | Loss: 0.3405579924583435 | Test loss: 0.3347534239292145\n",
      "Epoch: 182510 | Loss: 0.34054622054100037 | Test loss: 0.3347395658493042\n",
      "Epoch: 182520 | Loss: 0.340534508228302 | Test loss: 0.3347257077693939\n",
      "Epoch: 182530 | Loss: 0.34052273631095886 | Test loss: 0.33471187949180603\n",
      "Epoch: 182540 | Loss: 0.34051093459129333 | Test loss: 0.33469805121421814\n",
      "Epoch: 182550 | Loss: 0.3404991924762726 | Test loss: 0.33468419313430786\n",
      "Epoch: 182560 | Loss: 0.34048745036125183 | Test loss: 0.33467036485671997\n",
      "Epoch: 182570 | Loss: 0.3404756784439087 | Test loss: 0.3346565067768097\n",
      "Epoch: 182580 | Loss: 0.34046393632888794 | Test loss: 0.3346426486968994\n",
      "Epoch: 182590 | Loss: 0.3404521644115448 | Test loss: 0.3346288204193115\n",
      "Epoch: 182600 | Loss: 0.34044039249420166 | Test loss: 0.33461496233940125\n",
      "Epoch: 182610 | Loss: 0.3404286205768585 | Test loss: 0.33460113406181335\n",
      "Epoch: 182620 | Loss: 0.34041687846183777 | Test loss: 0.3345872461795807\n",
      "Epoch: 182630 | Loss: 0.340405136346817 | Test loss: 0.3345734179019928\n",
      "Epoch: 182640 | Loss: 0.3403933644294739 | Test loss: 0.3345595598220825\n",
      "Epoch: 182650 | Loss: 0.34038159251213074 | Test loss: 0.33454570174217224\n",
      "Epoch: 182660 | Loss: 0.3403698205947876 | Test loss: 0.33453187346458435\n",
      "Epoch: 182670 | Loss: 0.34035810828208923 | Test loss: 0.33451804518699646\n",
      "Epoch: 182680 | Loss: 0.3403463065624237 | Test loss: 0.3345041871070862\n",
      "Epoch: 182690 | Loss: 0.34033456444740295 | Test loss: 0.3344903588294983\n",
      "Epoch: 182700 | Loss: 0.3403228223323822 | Test loss: 0.3344764709472656\n",
      "Epoch: 182710 | Loss: 0.3403110206127167 | Test loss: 0.33446264266967773\n",
      "Epoch: 182720 | Loss: 0.3402992784976959 | Test loss: 0.33444881439208984\n",
      "Epoch: 182730 | Loss: 0.3402875065803528 | Test loss: 0.33443495631217957\n",
      "Epoch: 182740 | Loss: 0.34027576446533203 | Test loss: 0.3344211280345917\n",
      "Epoch: 182750 | Loss: 0.3402640223503113 | Test loss: 0.3344072997570038\n",
      "Epoch: 182760 | Loss: 0.34025225043296814 | Test loss: 0.3343934118747711\n",
      "Epoch: 182770 | Loss: 0.340240478515625 | Test loss: 0.3343795835971832\n",
      "Epoch: 182780 | Loss: 0.34022873640060425 | Test loss: 0.33436572551727295\n",
      "Epoch: 182790 | Loss: 0.3402169346809387 | Test loss: 0.33435186743736267\n",
      "Epoch: 182800 | Loss: 0.34020519256591797 | Test loss: 0.3343380391597748\n",
      "Epoch: 182810 | Loss: 0.3401934504508972 | Test loss: 0.3343242108821869\n",
      "Epoch: 182820 | Loss: 0.3401816785335541 | Test loss: 0.3343103528022766\n",
      "Epoch: 182830 | Loss: 0.34016990661621094 | Test loss: 0.33429649472236633\n",
      "Epoch: 182840 | Loss: 0.3401581943035126 | Test loss: 0.33428263664245605\n",
      "Epoch: 182850 | Loss: 0.34014639258384705 | Test loss: 0.33426880836486816\n",
      "Epoch: 182860 | Loss: 0.3401346504688263 | Test loss: 0.3342549502849579\n",
      "Epoch: 182870 | Loss: 0.34012290835380554 | Test loss: 0.33424112200737\n",
      "Epoch: 182880 | Loss: 0.3401111364364624 | Test loss: 0.3342272639274597\n",
      "Epoch: 182890 | Loss: 0.3400993347167969 | Test loss: 0.33421340584754944\n",
      "Epoch: 182900 | Loss: 0.3400875926017761 | Test loss: 0.33419957756996155\n",
      "Epoch: 182910 | Loss: 0.340075820684433 | Test loss: 0.33418571949005127\n",
      "Epoch: 182920 | Loss: 0.3400641083717346 | Test loss: 0.334171861410141\n",
      "Epoch: 182930 | Loss: 0.3400523364543915 | Test loss: 0.3341580331325531\n",
      "Epoch: 182940 | Loss: 0.34004053473472595 | Test loss: 0.3341442048549652\n",
      "Epoch: 182950 | Loss: 0.3400287926197052 | Test loss: 0.33413034677505493\n",
      "Epoch: 182960 | Loss: 0.34001705050468445 | Test loss: 0.33411651849746704\n",
      "Epoch: 182970 | Loss: 0.3400052785873413 | Test loss: 0.33410266041755676\n",
      "Epoch: 182980 | Loss: 0.33999353647232056 | Test loss: 0.3340888023376465\n",
      "Epoch: 182990 | Loss: 0.3399817645549774 | Test loss: 0.3340749740600586\n",
      "Epoch: 183000 | Loss: 0.3399699926376343 | Test loss: 0.3340611159801483\n",
      "Epoch: 183010 | Loss: 0.33995822072029114 | Test loss: 0.3340472877025604\n",
      "Epoch: 183020 | Loss: 0.3399464786052704 | Test loss: 0.33403339982032776\n",
      "Epoch: 183030 | Loss: 0.33993473649024963 | Test loss: 0.33401957154273987\n",
      "Epoch: 183040 | Loss: 0.3399229645729065 | Test loss: 0.3340057134628296\n",
      "Epoch: 183050 | Loss: 0.33991119265556335 | Test loss: 0.3339918553829193\n",
      "Epoch: 183060 | Loss: 0.3398994207382202 | Test loss: 0.3339780271053314\n",
      "Epoch: 183070 | Loss: 0.33988770842552185 | Test loss: 0.33396419882774353\n",
      "Epoch: 183080 | Loss: 0.3398759067058563 | Test loss: 0.33395034074783325\n",
      "Epoch: 183090 | Loss: 0.33986416459083557 | Test loss: 0.33393651247024536\n",
      "Epoch: 183100 | Loss: 0.3398524224758148 | Test loss: 0.3339226245880127\n",
      "Epoch: 183110 | Loss: 0.3398406207561493 | Test loss: 0.3339087963104248\n",
      "Epoch: 183120 | Loss: 0.33982887864112854 | Test loss: 0.3338949680328369\n",
      "Epoch: 183130 | Loss: 0.3398171067237854 | Test loss: 0.33388110995292664\n",
      "Epoch: 183140 | Loss: 0.33980536460876465 | Test loss: 0.33386728167533875\n",
      "Epoch: 183150 | Loss: 0.3397936224937439 | Test loss: 0.33385345339775085\n",
      "Epoch: 183160 | Loss: 0.33978185057640076 | Test loss: 0.3338395655155182\n",
      "Epoch: 183170 | Loss: 0.3397700786590576 | Test loss: 0.3338257372379303\n",
      "Epoch: 183180 | Loss: 0.33975833654403687 | Test loss: 0.33381187915802\n",
      "Epoch: 183190 | Loss: 0.33974653482437134 | Test loss: 0.33379802107810974\n",
      "Epoch: 183200 | Loss: 0.3397347927093506 | Test loss: 0.33378419280052185\n",
      "Epoch: 183210 | Loss: 0.33972305059432983 | Test loss: 0.33377036452293396\n",
      "Epoch: 183220 | Loss: 0.3397112786769867 | Test loss: 0.3337565064430237\n",
      "Epoch: 183230 | Loss: 0.33969950675964355 | Test loss: 0.3337426483631134\n",
      "Epoch: 183240 | Loss: 0.3396877944469452 | Test loss: 0.3337287902832031\n",
      "Epoch: 183250 | Loss: 0.33967599272727966 | Test loss: 0.33371496200561523\n",
      "Epoch: 183260 | Loss: 0.3396642506122589 | Test loss: 0.33370110392570496\n",
      "Epoch: 183270 | Loss: 0.33965250849723816 | Test loss: 0.33368727564811707\n",
      "Epoch: 183280 | Loss: 0.339640736579895 | Test loss: 0.3336734175682068\n",
      "Epoch: 183290 | Loss: 0.3396289348602295 | Test loss: 0.3336595594882965\n",
      "Epoch: 183300 | Loss: 0.33961719274520874 | Test loss: 0.3336457312107086\n",
      "Epoch: 183310 | Loss: 0.3396054208278656 | Test loss: 0.33363187313079834\n",
      "Epoch: 183320 | Loss: 0.33959370851516724 | Test loss: 0.33361801505088806\n",
      "Epoch: 183330 | Loss: 0.3395819365978241 | Test loss: 0.33360418677330017\n",
      "Epoch: 183340 | Loss: 0.33957013487815857 | Test loss: 0.3335903584957123\n",
      "Epoch: 183350 | Loss: 0.3395583927631378 | Test loss: 0.333576500415802\n",
      "Epoch: 183360 | Loss: 0.33954665064811707 | Test loss: 0.3335626721382141\n",
      "Epoch: 183370 | Loss: 0.3395348787307739 | Test loss: 0.33354881405830383\n",
      "Epoch: 183380 | Loss: 0.3395231366157532 | Test loss: 0.33353495597839355\n",
      "Epoch: 183390 | Loss: 0.33951136469841003 | Test loss: 0.33352112770080566\n",
      "Epoch: 183400 | Loss: 0.3394995927810669 | Test loss: 0.3335072696208954\n",
      "Epoch: 183410 | Loss: 0.33948782086372375 | Test loss: 0.3334934413433075\n",
      "Epoch: 183420 | Loss: 0.339476078748703 | Test loss: 0.33347955346107483\n",
      "Epoch: 183430 | Loss: 0.33946433663368225 | Test loss: 0.33346572518348694\n",
      "Epoch: 183440 | Loss: 0.3394525647163391 | Test loss: 0.33345186710357666\n",
      "Epoch: 183450 | Loss: 0.33944079279899597 | Test loss: 0.3334380090236664\n",
      "Epoch: 183460 | Loss: 0.33942902088165283 | Test loss: 0.3334241807460785\n",
      "Epoch: 183470 | Loss: 0.33941730856895447 | Test loss: 0.3334103524684906\n",
      "Epoch: 183480 | Loss: 0.33940550684928894 | Test loss: 0.3333964943885803\n",
      "Epoch: 183490 | Loss: 0.3393937647342682 | Test loss: 0.33338266611099243\n",
      "Epoch: 183500 | Loss: 0.33938202261924744 | Test loss: 0.33336877822875977\n",
      "Epoch: 183510 | Loss: 0.3393702208995819 | Test loss: 0.3333549499511719\n",
      "Epoch: 183520 | Loss: 0.33935847878456116 | Test loss: 0.333341121673584\n",
      "Epoch: 183530 | Loss: 0.339346706867218 | Test loss: 0.3333272635936737\n",
      "Epoch: 183540 | Loss: 0.33933496475219727 | Test loss: 0.3333134353160858\n",
      "Epoch: 183550 | Loss: 0.3393232226371765 | Test loss: 0.3332996070384979\n",
      "Epoch: 183560 | Loss: 0.3393114507198334 | Test loss: 0.33328571915626526\n",
      "Epoch: 183570 | Loss: 0.33929967880249023 | Test loss: 0.33327189087867737\n",
      "Epoch: 183580 | Loss: 0.3392879366874695 | Test loss: 0.3332580327987671\n",
      "Epoch: 183590 | Loss: 0.33927613496780396 | Test loss: 0.3332441747188568\n",
      "Epoch: 183600 | Loss: 0.3392643928527832 | Test loss: 0.3332303464412689\n",
      "Epoch: 183610 | Loss: 0.33925265073776245 | Test loss: 0.33321651816368103\n",
      "Epoch: 183620 | Loss: 0.3392408788204193 | Test loss: 0.33320266008377075\n",
      "Epoch: 183630 | Loss: 0.33922910690307617 | Test loss: 0.3331888020038605\n",
      "Epoch: 183640 | Loss: 0.3392173945903778 | Test loss: 0.3331749439239502\n",
      "Epoch: 183650 | Loss: 0.3392055928707123 | Test loss: 0.3331611156463623\n",
      "Epoch: 183660 | Loss: 0.33919385075569153 | Test loss: 0.333147257566452\n",
      "Epoch: 183670 | Loss: 0.3391821086406708 | Test loss: 0.33313342928886414\n",
      "Epoch: 183680 | Loss: 0.33917033672332764 | Test loss: 0.33311957120895386\n",
      "Epoch: 183690 | Loss: 0.3391585350036621 | Test loss: 0.3331057131290436\n",
      "Epoch: 183700 | Loss: 0.33914679288864136 | Test loss: 0.3330918848514557\n",
      "Epoch: 183710 | Loss: 0.3391350209712982 | Test loss: 0.3330780267715454\n",
      "Epoch: 183720 | Loss: 0.33912330865859985 | Test loss: 0.33306416869163513\n",
      "Epoch: 183730 | Loss: 0.3391115367412567 | Test loss: 0.33305034041404724\n",
      "Epoch: 183740 | Loss: 0.3390997350215912 | Test loss: 0.33303651213645935\n",
      "Epoch: 183750 | Loss: 0.33908799290657043 | Test loss: 0.3330226540565491\n",
      "Epoch: 183760 | Loss: 0.3390762507915497 | Test loss: 0.3330088257789612\n",
      "Epoch: 183770 | Loss: 0.33906447887420654 | Test loss: 0.3329949676990509\n",
      "Epoch: 183780 | Loss: 0.3390527367591858 | Test loss: 0.3329811096191406\n",
      "Epoch: 183790 | Loss: 0.33904096484184265 | Test loss: 0.33296728134155273\n",
      "Epoch: 183800 | Loss: 0.3390291929244995 | Test loss: 0.33295342326164246\n",
      "Epoch: 183810 | Loss: 0.33901742100715637 | Test loss: 0.33293959498405457\n",
      "Epoch: 183820 | Loss: 0.3390056788921356 | Test loss: 0.3329257071018219\n",
      "Epoch: 183830 | Loss: 0.33899393677711487 | Test loss: 0.332911878824234\n",
      "Epoch: 183840 | Loss: 0.33898216485977173 | Test loss: 0.33289802074432373\n",
      "Epoch: 183850 | Loss: 0.3389703929424286 | Test loss: 0.33288416266441345\n",
      "Epoch: 183860 | Loss: 0.33895862102508545 | Test loss: 0.33287033438682556\n",
      "Epoch: 183870 | Loss: 0.3389469087123871 | Test loss: 0.33285650610923767\n",
      "Epoch: 183880 | Loss: 0.33893510699272156 | Test loss: 0.3328426480293274\n",
      "Epoch: 183890 | Loss: 0.3389233648777008 | Test loss: 0.3328288197517395\n",
      "Epoch: 183900 | Loss: 0.33891162276268005 | Test loss: 0.33281493186950684\n",
      "Epoch: 183910 | Loss: 0.3388998210430145 | Test loss: 0.33280110359191895\n",
      "Epoch: 183920 | Loss: 0.3388880789279938 | Test loss: 0.33278727531433105\n",
      "Epoch: 183930 | Loss: 0.33887630701065063 | Test loss: 0.3327734172344208\n",
      "Epoch: 183940 | Loss: 0.3388645648956299 | Test loss: 0.3327595889568329\n",
      "Epoch: 183950 | Loss: 0.33885282278060913 | Test loss: 0.332745760679245\n",
      "Epoch: 183960 | Loss: 0.338841050863266 | Test loss: 0.33273187279701233\n",
      "Epoch: 183970 | Loss: 0.33882927894592285 | Test loss: 0.33271804451942444\n",
      "Epoch: 183980 | Loss: 0.3388175368309021 | Test loss: 0.33270418643951416\n",
      "Epoch: 183990 | Loss: 0.3388057351112366 | Test loss: 0.3326903283596039\n",
      "Epoch: 184000 | Loss: 0.3387939929962158 | Test loss: 0.332676500082016\n",
      "Epoch: 184010 | Loss: 0.33878225088119507 | Test loss: 0.3326626718044281\n",
      "Epoch: 184020 | Loss: 0.33877047896385193 | Test loss: 0.3326488137245178\n",
      "Epoch: 184030 | Loss: 0.3387587070465088 | Test loss: 0.33263495564460754\n",
      "Epoch: 184040 | Loss: 0.3387469947338104 | Test loss: 0.33262109756469727\n",
      "Epoch: 184050 | Loss: 0.3387351930141449 | Test loss: 0.3326072692871094\n",
      "Epoch: 184060 | Loss: 0.33872345089912415 | Test loss: 0.3325934112071991\n",
      "Epoch: 184070 | Loss: 0.3387117087841034 | Test loss: 0.3325795829296112\n",
      "Epoch: 184080 | Loss: 0.33869993686676025 | Test loss: 0.3325657248497009\n",
      "Epoch: 184090 | Loss: 0.3386881351470947 | Test loss: 0.33255186676979065\n",
      "Epoch: 184100 | Loss: 0.338676393032074 | Test loss: 0.33253803849220276\n",
      "Epoch: 184110 | Loss: 0.33866462111473083 | Test loss: 0.3325241804122925\n",
      "Epoch: 184120 | Loss: 0.33865290880203247 | Test loss: 0.3325103223323822\n",
      "Epoch: 184130 | Loss: 0.33864113688468933 | Test loss: 0.3324964940547943\n",
      "Epoch: 184140 | Loss: 0.3386293351650238 | Test loss: 0.3324826657772064\n",
      "Epoch: 184150 | Loss: 0.33861759305000305 | Test loss: 0.33246880769729614\n",
      "Epoch: 184160 | Loss: 0.3386058509349823 | Test loss: 0.33245497941970825\n",
      "Epoch: 184170 | Loss: 0.33859407901763916 | Test loss: 0.332441121339798\n",
      "Epoch: 184180 | Loss: 0.3385823369026184 | Test loss: 0.3324272632598877\n",
      "Epoch: 184190 | Loss: 0.33857056498527527 | Test loss: 0.3324134349822998\n",
      "Epoch: 184200 | Loss: 0.33855879306793213 | Test loss: 0.3323995769023895\n",
      "Epoch: 184210 | Loss: 0.338547021150589 | Test loss: 0.33238574862480164\n",
      "Epoch: 184220 | Loss: 0.33853527903556824 | Test loss: 0.33237186074256897\n",
      "Epoch: 184230 | Loss: 0.3385235369205475 | Test loss: 0.3323580324649811\n",
      "Epoch: 184240 | Loss: 0.33851176500320435 | Test loss: 0.3323441743850708\n",
      "Epoch: 184250 | Loss: 0.3384999930858612 | Test loss: 0.3323303163051605\n",
      "Epoch: 184260 | Loss: 0.33848822116851807 | Test loss: 0.33231648802757263\n",
      "Epoch: 184270 | Loss: 0.3384765088558197 | Test loss: 0.33230265974998474\n",
      "Epoch: 184280 | Loss: 0.3384647071361542 | Test loss: 0.33228880167007446\n",
      "Epoch: 184290 | Loss: 0.3384529650211334 | Test loss: 0.3322749733924866\n",
      "Epoch: 184300 | Loss: 0.33844122290611267 | Test loss: 0.3322610855102539\n",
      "Epoch: 184310 | Loss: 0.33842942118644714 | Test loss: 0.332247257232666\n",
      "Epoch: 184320 | Loss: 0.3384176790714264 | Test loss: 0.3322334289550781\n",
      "Epoch: 184330 | Loss: 0.33840590715408325 | Test loss: 0.33221957087516785\n",
      "Epoch: 184340 | Loss: 0.3383941650390625 | Test loss: 0.33220574259757996\n",
      "Epoch: 184350 | Loss: 0.33838242292404175 | Test loss: 0.33219191431999207\n",
      "Epoch: 184360 | Loss: 0.3383706510066986 | Test loss: 0.3321780264377594\n",
      "Epoch: 184370 | Loss: 0.33835887908935547 | Test loss: 0.3321641981601715\n",
      "Epoch: 184380 | Loss: 0.3383471369743347 | Test loss: 0.33215034008026123\n",
      "Epoch: 184390 | Loss: 0.3383353352546692 | Test loss: 0.33213648200035095\n",
      "Epoch: 184400 | Loss: 0.33832359313964844 | Test loss: 0.33212265372276306\n",
      "Epoch: 184410 | Loss: 0.3383118510246277 | Test loss: 0.33210882544517517\n",
      "Epoch: 184420 | Loss: 0.33830007910728455 | Test loss: 0.3320949673652649\n",
      "Epoch: 184430 | Loss: 0.3382883071899414 | Test loss: 0.3320811092853546\n",
      "Epoch: 184440 | Loss: 0.33827659487724304 | Test loss: 0.33206725120544434\n",
      "Epoch: 184450 | Loss: 0.3382647931575775 | Test loss: 0.33205342292785645\n",
      "Epoch: 184460 | Loss: 0.33825305104255676 | Test loss: 0.33203956484794617\n",
      "Epoch: 184470 | Loss: 0.338241308927536 | Test loss: 0.3320257365703583\n",
      "Epoch: 184480 | Loss: 0.33822953701019287 | Test loss: 0.332011878490448\n",
      "Epoch: 184490 | Loss: 0.33821773529052734 | Test loss: 0.3319980204105377\n",
      "Epoch: 184500 | Loss: 0.3382059931755066 | Test loss: 0.33198419213294983\n",
      "Epoch: 184510 | Loss: 0.33819422125816345 | Test loss: 0.33197033405303955\n",
      "Epoch: 184520 | Loss: 0.3381825089454651 | Test loss: 0.3319564759731293\n",
      "Epoch: 184530 | Loss: 0.33817073702812195 | Test loss: 0.3319426476955414\n",
      "Epoch: 184540 | Loss: 0.3381589353084564 | Test loss: 0.3319288194179535\n",
      "Epoch: 184550 | Loss: 0.33814719319343567 | Test loss: 0.3319149613380432\n",
      "Epoch: 184560 | Loss: 0.3381354510784149 | Test loss: 0.3319011330604553\n",
      "Epoch: 184570 | Loss: 0.3381236791610718 | Test loss: 0.33188727498054504\n",
      "Epoch: 184580 | Loss: 0.338111937046051 | Test loss: 0.33187341690063477\n",
      "Epoch: 184590 | Loss: 0.3381001651287079 | Test loss: 0.3318595886230469\n",
      "Epoch: 184600 | Loss: 0.33808839321136475 | Test loss: 0.3318457305431366\n",
      "Epoch: 184610 | Loss: 0.3380766212940216 | Test loss: 0.3318319022655487\n",
      "Epoch: 184620 | Loss: 0.33806487917900085 | Test loss: 0.33181801438331604\n",
      "Epoch: 184630 | Loss: 0.3380531370639801 | Test loss: 0.33180418610572815\n",
      "Epoch: 184640 | Loss: 0.33804136514663696 | Test loss: 0.33179032802581787\n",
      "Epoch: 184650 | Loss: 0.3380295932292938 | Test loss: 0.3317764699459076\n",
      "Epoch: 184660 | Loss: 0.3380178213119507 | Test loss: 0.3317626416683197\n",
      "Epoch: 184670 | Loss: 0.3380061089992523 | Test loss: 0.3317488133907318\n",
      "Epoch: 184680 | Loss: 0.3379943072795868 | Test loss: 0.33173495531082153\n",
      "Epoch: 184690 | Loss: 0.33798256516456604 | Test loss: 0.33172112703323364\n",
      "Epoch: 184700 | Loss: 0.3379708230495453 | Test loss: 0.331707239151001\n",
      "Epoch: 184710 | Loss: 0.33795902132987976 | Test loss: 0.3316934108734131\n",
      "Epoch: 184720 | Loss: 0.337947279214859 | Test loss: 0.3316795825958252\n",
      "Epoch: 184730 | Loss: 0.33793550729751587 | Test loss: 0.3316657245159149\n",
      "Epoch: 184740 | Loss: 0.3379237651824951 | Test loss: 0.331651896238327\n",
      "Epoch: 184750 | Loss: 0.33791202306747437 | Test loss: 0.33163806796073914\n",
      "Epoch: 184760 | Loss: 0.3379002511501312 | Test loss: 0.33162418007850647\n",
      "Epoch: 184770 | Loss: 0.3378884792327881 | Test loss: 0.3316103518009186\n",
      "Epoch: 184780 | Loss: 0.33787673711776733 | Test loss: 0.3315964639186859\n",
      "Epoch: 184790 | Loss: 0.3378649353981018 | Test loss: 0.331582635641098\n",
      "Epoch: 184800 | Loss: 0.33785319328308105 | Test loss: 0.33156880736351013\n",
      "Epoch: 184810 | Loss: 0.3378414511680603 | Test loss: 0.33155497908592224\n",
      "Epoch: 184820 | Loss: 0.33782967925071716 | Test loss: 0.33154112100601196\n",
      "Epoch: 184830 | Loss: 0.337817907333374 | Test loss: 0.3315272629261017\n",
      "Epoch: 184840 | Loss: 0.33780619502067566 | Test loss: 0.3315134048461914\n",
      "Epoch: 184850 | Loss: 0.33779439330101013 | Test loss: 0.3314995765686035\n",
      "Epoch: 184860 | Loss: 0.3377826511859894 | Test loss: 0.33148571848869324\n",
      "Epoch: 184870 | Loss: 0.33777090907096863 | Test loss: 0.33147189021110535\n",
      "Epoch: 184880 | Loss: 0.3377591371536255 | Test loss: 0.33145803213119507\n",
      "Epoch: 184890 | Loss: 0.33774733543395996 | Test loss: 0.3314441740512848\n",
      "Epoch: 184900 | Loss: 0.3377355933189392 | Test loss: 0.3314303457736969\n",
      "Epoch: 184910 | Loss: 0.33772382140159607 | Test loss: 0.3314164876937866\n",
      "Epoch: 184920 | Loss: 0.3377121090888977 | Test loss: 0.33140262961387634\n",
      "Epoch: 184930 | Loss: 0.33770033717155457 | Test loss: 0.33138880133628845\n",
      "Epoch: 184940 | Loss: 0.33768853545188904 | Test loss: 0.33137497305870056\n",
      "Epoch: 184950 | Loss: 0.3376767933368683 | Test loss: 0.3313611149787903\n",
      "Epoch: 184960 | Loss: 0.33766505122184753 | Test loss: 0.3313472867012024\n",
      "Epoch: 184970 | Loss: 0.3376532793045044 | Test loss: 0.3313334286212921\n",
      "Epoch: 184980 | Loss: 0.33764153718948364 | Test loss: 0.33131954073905945\n",
      "Epoch: 184990 | Loss: 0.3376297652721405 | Test loss: 0.33130574226379395\n",
      "Epoch: 185000 | Loss: 0.33761799335479736 | Test loss: 0.33129188418388367\n",
      "Epoch: 185010 | Loss: 0.3376062214374542 | Test loss: 0.3312780559062958\n",
      "Epoch: 185020 | Loss: 0.33759447932243347 | Test loss: 0.3312641680240631\n",
      "Epoch: 185030 | Loss: 0.3375827372074127 | Test loss: 0.3312503397464752\n",
      "Epoch: 185040 | Loss: 0.3375709652900696 | Test loss: 0.33123648166656494\n",
      "Epoch: 185050 | Loss: 0.33755919337272644 | Test loss: 0.33122262358665466\n",
      "Epoch: 185060 | Loss: 0.3375474214553833 | Test loss: 0.3312087953090668\n",
      "Epoch: 185070 | Loss: 0.33753570914268494 | Test loss: 0.3311949670314789\n",
      "Epoch: 185080 | Loss: 0.3375239074230194 | Test loss: 0.3311811089515686\n",
      "Epoch: 185090 | Loss: 0.33751216530799866 | Test loss: 0.3311672806739807\n",
      "Epoch: 185100 | Loss: 0.3375004231929779 | Test loss: 0.33115339279174805\n",
      "Epoch: 185110 | Loss: 0.3374886214733124 | Test loss: 0.33113956451416016\n",
      "Epoch: 185120 | Loss: 0.3374768793582916 | Test loss: 0.33112573623657227\n",
      "Epoch: 185130 | Loss: 0.3374651074409485 | Test loss: 0.331111878156662\n",
      "Epoch: 185140 | Loss: 0.33745336532592773 | Test loss: 0.3310980498790741\n",
      "Epoch: 185150 | Loss: 0.337441623210907 | Test loss: 0.3310842216014862\n",
      "Epoch: 185160 | Loss: 0.33742985129356384 | Test loss: 0.33107033371925354\n",
      "Epoch: 185170 | Loss: 0.3374180793762207 | Test loss: 0.33105650544166565\n",
      "Epoch: 185180 | Loss: 0.33740633726119995 | Test loss: 0.331042617559433\n",
      "Epoch: 185190 | Loss: 0.3373945653438568 | Test loss: 0.3310287892818451\n",
      "Epoch: 185200 | Loss: 0.33738279342651367 | Test loss: 0.3310149610042572\n",
      "Epoch: 185210 | Loss: 0.3373710513114929 | Test loss: 0.3310011327266693\n",
      "Epoch: 185220 | Loss: 0.3373592793941498 | Test loss: 0.33098727464675903\n",
      "Epoch: 185230 | Loss: 0.33734750747680664 | Test loss: 0.33097341656684875\n",
      "Epoch: 185240 | Loss: 0.3373357951641083 | Test loss: 0.3309595584869385\n",
      "Epoch: 185250 | Loss: 0.33732399344444275 | Test loss: 0.3309457302093506\n",
      "Epoch: 185260 | Loss: 0.337312251329422 | Test loss: 0.3309318721294403\n",
      "Epoch: 185270 | Loss: 0.33730050921440125 | Test loss: 0.3309180438518524\n",
      "Epoch: 185280 | Loss: 0.3372887372970581 | Test loss: 0.33090418577194214\n",
      "Epoch: 185290 | Loss: 0.3372769355773926 | Test loss: 0.33089032769203186\n",
      "Epoch: 185300 | Loss: 0.3372651934623718 | Test loss: 0.33087649941444397\n",
      "Epoch: 185310 | Loss: 0.3372534215450287 | Test loss: 0.3308626413345337\n",
      "Epoch: 185320 | Loss: 0.3372417092323303 | Test loss: 0.3308487832546234\n",
      "Epoch: 185330 | Loss: 0.3372299373149872 | Test loss: 0.3308349549770355\n",
      "Epoch: 185340 | Loss: 0.33721813559532166 | Test loss: 0.33082112669944763\n",
      "Epoch: 185350 | Loss: 0.3372063934803009 | Test loss: 0.33080726861953735\n",
      "Epoch: 185360 | Loss: 0.33719465136528015 | Test loss: 0.33079344034194946\n",
      "Epoch: 185370 | Loss: 0.337182879447937 | Test loss: 0.3307795822620392\n",
      "Epoch: 185380 | Loss: 0.33717113733291626 | Test loss: 0.3307656943798065\n",
      "Epoch: 185390 | Loss: 0.3371593654155731 | Test loss: 0.330751895904541\n",
      "Epoch: 185400 | Loss: 0.33714759349823 | Test loss: 0.33073803782463074\n",
      "Epoch: 185410 | Loss: 0.33713582158088684 | Test loss: 0.33072420954704285\n",
      "Epoch: 185420 | Loss: 0.3371240794658661 | Test loss: 0.3307103216648102\n",
      "Epoch: 185430 | Loss: 0.33711233735084534 | Test loss: 0.3306964933872223\n",
      "Epoch: 185440 | Loss: 0.3371005654335022 | Test loss: 0.330682635307312\n",
      "Epoch: 185450 | Loss: 0.33708879351615906 | Test loss: 0.33066877722740173\n",
      "Epoch: 185460 | Loss: 0.3370770215988159 | Test loss: 0.33065494894981384\n",
      "Epoch: 185470 | Loss: 0.33706530928611755 | Test loss: 0.33064112067222595\n",
      "Epoch: 185480 | Loss: 0.337053507566452 | Test loss: 0.3306272625923157\n",
      "Epoch: 185490 | Loss: 0.3370417654514313 | Test loss: 0.3306134343147278\n",
      "Epoch: 185500 | Loss: 0.3370300233364105 | Test loss: 0.3305995464324951\n",
      "Epoch: 185510 | Loss: 0.337018221616745 | Test loss: 0.3305857181549072\n",
      "Epoch: 185520 | Loss: 0.33700647950172424 | Test loss: 0.33057188987731934\n",
      "Epoch: 185530 | Loss: 0.3369947075843811 | Test loss: 0.33055803179740906\n",
      "Epoch: 185540 | Loss: 0.33698296546936035 | Test loss: 0.33054420351982117\n",
      "Epoch: 185550 | Loss: 0.3369712233543396 | Test loss: 0.3305303752422333\n",
      "Epoch: 185560 | Loss: 0.33695945143699646 | Test loss: 0.3305164873600006\n",
      "Epoch: 185570 | Loss: 0.3369476795196533 | Test loss: 0.3305026590824127\n",
      "Epoch: 185580 | Loss: 0.33693593740463257 | Test loss: 0.33048877120018005\n",
      "Epoch: 185590 | Loss: 0.33692416548728943 | Test loss: 0.33047494292259216\n",
      "Epoch: 185600 | Loss: 0.3369123935699463 | Test loss: 0.3304611146450043\n",
      "Epoch: 185610 | Loss: 0.33690065145492554 | Test loss: 0.3304472863674164\n",
      "Epoch: 185620 | Loss: 0.3368888795375824 | Test loss: 0.3304334282875061\n",
      "Epoch: 185630 | Loss: 0.33687710762023926 | Test loss: 0.3304195702075958\n",
      "Epoch: 185640 | Loss: 0.3368653953075409 | Test loss: 0.33040571212768555\n",
      "Epoch: 185650 | Loss: 0.33685359358787537 | Test loss: 0.33039188385009766\n",
      "Epoch: 185660 | Loss: 0.3368418514728546 | Test loss: 0.3303780257701874\n",
      "Epoch: 185670 | Loss: 0.33683010935783386 | Test loss: 0.3303641974925995\n",
      "Epoch: 185680 | Loss: 0.3368183374404907 | Test loss: 0.3303503394126892\n",
      "Epoch: 185690 | Loss: 0.3368065357208252 | Test loss: 0.33033648133277893\n",
      "Epoch: 185700 | Loss: 0.33679479360580444 | Test loss: 0.33032265305519104\n",
      "Epoch: 185710 | Loss: 0.3367830216884613 | Test loss: 0.33030879497528076\n",
      "Epoch: 185720 | Loss: 0.33677130937576294 | Test loss: 0.3302949368953705\n",
      "Epoch: 185730 | Loss: 0.3367595374584198 | Test loss: 0.3302811086177826\n",
      "Epoch: 185740 | Loss: 0.3367477357387543 | Test loss: 0.3302672803401947\n",
      "Epoch: 185750 | Loss: 0.3367359936237335 | Test loss: 0.3302534222602844\n",
      "Epoch: 185760 | Loss: 0.33672425150871277 | Test loss: 0.33023959398269653\n",
      "Epoch: 185770 | Loss: 0.33671247959136963 | Test loss: 0.33022573590278625\n",
      "Epoch: 185780 | Loss: 0.3367007374763489 | Test loss: 0.3302118480205536\n",
      "Epoch: 185790 | Loss: 0.33668896555900574 | Test loss: 0.3301980495452881\n",
      "Epoch: 185800 | Loss: 0.3366771936416626 | Test loss: 0.3301841914653778\n",
      "Epoch: 185810 | Loss: 0.33666542172431946 | Test loss: 0.3301703631877899\n",
      "Epoch: 185820 | Loss: 0.3366536796092987 | Test loss: 0.33015647530555725\n",
      "Epoch: 185830 | Loss: 0.33664193749427795 | Test loss: 0.33014264702796936\n",
      "Epoch: 185840 | Loss: 0.3366301655769348 | Test loss: 0.3301287889480591\n",
      "Epoch: 185850 | Loss: 0.3366183936595917 | Test loss: 0.3301149308681488\n",
      "Epoch: 185860 | Loss: 0.33660662174224854 | Test loss: 0.3301011025905609\n",
      "Epoch: 185870 | Loss: 0.33659490942955017 | Test loss: 0.330087274312973\n",
      "Epoch: 185880 | Loss: 0.33658310770988464 | Test loss: 0.33007341623306274\n",
      "Epoch: 185890 | Loss: 0.3365713655948639 | Test loss: 0.33005958795547485\n",
      "Epoch: 185900 | Loss: 0.33655962347984314 | Test loss: 0.3300457000732422\n",
      "Epoch: 185910 | Loss: 0.3365478217601776 | Test loss: 0.3300318717956543\n",
      "Epoch: 185920 | Loss: 0.33653607964515686 | Test loss: 0.3300180435180664\n",
      "Epoch: 185930 | Loss: 0.3365243077278137 | Test loss: 0.33000418543815613\n",
      "Epoch: 185940 | Loss: 0.33651256561279297 | Test loss: 0.32999035716056824\n",
      "Epoch: 185950 | Loss: 0.3365008234977722 | Test loss: 0.32997652888298035\n",
      "Epoch: 185960 | Loss: 0.3364890515804291 | Test loss: 0.3299626410007477\n",
      "Epoch: 185970 | Loss: 0.33647727966308594 | Test loss: 0.3299488127231598\n",
      "Epoch: 185980 | Loss: 0.3364655375480652 | Test loss: 0.3299349248409271\n",
      "Epoch: 185990 | Loss: 0.33645376563072205 | Test loss: 0.32992109656333923\n",
      "Epoch: 186000 | Loss: 0.3364419937133789 | Test loss: 0.32990726828575134\n",
      "Epoch: 186010 | Loss: 0.33643025159835815 | Test loss: 0.32989344000816345\n",
      "Epoch: 186020 | Loss: 0.336418479681015 | Test loss: 0.3298795819282532\n",
      "Epoch: 186030 | Loss: 0.3364067077636719 | Test loss: 0.3298657238483429\n",
      "Epoch: 186040 | Loss: 0.3363949954509735 | Test loss: 0.3298518657684326\n",
      "Epoch: 186050 | Loss: 0.336383193731308 | Test loss: 0.3298380374908447\n",
      "Epoch: 186060 | Loss: 0.33637145161628723 | Test loss: 0.32982417941093445\n",
      "Epoch: 186070 | Loss: 0.3363597095012665 | Test loss: 0.32981035113334656\n",
      "Epoch: 186080 | Loss: 0.33634793758392334 | Test loss: 0.3297964930534363\n",
      "Epoch: 186090 | Loss: 0.3363361358642578 | Test loss: 0.329782634973526\n",
      "Epoch: 186100 | Loss: 0.33632439374923706 | Test loss: 0.3297688066959381\n",
      "Epoch: 186110 | Loss: 0.3363126218318939 | Test loss: 0.32975494861602783\n",
      "Epoch: 186120 | Loss: 0.33630090951919556 | Test loss: 0.32974109053611755\n",
      "Epoch: 186130 | Loss: 0.3362891376018524 | Test loss: 0.32972726225852966\n",
      "Epoch: 186140 | Loss: 0.3362773358821869 | Test loss: 0.3297134339809418\n",
      "Epoch: 186150 | Loss: 0.33626559376716614 | Test loss: 0.3296995759010315\n",
      "Epoch: 186160 | Loss: 0.3362538516521454 | Test loss: 0.3296857476234436\n",
      "Epoch: 186170 | Loss: 0.33624207973480225 | Test loss: 0.3296718895435333\n",
      "Epoch: 186180 | Loss: 0.3362303376197815 | Test loss: 0.32965800166130066\n",
      "Epoch: 186190 | Loss: 0.33621856570243835 | Test loss: 0.32964420318603516\n",
      "Epoch: 186200 | Loss: 0.3362067937850952 | Test loss: 0.3296303451061249\n",
      "Epoch: 186210 | Loss: 0.3361950218677521 | Test loss: 0.329616516828537\n",
      "Epoch: 186220 | Loss: 0.3361832797527313 | Test loss: 0.3296026289463043\n",
      "Epoch: 186230 | Loss: 0.33617153763771057 | Test loss: 0.32958880066871643\n",
      "Epoch: 186240 | Loss: 0.33615976572036743 | Test loss: 0.32957494258880615\n",
      "Epoch: 186250 | Loss: 0.3361479938030243 | Test loss: 0.3295610845088959\n",
      "Epoch: 186260 | Loss: 0.33613622188568115 | Test loss: 0.329547256231308\n",
      "Epoch: 186270 | Loss: 0.3361245095729828 | Test loss: 0.3295334279537201\n",
      "Epoch: 186280 | Loss: 0.33611270785331726 | Test loss: 0.3295195698738098\n",
      "Epoch: 186290 | Loss: 0.3361009657382965 | Test loss: 0.3295057415962219\n",
      "Epoch: 186300 | Loss: 0.33608922362327576 | Test loss: 0.32949185371398926\n",
      "Epoch: 186310 | Loss: 0.33607742190361023 | Test loss: 0.32947802543640137\n",
      "Epoch: 186320 | Loss: 0.3360656797885895 | Test loss: 0.3294641971588135\n",
      "Epoch: 186330 | Loss: 0.33605390787124634 | Test loss: 0.3294503390789032\n",
      "Epoch: 186340 | Loss: 0.3360421657562256 | Test loss: 0.3294365108013153\n",
      "Epoch: 186350 | Loss: 0.33603042364120483 | Test loss: 0.3294226825237274\n",
      "Epoch: 186360 | Loss: 0.3360186517238617 | Test loss: 0.32940879464149475\n",
      "Epoch: 186370 | Loss: 0.33600687980651855 | Test loss: 0.32939496636390686\n",
      "Epoch: 186380 | Loss: 0.3359951376914978 | Test loss: 0.3293810784816742\n",
      "Epoch: 186390 | Loss: 0.33598336577415466 | Test loss: 0.3293672502040863\n",
      "Epoch: 186400 | Loss: 0.3359715938568115 | Test loss: 0.3293534219264984\n",
      "Epoch: 186410 | Loss: 0.33595985174179077 | Test loss: 0.3293395936489105\n",
      "Epoch: 186420 | Loss: 0.33594807982444763 | Test loss: 0.32932573556900024\n",
      "Epoch: 186430 | Loss: 0.3359363079071045 | Test loss: 0.32931187748908997\n",
      "Epoch: 186440 | Loss: 0.33592459559440613 | Test loss: 0.3292980194091797\n",
      "Epoch: 186450 | Loss: 0.3359127938747406 | Test loss: 0.3292841911315918\n",
      "Epoch: 186460 | Loss: 0.33590105175971985 | Test loss: 0.3292703330516815\n",
      "Epoch: 186470 | Loss: 0.3358893096446991 | Test loss: 0.32925650477409363\n",
      "Epoch: 186480 | Loss: 0.33587753772735596 | Test loss: 0.32924264669418335\n",
      "Epoch: 186490 | Loss: 0.33586573600769043 | Test loss: 0.32922878861427307\n",
      "Epoch: 186500 | Loss: 0.3358539938926697 | Test loss: 0.3292149603366852\n",
      "Epoch: 186510 | Loss: 0.33584222197532654 | Test loss: 0.3292011022567749\n",
      "Epoch: 186520 | Loss: 0.3358305096626282 | Test loss: 0.3291872441768646\n",
      "Epoch: 186530 | Loss: 0.33581873774528503 | Test loss: 0.32917341589927673\n",
      "Epoch: 186540 | Loss: 0.3358069360256195 | Test loss: 0.32915958762168884\n",
      "Epoch: 186550 | Loss: 0.33579519391059875 | Test loss: 0.32914572954177856\n",
      "Epoch: 186560 | Loss: 0.335783451795578 | Test loss: 0.3291319012641907\n",
      "Epoch: 186570 | Loss: 0.33577167987823486 | Test loss: 0.3291180431842804\n",
      "Epoch: 186580 | Loss: 0.3357599377632141 | Test loss: 0.32910415530204773\n",
      "Epoch: 186590 | Loss: 0.33574816584587097 | Test loss: 0.3290903568267822\n",
      "Epoch: 186600 | Loss: 0.33573639392852783 | Test loss: 0.32907649874687195\n",
      "Epoch: 186610 | Loss: 0.3357246220111847 | Test loss: 0.32906267046928406\n",
      "Epoch: 186620 | Loss: 0.33571287989616394 | Test loss: 0.3290487825870514\n",
      "Epoch: 186630 | Loss: 0.3357011377811432 | Test loss: 0.3290349543094635\n",
      "Epoch: 186640 | Loss: 0.33568936586380005 | Test loss: 0.3290210962295532\n",
      "Epoch: 186650 | Loss: 0.3356775939464569 | Test loss: 0.32900723814964294\n",
      "Epoch: 186660 | Loss: 0.33566582202911377 | Test loss: 0.32899340987205505\n",
      "Epoch: 186670 | Loss: 0.3356541097164154 | Test loss: 0.32897958159446716\n",
      "Epoch: 186680 | Loss: 0.3356423079967499 | Test loss: 0.3289657235145569\n",
      "Epoch: 186690 | Loss: 0.3356305658817291 | Test loss: 0.328951895236969\n",
      "Epoch: 186700 | Loss: 0.3356188237667084 | Test loss: 0.32893800735473633\n",
      "Epoch: 186710 | Loss: 0.33560702204704285 | Test loss: 0.32892417907714844\n",
      "Epoch: 186720 | Loss: 0.3355952799320221 | Test loss: 0.32891035079956055\n",
      "Epoch: 186730 | Loss: 0.33558350801467896 | Test loss: 0.32889649271965027\n",
      "Epoch: 186740 | Loss: 0.3355717658996582 | Test loss: 0.3288826644420624\n",
      "Epoch: 186750 | Loss: 0.33556002378463745 | Test loss: 0.3288688361644745\n",
      "Epoch: 186760 | Loss: 0.3355482518672943 | Test loss: 0.3288549482822418\n",
      "Epoch: 186770 | Loss: 0.33553647994995117 | Test loss: 0.32884112000465393\n",
      "Epoch: 186780 | Loss: 0.3355247378349304 | Test loss: 0.32882723212242126\n",
      "Epoch: 186790 | Loss: 0.3355129659175873 | Test loss: 0.3288134038448334\n",
      "Epoch: 186800 | Loss: 0.33550119400024414 | Test loss: 0.3287995755672455\n",
      "Epoch: 186810 | Loss: 0.3354894518852234 | Test loss: 0.3287857472896576\n",
      "Epoch: 186820 | Loss: 0.33547767996788025 | Test loss: 0.3287718892097473\n",
      "Epoch: 186830 | Loss: 0.3354659080505371 | Test loss: 0.32875803112983704\n",
      "Epoch: 186840 | Loss: 0.33545419573783875 | Test loss: 0.32874417304992676\n",
      "Epoch: 186850 | Loss: 0.3354423940181732 | Test loss: 0.32873034477233887\n",
      "Epoch: 186860 | Loss: 0.33543065190315247 | Test loss: 0.3287164866924286\n",
      "Epoch: 186870 | Loss: 0.3354189097881317 | Test loss: 0.3287026584148407\n",
      "Epoch: 186880 | Loss: 0.3354071378707886 | Test loss: 0.3286888003349304\n",
      "Epoch: 186890 | Loss: 0.33539533615112305 | Test loss: 0.32867494225502014\n",
      "Epoch: 186900 | Loss: 0.3353835940361023 | Test loss: 0.32866111397743225\n",
      "Epoch: 186910 | Loss: 0.33537182211875916 | Test loss: 0.328647255897522\n",
      "Epoch: 186920 | Loss: 0.3353601098060608 | Test loss: 0.3286333978176117\n",
      "Epoch: 186930 | Loss: 0.33534833788871765 | Test loss: 0.3286195695400238\n",
      "Epoch: 186940 | Loss: 0.3353365361690521 | Test loss: 0.3286057412624359\n",
      "Epoch: 186950 | Loss: 0.33532479405403137 | Test loss: 0.32859188318252563\n",
      "Epoch: 186960 | Loss: 0.3353130519390106 | Test loss: 0.32857805490493774\n",
      "Epoch: 186970 | Loss: 0.3353012800216675 | Test loss: 0.32856419682502747\n",
      "Epoch: 186980 | Loss: 0.33528953790664673 | Test loss: 0.3285503089427948\n",
      "Epoch: 186990 | Loss: 0.3352777659893036 | Test loss: 0.3285365104675293\n",
      "Epoch: 187000 | Loss: 0.33526599407196045 | Test loss: 0.328522652387619\n",
      "Epoch: 187010 | Loss: 0.3352542221546173 | Test loss: 0.32850882411003113\n",
      "Epoch: 187020 | Loss: 0.33524248003959656 | Test loss: 0.32849493622779846\n",
      "Epoch: 187030 | Loss: 0.3352307379245758 | Test loss: 0.32848110795021057\n",
      "Epoch: 187040 | Loss: 0.33521896600723267 | Test loss: 0.3284672498703003\n",
      "Epoch: 187050 | Loss: 0.3352071940898895 | Test loss: 0.32845339179039\n",
      "Epoch: 187060 | Loss: 0.3351954221725464 | Test loss: 0.3284395635128021\n",
      "Epoch: 187070 | Loss: 0.335183709859848 | Test loss: 0.32842573523521423\n",
      "Epoch: 187080 | Loss: 0.3351719081401825 | Test loss: 0.32841187715530396\n",
      "Epoch: 187090 | Loss: 0.33516016602516174 | Test loss: 0.32839804887771606\n",
      "Epoch: 187100 | Loss: 0.335148423910141 | Test loss: 0.3283841609954834\n",
      "Epoch: 187110 | Loss: 0.33513662219047546 | Test loss: 0.3283703327178955\n",
      "Epoch: 187120 | Loss: 0.3351248800754547 | Test loss: 0.3283565044403076\n",
      "Epoch: 187130 | Loss: 0.3351131081581116 | Test loss: 0.32834264636039734\n",
      "Epoch: 187140 | Loss: 0.3351013660430908 | Test loss: 0.32832881808280945\n",
      "Epoch: 187150 | Loss: 0.33508962392807007 | Test loss: 0.32831498980522156\n",
      "Epoch: 187160 | Loss: 0.33507785201072693 | Test loss: 0.3283011019229889\n",
      "Epoch: 187170 | Loss: 0.3350660800933838 | Test loss: 0.328287273645401\n",
      "Epoch: 187180 | Loss: 0.33505433797836304 | Test loss: 0.32827338576316833\n",
      "Epoch: 187190 | Loss: 0.3350425660610199 | Test loss: 0.32825955748558044\n",
      "Epoch: 187200 | Loss: 0.33503079414367676 | Test loss: 0.32824572920799255\n",
      "Epoch: 187210 | Loss: 0.335019052028656 | Test loss: 0.32823190093040466\n",
      "Epoch: 187220 | Loss: 0.33500728011131287 | Test loss: 0.3282180428504944\n",
      "Epoch: 187230 | Loss: 0.3349955081939697 | Test loss: 0.3282041847705841\n",
      "Epoch: 187240 | Loss: 0.33498379588127136 | Test loss: 0.32819032669067383\n",
      "Epoch: 187250 | Loss: 0.33497199416160583 | Test loss: 0.32817649841308594\n",
      "Epoch: 187260 | Loss: 0.3349602520465851 | Test loss: 0.32816264033317566\n",
      "Epoch: 187270 | Loss: 0.33494850993156433 | Test loss: 0.32814881205558777\n",
      "Epoch: 187280 | Loss: 0.3349367380142212 | Test loss: 0.3281349539756775\n",
      "Epoch: 187290 | Loss: 0.33492493629455566 | Test loss: 0.3281210958957672\n",
      "Epoch: 187300 | Loss: 0.3349131941795349 | Test loss: 0.3281072676181793\n",
      "Epoch: 187310 | Loss: 0.3349014222621918 | Test loss: 0.32809340953826904\n",
      "Epoch: 187320 | Loss: 0.3348897099494934 | Test loss: 0.32807955145835876\n",
      "Epoch: 187330 | Loss: 0.33487793803215027 | Test loss: 0.3280657231807709\n",
      "Epoch: 187340 | Loss: 0.33486613631248474 | Test loss: 0.328051894903183\n",
      "Epoch: 187350 | Loss: 0.334854394197464 | Test loss: 0.3280380368232727\n",
      "Epoch: 187360 | Loss: 0.33484265208244324 | Test loss: 0.3280242085456848\n",
      "Epoch: 187370 | Loss: 0.3348308801651001 | Test loss: 0.32801035046577454\n",
      "Epoch: 187380 | Loss: 0.33481913805007935 | Test loss: 0.32799646258354187\n",
      "Epoch: 187390 | Loss: 0.3348073661327362 | Test loss: 0.32798266410827637\n",
      "Epoch: 187400 | Loss: 0.33479559421539307 | Test loss: 0.3279688060283661\n",
      "Epoch: 187410 | Loss: 0.3347838222980499 | Test loss: 0.3279549777507782\n",
      "Epoch: 187420 | Loss: 0.3347720801830292 | Test loss: 0.32794108986854553\n",
      "Epoch: 187430 | Loss: 0.3347603380680084 | Test loss: 0.32792726159095764\n",
      "Epoch: 187440 | Loss: 0.3347485661506653 | Test loss: 0.32791340351104736\n",
      "Epoch: 187450 | Loss: 0.33473679423332214 | Test loss: 0.3278995454311371\n",
      "Epoch: 187460 | Loss: 0.334725022315979 | Test loss: 0.3278857171535492\n",
      "Epoch: 187470 | Loss: 0.33471331000328064 | Test loss: 0.3278718888759613\n",
      "Epoch: 187480 | Loss: 0.3347015082836151 | Test loss: 0.327858030796051\n",
      "Epoch: 187490 | Loss: 0.33468976616859436 | Test loss: 0.32784420251846313\n",
      "Epoch: 187500 | Loss: 0.3346780240535736 | Test loss: 0.32783031463623047\n",
      "Epoch: 187510 | Loss: 0.3346662223339081 | Test loss: 0.3278164863586426\n",
      "Epoch: 187520 | Loss: 0.33465448021888733 | Test loss: 0.3278026580810547\n",
      "Epoch: 187530 | Loss: 0.3346427083015442 | Test loss: 0.3277888000011444\n",
      "Epoch: 187540 | Loss: 0.33463096618652344 | Test loss: 0.3277749717235565\n",
      "Epoch: 187550 | Loss: 0.3346192240715027 | Test loss: 0.32776114344596863\n",
      "Epoch: 187560 | Loss: 0.33460745215415955 | Test loss: 0.32774725556373596\n",
      "Epoch: 187570 | Loss: 0.3345956802368164 | Test loss: 0.32773342728614807\n",
      "Epoch: 187580 | Loss: 0.33458393812179565 | Test loss: 0.3277195394039154\n",
      "Epoch: 187590 | Loss: 0.3345721662044525 | Test loss: 0.3277057111263275\n",
      "Epoch: 187600 | Loss: 0.3345603942871094 | Test loss: 0.3276918828487396\n",
      "Epoch: 187610 | Loss: 0.3345486521720886 | Test loss: 0.32767805457115173\n",
      "Epoch: 187620 | Loss: 0.3345368802547455 | Test loss: 0.32766419649124146\n",
      "Epoch: 187630 | Loss: 0.33452510833740234 | Test loss: 0.3276503384113312\n",
      "Epoch: 187640 | Loss: 0.334513396024704 | Test loss: 0.3276364803314209\n",
      "Epoch: 187650 | Loss: 0.33450159430503845 | Test loss: 0.327622652053833\n",
      "Epoch: 187660 | Loss: 0.3344898521900177 | Test loss: 0.32760879397392273\n",
      "Epoch: 187670 | Loss: 0.33447811007499695 | Test loss: 0.32759496569633484\n",
      "Epoch: 187680 | Loss: 0.3344663381576538 | Test loss: 0.32758110761642456\n",
      "Epoch: 187690 | Loss: 0.3344545364379883 | Test loss: 0.3275672495365143\n",
      "Epoch: 187700 | Loss: 0.33444279432296753 | Test loss: 0.3275534212589264\n",
      "Epoch: 187710 | Loss: 0.3344310224056244 | Test loss: 0.3275395631790161\n",
      "Epoch: 187720 | Loss: 0.334419310092926 | Test loss: 0.32752570509910583\n",
      "Epoch: 187730 | Loss: 0.3344075381755829 | Test loss: 0.32751187682151794\n",
      "Epoch: 187740 | Loss: 0.33439573645591736 | Test loss: 0.32749804854393005\n",
      "Epoch: 187750 | Loss: 0.3343839943408966 | Test loss: 0.3274841904640198\n",
      "Epoch: 187760 | Loss: 0.33437225222587585 | Test loss: 0.3274703621864319\n",
      "Epoch: 187770 | Loss: 0.3343604803085327 | Test loss: 0.3274565041065216\n",
      "Epoch: 187780 | Loss: 0.33434873819351196 | Test loss: 0.32744261622428894\n",
      "Epoch: 187790 | Loss: 0.3343369662761688 | Test loss: 0.32742881774902344\n",
      "Epoch: 187800 | Loss: 0.3343251943588257 | Test loss: 0.32741495966911316\n",
      "Epoch: 187810 | Loss: 0.33431342244148254 | Test loss: 0.32740113139152527\n",
      "Epoch: 187820 | Loss: 0.3343016803264618 | Test loss: 0.3273872435092926\n",
      "Epoch: 187830 | Loss: 0.33428993821144104 | Test loss: 0.3273734152317047\n",
      "Epoch: 187840 | Loss: 0.3342781662940979 | Test loss: 0.32735955715179443\n",
      "Epoch: 187850 | Loss: 0.33426639437675476 | Test loss: 0.32734569907188416\n",
      "Epoch: 187860 | Loss: 0.3342546224594116 | Test loss: 0.32733187079429626\n",
      "Epoch: 187870 | Loss: 0.33424291014671326 | Test loss: 0.3273180425167084\n",
      "Epoch: 187880 | Loss: 0.33423110842704773 | Test loss: 0.3273041844367981\n",
      "Epoch: 187890 | Loss: 0.334219366312027 | Test loss: 0.3272903561592102\n",
      "Epoch: 187900 | Loss: 0.3342076241970062 | Test loss: 0.32727646827697754\n",
      "Epoch: 187910 | Loss: 0.3341958224773407 | Test loss: 0.32726263999938965\n",
      "Epoch: 187920 | Loss: 0.33418408036231995 | Test loss: 0.32724881172180176\n",
      "Epoch: 187930 | Loss: 0.3341723084449768 | Test loss: 0.3272349536418915\n",
      "Epoch: 187940 | Loss: 0.33416056632995605 | Test loss: 0.3272211253643036\n",
      "Epoch: 187950 | Loss: 0.3341488242149353 | Test loss: 0.3272072970867157\n",
      "Epoch: 187960 | Loss: 0.33413705229759216 | Test loss: 0.32719340920448303\n",
      "Epoch: 187970 | Loss: 0.334125280380249 | Test loss: 0.32717958092689514\n",
      "Epoch: 187980 | Loss: 0.33411353826522827 | Test loss: 0.3271656930446625\n",
      "Epoch: 187990 | Loss: 0.33410176634788513 | Test loss: 0.3271518647670746\n",
      "Epoch: 188000 | Loss: 0.334089994430542 | Test loss: 0.3271380364894867\n",
      "Epoch: 188010 | Loss: 0.33407825231552124 | Test loss: 0.3271242082118988\n",
      "Epoch: 188020 | Loss: 0.3340664803981781 | Test loss: 0.3271103501319885\n",
      "Epoch: 188030 | Loss: 0.33405470848083496 | Test loss: 0.32709649205207825\n",
      "Epoch: 188040 | Loss: 0.3340429961681366 | Test loss: 0.32708263397216797\n",
      "Epoch: 188050 | Loss: 0.33403119444847107 | Test loss: 0.3270688056945801\n",
      "Epoch: 188060 | Loss: 0.3340194523334503 | Test loss: 0.3270549476146698\n",
      "Epoch: 188070 | Loss: 0.33400771021842957 | Test loss: 0.3270411193370819\n",
      "Epoch: 188080 | Loss: 0.3339959383010864 | Test loss: 0.32702726125717163\n",
      "Epoch: 188090 | Loss: 0.3339841365814209 | Test loss: 0.32701340317726135\n",
      "Epoch: 188100 | Loss: 0.33397239446640015 | Test loss: 0.32699957489967346\n",
      "Epoch: 188110 | Loss: 0.333960622549057 | Test loss: 0.3269857168197632\n",
      "Epoch: 188120 | Loss: 0.33394891023635864 | Test loss: 0.3269718587398529\n",
      "Epoch: 188130 | Loss: 0.3339371383190155 | Test loss: 0.326958030462265\n",
      "Epoch: 188140 | Loss: 0.33392533659935 | Test loss: 0.3269442021846771\n",
      "Epoch: 188150 | Loss: 0.3339135944843292 | Test loss: 0.32693034410476685\n",
      "Epoch: 188160 | Loss: 0.33390185236930847 | Test loss: 0.32691651582717896\n",
      "Epoch: 188170 | Loss: 0.33389008045196533 | Test loss: 0.3269026577472687\n",
      "Epoch: 188180 | Loss: 0.3338783383369446 | Test loss: 0.326888769865036\n",
      "Epoch: 188190 | Loss: 0.33386656641960144 | Test loss: 0.3268749713897705\n",
      "Epoch: 188200 | Loss: 0.3338547945022583 | Test loss: 0.32686111330986023\n",
      "Epoch: 188210 | Loss: 0.33384302258491516 | Test loss: 0.32684728503227234\n",
      "Epoch: 188220 | Loss: 0.3338312804698944 | Test loss: 0.3268333971500397\n",
      "Epoch: 188230 | Loss: 0.33381953835487366 | Test loss: 0.3268195688724518\n",
      "Epoch: 188240 | Loss: 0.3338077664375305 | Test loss: 0.3268057107925415\n",
      "Epoch: 188250 | Loss: 0.3337959945201874 | Test loss: 0.3267918527126312\n",
      "Epoch: 188260 | Loss: 0.33378422260284424 | Test loss: 0.32677802443504333\n",
      "Epoch: 188270 | Loss: 0.3337725102901459 | Test loss: 0.32676419615745544\n",
      "Epoch: 188280 | Loss: 0.33376070857048035 | Test loss: 0.32675033807754517\n",
      "Epoch: 188290 | Loss: 0.3337489664554596 | Test loss: 0.3267365097999573\n",
      "Epoch: 188300 | Loss: 0.33373722434043884 | Test loss: 0.3267226219177246\n",
      "Epoch: 188310 | Loss: 0.3337254226207733 | Test loss: 0.3267087936401367\n",
      "Epoch: 188320 | Loss: 0.33371368050575256 | Test loss: 0.32669496536254883\n",
      "Epoch: 188330 | Loss: 0.3337019085884094 | Test loss: 0.32668110728263855\n",
      "Epoch: 188340 | Loss: 0.33369016647338867 | Test loss: 0.32666727900505066\n",
      "Epoch: 188350 | Loss: 0.3336784243583679 | Test loss: 0.32665345072746277\n",
      "Epoch: 188360 | Loss: 0.3336666524410248 | Test loss: 0.3266395628452301\n",
      "Epoch: 188370 | Loss: 0.33365488052368164 | Test loss: 0.3266257345676422\n",
      "Epoch: 188380 | Loss: 0.3336431384086609 | Test loss: 0.32661184668540955\n",
      "Epoch: 188390 | Loss: 0.33363136649131775 | Test loss: 0.32659801840782166\n",
      "Epoch: 188400 | Loss: 0.3336195945739746 | Test loss: 0.32658419013023376\n",
      "Epoch: 188410 | Loss: 0.33360785245895386 | Test loss: 0.3265703618526459\n",
      "Epoch: 188420 | Loss: 0.3335960805416107 | Test loss: 0.3265565037727356\n",
      "Epoch: 188430 | Loss: 0.3335843086242676 | Test loss: 0.3265426456928253\n",
      "Epoch: 188440 | Loss: 0.3335725963115692 | Test loss: 0.32652878761291504\n",
      "Epoch: 188450 | Loss: 0.3335607945919037 | Test loss: 0.32651495933532715\n",
      "Epoch: 188460 | Loss: 0.33354905247688293 | Test loss: 0.32650110125541687\n",
      "Epoch: 188470 | Loss: 0.3335373103618622 | Test loss: 0.326487272977829\n",
      "Epoch: 188480 | Loss: 0.33352553844451904 | Test loss: 0.3264734148979187\n",
      "Epoch: 188490 | Loss: 0.3335137367248535 | Test loss: 0.3264595568180084\n",
      "Epoch: 188500 | Loss: 0.33350199460983276 | Test loss: 0.32644572854042053\n",
      "Epoch: 188510 | Loss: 0.3334902226924896 | Test loss: 0.32643187046051025\n",
      "Epoch: 188520 | Loss: 0.33347851037979126 | Test loss: 0.3264180123806\n",
      "Epoch: 188530 | Loss: 0.3334667384624481 | Test loss: 0.3264041841030121\n",
      "Epoch: 188540 | Loss: 0.3334549367427826 | Test loss: 0.3263903558254242\n",
      "Epoch: 188550 | Loss: 0.33344319462776184 | Test loss: 0.3263764977455139\n",
      "Epoch: 188560 | Loss: 0.3334314525127411 | Test loss: 0.326362669467926\n",
      "Epoch: 188570 | Loss: 0.33341968059539795 | Test loss: 0.32634881138801575\n",
      "Epoch: 188580 | Loss: 0.3334079384803772 | Test loss: 0.3263349235057831\n",
      "Epoch: 188590 | Loss: 0.33339616656303406 | Test loss: 0.3263211250305176\n",
      "Epoch: 188600 | Loss: 0.3333843946456909 | Test loss: 0.3263072669506073\n",
      "Epoch: 188610 | Loss: 0.3333726227283478 | Test loss: 0.3262934386730194\n",
      "Epoch: 188620 | Loss: 0.333360880613327 | Test loss: 0.32627955079078674\n",
      "Epoch: 188630 | Loss: 0.3333491384983063 | Test loss: 0.32626572251319885\n",
      "Epoch: 188640 | Loss: 0.33333736658096313 | Test loss: 0.3262518644332886\n",
      "Epoch: 188650 | Loss: 0.33332559466362 | Test loss: 0.3262380063533783\n",
      "Epoch: 188660 | Loss: 0.33331382274627686 | Test loss: 0.3262241780757904\n",
      "Epoch: 188670 | Loss: 0.3333021104335785 | Test loss: 0.3262103497982025\n",
      "Epoch: 188680 | Loss: 0.33329030871391296 | Test loss: 0.32619649171829224\n",
      "Epoch: 188690 | Loss: 0.3332785665988922 | Test loss: 0.32618266344070435\n",
      "Epoch: 188700 | Loss: 0.33326682448387146 | Test loss: 0.3261687755584717\n",
      "Epoch: 188710 | Loss: 0.33325502276420593 | Test loss: 0.3261549472808838\n",
      "Epoch: 188720 | Loss: 0.3332432806491852 | Test loss: 0.3261411190032959\n",
      "Epoch: 188730 | Loss: 0.33323150873184204 | Test loss: 0.3261272609233856\n",
      "Epoch: 188740 | Loss: 0.3332197666168213 | Test loss: 0.32611343264579773\n",
      "Epoch: 188750 | Loss: 0.33320802450180054 | Test loss: 0.32609960436820984\n",
      "Epoch: 188760 | Loss: 0.3331962525844574 | Test loss: 0.3260857164859772\n",
      "Epoch: 188770 | Loss: 0.33318448066711426 | Test loss: 0.3260718882083893\n",
      "Epoch: 188780 | Loss: 0.3331727385520935 | Test loss: 0.3260580003261566\n",
      "Epoch: 188790 | Loss: 0.33316096663475037 | Test loss: 0.3260441720485687\n",
      "Epoch: 188800 | Loss: 0.3331491947174072 | Test loss: 0.32603034377098083\n",
      "Epoch: 188810 | Loss: 0.3331374526023865 | Test loss: 0.32601651549339294\n",
      "Epoch: 188820 | Loss: 0.33312568068504333 | Test loss: 0.32600265741348267\n",
      "Epoch: 188830 | Loss: 0.3331139087677002 | Test loss: 0.3259887993335724\n",
      "Epoch: 188840 | Loss: 0.33310219645500183 | Test loss: 0.3259749412536621\n",
      "Epoch: 188850 | Loss: 0.3330903947353363 | Test loss: 0.3259611129760742\n",
      "Epoch: 188860 | Loss: 0.33307865262031555 | Test loss: 0.32594725489616394\n",
      "Epoch: 188870 | Loss: 0.3330669105052948 | Test loss: 0.32593342661857605\n",
      "Epoch: 188880 | Loss: 0.33305513858795166 | Test loss: 0.32591956853866577\n",
      "Epoch: 188890 | Loss: 0.33304333686828613 | Test loss: 0.3259057104587555\n",
      "Epoch: 188900 | Loss: 0.3330315947532654 | Test loss: 0.3258918821811676\n",
      "Epoch: 188910 | Loss: 0.33301982283592224 | Test loss: 0.3258780241012573\n",
      "Epoch: 188920 | Loss: 0.3330081105232239 | Test loss: 0.32586416602134705\n",
      "Epoch: 188930 | Loss: 0.33299633860588074 | Test loss: 0.32585033774375916\n",
      "Epoch: 188940 | Loss: 0.3329845368862152 | Test loss: 0.32583650946617126\n",
      "Epoch: 188950 | Loss: 0.33297279477119446 | Test loss: 0.325822651386261\n",
      "Epoch: 188960 | Loss: 0.3329610526561737 | Test loss: 0.3258088231086731\n",
      "Epoch: 188970 | Loss: 0.33294928073883057 | Test loss: 0.3257949650287628\n",
      "Epoch: 188980 | Loss: 0.3329375386238098 | Test loss: 0.32578107714653015\n",
      "Epoch: 188990 | Loss: 0.3329257667064667 | Test loss: 0.32576727867126465\n",
      "Epoch: 189000 | Loss: 0.33291399478912354 | Test loss: 0.32575342059135437\n",
      "Epoch: 189010 | Loss: 0.3329022228717804 | Test loss: 0.3257395923137665\n",
      "Epoch: 189020 | Loss: 0.33289048075675964 | Test loss: 0.3257257044315338\n",
      "Epoch: 189030 | Loss: 0.3328787386417389 | Test loss: 0.3257118761539459\n",
      "Epoch: 189040 | Loss: 0.33286696672439575 | Test loss: 0.32569801807403564\n",
      "Epoch: 189050 | Loss: 0.3328551948070526 | Test loss: 0.32568415999412537\n",
      "Epoch: 189060 | Loss: 0.3328434228897095 | Test loss: 0.3256703317165375\n",
      "Epoch: 189070 | Loss: 0.3328317105770111 | Test loss: 0.3256565034389496\n",
      "Epoch: 189080 | Loss: 0.3328199088573456 | Test loss: 0.3256426453590393\n",
      "Epoch: 189090 | Loss: 0.33280816674232483 | Test loss: 0.3256288170814514\n",
      "Epoch: 189100 | Loss: 0.3327964246273041 | Test loss: 0.32561492919921875\n",
      "Epoch: 189110 | Loss: 0.33278462290763855 | Test loss: 0.32560110092163086\n",
      "Epoch: 189120 | Loss: 0.3327728807926178 | Test loss: 0.32558727264404297\n",
      "Epoch: 189130 | Loss: 0.33276110887527466 | Test loss: 0.3255734145641327\n",
      "Epoch: 189140 | Loss: 0.3327493667602539 | Test loss: 0.3255595862865448\n",
      "Epoch: 189150 | Loss: 0.33273762464523315 | Test loss: 0.3255457580089569\n",
      "Epoch: 189160 | Loss: 0.33272585272789 | Test loss: 0.32553187012672424\n",
      "Epoch: 189170 | Loss: 0.3327140808105469 | Test loss: 0.32551804184913635\n",
      "Epoch: 189180 | Loss: 0.3327023386955261 | Test loss: 0.3255041539669037\n",
      "Epoch: 189190 | Loss: 0.332690566778183 | Test loss: 0.3254903256893158\n",
      "Epoch: 189200 | Loss: 0.33267879486083984 | Test loss: 0.3254764974117279\n",
      "Epoch: 189210 | Loss: 0.3326670527458191 | Test loss: 0.32546266913414\n",
      "Epoch: 189220 | Loss: 0.33265528082847595 | Test loss: 0.32544881105422974\n",
      "Epoch: 189230 | Loss: 0.3326435089111328 | Test loss: 0.32543495297431946\n",
      "Epoch: 189240 | Loss: 0.33263179659843445 | Test loss: 0.3254210948944092\n",
      "Epoch: 189250 | Loss: 0.3326199948787689 | Test loss: 0.3254072666168213\n",
      "Epoch: 189260 | Loss: 0.33260825276374817 | Test loss: 0.325393408536911\n",
      "Epoch: 189270 | Loss: 0.3325965106487274 | Test loss: 0.3253795802593231\n",
      "Epoch: 189280 | Loss: 0.3325847387313843 | Test loss: 0.32536572217941284\n",
      "Epoch: 189290 | Loss: 0.33257293701171875 | Test loss: 0.32535186409950256\n",
      "Epoch: 189300 | Loss: 0.3325612246990204 | Test loss: 0.3253380358219147\n",
      "Epoch: 189310 | Loss: 0.33254942297935486 | Test loss: 0.3253241777420044\n",
      "Epoch: 189320 | Loss: 0.3325377106666565 | Test loss: 0.3253103196620941\n",
      "Epoch: 189330 | Loss: 0.33252593874931335 | Test loss: 0.3252964913845062\n",
      "Epoch: 189340 | Loss: 0.3325141370296478 | Test loss: 0.32528266310691833\n",
      "Epoch: 189350 | Loss: 0.33250242471694946 | Test loss: 0.32526880502700806\n",
      "Epoch: 189360 | Loss: 0.3324906527996063 | Test loss: 0.32525497674942017\n",
      "Epoch: 189370 | Loss: 0.3324788808822632 | Test loss: 0.3252411186695099\n",
      "Epoch: 189380 | Loss: 0.33246713876724243 | Test loss: 0.3252272307872772\n",
      "Epoch: 189390 | Loss: 0.3324553668498993 | Test loss: 0.3252134323120117\n",
      "Epoch: 189400 | Loss: 0.33244359493255615 | Test loss: 0.32519957423210144\n",
      "Epoch: 189410 | Loss: 0.3324318528175354 | Test loss: 0.32518574595451355\n",
      "Epoch: 189420 | Loss: 0.33242008090019226 | Test loss: 0.3251718580722809\n",
      "Epoch: 189430 | Loss: 0.3324083387851715 | Test loss: 0.325158029794693\n",
      "Epoch: 189440 | Loss: 0.33239656686782837 | Test loss: 0.3251441717147827\n",
      "Epoch: 189450 | Loss: 0.33238479495048523 | Test loss: 0.32513031363487244\n",
      "Epoch: 189460 | Loss: 0.3323730528354645 | Test loss: 0.32511648535728455\n",
      "Epoch: 189470 | Loss: 0.3323613107204437 | Test loss: 0.32510265707969666\n",
      "Epoch: 189480 | Loss: 0.3323495388031006 | Test loss: 0.3250887989997864\n",
      "Epoch: 189490 | Loss: 0.33233776688575745 | Test loss: 0.3250749707221985\n",
      "Epoch: 189500 | Loss: 0.3323259949684143 | Test loss: 0.3250610828399658\n",
      "Epoch: 189510 | Loss: 0.33231422305107117 | Test loss: 0.32504725456237793\n",
      "Epoch: 189520 | Loss: 0.3323024809360504 | Test loss: 0.32503342628479004\n",
      "Epoch: 189530 | Loss: 0.3322907090187073 | Test loss: 0.32501956820487976\n",
      "Epoch: 189540 | Loss: 0.3322789669036865 | Test loss: 0.32500573992729187\n",
      "Epoch: 189550 | Loss: 0.3322671949863434 | Test loss: 0.324991911649704\n",
      "Epoch: 189560 | Loss: 0.33225545287132263 | Test loss: 0.3249780237674713\n",
      "Epoch: 189570 | Loss: 0.3322436809539795 | Test loss: 0.3249641954898834\n",
      "Epoch: 189580 | Loss: 0.33223193883895874 | Test loss: 0.32495030760765076\n",
      "Epoch: 189590 | Loss: 0.3322201669216156 | Test loss: 0.32493647933006287\n",
      "Epoch: 189600 | Loss: 0.33220839500427246 | Test loss: 0.324922651052475\n",
      "Epoch: 189610 | Loss: 0.3321966230869293 | Test loss: 0.3249088227748871\n",
      "Epoch: 189620 | Loss: 0.33218488097190857 | Test loss: 0.3248949646949768\n",
      "Epoch: 189630 | Loss: 0.33217310905456543 | Test loss: 0.32488110661506653\n",
      "Epoch: 189640 | Loss: 0.33216139674186707 | Test loss: 0.32486724853515625\n",
      "Epoch: 189650 | Loss: 0.33214959502220154 | Test loss: 0.32485342025756836\n",
      "Epoch: 189660 | Loss: 0.3321378231048584 | Test loss: 0.3248395621776581\n",
      "Epoch: 189670 | Loss: 0.33212611079216003 | Test loss: 0.3248257339000702\n",
      "Epoch: 189680 | Loss: 0.3321143090724945 | Test loss: 0.3248118758201599\n",
      "Epoch: 189690 | Loss: 0.33210253715515137 | Test loss: 0.32479801774024963\n",
      "Epoch: 189700 | Loss: 0.332090824842453 | Test loss: 0.32478418946266174\n",
      "Epoch: 189710 | Loss: 0.3320790231227875 | Test loss: 0.32477033138275146\n",
      "Epoch: 189720 | Loss: 0.3320673108100891 | Test loss: 0.3247564733028412\n",
      "Epoch: 189730 | Loss: 0.33205553889274597 | Test loss: 0.3247426450252533\n",
      "Epoch: 189740 | Loss: 0.33204373717308044 | Test loss: 0.3247288167476654\n",
      "Epoch: 189750 | Loss: 0.3320320248603821 | Test loss: 0.3247149586677551\n",
      "Epoch: 189760 | Loss: 0.33202025294303894 | Test loss: 0.32470113039016724\n",
      "Epoch: 189770 | Loss: 0.3320084810256958 | Test loss: 0.32468727231025696\n",
      "Epoch: 189780 | Loss: 0.33199673891067505 | Test loss: 0.3246733844280243\n",
      "Epoch: 189790 | Loss: 0.3319849669933319 | Test loss: 0.3246595859527588\n",
      "Epoch: 189800 | Loss: 0.33197319507598877 | Test loss: 0.3246457278728485\n",
      "Epoch: 189810 | Loss: 0.331961452960968 | Test loss: 0.3246318995952606\n",
      "Epoch: 189820 | Loss: 0.3319496810436249 | Test loss: 0.32461801171302795\n",
      "Epoch: 189830 | Loss: 0.3319379389286041 | Test loss: 0.32460418343544006\n",
      "Epoch: 189840 | Loss: 0.331926167011261 | Test loss: 0.3245903253555298\n",
      "Epoch: 189850 | Loss: 0.33191439509391785 | Test loss: 0.3245764672756195\n",
      "Epoch: 189860 | Loss: 0.3319026529788971 | Test loss: 0.3245626389980316\n",
      "Epoch: 189870 | Loss: 0.33189091086387634 | Test loss: 0.3245488107204437\n",
      "Epoch: 189880 | Loss: 0.3318791389465332 | Test loss: 0.32453495264053345\n",
      "Epoch: 189890 | Loss: 0.33186736702919006 | Test loss: 0.32452112436294556\n",
      "Epoch: 189900 | Loss: 0.3318555951118469 | Test loss: 0.3245072364807129\n",
      "Epoch: 189910 | Loss: 0.3318438231945038 | Test loss: 0.324493408203125\n",
      "Epoch: 189920 | Loss: 0.33183208107948303 | Test loss: 0.3244795799255371\n",
      "Epoch: 189930 | Loss: 0.3318203091621399 | Test loss: 0.32446572184562683\n",
      "Epoch: 189940 | Loss: 0.33180856704711914 | Test loss: 0.32445189356803894\n",
      "Epoch: 189950 | Loss: 0.331796795129776 | Test loss: 0.32443806529045105\n",
      "Epoch: 189960 | Loss: 0.33178505301475525 | Test loss: 0.3244241774082184\n",
      "Epoch: 189970 | Loss: 0.3317732810974121 | Test loss: 0.3244103491306305\n",
      "Epoch: 189980 | Loss: 0.33176153898239136 | Test loss: 0.3243964612483978\n",
      "Epoch: 189990 | Loss: 0.3317497670650482 | Test loss: 0.32438263297080994\n",
      "Epoch: 190000 | Loss: 0.3317379951477051 | Test loss: 0.32436880469322205\n",
      "Epoch: 190010 | Loss: 0.33172622323036194 | Test loss: 0.32435497641563416\n",
      "Epoch: 190020 | Loss: 0.3317144811153412 | Test loss: 0.3243411183357239\n",
      "Epoch: 190030 | Loss: 0.33170270919799805 | Test loss: 0.3243272602558136\n",
      "Epoch: 190040 | Loss: 0.3316909968852997 | Test loss: 0.3243134021759033\n",
      "Epoch: 190050 | Loss: 0.33167919516563416 | Test loss: 0.32429957389831543\n",
      "Epoch: 190060 | Loss: 0.331667423248291 | Test loss: 0.32428571581840515\n",
      "Epoch: 190070 | Loss: 0.33165571093559265 | Test loss: 0.32427188754081726\n",
      "Epoch: 190080 | Loss: 0.3316439092159271 | Test loss: 0.324258029460907\n",
      "Epoch: 190090 | Loss: 0.331632137298584 | Test loss: 0.3242441713809967\n",
      "Epoch: 190100 | Loss: 0.3316204249858856 | Test loss: 0.3242303431034088\n",
      "Epoch: 190110 | Loss: 0.3316086232662201 | Test loss: 0.32421648502349854\n",
      "Epoch: 190120 | Loss: 0.33159691095352173 | Test loss: 0.32420262694358826\n",
      "Epoch: 190130 | Loss: 0.3315851390361786 | Test loss: 0.32418879866600037\n",
      "Epoch: 190140 | Loss: 0.33157333731651306 | Test loss: 0.3241749703884125\n",
      "Epoch: 190150 | Loss: 0.3315616250038147 | Test loss: 0.3241611123085022\n",
      "Epoch: 190160 | Loss: 0.33154985308647156 | Test loss: 0.3241472840309143\n",
      "Epoch: 190170 | Loss: 0.3315380811691284 | Test loss: 0.32413342595100403\n",
      "Epoch: 190180 | Loss: 0.33152633905410767 | Test loss: 0.32411953806877136\n",
      "Epoch: 190190 | Loss: 0.3315145671367645 | Test loss: 0.32410573959350586\n",
      "Epoch: 190200 | Loss: 0.3315027952194214 | Test loss: 0.3240918815135956\n",
      "Epoch: 190210 | Loss: 0.33149105310440063 | Test loss: 0.3240780532360077\n",
      "Epoch: 190220 | Loss: 0.3314792811870575 | Test loss: 0.324064165353775\n",
      "Epoch: 190230 | Loss: 0.33146753907203674 | Test loss: 0.32405033707618713\n",
      "Epoch: 190240 | Loss: 0.3314557671546936 | Test loss: 0.32403647899627686\n",
      "Epoch: 190250 | Loss: 0.33144399523735046 | Test loss: 0.3240226209163666\n",
      "Epoch: 190260 | Loss: 0.3314322531223297 | Test loss: 0.3240087926387787\n",
      "Epoch: 190270 | Loss: 0.33142051100730896 | Test loss: 0.3239949643611908\n",
      "Epoch: 190280 | Loss: 0.3314087390899658 | Test loss: 0.3239811062812805\n",
      "Epoch: 190290 | Loss: 0.3313969671726227 | Test loss: 0.3239672780036926\n",
      "Epoch: 190300 | Loss: 0.33138519525527954 | Test loss: 0.32395339012145996\n",
      "Epoch: 190310 | Loss: 0.3313734233379364 | Test loss: 0.32393956184387207\n",
      "Epoch: 190320 | Loss: 0.33136168122291565 | Test loss: 0.3239257335662842\n",
      "Epoch: 190330 | Loss: 0.3313499093055725 | Test loss: 0.3239118754863739\n",
      "Epoch: 190340 | Loss: 0.33133816719055176 | Test loss: 0.323898047208786\n",
      "Epoch: 190350 | Loss: 0.3313263952732086 | Test loss: 0.3238842189311981\n",
      "Epoch: 190360 | Loss: 0.33131465315818787 | Test loss: 0.32387033104896545\n",
      "Epoch: 190370 | Loss: 0.3313028812408447 | Test loss: 0.32385650277137756\n",
      "Epoch: 190380 | Loss: 0.331291139125824 | Test loss: 0.3238426148891449\n",
      "Epoch: 190390 | Loss: 0.33127936720848083 | Test loss: 0.323828786611557\n",
      "Epoch: 190400 | Loss: 0.3312675952911377 | Test loss: 0.3238149583339691\n",
      "Epoch: 190410 | Loss: 0.33125582337379456 | Test loss: 0.3238011300563812\n",
      "Epoch: 190420 | Loss: 0.3312440812587738 | Test loss: 0.32378727197647095\n",
      "Epoch: 190430 | Loss: 0.33123230934143066 | Test loss: 0.32377341389656067\n",
      "Epoch: 190440 | Loss: 0.3312205970287323 | Test loss: 0.3237595558166504\n",
      "Epoch: 190450 | Loss: 0.3312087953090668 | Test loss: 0.3237457275390625\n",
      "Epoch: 190460 | Loss: 0.33119702339172363 | Test loss: 0.3237318694591522\n",
      "Epoch: 190470 | Loss: 0.33118531107902527 | Test loss: 0.32371804118156433\n",
      "Epoch: 190480 | Loss: 0.33117350935935974 | Test loss: 0.32370418310165405\n",
      "Epoch: 190490 | Loss: 0.3311617374420166 | Test loss: 0.3236903250217438\n",
      "Epoch: 190500 | Loss: 0.33115002512931824 | Test loss: 0.3236764967441559\n",
      "Epoch: 190510 | Loss: 0.3311382234096527 | Test loss: 0.3236626386642456\n",
      "Epoch: 190520 | Loss: 0.33112651109695435 | Test loss: 0.3236487805843353\n",
      "Epoch: 190530 | Loss: 0.3311147391796112 | Test loss: 0.32363495230674744\n",
      "Epoch: 190540 | Loss: 0.3311029374599457 | Test loss: 0.32362112402915955\n",
      "Epoch: 190550 | Loss: 0.3310912251472473 | Test loss: 0.32360726594924927\n",
      "Epoch: 190560 | Loss: 0.3310794532299042 | Test loss: 0.3235934376716614\n",
      "Epoch: 190570 | Loss: 0.33106768131256104 | Test loss: 0.3235795795917511\n",
      "Epoch: 190580 | Loss: 0.3310559391975403 | Test loss: 0.32356569170951843\n",
      "Epoch: 190590 | Loss: 0.33104416728019714 | Test loss: 0.32355189323425293\n",
      "Epoch: 190600 | Loss: 0.331032395362854 | Test loss: 0.32353803515434265\n",
      "Epoch: 190610 | Loss: 0.33102065324783325 | Test loss: 0.32352420687675476\n",
      "Epoch: 190620 | Loss: 0.3310088813304901 | Test loss: 0.3235103189945221\n",
      "Epoch: 190630 | Loss: 0.33099713921546936 | Test loss: 0.3234964907169342\n",
      "Epoch: 190640 | Loss: 0.3309853672981262 | Test loss: 0.3234826326370239\n",
      "Epoch: 190650 | Loss: 0.3309735953807831 | Test loss: 0.32346877455711365\n",
      "Epoch: 190660 | Loss: 0.33096185326576233 | Test loss: 0.32345494627952576\n",
      "Epoch: 190670 | Loss: 0.3309501111507416 | Test loss: 0.32344111800193787\n",
      "Epoch: 190680 | Loss: 0.33093833923339844 | Test loss: 0.3234272599220276\n",
      "Epoch: 190690 | Loss: 0.3309265673160553 | Test loss: 0.3234134316444397\n",
      "Epoch: 190700 | Loss: 0.33091479539871216 | Test loss: 0.32339954376220703\n",
      "Epoch: 190710 | Loss: 0.330903023481369 | Test loss: 0.32338571548461914\n",
      "Epoch: 190720 | Loss: 0.33089128136634827 | Test loss: 0.32337188720703125\n",
      "Epoch: 190730 | Loss: 0.3308795094490051 | Test loss: 0.32335802912712097\n",
      "Epoch: 190740 | Loss: 0.3308677673339844 | Test loss: 0.3233442008495331\n",
      "Epoch: 190750 | Loss: 0.33085599541664124 | Test loss: 0.3233303725719452\n",
      "Epoch: 190760 | Loss: 0.3308442533016205 | Test loss: 0.3233164846897125\n",
      "Epoch: 190770 | Loss: 0.33083248138427734 | Test loss: 0.32330265641212463\n",
      "Epoch: 190780 | Loss: 0.3308207392692566 | Test loss: 0.32328876852989197\n",
      "Epoch: 190790 | Loss: 0.33080896735191345 | Test loss: 0.3232749402523041\n",
      "Epoch: 190800 | Loss: 0.3307971954345703 | Test loss: 0.3232611119747162\n",
      "Epoch: 190810 | Loss: 0.3307854235172272 | Test loss: 0.3232472836971283\n",
      "Epoch: 190820 | Loss: 0.3307736814022064 | Test loss: 0.323233425617218\n",
      "Epoch: 190830 | Loss: 0.3307619094848633 | Test loss: 0.32321956753730774\n",
      "Epoch: 190840 | Loss: 0.3307501971721649 | Test loss: 0.32320570945739746\n",
      "Epoch: 190850 | Loss: 0.3307383954524994 | Test loss: 0.32319188117980957\n",
      "Epoch: 190860 | Loss: 0.33072662353515625 | Test loss: 0.3231780230998993\n",
      "Epoch: 190870 | Loss: 0.3307149112224579 | Test loss: 0.3231641948223114\n",
      "Epoch: 190880 | Loss: 0.33070310950279236 | Test loss: 0.3231503367424011\n",
      "Epoch: 190890 | Loss: 0.3306913375854492 | Test loss: 0.32313647866249084\n",
      "Epoch: 190900 | Loss: 0.33067962527275085 | Test loss: 0.32312265038490295\n",
      "Epoch: 190910 | Loss: 0.3306678235530853 | Test loss: 0.3231087923049927\n",
      "Epoch: 190920 | Loss: 0.33065611124038696 | Test loss: 0.3230949342250824\n",
      "Epoch: 190930 | Loss: 0.3306443393230438 | Test loss: 0.3230811059474945\n",
      "Epoch: 190940 | Loss: 0.3306325376033783 | Test loss: 0.3230672776699066\n",
      "Epoch: 190950 | Loss: 0.33062082529067993 | Test loss: 0.32305341958999634\n",
      "Epoch: 190960 | Loss: 0.3306090533733368 | Test loss: 0.32303959131240845\n",
      "Epoch: 190970 | Loss: 0.33059728145599365 | Test loss: 0.32302573323249817\n",
      "Epoch: 190980 | Loss: 0.3305855393409729 | Test loss: 0.3230118453502655\n",
      "Epoch: 190990 | Loss: 0.33057376742362976 | Test loss: 0.322998046875\n",
      "Epoch: 191000 | Loss: 0.3305619955062866 | Test loss: 0.3229841887950897\n",
      "Epoch: 191010 | Loss: 0.33055025339126587 | Test loss: 0.32297036051750183\n",
      "Epoch: 191020 | Loss: 0.33053848147392273 | Test loss: 0.32295647263526917\n",
      "Epoch: 191030 | Loss: 0.330526739358902 | Test loss: 0.3229426443576813\n",
      "Epoch: 191040 | Loss: 0.33051496744155884 | Test loss: 0.322928786277771\n",
      "Epoch: 191050 | Loss: 0.3305031955242157 | Test loss: 0.3229149281978607\n",
      "Epoch: 191060 | Loss: 0.33049145340919495 | Test loss: 0.3229010999202728\n",
      "Epoch: 191070 | Loss: 0.3304797112941742 | Test loss: 0.32288727164268494\n",
      "Epoch: 191080 | Loss: 0.33046793937683105 | Test loss: 0.32287341356277466\n",
      "Epoch: 191090 | Loss: 0.3304561674594879 | Test loss: 0.32285958528518677\n",
      "Epoch: 191100 | Loss: 0.3304443955421448 | Test loss: 0.3228456974029541\n",
      "Epoch: 191110 | Loss: 0.33043262362480164 | Test loss: 0.3228318691253662\n",
      "Epoch: 191120 | Loss: 0.3304208815097809 | Test loss: 0.3228180408477783\n",
      "Epoch: 191130 | Loss: 0.33040910959243774 | Test loss: 0.32280418276786804\n",
      "Epoch: 191140 | Loss: 0.330397367477417 | Test loss: 0.32279035449028015\n",
      "Epoch: 191150 | Loss: 0.33038559556007385 | Test loss: 0.32277652621269226\n",
      "Epoch: 191160 | Loss: 0.3303738534450531 | Test loss: 0.3227626383304596\n",
      "Epoch: 191170 | Loss: 0.33036208152770996 | Test loss: 0.3227488100528717\n",
      "Epoch: 191180 | Loss: 0.3303503394126892 | Test loss: 0.32273492217063904\n",
      "Epoch: 191190 | Loss: 0.33033856749534607 | Test loss: 0.32272109389305115\n",
      "Epoch: 191200 | Loss: 0.33032679557800293 | Test loss: 0.32270726561546326\n",
      "Epoch: 191210 | Loss: 0.3303150236606598 | Test loss: 0.32269343733787537\n",
      "Epoch: 191220 | Loss: 0.33030328154563904 | Test loss: 0.3226795792579651\n",
      "Epoch: 191230 | Loss: 0.3302915096282959 | Test loss: 0.3226657211780548\n",
      "Epoch: 191240 | Loss: 0.33027979731559753 | Test loss: 0.32265186309814453\n",
      "Epoch: 191250 | Loss: 0.330267995595932 | Test loss: 0.32263803482055664\n",
      "Epoch: 191260 | Loss: 0.33025622367858887 | Test loss: 0.32262417674064636\n",
      "Epoch: 191270 | Loss: 0.3302445113658905 | Test loss: 0.32261034846305847\n",
      "Epoch: 191280 | Loss: 0.330232709646225 | Test loss: 0.3225964903831482\n",
      "Epoch: 191290 | Loss: 0.33022093772888184 | Test loss: 0.3225826323032379\n",
      "Epoch: 191300 | Loss: 0.33020922541618347 | Test loss: 0.32256880402565\n",
      "Epoch: 191310 | Loss: 0.33019742369651794 | Test loss: 0.32255494594573975\n",
      "Epoch: 191320 | Loss: 0.3301857113838196 | Test loss: 0.32254108786582947\n",
      "Epoch: 191330 | Loss: 0.33017393946647644 | Test loss: 0.3225272595882416\n",
      "Epoch: 191340 | Loss: 0.3301621377468109 | Test loss: 0.3225134313106537\n",
      "Epoch: 191350 | Loss: 0.33015042543411255 | Test loss: 0.3224995732307434\n",
      "Epoch: 191360 | Loss: 0.3301386535167694 | Test loss: 0.3224857449531555\n",
      "Epoch: 191370 | Loss: 0.33012688159942627 | Test loss: 0.32247188687324524\n",
      "Epoch: 191380 | Loss: 0.3301151394844055 | Test loss: 0.3224579989910126\n",
      "Epoch: 191390 | Loss: 0.3301033675670624 | Test loss: 0.32244420051574707\n",
      "Epoch: 191400 | Loss: 0.33009159564971924 | Test loss: 0.3224303424358368\n",
      "Epoch: 191410 | Loss: 0.3300798535346985 | Test loss: 0.3224165141582489\n",
      "Epoch: 191420 | Loss: 0.33006808161735535 | Test loss: 0.32240262627601624\n",
      "Epoch: 191430 | Loss: 0.3300563395023346 | Test loss: 0.32238879799842834\n",
      "Epoch: 191440 | Loss: 0.33004456758499146 | Test loss: 0.32237493991851807\n",
      "Epoch: 191450 | Loss: 0.3300327956676483 | Test loss: 0.3223610818386078\n",
      "Epoch: 191460 | Loss: 0.33002105355262756 | Test loss: 0.3223472535610199\n",
      "Epoch: 191470 | Loss: 0.3300093114376068 | Test loss: 0.322333425283432\n",
      "Epoch: 191480 | Loss: 0.32999753952026367 | Test loss: 0.32231956720352173\n",
      "Epoch: 191490 | Loss: 0.32998576760292053 | Test loss: 0.32230573892593384\n",
      "Epoch: 191500 | Loss: 0.3299739956855774 | Test loss: 0.32229185104370117\n",
      "Epoch: 191510 | Loss: 0.32996222376823425 | Test loss: 0.3222780227661133\n",
      "Epoch: 191520 | Loss: 0.3299504816532135 | Test loss: 0.3222641944885254\n",
      "Epoch: 191530 | Loss: 0.32993870973587036 | Test loss: 0.3222503364086151\n",
      "Epoch: 191540 | Loss: 0.3299269676208496 | Test loss: 0.3222365081310272\n",
      "Epoch: 191550 | Loss: 0.32991519570350647 | Test loss: 0.32222267985343933\n",
      "Epoch: 191560 | Loss: 0.3299034535884857 | Test loss: 0.32220879197120667\n",
      "Epoch: 191570 | Loss: 0.3298916816711426 | Test loss: 0.3221949636936188\n",
      "Epoch: 191580 | Loss: 0.3298799395561218 | Test loss: 0.3221810758113861\n",
      "Epoch: 191590 | Loss: 0.3298681676387787 | Test loss: 0.3221672475337982\n",
      "Epoch: 191600 | Loss: 0.32985639572143555 | Test loss: 0.3221534192562103\n",
      "Epoch: 191610 | Loss: 0.3298446238040924 | Test loss: 0.32213959097862244\n",
      "Epoch: 191620 | Loss: 0.32983288168907166 | Test loss: 0.32212573289871216\n",
      "Epoch: 191630 | Loss: 0.3298211097717285 | Test loss: 0.3221118748188019\n",
      "Epoch: 191640 | Loss: 0.32980939745903015 | Test loss: 0.3220980167388916\n",
      "Epoch: 191650 | Loss: 0.3297975957393646 | Test loss: 0.3220841884613037\n",
      "Epoch: 191660 | Loss: 0.3297858238220215 | Test loss: 0.32207033038139343\n",
      "Epoch: 191670 | Loss: 0.3297741115093231 | Test loss: 0.32205650210380554\n",
      "Epoch: 191680 | Loss: 0.3297623097896576 | Test loss: 0.32204264402389526\n",
      "Epoch: 191690 | Loss: 0.32975053787231445 | Test loss: 0.322028785943985\n",
      "Epoch: 191700 | Loss: 0.3297388255596161 | Test loss: 0.3220149576663971\n",
      "Epoch: 191710 | Loss: 0.32972702383995056 | Test loss: 0.3220010995864868\n",
      "Epoch: 191720 | Loss: 0.3297153115272522 | Test loss: 0.32198724150657654\n",
      "Epoch: 191730 | Loss: 0.32970353960990906 | Test loss: 0.32197341322898865\n",
      "Epoch: 191740 | Loss: 0.32969173789024353 | Test loss: 0.32195958495140076\n",
      "Epoch: 191750 | Loss: 0.32968002557754517 | Test loss: 0.3219457268714905\n",
      "Epoch: 191760 | Loss: 0.329668253660202 | Test loss: 0.3219318985939026\n",
      "Epoch: 191770 | Loss: 0.3296564817428589 | Test loss: 0.3219180405139923\n",
      "Epoch: 191780 | Loss: 0.32964473962783813 | Test loss: 0.32190415263175964\n",
      "Epoch: 191790 | Loss: 0.329632967710495 | Test loss: 0.32189035415649414\n",
      "Epoch: 191800 | Loss: 0.32962119579315186 | Test loss: 0.32187649607658386\n",
      "Epoch: 191810 | Loss: 0.3296094536781311 | Test loss: 0.32186266779899597\n",
      "Epoch: 191820 | Loss: 0.32959768176078796 | Test loss: 0.3218487799167633\n",
      "Epoch: 191830 | Loss: 0.3295859396457672 | Test loss: 0.3218349516391754\n",
      "Epoch: 191840 | Loss: 0.3295741677284241 | Test loss: 0.32182109355926514\n",
      "Epoch: 191850 | Loss: 0.32956239581108093 | Test loss: 0.32180723547935486\n",
      "Epoch: 191860 | Loss: 0.3295506536960602 | Test loss: 0.32179340720176697\n",
      "Epoch: 191870 | Loss: 0.32953891158103943 | Test loss: 0.3217795789241791\n",
      "Epoch: 191880 | Loss: 0.3295271396636963 | Test loss: 0.3217657208442688\n",
      "Epoch: 191890 | Loss: 0.32951536774635315 | Test loss: 0.3217518925666809\n",
      "Epoch: 191900 | Loss: 0.32950359582901 | Test loss: 0.32173800468444824\n",
      "Epoch: 191910 | Loss: 0.32949182391166687 | Test loss: 0.32172417640686035\n",
      "Epoch: 191920 | Loss: 0.3294800817966461 | Test loss: 0.32171034812927246\n",
      "Epoch: 191930 | Loss: 0.329468309879303 | Test loss: 0.3216964900493622\n",
      "Epoch: 191940 | Loss: 0.3294565677642822 | Test loss: 0.3216826617717743\n",
      "Epoch: 191950 | Loss: 0.3294447958469391 | Test loss: 0.3216688334941864\n",
      "Epoch: 191960 | Loss: 0.32943305373191833 | Test loss: 0.32165494561195374\n",
      "Epoch: 191970 | Loss: 0.3294212818145752 | Test loss: 0.32164111733436584\n",
      "Epoch: 191980 | Loss: 0.32940953969955444 | Test loss: 0.3216272294521332\n",
      "Epoch: 191990 | Loss: 0.3293977677822113 | Test loss: 0.3216134011745453\n",
      "Epoch: 192000 | Loss: 0.32938599586486816 | Test loss: 0.3215995728969574\n",
      "Epoch: 192010 | Loss: 0.329374223947525 | Test loss: 0.3215857446193695\n",
      "Epoch: 192020 | Loss: 0.3293624818325043 | Test loss: 0.32157188653945923\n",
      "Epoch: 192030 | Loss: 0.32935070991516113 | Test loss: 0.32155802845954895\n",
      "Epoch: 192040 | Loss: 0.32933899760246277 | Test loss: 0.32154417037963867\n",
      "Epoch: 192050 | Loss: 0.32932719588279724 | Test loss: 0.3215303421020508\n",
      "Epoch: 192060 | Loss: 0.3293154239654541 | Test loss: 0.3215164840221405\n",
      "Epoch: 192070 | Loss: 0.32930371165275574 | Test loss: 0.3215026557445526\n",
      "Epoch: 192080 | Loss: 0.3292919099330902 | Test loss: 0.32148879766464233\n",
      "Epoch: 192090 | Loss: 0.32928013801574707 | Test loss: 0.32147493958473206\n",
      "Epoch: 192100 | Loss: 0.3292684257030487 | Test loss: 0.32146111130714417\n",
      "Epoch: 192110 | Loss: 0.3292566239833832 | Test loss: 0.3214472532272339\n",
      "Epoch: 192120 | Loss: 0.3292449116706848 | Test loss: 0.3214333951473236\n",
      "Epoch: 192130 | Loss: 0.3292331397533417 | Test loss: 0.3214195668697357\n",
      "Epoch: 192140 | Loss: 0.32922133803367615 | Test loss: 0.3214057385921478\n",
      "Epoch: 192150 | Loss: 0.3292096257209778 | Test loss: 0.32139188051223755\n",
      "Epoch: 192160 | Loss: 0.32919785380363464 | Test loss: 0.32137805223464966\n",
      "Epoch: 192170 | Loss: 0.3291860818862915 | Test loss: 0.3213641941547394\n",
      "Epoch: 192180 | Loss: 0.32917433977127075 | Test loss: 0.3213503062725067\n",
      "Epoch: 192190 | Loss: 0.3291625678539276 | Test loss: 0.3213365077972412\n",
      "Epoch: 192200 | Loss: 0.3291507959365845 | Test loss: 0.32132264971733093\n",
      "Epoch: 192210 | Loss: 0.3291390538215637 | Test loss: 0.32130882143974304\n",
      "Epoch: 192220 | Loss: 0.3291272819042206 | Test loss: 0.3212949335575104\n",
      "Epoch: 192230 | Loss: 0.32911553978919983 | Test loss: 0.3212811052799225\n",
      "Epoch: 192240 | Loss: 0.3291037678718567 | Test loss: 0.3212672472000122\n",
      "Epoch: 192250 | Loss: 0.32909199595451355 | Test loss: 0.32125338912010193\n",
      "Epoch: 192260 | Loss: 0.3290802538394928 | Test loss: 0.32123956084251404\n",
      "Epoch: 192270 | Loss: 0.32906851172447205 | Test loss: 0.32122573256492615\n",
      "Epoch: 192280 | Loss: 0.3290567398071289 | Test loss: 0.32121187448501587\n",
      "Epoch: 192290 | Loss: 0.32904496788978577 | Test loss: 0.321198046207428\n",
      "Epoch: 192300 | Loss: 0.3290331959724426 | Test loss: 0.3211841583251953\n",
      "Epoch: 192310 | Loss: 0.3290214240550995 | Test loss: 0.3211703300476074\n",
      "Epoch: 192320 | Loss: 0.32900968194007874 | Test loss: 0.32115650177001953\n",
      "Epoch: 192330 | Loss: 0.3289979100227356 | Test loss: 0.32114264369010925\n",
      "Epoch: 192340 | Loss: 0.32898616790771484 | Test loss: 0.32112881541252136\n",
      "Epoch: 192350 | Loss: 0.3289743959903717 | Test loss: 0.32111498713493347\n",
      "Epoch: 192360 | Loss: 0.32896265387535095 | Test loss: 0.3211010992527008\n",
      "Epoch: 192370 | Loss: 0.3289508819580078 | Test loss: 0.3210872709751129\n",
      "Epoch: 192380 | Loss: 0.32893913984298706 | Test loss: 0.32107338309288025\n",
      "Epoch: 192390 | Loss: 0.3289273679256439 | Test loss: 0.32105955481529236\n",
      "Epoch: 192400 | Loss: 0.3289155960083008 | Test loss: 0.32104572653770447\n",
      "Epoch: 192410 | Loss: 0.32890382409095764 | Test loss: 0.3210318982601166\n",
      "Epoch: 192420 | Loss: 0.3288920819759369 | Test loss: 0.3210180401802063\n",
      "Epoch: 192430 | Loss: 0.32888031005859375 | Test loss: 0.321004182100296\n",
      "Epoch: 192440 | Loss: 0.3288685977458954 | Test loss: 0.32099032402038574\n",
      "Epoch: 192450 | Loss: 0.32885679602622986 | Test loss: 0.32097649574279785\n",
      "Epoch: 192460 | Loss: 0.3288450241088867 | Test loss: 0.3209626376628876\n",
      "Epoch: 192470 | Loss: 0.32883331179618835 | Test loss: 0.3209488093852997\n",
      "Epoch: 192480 | Loss: 0.3288215100765228 | Test loss: 0.3209349513053894\n",
      "Epoch: 192490 | Loss: 0.3288097381591797 | Test loss: 0.3209210932254791\n",
      "Epoch: 192500 | Loss: 0.3287980258464813 | Test loss: 0.32090726494789124\n",
      "Epoch: 192510 | Loss: 0.3287862241268158 | Test loss: 0.32089340686798096\n",
      "Epoch: 192520 | Loss: 0.32877451181411743 | Test loss: 0.3208795487880707\n",
      "Epoch: 192530 | Loss: 0.3287627398967743 | Test loss: 0.3208657205104828\n",
      "Epoch: 192540 | Loss: 0.32875093817710876 | Test loss: 0.3208518922328949\n",
      "Epoch: 192550 | Loss: 0.3287392258644104 | Test loss: 0.3208380341529846\n",
      "Epoch: 192560 | Loss: 0.32872745394706726 | Test loss: 0.32082420587539673\n",
      "Epoch: 192570 | Loss: 0.3287156820297241 | Test loss: 0.32081034779548645\n",
      "Epoch: 192580 | Loss: 0.32870393991470337 | Test loss: 0.3207964599132538\n",
      "Epoch: 192590 | Loss: 0.32869216799736023 | Test loss: 0.3207826614379883\n",
      "Epoch: 192600 | Loss: 0.3286803960800171 | Test loss: 0.320768803358078\n",
      "Epoch: 192610 | Loss: 0.32866865396499634 | Test loss: 0.3207549750804901\n",
      "Epoch: 192620 | Loss: 0.3286568820476532 | Test loss: 0.32074108719825745\n",
      "Epoch: 192630 | Loss: 0.32864513993263245 | Test loss: 0.32072725892066956\n",
      "Epoch: 192640 | Loss: 0.3286333680152893 | Test loss: 0.3207134008407593\n",
      "Epoch: 192650 | Loss: 0.32862159609794617 | Test loss: 0.320699542760849\n",
      "Epoch: 192660 | Loss: 0.3286098539829254 | Test loss: 0.3206857144832611\n",
      "Epoch: 192670 | Loss: 0.32859811186790466 | Test loss: 0.3206718862056732\n",
      "Epoch: 192680 | Loss: 0.3285863399505615 | Test loss: 0.32065802812576294\n",
      "Epoch: 192690 | Loss: 0.3285745680332184 | Test loss: 0.32064419984817505\n",
      "Epoch: 192700 | Loss: 0.32856279611587524 | Test loss: 0.3206303119659424\n",
      "Epoch: 192710 | Loss: 0.3285510241985321 | Test loss: 0.3206164836883545\n",
      "Epoch: 192720 | Loss: 0.32853928208351135 | Test loss: 0.3206026554107666\n",
      "Epoch: 192730 | Loss: 0.3285275101661682 | Test loss: 0.3205887973308563\n",
      "Epoch: 192740 | Loss: 0.32851576805114746 | Test loss: 0.32057496905326843\n",
      "Epoch: 192750 | Loss: 0.3285039961338043 | Test loss: 0.32056114077568054\n",
      "Epoch: 192760 | Loss: 0.32849225401878357 | Test loss: 0.3205472528934479\n",
      "Epoch: 192770 | Loss: 0.32848048210144043 | Test loss: 0.32053342461586\n",
      "Epoch: 192780 | Loss: 0.3284687399864197 | Test loss: 0.3205195367336273\n",
      "Epoch: 192790 | Loss: 0.32845696806907654 | Test loss: 0.32050570845603943\n",
      "Epoch: 192800 | Loss: 0.3284451961517334 | Test loss: 0.32049188017845154\n",
      "Epoch: 192810 | Loss: 0.32843342423439026 | Test loss: 0.32047805190086365\n",
      "Epoch: 192820 | Loss: 0.3284216821193695 | Test loss: 0.32046419382095337\n",
      "Epoch: 192830 | Loss: 0.32840991020202637 | Test loss: 0.3204503357410431\n",
      "Epoch: 192840 | Loss: 0.328398197889328 | Test loss: 0.3204364776611328\n",
      "Epoch: 192850 | Loss: 0.3283863961696625 | Test loss: 0.3204226493835449\n",
      "Epoch: 192860 | Loss: 0.32837462425231934 | Test loss: 0.32040879130363464\n",
      "Epoch: 192870 | Loss: 0.32836291193962097 | Test loss: 0.32039496302604675\n",
      "Epoch: 192880 | Loss: 0.32835111021995544 | Test loss: 0.3203811049461365\n",
      "Epoch: 192890 | Loss: 0.3283393383026123 | Test loss: 0.3203672468662262\n",
      "Epoch: 192900 | Loss: 0.32832762598991394 | Test loss: 0.3203534185886383\n",
      "Epoch: 192910 | Loss: 0.3283158242702484 | Test loss: 0.320339560508728\n",
      "Epoch: 192920 | Loss: 0.32830411195755005 | Test loss: 0.32032570242881775\n",
      "Epoch: 192930 | Loss: 0.3282923400402069 | Test loss: 0.32031187415122986\n",
      "Epoch: 192940 | Loss: 0.3282805383205414 | Test loss: 0.32029804587364197\n",
      "Epoch: 192950 | Loss: 0.328268826007843 | Test loss: 0.3202841877937317\n",
      "Epoch: 192960 | Loss: 0.3282570540904999 | Test loss: 0.3202703595161438\n",
      "Epoch: 192970 | Loss: 0.32824528217315674 | Test loss: 0.3202565014362335\n",
      "Epoch: 192980 | Loss: 0.328233540058136 | Test loss: 0.32024261355400085\n",
      "Epoch: 192990 | Loss: 0.32822176814079285 | Test loss: 0.32022881507873535\n",
      "Epoch: 193000 | Loss: 0.3282099962234497 | Test loss: 0.3202149569988251\n",
      "Epoch: 193010 | Loss: 0.32819825410842896 | Test loss: 0.3202011287212372\n",
      "Epoch: 193020 | Loss: 0.3281864821910858 | Test loss: 0.3201872408390045\n",
      "Epoch: 193030 | Loss: 0.32817474007606506 | Test loss: 0.3201734125614166\n",
      "Epoch: 193040 | Loss: 0.3281629681587219 | Test loss: 0.32015955448150635\n",
      "Epoch: 193050 | Loss: 0.3281511962413788 | Test loss: 0.32014569640159607\n",
      "Epoch: 193060 | Loss: 0.32813945412635803 | Test loss: 0.3201318681240082\n",
      "Epoch: 193070 | Loss: 0.3281277120113373 | Test loss: 0.3201180398464203\n",
      "Epoch: 193080 | Loss: 0.32811594009399414 | Test loss: 0.32010418176651\n",
      "Epoch: 193090 | Loss: 0.328104168176651 | Test loss: 0.3200903534889221\n",
      "Epoch: 193100 | Loss: 0.32809239625930786 | Test loss: 0.32007646560668945\n",
      "Epoch: 193110 | Loss: 0.3280806243419647 | Test loss: 0.32006263732910156\n",
      "Epoch: 193120 | Loss: 0.32806888222694397 | Test loss: 0.32004880905151367\n",
      "Epoch: 193130 | Loss: 0.32805711030960083 | Test loss: 0.3200349509716034\n",
      "Epoch: 193140 | Loss: 0.3280453681945801 | Test loss: 0.3200211226940155\n",
      "Epoch: 193150 | Loss: 0.32803359627723694 | Test loss: 0.3200072944164276\n",
      "Epoch: 193160 | Loss: 0.3280218541622162 | Test loss: 0.31999340653419495\n",
      "Epoch: 193170 | Loss: 0.32801008224487305 | Test loss: 0.31997957825660706\n",
      "Epoch: 193180 | Loss: 0.3279983401298523 | Test loss: 0.3199656903743744\n",
      "Epoch: 193190 | Loss: 0.32798656821250916 | Test loss: 0.3199518620967865\n",
      "Epoch: 193200 | Loss: 0.327974796295166 | Test loss: 0.3199380338191986\n",
      "Epoch: 193210 | Loss: 0.3279630243778229 | Test loss: 0.3199242055416107\n",
      "Epoch: 193220 | Loss: 0.3279512822628021 | Test loss: 0.31991034746170044\n",
      "Epoch: 193230 | Loss: 0.327939510345459 | Test loss: 0.31989648938179016\n",
      "Epoch: 193240 | Loss: 0.3279277980327606 | Test loss: 0.3198826313018799\n",
      "Epoch: 193250 | Loss: 0.3279159963130951 | Test loss: 0.319868803024292\n",
      "Epoch: 193260 | Loss: 0.32790422439575195 | Test loss: 0.3198549449443817\n",
      "Epoch: 193270 | Loss: 0.3278925120830536 | Test loss: 0.3198411166667938\n",
      "Epoch: 193280 | Loss: 0.32788071036338806 | Test loss: 0.31982725858688354\n",
      "Epoch: 193290 | Loss: 0.3278689384460449 | Test loss: 0.31981340050697327\n",
      "Epoch: 193300 | Loss: 0.32785722613334656 | Test loss: 0.3197995722293854\n",
      "Epoch: 193310 | Loss: 0.32784542441368103 | Test loss: 0.3197857141494751\n",
      "Epoch: 193320 | Loss: 0.32783371210098267 | Test loss: 0.3197718560695648\n",
      "Epoch: 193330 | Loss: 0.3278219401836395 | Test loss: 0.31975802779197693\n",
      "Epoch: 193340 | Loss: 0.327810138463974 | Test loss: 0.31974419951438904\n",
      "Epoch: 193350 | Loss: 0.32779842615127563 | Test loss: 0.31973034143447876\n",
      "Epoch: 193360 | Loss: 0.3277866542339325 | Test loss: 0.31971651315689087\n",
      "Epoch: 193370 | Loss: 0.32777488231658936 | Test loss: 0.3197026550769806\n",
      "Epoch: 193380 | Loss: 0.3277631402015686 | Test loss: 0.3196887671947479\n",
      "Epoch: 193390 | Loss: 0.32775136828422546 | Test loss: 0.3196749687194824\n",
      "Epoch: 193400 | Loss: 0.3277395963668823 | Test loss: 0.31966111063957214\n",
      "Epoch: 193410 | Loss: 0.3277278542518616 | Test loss: 0.31964728236198425\n",
      "Epoch: 193420 | Loss: 0.32771608233451843 | Test loss: 0.3196333944797516\n",
      "Epoch: 193430 | Loss: 0.3277043402194977 | Test loss: 0.3196195662021637\n",
      "Epoch: 193440 | Loss: 0.32769256830215454 | Test loss: 0.3196057081222534\n",
      "Epoch: 193450 | Loss: 0.3276807963848114 | Test loss: 0.31959185004234314\n",
      "Epoch: 193460 | Loss: 0.32766905426979065 | Test loss: 0.31957802176475525\n",
      "Epoch: 193470 | Loss: 0.3276573121547699 | Test loss: 0.31956419348716736\n",
      "Epoch: 193480 | Loss: 0.32764554023742676 | Test loss: 0.3195503354072571\n",
      "Epoch: 193490 | Loss: 0.3276337683200836 | Test loss: 0.3195365071296692\n",
      "Epoch: 193500 | Loss: 0.3276219964027405 | Test loss: 0.3195226192474365\n",
      "Epoch: 193510 | Loss: 0.32761022448539734 | Test loss: 0.31950879096984863\n",
      "Epoch: 193520 | Loss: 0.3275984823703766 | Test loss: 0.31949496269226074\n",
      "Epoch: 193530 | Loss: 0.32758671045303345 | Test loss: 0.31948110461235046\n",
      "Epoch: 193540 | Loss: 0.3275749683380127 | Test loss: 0.3194672763347626\n",
      "Epoch: 193550 | Loss: 0.32756319642066956 | Test loss: 0.3194534480571747\n",
      "Epoch: 193560 | Loss: 0.3275514543056488 | Test loss: 0.319439560174942\n",
      "Epoch: 193570 | Loss: 0.32753968238830566 | Test loss: 0.3194257318973541\n",
      "Epoch: 193580 | Loss: 0.3275279402732849 | Test loss: 0.31941184401512146\n",
      "Epoch: 193590 | Loss: 0.3275161683559418 | Test loss: 0.31939801573753357\n",
      "Epoch: 193600 | Loss: 0.32750439643859863 | Test loss: 0.3193841874599457\n",
      "Epoch: 193610 | Loss: 0.3274926245212555 | Test loss: 0.3193703591823578\n",
      "Epoch: 193620 | Loss: 0.32748088240623474 | Test loss: 0.3193565011024475\n",
      "Epoch: 193630 | Loss: 0.3274691104888916 | Test loss: 0.31934264302253723\n",
      "Epoch: 193640 | Loss: 0.32745739817619324 | Test loss: 0.31932878494262695\n",
      "Epoch: 193650 | Loss: 0.3274455964565277 | Test loss: 0.31931495666503906\n",
      "Epoch: 193660 | Loss: 0.32743382453918457 | Test loss: 0.3193010985851288\n",
      "Epoch: 193670 | Loss: 0.3274221122264862 | Test loss: 0.3192872703075409\n",
      "Epoch: 193680 | Loss: 0.3274103105068207 | Test loss: 0.3192734122276306\n",
      "Epoch: 193690 | Loss: 0.32739853858947754 | Test loss: 0.31925955414772034\n",
      "Epoch: 193700 | Loss: 0.3273868262767792 | Test loss: 0.31924572587013245\n",
      "Epoch: 193710 | Loss: 0.32737502455711365 | Test loss: 0.31923186779022217\n",
      "Epoch: 193720 | Loss: 0.3273633122444153 | Test loss: 0.3192180097103119\n",
      "Epoch: 193730 | Loss: 0.32735154032707214 | Test loss: 0.319204181432724\n",
      "Epoch: 193740 | Loss: 0.3273397386074066 | Test loss: 0.3191903531551361\n",
      "Epoch: 193750 | Loss: 0.32732802629470825 | Test loss: 0.31917649507522583\n",
      "Epoch: 193760 | Loss: 0.3273162543773651 | Test loss: 0.31916266679763794\n",
      "Epoch: 193770 | Loss: 0.327304482460022 | Test loss: 0.31914880871772766\n",
      "Epoch: 193780 | Loss: 0.3272927403450012 | Test loss: 0.319134920835495\n",
      "Epoch: 193790 | Loss: 0.3272809684276581 | Test loss: 0.3191211223602295\n",
      "Epoch: 193800 | Loss: 0.32726919651031494 | Test loss: 0.3191072642803192\n",
      "Epoch: 193810 | Loss: 0.3272574543952942 | Test loss: 0.3190934360027313\n",
      "Epoch: 193820 | Loss: 0.32724568247795105 | Test loss: 0.31907954812049866\n",
      "Epoch: 193830 | Loss: 0.3272339403629303 | Test loss: 0.31906571984291077\n",
      "Epoch: 193840 | Loss: 0.32722216844558716 | Test loss: 0.3190518617630005\n",
      "Epoch: 193850 | Loss: 0.327210396528244 | Test loss: 0.3190380036830902\n",
      "Epoch: 193860 | Loss: 0.32719865441322327 | Test loss: 0.3190241754055023\n",
      "Epoch: 193870 | Loss: 0.3271869122982025 | Test loss: 0.31901034712791443\n",
      "Epoch: 193880 | Loss: 0.3271751403808594 | Test loss: 0.31899648904800415\n",
      "Epoch: 193890 | Loss: 0.32716336846351624 | Test loss: 0.31898266077041626\n",
      "Epoch: 193900 | Loss: 0.3271515965461731 | Test loss: 0.3189687728881836\n",
      "Epoch: 193910 | Loss: 0.32713982462882996 | Test loss: 0.3189549446105957\n",
      "Epoch: 193920 | Loss: 0.3271280825138092 | Test loss: 0.3189411163330078\n",
      "Epoch: 193930 | Loss: 0.32711631059646606 | Test loss: 0.31892725825309753\n",
      "Epoch: 193940 | Loss: 0.3271045684814453 | Test loss: 0.31891342997550964\n",
      "Epoch: 193950 | Loss: 0.3270927965641022 | Test loss: 0.31889960169792175\n",
      "Epoch: 193960 | Loss: 0.3270810544490814 | Test loss: 0.3188857138156891\n",
      "Epoch: 193970 | Loss: 0.3270692825317383 | Test loss: 0.3188718855381012\n",
      "Epoch: 193980 | Loss: 0.32705754041671753 | Test loss: 0.31885799765586853\n",
      "Epoch: 193990 | Loss: 0.3270457684993744 | Test loss: 0.31884416937828064\n",
      "Epoch: 194000 | Loss: 0.32703399658203125 | Test loss: 0.31883034110069275\n",
      "Epoch: 194010 | Loss: 0.3270222246646881 | Test loss: 0.31881651282310486\n",
      "Epoch: 194020 | Loss: 0.32701048254966736 | Test loss: 0.3188026547431946\n",
      "Epoch: 194030 | Loss: 0.3269987106323242 | Test loss: 0.3187887966632843\n",
      "Epoch: 194040 | Loss: 0.32698699831962585 | Test loss: 0.318774938583374\n",
      "Epoch: 194050 | Loss: 0.3269751965999603 | Test loss: 0.31876111030578613\n",
      "Epoch: 194060 | Loss: 0.3269634246826172 | Test loss: 0.31874725222587585\n",
      "Epoch: 194070 | Loss: 0.3269517123699188 | Test loss: 0.31873342394828796\n",
      "Epoch: 194080 | Loss: 0.3269399106502533 | Test loss: 0.3187195658683777\n",
      "Epoch: 194090 | Loss: 0.32692813873291016 | Test loss: 0.3187057077884674\n",
      "Epoch: 194100 | Loss: 0.3269164264202118 | Test loss: 0.3186918795108795\n",
      "Epoch: 194110 | Loss: 0.32690462470054626 | Test loss: 0.31867802143096924\n",
      "Epoch: 194120 | Loss: 0.3268929123878479 | Test loss: 0.31866416335105896\n",
      "Epoch: 194130 | Loss: 0.32688114047050476 | Test loss: 0.31865033507347107\n",
      "Epoch: 194140 | Loss: 0.32686933875083923 | Test loss: 0.3186365067958832\n",
      "Epoch: 194150 | Loss: 0.32685762643814087 | Test loss: 0.3186226487159729\n",
      "Epoch: 194160 | Loss: 0.32684585452079773 | Test loss: 0.318608820438385\n",
      "Epoch: 194170 | Loss: 0.3268340826034546 | Test loss: 0.31859496235847473\n",
      "Epoch: 194180 | Loss: 0.32682234048843384 | Test loss: 0.31858107447624207\n",
      "Epoch: 194190 | Loss: 0.3268105685710907 | Test loss: 0.31856727600097656\n",
      "Epoch: 194200 | Loss: 0.32679879665374756 | Test loss: 0.3185534179210663\n",
      "Epoch: 194210 | Loss: 0.3267870545387268 | Test loss: 0.3185395896434784\n",
      "Epoch: 194220 | Loss: 0.32677528262138367 | Test loss: 0.3185257017612457\n",
      "Epoch: 194230 | Loss: 0.3267635405063629 | Test loss: 0.31851187348365784\n",
      "Epoch: 194240 | Loss: 0.3267517685890198 | Test loss: 0.31849801540374756\n",
      "Epoch: 194250 | Loss: 0.32673999667167664 | Test loss: 0.3184841573238373\n",
      "Epoch: 194260 | Loss: 0.3267282545566559 | Test loss: 0.3184703290462494\n",
      "Epoch: 194270 | Loss: 0.32671651244163513 | Test loss: 0.3184565007686615\n",
      "Epoch: 194280 | Loss: 0.326704740524292 | Test loss: 0.3184426426887512\n",
      "Epoch: 194290 | Loss: 0.32669296860694885 | Test loss: 0.31842881441116333\n",
      "Epoch: 194300 | Loss: 0.3266811966896057 | Test loss: 0.31841492652893066\n",
      "Epoch: 194310 | Loss: 0.3266694247722626 | Test loss: 0.3184010982513428\n",
      "Epoch: 194320 | Loss: 0.3266576826572418 | Test loss: 0.3183872699737549\n",
      "Epoch: 194330 | Loss: 0.3266459107398987 | Test loss: 0.3183734118938446\n",
      "Epoch: 194340 | Loss: 0.32663416862487793 | Test loss: 0.3183595836162567\n",
      "Epoch: 194350 | Loss: 0.3266223967075348 | Test loss: 0.3183457553386688\n",
      "Epoch: 194360 | Loss: 0.32661065459251404 | Test loss: 0.31833186745643616\n",
      "Epoch: 194370 | Loss: 0.3265988826751709 | Test loss: 0.31831803917884827\n",
      "Epoch: 194380 | Loss: 0.32658714056015015 | Test loss: 0.3183041512966156\n",
      "Epoch: 194390 | Loss: 0.326575368642807 | Test loss: 0.3182903230190277\n",
      "Epoch: 194400 | Loss: 0.32656359672546387 | Test loss: 0.3182764947414398\n",
      "Epoch: 194410 | Loss: 0.3265518248081207 | Test loss: 0.31826266646385193\n",
      "Epoch: 194420 | Loss: 0.3265400826931 | Test loss: 0.31824880838394165\n",
      "Epoch: 194430 | Loss: 0.32652831077575684 | Test loss: 0.31823495030403137\n",
      "Epoch: 194440 | Loss: 0.32651659846305847 | Test loss: 0.3182210922241211\n",
      "Epoch: 194450 | Loss: 0.32650479674339294 | Test loss: 0.3182072639465332\n",
      "Epoch: 194460 | Loss: 0.3264930248260498 | Test loss: 0.3181934058666229\n",
      "Epoch: 194470 | Loss: 0.32648131251335144 | Test loss: 0.31817957758903503\n",
      "Epoch: 194480 | Loss: 0.3264695107936859 | Test loss: 0.31816571950912476\n",
      "Epoch: 194490 | Loss: 0.3264577388763428 | Test loss: 0.3181518614292145\n",
      "Epoch: 194500 | Loss: 0.3264460265636444 | Test loss: 0.3181380331516266\n",
      "Epoch: 194510 | Loss: 0.3264342248439789 | Test loss: 0.3181241750717163\n",
      "Epoch: 194520 | Loss: 0.3264225125312805 | Test loss: 0.31811031699180603\n",
      "Epoch: 194530 | Loss: 0.3264107406139374 | Test loss: 0.31809648871421814\n",
      "Epoch: 194540 | Loss: 0.32639893889427185 | Test loss: 0.31808266043663025\n",
      "Epoch: 194550 | Loss: 0.3263872265815735 | Test loss: 0.31806880235671997\n",
      "Epoch: 194560 | Loss: 0.32637545466423035 | Test loss: 0.3180549740791321\n",
      "Epoch: 194570 | Loss: 0.3263636827468872 | Test loss: 0.3180411159992218\n",
      "Epoch: 194580 | Loss: 0.32635194063186646 | Test loss: 0.31802722811698914\n",
      "Epoch: 194590 | Loss: 0.3263401687145233 | Test loss: 0.31801342964172363\n",
      "Epoch: 194600 | Loss: 0.3263283967971802 | Test loss: 0.31799957156181335\n",
      "Epoch: 194610 | Loss: 0.3263166546821594 | Test loss: 0.31798574328422546\n",
      "Epoch: 194620 | Loss: 0.3263048827648163 | Test loss: 0.3179718554019928\n",
      "Epoch: 194630 | Loss: 0.32629314064979553 | Test loss: 0.3179580271244049\n",
      "Epoch: 194640 | Loss: 0.3262813687324524 | Test loss: 0.31794416904449463\n",
      "Epoch: 194650 | Loss: 0.32626959681510925 | Test loss: 0.31793031096458435\n",
      "Epoch: 194660 | Loss: 0.3262578547000885 | Test loss: 0.31791648268699646\n",
      "Epoch: 194670 | Loss: 0.32624611258506775 | Test loss: 0.31790265440940857\n",
      "Epoch: 194680 | Loss: 0.3262343406677246 | Test loss: 0.3178887963294983\n",
      "Epoch: 194690 | Loss: 0.32622256875038147 | Test loss: 0.3178749680519104\n",
      "Epoch: 194700 | Loss: 0.32621079683303833 | Test loss: 0.31786108016967773\n",
      "Epoch: 194710 | Loss: 0.3261990249156952 | Test loss: 0.31784725189208984\n",
      "Epoch: 194720 | Loss: 0.32618728280067444 | Test loss: 0.31783342361450195\n",
      "Epoch: 194730 | Loss: 0.3261755108833313 | Test loss: 0.3178195655345917\n",
      "Epoch: 194740 | Loss: 0.32616376876831055 | Test loss: 0.3178057372570038\n",
      "Epoch: 194750 | Loss: 0.3261519968509674 | Test loss: 0.3177919089794159\n",
      "Epoch: 194760 | Loss: 0.32614025473594666 | Test loss: 0.3177780210971832\n",
      "Epoch: 194770 | Loss: 0.3261284828186035 | Test loss: 0.31776419281959534\n",
      "Epoch: 194780 | Loss: 0.32611674070358276 | Test loss: 0.31775030493736267\n",
      "Epoch: 194790 | Loss: 0.3261049687862396 | Test loss: 0.3177364766597748\n",
      "Epoch: 194800 | Loss: 0.3260931968688965 | Test loss: 0.3177226483821869\n",
      "Epoch: 194810 | Loss: 0.32608142495155334 | Test loss: 0.317708820104599\n",
      "Epoch: 194820 | Loss: 0.3260696828365326 | Test loss: 0.3176949620246887\n",
      "Epoch: 194830 | Loss: 0.32605791091918945 | Test loss: 0.31768110394477844\n",
      "Epoch: 194840 | Loss: 0.3260461986064911 | Test loss: 0.31766724586486816\n",
      "Epoch: 194850 | Loss: 0.32603439688682556 | Test loss: 0.3176534175872803\n",
      "Epoch: 194860 | Loss: 0.3260226249694824 | Test loss: 0.31763955950737\n",
      "Epoch: 194870 | Loss: 0.32601091265678406 | Test loss: 0.3176257312297821\n",
      "Epoch: 194880 | Loss: 0.32599911093711853 | Test loss: 0.3176118731498718\n",
      "Epoch: 194890 | Loss: 0.3259873390197754 | Test loss: 0.31759801506996155\n",
      "Epoch: 194900 | Loss: 0.325975626707077 | Test loss: 0.31758418679237366\n",
      "Epoch: 194910 | Loss: 0.3259638249874115 | Test loss: 0.3175703287124634\n",
      "Epoch: 194920 | Loss: 0.32595211267471313 | Test loss: 0.3175564706325531\n",
      "Epoch: 194930 | Loss: 0.32594034075737 | Test loss: 0.3175426423549652\n",
      "Epoch: 194940 | Loss: 0.32592853903770447 | Test loss: 0.3175288140773773\n",
      "Epoch: 194950 | Loss: 0.3259168267250061 | Test loss: 0.31751495599746704\n",
      "Epoch: 194960 | Loss: 0.32590505480766296 | Test loss: 0.31750112771987915\n",
      "Epoch: 194970 | Loss: 0.3258932828903198 | Test loss: 0.31748726963996887\n",
      "Epoch: 194980 | Loss: 0.3258815407752991 | Test loss: 0.3174733817577362\n",
      "Epoch: 194990 | Loss: 0.32586976885795593 | Test loss: 0.3174595832824707\n",
      "Epoch: 195000 | Loss: 0.3258579969406128 | Test loss: 0.3174457252025604\n",
      "Epoch: 195010 | Loss: 0.32584625482559204 | Test loss: 0.31743189692497253\n",
      "Epoch: 195020 | Loss: 0.3258344829082489 | Test loss: 0.31741800904273987\n",
      "Epoch: 195030 | Loss: 0.32582274079322815 | Test loss: 0.317404180765152\n",
      "Epoch: 195040 | Loss: 0.325810968875885 | Test loss: 0.3173903226852417\n",
      "Epoch: 195050 | Loss: 0.32579919695854187 | Test loss: 0.3173764646053314\n",
      "Epoch: 195060 | Loss: 0.3257874548435211 | Test loss: 0.31736263632774353\n",
      "Epoch: 195070 | Loss: 0.32577571272850037 | Test loss: 0.31734880805015564\n",
      "Epoch: 195080 | Loss: 0.3257639408111572 | Test loss: 0.31733494997024536\n",
      "Epoch: 195090 | Loss: 0.3257521688938141 | Test loss: 0.31732112169265747\n",
      "Epoch: 195100 | Loss: 0.32574039697647095 | Test loss: 0.3173072338104248\n",
      "Epoch: 195110 | Loss: 0.3257286250591278 | Test loss: 0.3172934055328369\n",
      "Epoch: 195120 | Loss: 0.32571688294410706 | Test loss: 0.317279577255249\n",
      "Epoch: 195130 | Loss: 0.3257051110267639 | Test loss: 0.31726571917533875\n",
      "Epoch: 195140 | Loss: 0.32569336891174316 | Test loss: 0.31725189089775085\n",
      "Epoch: 195150 | Loss: 0.3256815969944 | Test loss: 0.31723806262016296\n",
      "Epoch: 195160 | Loss: 0.3256698548793793 | Test loss: 0.3172241747379303\n",
      "Epoch: 195170 | Loss: 0.32565808296203613 | Test loss: 0.3172103464603424\n",
      "Epoch: 195180 | Loss: 0.3256463408470154 | Test loss: 0.31719645857810974\n",
      "Epoch: 195190 | Loss: 0.32563456892967224 | Test loss: 0.31718263030052185\n",
      "Epoch: 195200 | Loss: 0.3256227970123291 | Test loss: 0.31716880202293396\n",
      "Epoch: 195210 | Loss: 0.32561102509498596 | Test loss: 0.31715497374534607\n",
      "Epoch: 195220 | Loss: 0.3255992829799652 | Test loss: 0.3171411156654358\n",
      "Epoch: 195230 | Loss: 0.32558751106262207 | Test loss: 0.3171272575855255\n",
      "Epoch: 195240 | Loss: 0.3255757987499237 | Test loss: 0.31711339950561523\n",
      "Epoch: 195250 | Loss: 0.3255639970302582 | Test loss: 0.31709957122802734\n",
      "Epoch: 195260 | Loss: 0.32555222511291504 | Test loss: 0.31708571314811707\n",
      "Epoch: 195270 | Loss: 0.3255405128002167 | Test loss: 0.3170718848705292\n",
      "Epoch: 195280 | Loss: 0.32552871108055115 | Test loss: 0.3170580267906189\n",
      "Epoch: 195290 | Loss: 0.325516939163208 | Test loss: 0.3170441687107086\n",
      "Epoch: 195300 | Loss: 0.32550522685050964 | Test loss: 0.3170303404331207\n",
      "Epoch: 195310 | Loss: 0.3254934251308441 | Test loss: 0.31701648235321045\n",
      "Epoch: 195320 | Loss: 0.32548171281814575 | Test loss: 0.31700262427330017\n",
      "Epoch: 195330 | Loss: 0.3254699409008026 | Test loss: 0.3169887959957123\n",
      "Epoch: 195340 | Loss: 0.3254581391811371 | Test loss: 0.3169749677181244\n",
      "Epoch: 195350 | Loss: 0.3254464268684387 | Test loss: 0.3169611096382141\n",
      "Epoch: 195360 | Loss: 0.3254346549510956 | Test loss: 0.3169472813606262\n",
      "Epoch: 195370 | Loss: 0.32542288303375244 | Test loss: 0.31693342328071594\n",
      "Epoch: 195380 | Loss: 0.3254111409187317 | Test loss: 0.3169195353984833\n",
      "Epoch: 195390 | Loss: 0.32539936900138855 | Test loss: 0.3169057369232178\n",
      "Epoch: 195400 | Loss: 0.3253875970840454 | Test loss: 0.3168918788433075\n",
      "Epoch: 195410 | Loss: 0.32537585496902466 | Test loss: 0.3168780505657196\n",
      "Epoch: 195420 | Loss: 0.3253640830516815 | Test loss: 0.31686416268348694\n",
      "Epoch: 195430 | Loss: 0.32535234093666077 | Test loss: 0.31685033440589905\n",
      "Epoch: 195440 | Loss: 0.3253405690193176 | Test loss: 0.31683647632598877\n",
      "Epoch: 195450 | Loss: 0.3253287971019745 | Test loss: 0.3168226182460785\n",
      "Epoch: 195460 | Loss: 0.32531705498695374 | Test loss: 0.3168087899684906\n",
      "Epoch: 195470 | Loss: 0.325305312871933 | Test loss: 0.3167949616909027\n",
      "Epoch: 195480 | Loss: 0.32529354095458984 | Test loss: 0.31678110361099243\n",
      "Epoch: 195490 | Loss: 0.3252817690372467 | Test loss: 0.31676727533340454\n",
      "Epoch: 195500 | Loss: 0.32526999711990356 | Test loss: 0.3167533874511719\n",
      "Epoch: 195510 | Loss: 0.3252582252025604 | Test loss: 0.316739559173584\n",
      "Epoch: 195520 | Loss: 0.3252464830875397 | Test loss: 0.3167257308959961\n",
      "Epoch: 195530 | Loss: 0.32523471117019653 | Test loss: 0.3167118728160858\n",
      "Epoch: 195540 | Loss: 0.3252229690551758 | Test loss: 0.3166980445384979\n",
      "Epoch: 195550 | Loss: 0.32521119713783264 | Test loss: 0.31668421626091003\n",
      "Epoch: 195560 | Loss: 0.3251994550228119 | Test loss: 0.31667032837867737\n",
      "Epoch: 195570 | Loss: 0.32518768310546875 | Test loss: 0.3166565001010895\n",
      "Epoch: 195580 | Loss: 0.325175940990448 | Test loss: 0.3166426122188568\n",
      "Epoch: 195590 | Loss: 0.32516416907310486 | Test loss: 0.3166287839412689\n",
      "Epoch: 195600 | Loss: 0.3251523971557617 | Test loss: 0.31661495566368103\n",
      "Epoch: 195610 | Loss: 0.3251406252384186 | Test loss: 0.31660112738609314\n",
      "Epoch: 195620 | Loss: 0.3251288831233978 | Test loss: 0.31658726930618286\n",
      "Epoch: 195630 | Loss: 0.3251171112060547 | Test loss: 0.3165734112262726\n",
      "Epoch: 195640 | Loss: 0.3251053988933563 | Test loss: 0.3165595531463623\n",
      "Epoch: 195650 | Loss: 0.3250935971736908 | Test loss: 0.3165457248687744\n",
      "Epoch: 195660 | Loss: 0.32508182525634766 | Test loss: 0.31653186678886414\n",
      "Epoch: 195670 | Loss: 0.3250701129436493 | Test loss: 0.31651803851127625\n",
      "Epoch: 195680 | Loss: 0.32505831122398376 | Test loss: 0.31650418043136597\n",
      "Epoch: 195690 | Loss: 0.3250465393066406 | Test loss: 0.3164903223514557\n",
      "Epoch: 195700 | Loss: 0.32503482699394226 | Test loss: 0.3164764940738678\n",
      "Epoch: 195710 | Loss: 0.32502302527427673 | Test loss: 0.3164626359939575\n",
      "Epoch: 195720 | Loss: 0.32501131296157837 | Test loss: 0.31644877791404724\n",
      "Epoch: 195730 | Loss: 0.32499954104423523 | Test loss: 0.31643494963645935\n",
      "Epoch: 195740 | Loss: 0.3249877393245697 | Test loss: 0.31642112135887146\n",
      "Epoch: 195750 | Loss: 0.32497602701187134 | Test loss: 0.3164072632789612\n",
      "Epoch: 195760 | Loss: 0.3249642550945282 | Test loss: 0.3163934350013733\n",
      "Epoch: 195770 | Loss: 0.32495248317718506 | Test loss: 0.316379576921463\n",
      "Epoch: 195780 | Loss: 0.3249407410621643 | Test loss: 0.31636568903923035\n",
      "Epoch: 195790 | Loss: 0.32492896914482117 | Test loss: 0.31635189056396484\n",
      "Epoch: 195800 | Loss: 0.324917197227478 | Test loss: 0.31633803248405457\n",
      "Epoch: 195810 | Loss: 0.3249054551124573 | Test loss: 0.3163242042064667\n",
      "Epoch: 195820 | Loss: 0.32489368319511414 | Test loss: 0.316310316324234\n",
      "Epoch: 195830 | Loss: 0.3248819410800934 | Test loss: 0.3162964880466461\n",
      "Epoch: 195840 | Loss: 0.32487016916275024 | Test loss: 0.31628262996673584\n",
      "Epoch: 195850 | Loss: 0.3248583972454071 | Test loss: 0.31626877188682556\n",
      "Epoch: 195860 | Loss: 0.32484665513038635 | Test loss: 0.31625494360923767\n",
      "Epoch: 195870 | Loss: 0.3248349130153656 | Test loss: 0.3162411153316498\n",
      "Epoch: 195880 | Loss: 0.32482314109802246 | Test loss: 0.3162272572517395\n",
      "Epoch: 195890 | Loss: 0.3248113691806793 | Test loss: 0.3162134289741516\n",
      "Epoch: 195900 | Loss: 0.3247995972633362 | Test loss: 0.31619954109191895\n",
      "Epoch: 195910 | Loss: 0.32478782534599304 | Test loss: 0.31618571281433105\n",
      "Epoch: 195920 | Loss: 0.3247760832309723 | Test loss: 0.31617188453674316\n",
      "Epoch: 195930 | Loss: 0.32476431131362915 | Test loss: 0.3161580264568329\n",
      "Epoch: 195940 | Loss: 0.3247525691986084 | Test loss: 0.316144198179245\n",
      "Epoch: 195950 | Loss: 0.32474079728126526 | Test loss: 0.3161303699016571\n",
      "Epoch: 195960 | Loss: 0.3247290551662445 | Test loss: 0.31611648201942444\n",
      "Epoch: 195970 | Loss: 0.32471728324890137 | Test loss: 0.31610265374183655\n",
      "Epoch: 195980 | Loss: 0.3247055411338806 | Test loss: 0.3160887658596039\n",
      "Epoch: 195990 | Loss: 0.3246937692165375 | Test loss: 0.316074937582016\n",
      "Epoch: 196000 | Loss: 0.32468199729919434 | Test loss: 0.3160611093044281\n",
      "Epoch: 196010 | Loss: 0.3246702253818512 | Test loss: 0.3160472810268402\n",
      "Epoch: 196020 | Loss: 0.32465848326683044 | Test loss: 0.31603342294692993\n",
      "Epoch: 196030 | Loss: 0.3246467113494873 | Test loss: 0.31601956486701965\n",
      "Epoch: 196040 | Loss: 0.32463499903678894 | Test loss: 0.3160057067871094\n",
      "Epoch: 196050 | Loss: 0.3246231973171234 | Test loss: 0.3159918785095215\n",
      "Epoch: 196060 | Loss: 0.3246114253997803 | Test loss: 0.3159780204296112\n",
      "Epoch: 196070 | Loss: 0.3245997130870819 | Test loss: 0.3159641921520233\n",
      "Epoch: 196080 | Loss: 0.3245879113674164 | Test loss: 0.31595033407211304\n",
      "Epoch: 196090 | Loss: 0.32457613945007324 | Test loss: 0.31593647599220276\n",
      "Epoch: 196100 | Loss: 0.3245644271373749 | Test loss: 0.31592264771461487\n",
      "Epoch: 196110 | Loss: 0.32455262541770935 | Test loss: 0.3159087896347046\n",
      "Epoch: 196120 | Loss: 0.324540913105011 | Test loss: 0.3158949315547943\n",
      "Epoch: 196130 | Loss: 0.32452914118766785 | Test loss: 0.3158811032772064\n",
      "Epoch: 196140 | Loss: 0.3245173394680023 | Test loss: 0.31586727499961853\n",
      "Epoch: 196150 | Loss: 0.32450562715530396 | Test loss: 0.31585341691970825\n",
      "Epoch: 196160 | Loss: 0.3244938552379608 | Test loss: 0.31583958864212036\n",
      "Epoch: 196170 | Loss: 0.3244820833206177 | Test loss: 0.3158257305622101\n",
      "Epoch: 196180 | Loss: 0.3244703412055969 | Test loss: 0.3158118426799774\n",
      "Epoch: 196190 | Loss: 0.3244585692882538 | Test loss: 0.3157980442047119\n",
      "Epoch: 196200 | Loss: 0.32444679737091064 | Test loss: 0.31578418612480164\n",
      "Epoch: 196210 | Loss: 0.3244350552558899 | Test loss: 0.31577035784721375\n",
      "Epoch: 196220 | Loss: 0.32442328333854675 | Test loss: 0.3157564699649811\n",
      "Epoch: 196230 | Loss: 0.324411541223526 | Test loss: 0.3157426416873932\n",
      "Epoch: 196240 | Loss: 0.32439976930618286 | Test loss: 0.3157287836074829\n",
      "Epoch: 196250 | Loss: 0.3243879973888397 | Test loss: 0.31571492552757263\n",
      "Epoch: 196260 | Loss: 0.32437625527381897 | Test loss: 0.31570109724998474\n",
      "Epoch: 196270 | Loss: 0.3243645131587982 | Test loss: 0.31568726897239685\n",
      "Epoch: 196280 | Loss: 0.3243527412414551 | Test loss: 0.3156734108924866\n",
      "Epoch: 196290 | Loss: 0.32434096932411194 | Test loss: 0.3156595826148987\n",
      "Epoch: 196300 | Loss: 0.3243291974067688 | Test loss: 0.315645694732666\n",
      "Epoch: 196310 | Loss: 0.32431745529174805 | Test loss: 0.3156318664550781\n",
      "Epoch: 196320 | Loss: 0.3243056833744049 | Test loss: 0.31561803817749023\n",
      "Epoch: 196330 | Loss: 0.32429391145706177 | Test loss: 0.31560418009757996\n",
      "Epoch: 196340 | Loss: 0.324282169342041 | Test loss: 0.31559035181999207\n",
      "Epoch: 196350 | Loss: 0.3242703974246979 | Test loss: 0.3155765235424042\n",
      "Epoch: 196360 | Loss: 0.3242586553096771 | Test loss: 0.3155626356601715\n",
      "Epoch: 196370 | Loss: 0.324246883392334 | Test loss: 0.3155488073825836\n",
      "Epoch: 196380 | Loss: 0.32423514127731323 | Test loss: 0.31553491950035095\n",
      "Epoch: 196390 | Loss: 0.3242233693599701 | Test loss: 0.31552109122276306\n",
      "Epoch: 196400 | Loss: 0.32421159744262695 | Test loss: 0.31550726294517517\n",
      "Epoch: 196410 | Loss: 0.3241998553276062 | Test loss: 0.3154934346675873\n",
      "Epoch: 196420 | Loss: 0.32418808341026306 | Test loss: 0.315479576587677\n",
      "Epoch: 196430 | Loss: 0.3241763114929199 | Test loss: 0.3154657185077667\n",
      "Epoch: 196440 | Loss: 0.32416459918022156 | Test loss: 0.31545186042785645\n",
      "Epoch: 196450 | Loss: 0.32415279746055603 | Test loss: 0.31543803215026855\n",
      "Epoch: 196460 | Loss: 0.3241410255432129 | Test loss: 0.3154241740703583\n",
      "Epoch: 196470 | Loss: 0.3241293132305145 | Test loss: 0.3154103457927704\n",
      "Epoch: 196480 | Loss: 0.324117511510849 | Test loss: 0.3153964877128601\n",
      "Epoch: 196490 | Loss: 0.32410573959350586 | Test loss: 0.31538262963294983\n",
      "Epoch: 196500 | Loss: 0.3240940272808075 | Test loss: 0.31536880135536194\n",
      "Epoch: 196510 | Loss: 0.32408222556114197 | Test loss: 0.31535494327545166\n",
      "Epoch: 196520 | Loss: 0.3240705132484436 | Test loss: 0.3153410851955414\n",
      "Epoch: 196530 | Loss: 0.32405874133110046 | Test loss: 0.3153272569179535\n",
      "Epoch: 196540 | Loss: 0.32404693961143494 | Test loss: 0.3153134286403656\n",
      "Epoch: 196550 | Loss: 0.3240352272987366 | Test loss: 0.3152995705604553\n",
      "Epoch: 196560 | Loss: 0.32402345538139343 | Test loss: 0.31528574228286743\n",
      "Epoch: 196570 | Loss: 0.3240116834640503 | Test loss: 0.31527188420295715\n",
      "Epoch: 196580 | Loss: 0.32399994134902954 | Test loss: 0.3152579963207245\n",
      "Epoch: 196590 | Loss: 0.3239881694316864 | Test loss: 0.315244197845459\n",
      "Epoch: 196600 | Loss: 0.32397639751434326 | Test loss: 0.3152303397655487\n",
      "Epoch: 196610 | Loss: 0.3239646553993225 | Test loss: 0.3152165114879608\n",
      "Epoch: 196620 | Loss: 0.32395288348197937 | Test loss: 0.31520262360572815\n",
      "Epoch: 196630 | Loss: 0.3239411413669586 | Test loss: 0.31518879532814026\n",
      "Epoch: 196640 | Loss: 0.3239293694496155 | Test loss: 0.31517493724823\n",
      "Epoch: 196650 | Loss: 0.32391759753227234 | Test loss: 0.3151610791683197\n",
      "Epoch: 196660 | Loss: 0.3239058554172516 | Test loss: 0.3151472508907318\n",
      "Epoch: 196670 | Loss: 0.32389411330223083 | Test loss: 0.3151334226131439\n",
      "Epoch: 196680 | Loss: 0.3238823413848877 | Test loss: 0.31511956453323364\n",
      "Epoch: 196690 | Loss: 0.32387056946754456 | Test loss: 0.31510573625564575\n",
      "Epoch: 196700 | Loss: 0.3238587975502014 | Test loss: 0.3150918483734131\n",
      "Epoch: 196710 | Loss: 0.32384705543518066 | Test loss: 0.3150780200958252\n",
      "Epoch: 196720 | Loss: 0.3238352835178375 | Test loss: 0.3150641918182373\n",
      "Epoch: 196730 | Loss: 0.3238235116004944 | Test loss: 0.315050333738327\n",
      "Epoch: 196740 | Loss: 0.32381176948547363 | Test loss: 0.31503650546073914\n",
      "Epoch: 196750 | Loss: 0.3237999975681305 | Test loss: 0.31502267718315125\n",
      "Epoch: 196760 | Loss: 0.32378825545310974 | Test loss: 0.3150087893009186\n",
      "Epoch: 196770 | Loss: 0.3237764835357666 | Test loss: 0.3149949610233307\n",
      "Epoch: 196780 | Loss: 0.32376474142074585 | Test loss: 0.314981073141098\n",
      "Epoch: 196790 | Loss: 0.3237529695034027 | Test loss: 0.31496724486351013\n",
      "Epoch: 196800 | Loss: 0.32374119758605957 | Test loss: 0.31495341658592224\n",
      "Epoch: 196810 | Loss: 0.3237294554710388 | Test loss: 0.31493958830833435\n",
      "Epoch: 196820 | Loss: 0.3237176835536957 | Test loss: 0.3149257302284241\n",
      "Epoch: 196830 | Loss: 0.32370591163635254 | Test loss: 0.3149118721485138\n",
      "Epoch: 196840 | Loss: 0.3236941993236542 | Test loss: 0.3148980140686035\n",
      "Epoch: 196850 | Loss: 0.32368239760398865 | Test loss: 0.3148841857910156\n",
      "Epoch: 196860 | Loss: 0.3236706256866455 | Test loss: 0.31487032771110535\n",
      "Epoch: 196870 | Loss: 0.32365891337394714 | Test loss: 0.31485649943351746\n",
      "Epoch: 196880 | Loss: 0.3236471116542816 | Test loss: 0.3148426413536072\n",
      "Epoch: 196890 | Loss: 0.3236353397369385 | Test loss: 0.3148287832736969\n",
      "Epoch: 196900 | Loss: 0.3236236274242401 | Test loss: 0.314814954996109\n",
      "Epoch: 196910 | Loss: 0.3236118257045746 | Test loss: 0.31480109691619873\n",
      "Epoch: 196920 | Loss: 0.3236001133918762 | Test loss: 0.31478723883628845\n",
      "Epoch: 196930 | Loss: 0.3235883414745331 | Test loss: 0.31477341055870056\n",
      "Epoch: 196940 | Loss: 0.32357653975486755 | Test loss: 0.31475958228111267\n",
      "Epoch: 196950 | Loss: 0.3235648274421692 | Test loss: 0.3147457242012024\n",
      "Epoch: 196960 | Loss: 0.32355305552482605 | Test loss: 0.3147318959236145\n",
      "Epoch: 196970 | Loss: 0.3235412836074829 | Test loss: 0.3147180378437042\n",
      "Epoch: 196980 | Loss: 0.32352954149246216 | Test loss: 0.31470414996147156\n",
      "Epoch: 196990 | Loss: 0.323517769575119 | Test loss: 0.31469035148620605\n",
      "Epoch: 197000 | Loss: 0.3235059976577759 | Test loss: 0.3146764934062958\n",
      "Epoch: 197010 | Loss: 0.3234942555427551 | Test loss: 0.3146626651287079\n",
      "Epoch: 197020 | Loss: 0.323482483625412 | Test loss: 0.3146487772464752\n",
      "Epoch: 197030 | Loss: 0.32347074151039124 | Test loss: 0.31463494896888733\n",
      "Epoch: 197040 | Loss: 0.3234589695930481 | Test loss: 0.31462109088897705\n",
      "Epoch: 197050 | Loss: 0.32344719767570496 | Test loss: 0.3146072328090668\n",
      "Epoch: 197060 | Loss: 0.3234354555606842 | Test loss: 0.3145934045314789\n",
      "Epoch: 197070 | Loss: 0.32342371344566345 | Test loss: 0.314579576253891\n",
      "Epoch: 197080 | Loss: 0.3234119415283203 | Test loss: 0.3145657181739807\n",
      "Epoch: 197090 | Loss: 0.3234001696109772 | Test loss: 0.3145518898963928\n",
      "Epoch: 197100 | Loss: 0.32338839769363403 | Test loss: 0.31453800201416016\n",
      "Epoch: 197110 | Loss: 0.3233766555786133 | Test loss: 0.31452417373657227\n",
      "Epoch: 197120 | Loss: 0.32336488366127014 | Test loss: 0.3145103454589844\n",
      "Epoch: 197130 | Loss: 0.323353111743927 | Test loss: 0.3144964873790741\n",
      "Epoch: 197140 | Loss: 0.32334136962890625 | Test loss: 0.3144826591014862\n",
      "Epoch: 197150 | Loss: 0.3233295977115631 | Test loss: 0.3144688308238983\n",
      "Epoch: 197160 | Loss: 0.32331785559654236 | Test loss: 0.31445494294166565\n",
      "Epoch: 197170 | Loss: 0.3233060836791992 | Test loss: 0.31444111466407776\n",
      "Epoch: 197180 | Loss: 0.32329434156417847 | Test loss: 0.3144272267818451\n",
      "Epoch: 197190 | Loss: 0.3232825696468353 | Test loss: 0.3144133985042572\n",
      "Epoch: 197200 | Loss: 0.3232707977294922 | Test loss: 0.3143995702266693\n",
      "Epoch: 197210 | Loss: 0.32325905561447144 | Test loss: 0.3143857419490814\n",
      "Epoch: 197220 | Loss: 0.3232472836971283 | Test loss: 0.31437188386917114\n",
      "Epoch: 197230 | Loss: 0.32323551177978516 | Test loss: 0.31435802578926086\n",
      "Epoch: 197240 | Loss: 0.3232237994670868 | Test loss: 0.3143441677093506\n",
      "Epoch: 197250 | Loss: 0.32321199774742126 | Test loss: 0.3143303394317627\n",
      "Epoch: 197260 | Loss: 0.3232002258300781 | Test loss: 0.3143164813518524\n",
      "Epoch: 197270 | Loss: 0.32318851351737976 | Test loss: 0.3143026530742645\n",
      "Epoch: 197280 | Loss: 0.32317671179771423 | Test loss: 0.31428879499435425\n",
      "Epoch: 197290 | Loss: 0.3231649398803711 | Test loss: 0.31427493691444397\n",
      "Epoch: 197300 | Loss: 0.32315322756767273 | Test loss: 0.3142611086368561\n",
      "Epoch: 197310 | Loss: 0.3231414258480072 | Test loss: 0.3142472505569458\n",
      "Epoch: 197320 | Loss: 0.32312971353530884 | Test loss: 0.3142333924770355\n",
      "Epoch: 197330 | Loss: 0.3231179416179657 | Test loss: 0.31421956419944763\n",
      "Epoch: 197340 | Loss: 0.32310613989830017 | Test loss: 0.31420573592185974\n",
      "Epoch: 197350 | Loss: 0.3230944275856018 | Test loss: 0.31419187784194946\n",
      "Epoch: 197360 | Loss: 0.32308265566825867 | Test loss: 0.3141780495643616\n",
      "Epoch: 197370 | Loss: 0.3230708837509155 | Test loss: 0.3141641914844513\n",
      "Epoch: 197380 | Loss: 0.3230591416358948 | Test loss: 0.31415030360221863\n",
      "Epoch: 197390 | Loss: 0.32304736971855164 | Test loss: 0.3141365051269531\n",
      "Epoch: 197400 | Loss: 0.3230355978012085 | Test loss: 0.31412264704704285\n",
      "Epoch: 197410 | Loss: 0.32302385568618774 | Test loss: 0.31410881876945496\n",
      "Epoch: 197420 | Loss: 0.3230120837688446 | Test loss: 0.3140949308872223\n",
      "Epoch: 197430 | Loss: 0.32300034165382385 | Test loss: 0.3140811026096344\n",
      "Epoch: 197440 | Loss: 0.3229885697364807 | Test loss: 0.3140672445297241\n",
      "Epoch: 197450 | Loss: 0.3229767978191376 | Test loss: 0.31405338644981384\n",
      "Epoch: 197460 | Loss: 0.3229650557041168 | Test loss: 0.31403955817222595\n",
      "Epoch: 197470 | Loss: 0.32295331358909607 | Test loss: 0.31402572989463806\n",
      "Epoch: 197480 | Loss: 0.32294154167175293 | Test loss: 0.3140118718147278\n",
      "Epoch: 197490 | Loss: 0.3229297697544098 | Test loss: 0.3139980435371399\n",
      "Epoch: 197500 | Loss: 0.32291799783706665 | Test loss: 0.3139841556549072\n",
      "Epoch: 197510 | Loss: 0.3229062557220459 | Test loss: 0.31397032737731934\n",
      "Epoch: 197520 | Loss: 0.32289448380470276 | Test loss: 0.31395649909973145\n",
      "Epoch: 197530 | Loss: 0.3228827118873596 | Test loss: 0.31394264101982117\n",
      "Epoch: 197540 | Loss: 0.32287096977233887 | Test loss: 0.3139288127422333\n",
      "Epoch: 197550 | Loss: 0.3228591978549957 | Test loss: 0.3139149844646454\n",
      "Epoch: 197560 | Loss: 0.322847455739975 | Test loss: 0.3139010965824127\n",
      "Epoch: 197570 | Loss: 0.32283568382263184 | Test loss: 0.31388726830482483\n",
      "Epoch: 197580 | Loss: 0.3228239417076111 | Test loss: 0.31387338042259216\n",
      "Epoch: 197590 | Loss: 0.32281216979026794 | Test loss: 0.3138595521450043\n",
      "Epoch: 197600 | Loss: 0.3228003978729248 | Test loss: 0.3138457238674164\n",
      "Epoch: 197610 | Loss: 0.32278865575790405 | Test loss: 0.3138318955898285\n",
      "Epoch: 197620 | Loss: 0.3227768838405609 | Test loss: 0.3138180375099182\n",
      "Epoch: 197630 | Loss: 0.3227651119232178 | Test loss: 0.31380417943000793\n",
      "Epoch: 197640 | Loss: 0.3227533996105194 | Test loss: 0.31379032135009766\n",
      "Epoch: 197650 | Loss: 0.3227415978908539 | Test loss: 0.31377649307250977\n",
      "Epoch: 197660 | Loss: 0.32272982597351074 | Test loss: 0.3137626349925995\n",
      "Epoch: 197670 | Loss: 0.3227181136608124 | Test loss: 0.3137488067150116\n",
      "Epoch: 197680 | Loss: 0.32270631194114685 | Test loss: 0.3137349486351013\n",
      "Epoch: 197690 | Loss: 0.3226945400238037 | Test loss: 0.31372109055519104\n",
      "Epoch: 197700 | Loss: 0.32268282771110535 | Test loss: 0.31370726227760315\n",
      "Epoch: 197710 | Loss: 0.3226710259914398 | Test loss: 0.31369340419769287\n",
      "Epoch: 197720 | Loss: 0.32265931367874146 | Test loss: 0.3136795461177826\n",
      "Epoch: 197730 | Loss: 0.3226475417613983 | Test loss: 0.3136657178401947\n",
      "Epoch: 197740 | Loss: 0.3226357400417328 | Test loss: 0.3136518895626068\n",
      "Epoch: 197750 | Loss: 0.3226240277290344 | Test loss: 0.31363803148269653\n",
      "Epoch: 197760 | Loss: 0.3226122558116913 | Test loss: 0.31362420320510864\n",
      "Epoch: 197770 | Loss: 0.32260048389434814 | Test loss: 0.31361034512519836\n",
      "Epoch: 197780 | Loss: 0.3225887417793274 | Test loss: 0.3135964572429657\n",
      "Epoch: 197790 | Loss: 0.32257696986198425 | Test loss: 0.3135826587677002\n",
      "Epoch: 197800 | Loss: 0.3225651979446411 | Test loss: 0.3135688006877899\n",
      "Epoch: 197810 | Loss: 0.32255345582962036 | Test loss: 0.313554972410202\n",
      "Epoch: 197820 | Loss: 0.3225416839122772 | Test loss: 0.31354108452796936\n",
      "Epoch: 197830 | Loss: 0.32252994179725647 | Test loss: 0.31352725625038147\n",
      "Epoch: 197840 | Loss: 0.32251816987991333 | Test loss: 0.3135133981704712\n",
      "Epoch: 197850 | Loss: 0.3225063979625702 | Test loss: 0.3134995400905609\n",
      "Epoch: 197860 | Loss: 0.32249465584754944 | Test loss: 0.313485711812973\n",
      "Epoch: 197870 | Loss: 0.3224829137325287 | Test loss: 0.31347188353538513\n",
      "Epoch: 197880 | Loss: 0.32247114181518555 | Test loss: 0.31345802545547485\n",
      "Epoch: 197890 | Loss: 0.3224593698978424 | Test loss: 0.31344419717788696\n",
      "Epoch: 197900 | Loss: 0.32244759798049927 | Test loss: 0.3134303092956543\n",
      "Epoch: 197910 | Loss: 0.3224358558654785 | Test loss: 0.3134164810180664\n",
      "Epoch: 197920 | Loss: 0.3224240839481354 | Test loss: 0.3134026527404785\n",
      "Epoch: 197930 | Loss: 0.32241231203079224 | Test loss: 0.31338879466056824\n",
      "Epoch: 197940 | Loss: 0.3224005699157715 | Test loss: 0.31337496638298035\n",
      "Epoch: 197950 | Loss: 0.32238879799842834 | Test loss: 0.31336113810539246\n",
      "Epoch: 197960 | Loss: 0.3223770558834076 | Test loss: 0.3133472502231598\n",
      "Epoch: 197970 | Loss: 0.32236528396606445 | Test loss: 0.3133334219455719\n",
      "Epoch: 197980 | Loss: 0.3223535418510437 | Test loss: 0.31331953406333923\n",
      "Epoch: 197990 | Loss: 0.32234176993370056 | Test loss: 0.31330570578575134\n",
      "Epoch: 198000 | Loss: 0.3223299980163574 | Test loss: 0.31329187750816345\n",
      "Epoch: 198010 | Loss: 0.32231825590133667 | Test loss: 0.31327804923057556\n",
      "Epoch: 198020 | Loss: 0.32230648398399353 | Test loss: 0.3132641911506653\n",
      "Epoch: 198030 | Loss: 0.3222947120666504 | Test loss: 0.313250333070755\n",
      "Epoch: 198040 | Loss: 0.322282999753952 | Test loss: 0.3132364749908447\n",
      "Epoch: 198050 | Loss: 0.3222711980342865 | Test loss: 0.31322264671325684\n",
      "Epoch: 198060 | Loss: 0.32225942611694336 | Test loss: 0.31320878863334656\n",
      "Epoch: 198070 | Loss: 0.322247713804245 | Test loss: 0.31319496035575867\n",
      "Epoch: 198080 | Loss: 0.32223591208457947 | Test loss: 0.3131811022758484\n",
      "Epoch: 198090 | Loss: 0.32222414016723633 | Test loss: 0.3131672441959381\n",
      "Epoch: 198100 | Loss: 0.32221242785453796 | Test loss: 0.3131534159183502\n",
      "Epoch: 198110 | Loss: 0.32220062613487244 | Test loss: 0.31313955783843994\n",
      "Epoch: 198120 | Loss: 0.3221889138221741 | Test loss: 0.31312569975852966\n",
      "Epoch: 198130 | Loss: 0.32217714190483093 | Test loss: 0.3131118714809418\n",
      "Epoch: 198140 | Loss: 0.3221653401851654 | Test loss: 0.3130980432033539\n",
      "Epoch: 198150 | Loss: 0.32215362787246704 | Test loss: 0.3130841851234436\n",
      "Epoch: 198160 | Loss: 0.3221418559551239 | Test loss: 0.3130703568458557\n",
      "Epoch: 198170 | Loss: 0.32213008403778076 | Test loss: 0.31305649876594543\n",
      "Epoch: 198180 | Loss: 0.32211834192276 | Test loss: 0.31304261088371277\n",
      "Epoch: 198190 | Loss: 0.32210657000541687 | Test loss: 0.31302881240844727\n",
      "Epoch: 198200 | Loss: 0.32209479808807373 | Test loss: 0.313014954328537\n",
      "Epoch: 198210 | Loss: 0.322083055973053 | Test loss: 0.3130011260509491\n",
      "Epoch: 198220 | Loss: 0.32207128405570984 | Test loss: 0.31298723816871643\n",
      "Epoch: 198230 | Loss: 0.3220595419406891 | Test loss: 0.31297340989112854\n",
      "Epoch: 198240 | Loss: 0.32204777002334595 | Test loss: 0.31295955181121826\n",
      "Epoch: 198250 | Loss: 0.3220359981060028 | Test loss: 0.312945693731308\n",
      "Epoch: 198260 | Loss: 0.32202425599098206 | Test loss: 0.3129318654537201\n",
      "Epoch: 198270 | Loss: 0.3220125138759613 | Test loss: 0.3129180371761322\n",
      "Epoch: 198280 | Loss: 0.32200074195861816 | Test loss: 0.3129041790962219\n",
      "Epoch: 198290 | Loss: 0.321988970041275 | Test loss: 0.31289035081863403\n",
      "Epoch: 198300 | Loss: 0.3219771981239319 | Test loss: 0.31287646293640137\n",
      "Epoch: 198310 | Loss: 0.32196545600891113 | Test loss: 0.3128626346588135\n",
      "Epoch: 198320 | Loss: 0.321953684091568 | Test loss: 0.3128488063812256\n",
      "Epoch: 198330 | Loss: 0.32194191217422485 | Test loss: 0.3128349483013153\n",
      "Epoch: 198340 | Loss: 0.3219301700592041 | Test loss: 0.3128211200237274\n",
      "Epoch: 198350 | Loss: 0.32191839814186096 | Test loss: 0.3128072917461395\n",
      "Epoch: 198360 | Loss: 0.3219066560268402 | Test loss: 0.31279340386390686\n",
      "Epoch: 198370 | Loss: 0.32189488410949707 | Test loss: 0.31277957558631897\n",
      "Epoch: 198380 | Loss: 0.3218831419944763 | Test loss: 0.3127656877040863\n",
      "Epoch: 198390 | Loss: 0.3218713700771332 | Test loss: 0.3127518594264984\n",
      "Epoch: 198400 | Loss: 0.32185959815979004 | Test loss: 0.3127380311489105\n",
      "Epoch: 198410 | Loss: 0.3218478560447693 | Test loss: 0.31272420287132263\n",
      "Epoch: 198420 | Loss: 0.32183608412742615 | Test loss: 0.31271034479141235\n",
      "Epoch: 198430 | Loss: 0.321824312210083 | Test loss: 0.3126964867115021\n",
      "Epoch: 198440 | Loss: 0.32181259989738464 | Test loss: 0.3126826286315918\n",
      "Epoch: 198450 | Loss: 0.3218007981777191 | Test loss: 0.3126688003540039\n",
      "Epoch: 198460 | Loss: 0.321789026260376 | Test loss: 0.31265494227409363\n",
      "Epoch: 198470 | Loss: 0.3217773139476776 | Test loss: 0.31264111399650574\n",
      "Epoch: 198480 | Loss: 0.3217655122280121 | Test loss: 0.31262725591659546\n",
      "Epoch: 198490 | Loss: 0.32175374031066895 | Test loss: 0.3126133978366852\n",
      "Epoch: 198500 | Loss: 0.3217420279979706 | Test loss: 0.3125995695590973\n",
      "Epoch: 198510 | Loss: 0.32173022627830505 | Test loss: 0.312585711479187\n",
      "Epoch: 198520 | Loss: 0.3217185139656067 | Test loss: 0.31257185339927673\n",
      "Epoch: 198530 | Loss: 0.32170674204826355 | Test loss: 0.31255802512168884\n",
      "Epoch: 198540 | Loss: 0.321694940328598 | Test loss: 0.31254419684410095\n",
      "Epoch: 198550 | Loss: 0.32168322801589966 | Test loss: 0.3125303387641907\n",
      "Epoch: 198560 | Loss: 0.3216714560985565 | Test loss: 0.3125165104866028\n",
      "Epoch: 198570 | Loss: 0.3216596841812134 | Test loss: 0.3125026524066925\n",
      "Epoch: 198580 | Loss: 0.3216479420661926 | Test loss: 0.31248876452445984\n",
      "Epoch: 198590 | Loss: 0.3216361701488495 | Test loss: 0.31247496604919434\n",
      "Epoch: 198600 | Loss: 0.32162439823150635 | Test loss: 0.31246110796928406\n",
      "Epoch: 198610 | Loss: 0.3216126561164856 | Test loss: 0.31244727969169617\n",
      "Epoch: 198620 | Loss: 0.32160088419914246 | Test loss: 0.3124333918094635\n",
      "Epoch: 198630 | Loss: 0.3215891420841217 | Test loss: 0.3124195635318756\n",
      "Epoch: 198640 | Loss: 0.32157737016677856 | Test loss: 0.31240570545196533\n",
      "Epoch: 198650 | Loss: 0.3215655982494354 | Test loss: 0.31239184737205505\n",
      "Epoch: 198660 | Loss: 0.3215538561344147 | Test loss: 0.31237801909446716\n",
      "Epoch: 198670 | Loss: 0.3215421140193939 | Test loss: 0.3123641908168793\n",
      "Epoch: 198680 | Loss: 0.3215303421020508 | Test loss: 0.312350332736969\n",
      "Epoch: 198690 | Loss: 0.32151857018470764 | Test loss: 0.3123365044593811\n",
      "Epoch: 198700 | Loss: 0.3215067982673645 | Test loss: 0.31232261657714844\n",
      "Epoch: 198710 | Loss: 0.32149505615234375 | Test loss: 0.31230878829956055\n",
      "Epoch: 198720 | Loss: 0.3214832842350006 | Test loss: 0.31229496002197266\n",
      "Epoch: 198730 | Loss: 0.32147151231765747 | Test loss: 0.3122811019420624\n",
      "Epoch: 198740 | Loss: 0.3214597702026367 | Test loss: 0.3122672736644745\n",
      "Epoch: 198750 | Loss: 0.3214479982852936 | Test loss: 0.3122534453868866\n",
      "Epoch: 198760 | Loss: 0.3214362561702728 | Test loss: 0.31223955750465393\n",
      "Epoch: 198770 | Loss: 0.3214244842529297 | Test loss: 0.31222572922706604\n",
      "Epoch: 198780 | Loss: 0.32141274213790894 | Test loss: 0.3122118413448334\n",
      "Epoch: 198790 | Loss: 0.3214009702205658 | Test loss: 0.3121980130672455\n",
      "Epoch: 198800 | Loss: 0.32138919830322266 | Test loss: 0.3121841847896576\n",
      "Epoch: 198810 | Loss: 0.3213774561882019 | Test loss: 0.3121703565120697\n",
      "Epoch: 198820 | Loss: 0.32136568427085876 | Test loss: 0.3121564984321594\n",
      "Epoch: 198830 | Loss: 0.3213539123535156 | Test loss: 0.31214264035224915\n",
      "Epoch: 198840 | Loss: 0.32134220004081726 | Test loss: 0.31212878227233887\n",
      "Epoch: 198850 | Loss: 0.32133039832115173 | Test loss: 0.312114953994751\n",
      "Epoch: 198860 | Loss: 0.3213186264038086 | Test loss: 0.3121010959148407\n",
      "Epoch: 198870 | Loss: 0.32130691409111023 | Test loss: 0.3120872676372528\n",
      "Epoch: 198880 | Loss: 0.3212951123714447 | Test loss: 0.31207340955734253\n",
      "Epoch: 198890 | Loss: 0.32128334045410156 | Test loss: 0.31205955147743225\n",
      "Epoch: 198900 | Loss: 0.3212716281414032 | Test loss: 0.31204572319984436\n",
      "Epoch: 198910 | Loss: 0.32125982642173767 | Test loss: 0.3120318651199341\n",
      "Epoch: 198920 | Loss: 0.3212481141090393 | Test loss: 0.3120180070400238\n",
      "Epoch: 198930 | Loss: 0.32123634219169617 | Test loss: 0.3120041787624359\n",
      "Epoch: 198940 | Loss: 0.32122454047203064 | Test loss: 0.311990350484848\n",
      "Epoch: 198950 | Loss: 0.3212128281593323 | Test loss: 0.31197649240493774\n",
      "Epoch: 198960 | Loss: 0.32120105624198914 | Test loss: 0.31196266412734985\n",
      "Epoch: 198970 | Loss: 0.321189284324646 | Test loss: 0.3119488060474396\n",
      "Epoch: 198980 | Loss: 0.32117754220962524 | Test loss: 0.3119349181652069\n",
      "Epoch: 198990 | Loss: 0.3211657702922821 | Test loss: 0.3119211196899414\n",
      "Epoch: 199000 | Loss: 0.32115399837493896 | Test loss: 0.31190726161003113\n",
      "Epoch: 199010 | Loss: 0.3211422562599182 | Test loss: 0.31189343333244324\n",
      "Epoch: 199020 | Loss: 0.3211304843425751 | Test loss: 0.31187954545021057\n",
      "Epoch: 199030 | Loss: 0.3211187422275543 | Test loss: 0.3118657171726227\n",
      "Epoch: 199040 | Loss: 0.3211069703102112 | Test loss: 0.3118518590927124\n",
      "Epoch: 199050 | Loss: 0.32109519839286804 | Test loss: 0.3118380010128021\n",
      "Epoch: 199060 | Loss: 0.3210834562778473 | Test loss: 0.31182417273521423\n",
      "Epoch: 199070 | Loss: 0.32107171416282654 | Test loss: 0.31181034445762634\n",
      "Epoch: 199080 | Loss: 0.3210599422454834 | Test loss: 0.31179648637771606\n",
      "Epoch: 199090 | Loss: 0.32104817032814026 | Test loss: 0.3117826581001282\n",
      "Epoch: 199100 | Loss: 0.3210363984107971 | Test loss: 0.3117687702178955\n",
      "Epoch: 199110 | Loss: 0.32102465629577637 | Test loss: 0.3117549419403076\n",
      "Epoch: 199120 | Loss: 0.3210128843784332 | Test loss: 0.3117411136627197\n",
      "Epoch: 199130 | Loss: 0.3210011124610901 | Test loss: 0.31172725558280945\n",
      "Epoch: 199140 | Loss: 0.32098937034606934 | Test loss: 0.31171342730522156\n",
      "Epoch: 199150 | Loss: 0.3209775984287262 | Test loss: 0.31169959902763367\n",
      "Epoch: 199160 | Loss: 0.32096585631370544 | Test loss: 0.311685711145401\n",
      "Epoch: 199170 | Loss: 0.3209540843963623 | Test loss: 0.3116718828678131\n",
      "Epoch: 199180 | Loss: 0.32094234228134155 | Test loss: 0.31165799498558044\n",
      "Epoch: 199190 | Loss: 0.3209305703639984 | Test loss: 0.31164416670799255\n",
      "Epoch: 199200 | Loss: 0.3209187984466553 | Test loss: 0.31163033843040466\n",
      "Epoch: 199210 | Loss: 0.3209070563316345 | Test loss: 0.3116165101528168\n",
      "Epoch: 199220 | Loss: 0.3208952844142914 | Test loss: 0.3116026520729065\n",
      "Epoch: 199230 | Loss: 0.32088351249694824 | Test loss: 0.3115887939929962\n",
      "Epoch: 199240 | Loss: 0.3208718001842499 | Test loss: 0.31157493591308594\n",
      "Epoch: 199250 | Loss: 0.32085999846458435 | Test loss: 0.31156110763549805\n",
      "Epoch: 199260 | Loss: 0.3208482265472412 | Test loss: 0.31154724955558777\n",
      "Epoch: 199270 | Loss: 0.32083651423454285 | Test loss: 0.3115334212779999\n",
      "Epoch: 199280 | Loss: 0.3208247125148773 | Test loss: 0.3115195631980896\n",
      "Epoch: 199290 | Loss: 0.3208129405975342 | Test loss: 0.3115057051181793\n",
      "Epoch: 199300 | Loss: 0.3208012282848358 | Test loss: 0.31149187684059143\n",
      "Epoch: 199310 | Loss: 0.3207894265651703 | Test loss: 0.31147801876068115\n",
      "Epoch: 199320 | Loss: 0.3207777142524719 | Test loss: 0.3114641606807709\n",
      "Epoch: 199330 | Loss: 0.3207659423351288 | Test loss: 0.311450332403183\n",
      "Epoch: 199340 | Loss: 0.32075414061546326 | Test loss: 0.3114365041255951\n",
      "Epoch: 199350 | Loss: 0.3207424283027649 | Test loss: 0.3114226460456848\n",
      "Epoch: 199360 | Loss: 0.32073065638542175 | Test loss: 0.3114088177680969\n",
      "Epoch: 199370 | Loss: 0.3207188844680786 | Test loss: 0.31139495968818665\n",
      "Epoch: 199380 | Loss: 0.32070714235305786 | Test loss: 0.311381071805954\n",
      "Epoch: 199390 | Loss: 0.3206953704357147 | Test loss: 0.3113672733306885\n",
      "Epoch: 199400 | Loss: 0.3206835985183716 | Test loss: 0.3113534152507782\n",
      "Epoch: 199410 | Loss: 0.32067185640335083 | Test loss: 0.3113395869731903\n",
      "Epoch: 199420 | Loss: 0.3206600844860077 | Test loss: 0.31132569909095764\n",
      "Epoch: 199430 | Loss: 0.32064834237098694 | Test loss: 0.31131187081336975\n",
      "Epoch: 199440 | Loss: 0.3206365704536438 | Test loss: 0.3112980127334595\n",
      "Epoch: 199450 | Loss: 0.32062479853630066 | Test loss: 0.3112841546535492\n",
      "Epoch: 199460 | Loss: 0.3206130564212799 | Test loss: 0.3112703263759613\n",
      "Epoch: 199470 | Loss: 0.32060131430625916 | Test loss: 0.3112564980983734\n",
      "Epoch: 199480 | Loss: 0.320589542388916 | Test loss: 0.31124264001846313\n",
      "Epoch: 199490 | Loss: 0.3205777704715729 | Test loss: 0.31122881174087524\n",
      "Epoch: 199500 | Loss: 0.32056599855422974 | Test loss: 0.3112149238586426\n",
      "Epoch: 199510 | Loss: 0.320554256439209 | Test loss: 0.3112010955810547\n",
      "Epoch: 199520 | Loss: 0.32054248452186584 | Test loss: 0.3111872673034668\n",
      "Epoch: 199530 | Loss: 0.3205307126045227 | Test loss: 0.3111734092235565\n",
      "Epoch: 199540 | Loss: 0.32051897048950195 | Test loss: 0.31115958094596863\n",
      "Epoch: 199550 | Loss: 0.3205071985721588 | Test loss: 0.31114575266838074\n",
      "Epoch: 199560 | Loss: 0.32049545645713806 | Test loss: 0.31113186478614807\n",
      "Epoch: 199570 | Loss: 0.3204836845397949 | Test loss: 0.3111180365085602\n",
      "Epoch: 199580 | Loss: 0.32047194242477417 | Test loss: 0.3111041486263275\n",
      "Epoch: 199590 | Loss: 0.32046017050743103 | Test loss: 0.3110903203487396\n",
      "Epoch: 199600 | Loss: 0.3204483985900879 | Test loss: 0.31107649207115173\n",
      "Epoch: 199610 | Loss: 0.32043665647506714 | Test loss: 0.31106266379356384\n",
      "Epoch: 199620 | Loss: 0.320424884557724 | Test loss: 0.31104880571365356\n",
      "Epoch: 199630 | Loss: 0.32041311264038086 | Test loss: 0.3110349476337433\n",
      "Epoch: 199640 | Loss: 0.3204014003276825 | Test loss: 0.311021089553833\n",
      "Epoch: 199650 | Loss: 0.32038959860801697 | Test loss: 0.3110072612762451\n",
      "Epoch: 199660 | Loss: 0.32037782669067383 | Test loss: 0.31099340319633484\n",
      "Epoch: 199670 | Loss: 0.32036611437797546 | Test loss: 0.31097957491874695\n",
      "Epoch: 199680 | Loss: 0.32035431265830994 | Test loss: 0.31096571683883667\n",
      "Epoch: 199690 | Loss: 0.3203425407409668 | Test loss: 0.3109518587589264\n",
      "Epoch: 199700 | Loss: 0.32033082842826843 | Test loss: 0.3109380304813385\n",
      "Epoch: 199710 | Loss: 0.3203190267086029 | Test loss: 0.3109241724014282\n",
      "Epoch: 199720 | Loss: 0.32030731439590454 | Test loss: 0.31091031432151794\n",
      "Epoch: 199730 | Loss: 0.3202955424785614 | Test loss: 0.31089648604393005\n",
      "Epoch: 199740 | Loss: 0.3202837407588959 | Test loss: 0.31088265776634216\n",
      "Epoch: 199750 | Loss: 0.3202720284461975 | Test loss: 0.3108687996864319\n",
      "Epoch: 199760 | Loss: 0.32026025652885437 | Test loss: 0.310854971408844\n",
      "Epoch: 199770 | Loss: 0.32024848461151123 | Test loss: 0.3108411133289337\n",
      "Epoch: 199780 | Loss: 0.3202367424964905 | Test loss: 0.31082722544670105\n",
      "Epoch: 199790 | Loss: 0.32022497057914734 | Test loss: 0.31081342697143555\n",
      "Epoch: 199800 | Loss: 0.3202131986618042 | Test loss: 0.31079956889152527\n",
      "Epoch: 199810 | Loss: 0.32020145654678345 | Test loss: 0.3107857406139374\n",
      "Epoch: 199820 | Loss: 0.3201896846294403 | Test loss: 0.3107718527317047\n",
      "Epoch: 199830 | Loss: 0.32017794251441956 | Test loss: 0.3107580244541168\n",
      "Epoch: 199840 | Loss: 0.3201661705970764 | Test loss: 0.31074416637420654\n",
      "Epoch: 199850 | Loss: 0.3201543986797333 | Test loss: 0.31073030829429626\n",
      "Epoch: 199860 | Loss: 0.3201426565647125 | Test loss: 0.3107164800167084\n",
      "Epoch: 199870 | Loss: 0.3201309144496918 | Test loss: 0.3107026517391205\n",
      "Epoch: 199880 | Loss: 0.32011914253234863 | Test loss: 0.3106887936592102\n",
      "Epoch: 199890 | Loss: 0.3201073706150055 | Test loss: 0.3106749653816223\n",
      "Epoch: 199900 | Loss: 0.32009559869766235 | Test loss: 0.31066107749938965\n",
      "Epoch: 199910 | Loss: 0.3200838565826416 | Test loss: 0.31064724922180176\n",
      "Epoch: 199920 | Loss: 0.32007208466529846 | Test loss: 0.31063342094421387\n",
      "Epoch: 199930 | Loss: 0.3200603127479553 | Test loss: 0.3106195628643036\n",
      "Epoch: 199940 | Loss: 0.32004857063293457 | Test loss: 0.3106057345867157\n",
      "Epoch: 199950 | Loss: 0.32003679871559143 | Test loss: 0.3105919063091278\n",
      "Epoch: 199960 | Loss: 0.3200250566005707 | Test loss: 0.31057801842689514\n",
      "Epoch: 199970 | Loss: 0.32001328468322754 | Test loss: 0.31056419014930725\n",
      "Epoch: 199980 | Loss: 0.3200015425682068 | Test loss: 0.3105503022670746\n",
      "Epoch: 199990 | Loss: 0.31998977065086365 | Test loss: 0.3105364739894867\n"
     ]
    }
   ],
   "source": [
    "# Let's write a training loop\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 200000\n",
    "\n",
    "# Put data on the target device (device agnostic code for data) \n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  model_1.train()\n",
    "\n",
    "  # 1. Forward pass\n",
    "  y_pred = model_1(X_train)\n",
    "\n",
    "  # 2. Calculate the loss\n",
    "  loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "  # 3. Optimizer zero grad\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # 4. Perform backpropagation\n",
    "  loss.backward()\n",
    "\n",
    "  # 5. Optimizer step\n",
    "  optimizer.step()\n",
    "\n",
    "  ### Testing\n",
    "  model_1.eval()\n",
    "  with torch.inference_mode():\n",
    "    test_pred = model_1(X_test)\n",
    "\n",
    "    test_loss = loss_fn(test_pred, y_test)\n",
    "\n",
    "  # Print out what's happening\n",
    "  if epoch % 10 == 0: \n",
    "    print(f\"Epoch: {epoch} | Loss: {loss} | Test loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2qJjO4ko6x_",
    "outputId": "7125869b-b8f4-4ee2-ab14-571afa54c316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.6811]], device='cuda:0')),\n",
       "             ('linear_layer.bias', tensor([0.6274], device='cuda:0'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28o1G0gnpYRj",
    "outputId": "ae9098ca-52bb-440f-84f6-7979f2ccd2c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight, bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_Y2nC9tpaDz"
   },
   "source": [
    "### 6.4 Making and evaluating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ngw4JbJQqubf",
    "outputId": "6a05ad5c-98bd-4e01-e951-a40bf84e46a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1722],\n",
       "        [1.1858],\n",
       "        [1.1995],\n",
       "        [1.2131],\n",
       "        [1.2267],\n",
       "        [1.2403],\n",
       "        [1.2540],\n",
       "        [1.2676],\n",
       "        [1.2812],\n",
       "        [1.2948]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn model into evaluation mode\n",
    "model_1.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "  y_preds = model_1(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "uUIbkzHIq5U8",
    "outputId": "d1e6ead5-4ce9-42d9-c2a7-332e143408be"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "/* Put everything inside the global mpl namespace */\n/* global mpl */\nwindow.mpl = {};\n\nmpl.get_websocket_type = function () {\n    if (typeof WebSocket !== 'undefined') {\n        return WebSocket;\n    } else if (typeof MozWebSocket !== 'undefined') {\n        return MozWebSocket;\n    } else {\n        alert(\n            'Your browser does not have WebSocket support. ' +\n                'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n                'Firefox 4 and 5 are also supported but you ' +\n                'have to enable WebSockets in about:config.'\n        );\n    }\n};\n\nmpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n    this.id = figure_id;\n\n    this.ws = websocket;\n\n    this.supports_binary = this.ws.binaryType !== undefined;\n\n    if (!this.supports_binary) {\n        var warnings = document.getElementById('mpl-warnings');\n        if (warnings) {\n            warnings.style.display = 'block';\n            warnings.textContent =\n                'This browser does not support binary websocket messages. ' +\n                'Performance may be slow.';\n        }\n    }\n\n    this.imageObj = new Image();\n\n    this.context = undefined;\n    this.message = undefined;\n    this.canvas = undefined;\n    this.rubberband_canvas = undefined;\n    this.rubberband_context = undefined;\n    this.format_dropdown = undefined;\n\n    this.image_mode = 'full';\n\n    this.root = document.createElement('div');\n    this.root.setAttribute('style', 'display: inline-block');\n    this._root_extra_style(this.root);\n\n    parent_element.appendChild(this.root);\n\n    this._init_header(this);\n    this._init_canvas(this);\n    this._init_toolbar(this);\n\n    var fig = this;\n\n    this.waiting = false;\n\n    this.ws.onopen = function () {\n        fig.send_message('supports_binary', { value: fig.supports_binary });\n        fig.send_message('send_image_mode', {});\n        if (fig.ratio !== 1) {\n            fig.send_message('set_device_pixel_ratio', {\n                device_pixel_ratio: fig.ratio,\n            });\n        }\n        fig.send_message('refresh', {});\n    };\n\n    this.imageObj.onload = function () {\n        if (fig.image_mode === 'full') {\n            // Full images could contain transparency (where diff images\n            // almost always do), so we need to clear the canvas so that\n            // there is no ghosting.\n            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n        }\n        fig.context.drawImage(fig.imageObj, 0, 0);\n    };\n\n    this.imageObj.onunload = function () {\n        fig.ws.close();\n    };\n\n    this.ws.onmessage = this._make_on_message_function(this);\n\n    this.ondownload = ondownload;\n};\n\nmpl.figure.prototype._init_header = function () {\n    var titlebar = document.createElement('div');\n    titlebar.classList =\n        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n    var titletext = document.createElement('div');\n    titletext.classList = 'ui-dialog-title';\n    titletext.setAttribute(\n        'style',\n        'width: 100%; text-align: center; padding: 3px;'\n    );\n    titlebar.appendChild(titletext);\n    this.root.appendChild(titlebar);\n    this.header = titletext;\n};\n\nmpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n\nmpl.figure.prototype._init_canvas = function () {\n    var fig = this;\n\n    var canvas_div = (this.canvas_div = document.createElement('div'));\n    canvas_div.setAttribute('tabindex', '0');\n    canvas_div.setAttribute(\n        'style',\n        'border: 1px solid #ddd;' +\n            'box-sizing: content-box;' +\n            'clear: both;' +\n            'min-height: 1px;' +\n            'min-width: 1px;' +\n            'outline: 0;' +\n            'overflow: hidden;' +\n            'position: relative;' +\n            'resize: both;' +\n            'z-index: 2;'\n    );\n\n    function on_keyboard_event_closure(name) {\n        return function (event) {\n            return fig.key_event(event, name);\n        };\n    }\n\n    canvas_div.addEventListener(\n        'keydown',\n        on_keyboard_event_closure('key_press')\n    );\n    canvas_div.addEventListener(\n        'keyup',\n        on_keyboard_event_closure('key_release')\n    );\n\n    this._canvas_extra_style(canvas_div);\n    this.root.appendChild(canvas_div);\n\n    var canvas = (this.canvas = document.createElement('canvas'));\n    canvas.classList.add('mpl-canvas');\n    canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'pointer-events: none;' +\n            'position: relative;' +\n            'z-index: 0;'\n    );\n\n    this.context = canvas.getContext('2d');\n\n    var backingStore =\n        this.context.backingStorePixelRatio ||\n        this.context.webkitBackingStorePixelRatio ||\n        this.context.mozBackingStorePixelRatio ||\n        this.context.msBackingStorePixelRatio ||\n        this.context.oBackingStorePixelRatio ||\n        this.context.backingStorePixelRatio ||\n        1;\n\n    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n\n    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n        'canvas'\n    ));\n    rubberband_canvas.setAttribute(\n        'style',\n        'box-sizing: content-box;' +\n            'left: 0;' +\n            'pointer-events: none;' +\n            'position: absolute;' +\n            'top: 0;' +\n            'z-index: 1;'\n    );\n\n    // Apply a ponyfill if ResizeObserver is not implemented by browser.\n    if (this.ResizeObserver === undefined) {\n        if (window.ResizeObserver !== undefined) {\n            this.ResizeObserver = window.ResizeObserver;\n        } else {\n            var obs = _JSXTOOLS_RESIZE_OBSERVER({});\n            this.ResizeObserver = obs.ResizeObserver;\n        }\n    }\n\n    this.resizeObserverInstance = new this.ResizeObserver(function (entries) {\n        var nentries = entries.length;\n        for (var i = 0; i < nentries; i++) {\n            var entry = entries[i];\n            var width, height;\n            if (entry.contentBoxSize) {\n                if (entry.contentBoxSize instanceof Array) {\n                    // Chrome 84 implements new version of spec.\n                    width = entry.contentBoxSize[0].inlineSize;\n                    height = entry.contentBoxSize[0].blockSize;\n                } else {\n                    // Firefox implements old version of spec.\n                    width = entry.contentBoxSize.inlineSize;\n                    height = entry.contentBoxSize.blockSize;\n                }\n            } else {\n                // Chrome <84 implements even older version of spec.\n                width = entry.contentRect.width;\n                height = entry.contentRect.height;\n            }\n\n            // Keep the size of the canvas and rubber band canvas in sync with\n            // the canvas container.\n            if (entry.devicePixelContentBoxSize) {\n                // Chrome 84 implements new version of spec.\n                canvas.setAttribute(\n                    'width',\n                    entry.devicePixelContentBoxSize[0].inlineSize\n                );\n                canvas.setAttribute(\n                    'height',\n                    entry.devicePixelContentBoxSize[0].blockSize\n                );\n            } else {\n                canvas.setAttribute('width', width * fig.ratio);\n                canvas.setAttribute('height', height * fig.ratio);\n            }\n            /* This rescales the canvas back to display pixels, so that it\n             * appears correct on HiDPI screens. */\n            canvas.style.width = width + 'px';\n            canvas.style.height = height + 'px';\n\n            rubberband_canvas.setAttribute('width', width);\n            rubberband_canvas.setAttribute('height', height);\n\n            // And update the size in Python. We ignore the initial 0/0 size\n            // that occurs as the element is placed into the DOM, which should\n            // otherwise not happen due to the minimum size styling.\n            if (fig.ws.readyState == 1 && width != 0 && height != 0) {\n                fig.request_resize(width, height);\n            }\n        }\n    });\n    this.resizeObserverInstance.observe(canvas_div);\n\n    function on_mouse_event_closure(name) {\n        /* User Agent sniffing is bad, but WebKit is busted:\n         * https://bugs.webkit.org/show_bug.cgi?id=144526\n         * https://bugs.webkit.org/show_bug.cgi?id=181818\n         * The worst that happens here is that they get an extra browser\n         * selection when dragging, if this check fails to catch them.\n         */\n        var UA = navigator.userAgent;\n        var isWebKit = /AppleWebKit/.test(UA) && !/Chrome/.test(UA);\n        if(isWebKit) {\n            return function (event) {\n                /* This prevents the web browser from automatically changing to\n                 * the text insertion cursor when the button is pressed. We\n                 * want to control all of the cursor setting manually through\n                 * the 'cursor' event from matplotlib */\n                event.preventDefault()\n                return fig.mouse_event(event, name);\n            };\n        } else {\n            return function (event) {\n                return fig.mouse_event(event, name);\n            };\n        }\n    }\n\n    canvas_div.addEventListener(\n        'mousedown',\n        on_mouse_event_closure('button_press')\n    );\n    canvas_div.addEventListener(\n        'mouseup',\n        on_mouse_event_closure('button_release')\n    );\n    canvas_div.addEventListener(\n        'dblclick',\n        on_mouse_event_closure('dblclick')\n    );\n    // Throttle sequential mouse events to 1 every 20ms.\n    canvas_div.addEventListener(\n        'mousemove',\n        on_mouse_event_closure('motion_notify')\n    );\n\n    canvas_div.addEventListener(\n        'mouseenter',\n        on_mouse_event_closure('figure_enter')\n    );\n    canvas_div.addEventListener(\n        'mouseleave',\n        on_mouse_event_closure('figure_leave')\n    );\n\n    canvas_div.addEventListener('wheel', function (event) {\n        if (event.deltaY < 0) {\n            event.step = 1;\n        } else {\n            event.step = -1;\n        }\n        on_mouse_event_closure('scroll')(event);\n    });\n\n    canvas_div.appendChild(canvas);\n    canvas_div.appendChild(rubberband_canvas);\n\n    this.rubberband_context = rubberband_canvas.getContext('2d');\n    this.rubberband_context.strokeStyle = '#000000';\n\n    this._resize_canvas = function (width, height, forward) {\n        if (forward) {\n            canvas_div.style.width = width + 'px';\n            canvas_div.style.height = height + 'px';\n        }\n    };\n\n    // Disable right mouse context menu.\n    canvas_div.addEventListener('contextmenu', function (_e) {\n        event.preventDefault();\n        return false;\n    });\n\n    function set_focus() {\n        canvas.focus();\n        canvas_div.focus();\n    }\n\n    window.setTimeout(set_focus, 100);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'mpl-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'mpl-button-group';\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'mpl-button-group';\n            continue;\n        }\n\n        var button = (fig.buttons[name] = document.createElement('button'));\n        button.classList = 'mpl-widget';\n        button.setAttribute('role', 'button');\n        button.setAttribute('aria-disabled', 'false');\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n\n        var icon_img = document.createElement('img');\n        icon_img.src = '_images/' + image + '.png';\n        icon_img.srcset = '_images/' + image + '_large.png 2x';\n        icon_img.alt = tooltip;\n        button.appendChild(icon_img);\n\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    var fmt_picker = document.createElement('select');\n    fmt_picker.classList = 'mpl-widget';\n    toolbar.appendChild(fmt_picker);\n    this.format_dropdown = fmt_picker;\n\n    for (var ind in mpl.extensions) {\n        var fmt = mpl.extensions[ind];\n        var option = document.createElement('option');\n        option.selected = fmt === mpl.default_extension;\n        option.innerHTML = fmt;\n        fmt_picker.appendChild(option);\n    }\n\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n};\n\nmpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n    // which will in turn request a refresh of the image.\n    this.send_message('resize', { width: x_pixels, height: y_pixels });\n};\n\nmpl.figure.prototype.send_message = function (type, properties) {\n    properties['type'] = type;\n    properties['figure_id'] = this.id;\n    this.ws.send(JSON.stringify(properties));\n};\n\nmpl.figure.prototype.send_draw_message = function () {\n    if (!this.waiting) {\n        this.waiting = true;\n        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    var format_dropdown = fig.format_dropdown;\n    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n    fig.ondownload(fig, format);\n};\n\nmpl.figure.prototype.handle_resize = function (fig, msg) {\n    var size = msg['size'];\n    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n        fig._resize_canvas(size[0], size[1], msg['forward']);\n        fig.send_message('refresh', {});\n    }\n};\n\nmpl.figure.prototype.handle_rubberband = function (fig, msg) {\n    var x0 = msg['x0'] / fig.ratio;\n    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n    var x1 = msg['x1'] / fig.ratio;\n    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n    x0 = Math.floor(x0) + 0.5;\n    y0 = Math.floor(y0) + 0.5;\n    x1 = Math.floor(x1) + 0.5;\n    y1 = Math.floor(y1) + 0.5;\n    var min_x = Math.min(x0, x1);\n    var min_y = Math.min(y0, y1);\n    var width = Math.abs(x1 - x0);\n    var height = Math.abs(y1 - y0);\n\n    fig.rubberband_context.clearRect(\n        0,\n        0,\n        fig.canvas.width / fig.ratio,\n        fig.canvas.height / fig.ratio\n    );\n\n    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n};\n\nmpl.figure.prototype.handle_figure_label = function (fig, msg) {\n    // Updates the figure title.\n    fig.header.textContent = msg['label'];\n};\n\nmpl.figure.prototype.handle_cursor = function (fig, msg) {\n    fig.canvas_div.style.cursor = msg['cursor'];\n};\n\nmpl.figure.prototype.handle_message = function (fig, msg) {\n    fig.message.textContent = msg['message'];\n};\n\nmpl.figure.prototype.handle_draw = function (fig, _msg) {\n    // Request the server to send over a new figure.\n    fig.send_draw_message();\n};\n\nmpl.figure.prototype.handle_image_mode = function (fig, msg) {\n    fig.image_mode = msg['mode'];\n};\n\nmpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n    for (var key in msg) {\n        if (!(key in fig.buttons)) {\n            continue;\n        }\n        fig.buttons[key].disabled = !msg[key];\n        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n    }\n};\n\nmpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n    if (msg['mode'] === 'PAN') {\n        fig.buttons['Pan'].classList.add('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    } else if (msg['mode'] === 'ZOOM') {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.add('active');\n    } else {\n        fig.buttons['Pan'].classList.remove('active');\n        fig.buttons['Zoom'].classList.remove('active');\n    }\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Called whenever the canvas gets updated.\n    this.send_message('ack', {});\n};\n\n// A function to construct a web socket function for onmessage handling.\n// Called in the figure constructor.\nmpl.figure.prototype._make_on_message_function = function (fig) {\n    return function socket_on_message(evt) {\n        if (evt.data instanceof Blob) {\n            var img = evt.data;\n            if (img.type !== 'image/png') {\n                /* FIXME: We get \"Resource interpreted as Image but\n                 * transferred with MIME type text/plain:\" errors on\n                 * Chrome.  But how to set the MIME type?  It doesn't seem\n                 * to be part of the websocket stream */\n                img.type = 'image/png';\n            }\n\n            /* Free the memory for the previous frames */\n            if (fig.imageObj.src) {\n                (window.URL || window.webkitURL).revokeObjectURL(\n                    fig.imageObj.src\n                );\n            }\n\n            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n                img\n            );\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        } else if (\n            typeof evt.data === 'string' &&\n            evt.data.slice(0, 21) === 'data:image/png;base64'\n        ) {\n            fig.imageObj.src = evt.data;\n            fig.updated_canvas_event();\n            fig.waiting = false;\n            return;\n        }\n\n        var msg = JSON.parse(evt.data);\n        var msg_type = msg['type'];\n\n        // Call the  \"handle_{type}\" callback, which takes\n        // the figure and JSON message as its only arguments.\n        try {\n            var callback = fig['handle_' + msg_type];\n        } catch (e) {\n            console.log(\n                \"No handler for the '\" + msg_type + \"' message type: \",\n                msg\n            );\n            return;\n        }\n\n        if (callback) {\n            try {\n                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n                callback(fig, msg);\n            } catch (e) {\n                console.log(\n                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n                    e,\n                    e.stack,\n                    msg\n                );\n            }\n        }\n    };\n};\n\nfunction getModifiers(event) {\n    var mods = [];\n    if (event.ctrlKey) {\n        mods.push('ctrl');\n    }\n    if (event.altKey) {\n        mods.push('alt');\n    }\n    if (event.shiftKey) {\n        mods.push('shift');\n    }\n    if (event.metaKey) {\n        mods.push('meta');\n    }\n    return mods;\n}\n\n/*\n * return a copy of an object with only non-object keys\n * we need this to avoid circular references\n * https://stackoverflow.com/a/24161582/3208463\n */\nfunction simpleKeys(original) {\n    return Object.keys(original).reduce(function (obj, key) {\n        if (typeof original[key] !== 'object') {\n            obj[key] = original[key];\n        }\n        return obj;\n    }, {});\n}\n\nmpl.figure.prototype.mouse_event = function (event, name) {\n    if (name === 'button_press') {\n        this.canvas.focus();\n        this.canvas_div.focus();\n    }\n\n    // from https://stackoverflow.com/q/1114465\n    var boundingRect = this.canvas.getBoundingClientRect();\n    var x = (event.clientX - boundingRect.left) * this.ratio;\n    var y = (event.clientY - boundingRect.top) * this.ratio;\n\n    this.send_message(name, {\n        x: x,\n        y: y,\n        button: event.button,\n        step: event.step,\n        modifiers: getModifiers(event),\n        guiEvent: simpleKeys(event),\n    });\n\n    return false;\n};\n\nmpl.figure.prototype._key_event_extra = function (_event, _name) {\n    // Handle any extra behaviour associated with a key event\n};\n\nmpl.figure.prototype.key_event = function (event, name) {\n    // Prevent repeat events\n    if (name === 'key_press') {\n        if (event.key === this._key) {\n            return;\n        } else {\n            this._key = event.key;\n        }\n    }\n    if (name === 'key_release') {\n        this._key = null;\n    }\n\n    var value = '';\n    if (event.ctrlKey && event.key !== 'Control') {\n        value += 'ctrl+';\n    }\n    else if (event.altKey && event.key !== 'Alt') {\n        value += 'alt+';\n    }\n    else if (event.shiftKey && event.key !== 'Shift') {\n        value += 'shift+';\n    }\n\n    value += 'k' + event.key;\n\n    this._key_event_extra(event, name);\n\n    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n    return false;\n};\n\nmpl.figure.prototype.toolbar_button_onclick = function (name) {\n    if (name === 'download') {\n        this.handle_save(this, null);\n    } else {\n        this.send_message('toolbar_button', { name: name });\n    }\n};\n\nmpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n    this.message.textContent = tooltip;\n};\n\n///////////////// REMAINING CONTENT GENERATED BY embed_js.py /////////////////\n// prettier-ignore\nvar _JSXTOOLS_RESIZE_OBSERVER=function(A){var t,i=new WeakMap,n=new WeakMap,a=new WeakMap,r=new WeakMap,o=new Set;function s(e){if(!(this instanceof s))throw new TypeError(\"Constructor requires 'new' operator\");i.set(this,e)}function h(){throw new TypeError(\"Function is not a constructor\")}function c(e,t,i,n){e=0 in arguments?Number(arguments[0]):0,t=1 in arguments?Number(arguments[1]):0,i=2 in arguments?Number(arguments[2]):0,n=3 in arguments?Number(arguments[3]):0,this.right=(this.x=this.left=e)+(this.width=i),this.bottom=(this.y=this.top=t)+(this.height=n),Object.freeze(this)}function d(){t=requestAnimationFrame(d);var s=new WeakMap,p=new Set;o.forEach((function(t){r.get(t).forEach((function(i){var r=t instanceof window.SVGElement,o=a.get(t),d=r?0:parseFloat(o.paddingTop),f=r?0:parseFloat(o.paddingRight),l=r?0:parseFloat(o.paddingBottom),u=r?0:parseFloat(o.paddingLeft),g=r?0:parseFloat(o.borderTopWidth),m=r?0:parseFloat(o.borderRightWidth),w=r?0:parseFloat(o.borderBottomWidth),b=u+f,F=d+l,v=(r?0:parseFloat(o.borderLeftWidth))+m,W=g+w,y=r?0:t.offsetHeight-W-t.clientHeight,E=r?0:t.offsetWidth-v-t.clientWidth,R=b+v,z=F+W,M=r?t.width:parseFloat(o.width)-R-E,O=r?t.height:parseFloat(o.height)-z-y;if(n.has(t)){var k=n.get(t);if(k[0]===M&&k[1]===O)return}n.set(t,[M,O]);var S=Object.create(h.prototype);S.target=t,S.contentRect=new c(u,d,M,O),s.has(i)||(s.set(i,[]),p.add(i)),s.get(i).push(S)}))})),p.forEach((function(e){i.get(e).call(e,s.get(e),e)}))}return s.prototype.observe=function(i){if(i instanceof window.Element){r.has(i)||(r.set(i,new Set),o.add(i),a.set(i,window.getComputedStyle(i)));var n=r.get(i);n.has(this)||n.add(this),cancelAnimationFrame(t),t=requestAnimationFrame(d)}},s.prototype.unobserve=function(i){if(i instanceof window.Element&&r.has(i)){var n=r.get(i);n.has(this)&&(n.delete(this),n.size||(r.delete(i),o.delete(i))),n.size||r.delete(i),o.size||cancelAnimationFrame(t)}},A.DOMRectReadOnly=c,A.ResizeObserver=s,A.ResizeObserverEntry=h,A}; // eslint-disable-line\nmpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis\", \"fa fa-square-o\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o\", \"download\"]];\n\nmpl.extensions = [\"eps\", \"jpeg\", \"pgf\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\", \"webp\"];\n\nmpl.default_extension = \"png\";/* global mpl */\n\nvar comm_websocket_adapter = function (comm) {\n    // Create a \"websocket\"-like object which calls the given IPython comm\n    // object with the appropriate methods. Currently this is a non binary\n    // socket, so there is still some room for performance tuning.\n    var ws = {};\n\n    ws.binaryType = comm.kernel.ws.binaryType;\n    ws.readyState = comm.kernel.ws.readyState;\n    function updateReadyState(_event) {\n        if (comm.kernel.ws) {\n            ws.readyState = comm.kernel.ws.readyState;\n        } else {\n            ws.readyState = 3; // Closed state.\n        }\n    }\n    comm.kernel.ws.addEventListener('open', updateReadyState);\n    comm.kernel.ws.addEventListener('close', updateReadyState);\n    comm.kernel.ws.addEventListener('error', updateReadyState);\n\n    ws.close = function () {\n        comm.close();\n    };\n    ws.send = function (m) {\n        //console.log('sending', m);\n        comm.send(m);\n    };\n    // Register the callback with on_msg.\n    comm.on_msg(function (msg) {\n        //console.log('receiving', msg['content']['data'], msg);\n        var data = msg['content']['data'];\n        if (data['blob'] !== undefined) {\n            data = {\n                data: new Blob(msg['buffers'], { type: data['blob'] }),\n            };\n        }\n        // Pass the mpl event to the overridden (by mpl) onmessage function.\n        ws.onmessage(data);\n    });\n    return ws;\n};\n\nmpl.mpl_figure_comm = function (comm, msg) {\n    // This is the function which gets called when the mpl process\n    // starts-up an IPython Comm through the \"matplotlib\" channel.\n\n    var id = msg.content.data.id;\n    // Get hold of the div created by the display call when the Comm\n    // socket was opened in Python.\n    var element = document.getElementById(id);\n    var ws_proxy = comm_websocket_adapter(comm);\n\n    function ondownload(figure, _format) {\n        window.open(figure.canvas.toDataURL());\n    }\n\n    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n\n    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n    // web socket which is closed, not our websocket->open comm proxy.\n    ws_proxy.onopen();\n\n    fig.parent_element = element;\n    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n    if (!fig.cell_info) {\n        console.error('Failed to find cell for figure', id, fig);\n        return;\n    }\n    fig.cell_info[0].output_area.element.on(\n        'cleared',\n        { fig: fig },\n        fig._remove_fig_handler\n    );\n};\n\nmpl.figure.prototype.handle_close = function (fig, msg) {\n    var width = fig.canvas.width / fig.ratio;\n    fig.cell_info[0].output_area.element.off(\n        'cleared',\n        fig._remove_fig_handler\n    );\n    fig.resizeObserverInstance.unobserve(fig.canvas_div);\n\n    // Update the output cell to use the data from the current canvas.\n    fig.push_to_output();\n    var dataURL = fig.canvas.toDataURL();\n    // Re-enable the keyboard manager in IPython - without this line, in FF,\n    // the notebook keyboard shortcuts fail.\n    IPython.keyboard_manager.enable();\n    fig.parent_element.innerHTML =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n    fig.close_ws(fig, msg);\n};\n\nmpl.figure.prototype.close_ws = function (fig, msg) {\n    fig.send_message('closing', msg);\n    // fig.ws.close()\n};\n\nmpl.figure.prototype.push_to_output = function (_remove_interactive) {\n    // Turn the data on the canvas into data in the output cell.\n    var width = this.canvas.width / this.ratio;\n    var dataURL = this.canvas.toDataURL();\n    this.cell_info[1]['text/html'] =\n        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n};\n\nmpl.figure.prototype.updated_canvas_event = function () {\n    // Tell IPython that the notebook contents must change.\n    IPython.notebook.set_dirty(true);\n    this.send_message('ack', {});\n    var fig = this;\n    // Wait a second, then push the new image to the DOM so\n    // that it is saved nicely (might be nice to debounce this).\n    setTimeout(function () {\n        fig.push_to_output();\n    }, 1000);\n};\n\nmpl.figure.prototype._init_toolbar = function () {\n    var fig = this;\n\n    var toolbar = document.createElement('div');\n    toolbar.classList = 'btn-toolbar';\n    this.root.appendChild(toolbar);\n\n    function on_click_closure(name) {\n        return function (_event) {\n            return fig.toolbar_button_onclick(name);\n        };\n    }\n\n    function on_mouseover_closure(tooltip) {\n        return function (event) {\n            if (!event.currentTarget.disabled) {\n                return fig.toolbar_button_onmouseover(tooltip);\n            }\n        };\n    }\n\n    fig.buttons = {};\n    var buttonGroup = document.createElement('div');\n    buttonGroup.classList = 'btn-group';\n    var button;\n    for (var toolbar_ind in mpl.toolbar_items) {\n        var name = mpl.toolbar_items[toolbar_ind][0];\n        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n        var image = mpl.toolbar_items[toolbar_ind][2];\n        var method_name = mpl.toolbar_items[toolbar_ind][3];\n\n        if (!name) {\n            /* Instead of a spacer, we start a new button group. */\n            if (buttonGroup.hasChildNodes()) {\n                toolbar.appendChild(buttonGroup);\n            }\n            buttonGroup = document.createElement('div');\n            buttonGroup.classList = 'btn-group';\n            continue;\n        }\n\n        button = fig.buttons[name] = document.createElement('button');\n        button.classList = 'btn btn-default';\n        button.href = '#';\n        button.title = name;\n        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n        button.addEventListener('click', on_click_closure(method_name));\n        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n        buttonGroup.appendChild(button);\n    }\n\n    if (buttonGroup.hasChildNodes()) {\n        toolbar.appendChild(buttonGroup);\n    }\n\n    // Add the status bar.\n    var status_bar = document.createElement('span');\n    status_bar.classList = 'mpl-message pull-right';\n    toolbar.appendChild(status_bar);\n    this.message = status_bar;\n\n    // Add the close button to the window.\n    var buttongrp = document.createElement('div');\n    buttongrp.classList = 'btn-group inline pull-right';\n    button = document.createElement('button');\n    button.classList = 'btn btn-mini btn-primary';\n    button.href = '#';\n    button.title = 'Stop Interaction';\n    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n    button.addEventListener('click', function (_evt) {\n        fig.handle_close(fig, {});\n    });\n    button.addEventListener(\n        'mouseover',\n        on_mouseover_closure('Stop Interaction')\n    );\n    buttongrp.appendChild(button);\n    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n};\n\nmpl.figure.prototype._remove_fig_handler = function (event) {\n    var fig = event.data.fig;\n    if (event.target !== this) {\n        // Ignore bubbled events from children.\n        return;\n    }\n    fig.close_ws(fig, {});\n};\n\nmpl.figure.prototype._root_extra_style = function (el) {\n    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n};\n\nmpl.figure.prototype._canvas_extra_style = function (el) {\n    // this is important to make the div 'focusable\n    el.setAttribute('tabindex', 0);\n    // reach out to IPython and tell the keyboard manager to turn it's self\n    // off when our div gets focus\n\n    // location in version 3\n    if (IPython.notebook.keyboard_manager) {\n        IPython.notebook.keyboard_manager.register_events(el);\n    } else {\n        // location in version 2\n        IPython.keyboard_manager.register_events(el);\n    }\n};\n\nmpl.figure.prototype._key_event_extra = function (event, _name) {\n    // Check for shift+enter\n    if (event.shiftKey && event.which === 13) {\n        this.canvas_div.blur();\n        // select the cell after this one\n        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n        IPython.notebook.select(index + 1);\n    }\n};\n\nmpl.figure.prototype.handle_save = function (fig, _msg) {\n    fig.ondownload(fig, null);\n};\n\nmpl.find_output_cell = function (html_output) {\n    // Return the cell and output element which can be found *uniquely* in the notebook.\n    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n    // IPython event is triggered only after the cells have been serialised, which for\n    // our purposes (turning an active figure into a static one), is too late.\n    var cells = IPython.notebook.get_cells();\n    var ncells = cells.length;\n    for (var i = 0; i < ncells; i++) {\n        var cell = cells[i];\n        if (cell.cell_type === 'code') {\n            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n                var data = cell.output_area.outputs[j];\n                if (data.data) {\n                    // IPython >= 3 moved mimebundle to data attribute of output\n                    data = data.data;\n                }\n                if (data['text/html'] === html_output) {\n                    return [cell, data, j];\n                }\n            }\n        }\n    }\n};\n\n// Register the function which deals with the matplotlib target/channel.\n// The kernel may be null if the page has been refreshed.\nif (IPython.notebook.kernel !== null) {\n    IPython.notebook.kernel.comm_manager.register_target(\n        'matplotlib',\n        mpl.mpl_figure_comm\n    );\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='bef164cd-b609-4bf3-80ef-c70ae5b92d1b'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out our model predictions visually\n",
    "plot_predictions(predictions=y_preds.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qdw43z6rEZB"
   },
   "source": [
    "### 6.5 Saving & loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkPtfsseriP_",
    "outputId": "5cc74c45-2adc-43d5-806c-b1446c199eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models\\01_pytorch_workflow_model_1.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path\n",
    "MODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_1.state_dict(),\n",
    "           f=MODEL_SAVE_PATH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5n_xTjssNZ2",
    "outputId": "14c63c26-3ae1-458c-d172-2ba03e99479d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.6811]], device='cuda:0')),\n",
       "             ('linear_layer.bias', tensor([0.6274], device='cuda:0'))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yj8GttgnsNXq",
    "outputId": "d934cb26-e89a-4344-f904-26d6cb06487c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModelV2(\n",
       "  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a PyTorch model\n",
    "\n",
    "# Create a new instance of lienar regression model V2\n",
    "loaded_model_1 = LinearRegressionModelV2()\n",
    "\n",
    "# Load the saved model_1 state_dict\n",
    "loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "# Put the loaded model to device\n",
    "loaded_model_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pSqRcShsNVE",
    "outputId": "fde1b8e2-cab1-45e8-e8d4-2b2255a2b9d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(loaded_model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1nhEK4FsNSy",
    "outputId": "85d3f5b0-52b8-444f-c1cf-f2d2752cc78c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_layer.weight', tensor([[0.6811]], device='cuda:0')),\n",
       "             ('linear_layer.bias', tensor([0.6274], device='cuda:0'))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOZBAa-JsNQJ",
    "outputId": "9b722c03-be13-44a8-934d-33613490bb4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]], device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate loaded model\n",
    "loaded_model_1.eval()\n",
    "with torch.inference_mode():\n",
    "  loaded_model_1_preds = loaded_model_1(X_test)\n",
    "y_preds == loaded_model_1_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh1HuUNmtxt_"
   },
   "source": [
    "## Exercises & Extra-curriculum\n",
    "\n",
    "For exercise & extra-curriculum, refer to: https://www.learnpytorch.io/01_pytorch_workflow/#exercises "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DNZm0YkvEWZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPiZqHPF/YamI5YlikNi4KW",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01_pytorch_workflow_video.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
